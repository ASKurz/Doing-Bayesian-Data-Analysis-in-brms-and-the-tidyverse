---
title: "Chapter 20. Metric Predicted Variable with Multiple Nominal Predictors"
author: "A Solomon Kurz"
date: "2020-09-22"
output:
  github_document
---



# Metric Predicted Variable with Multiple Nominal Predictors

> This chapter considers data structures that consist of a metric predicted variable and two (or more) nominal predictors. This chapter extends ideas introduced in the previous chapter, so please read the previous chapter if you have not already...
>
> The traditional treatment of this sort of data structure is called multifactor analysis of variance (ANOVA). Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups. [@kruschkeDoingBayesianData2015, pp. 583--584]

## Describing groups of metric data with multiple nominal predictors

Quick reprise:

> Suppose we have two nominal predictors, denoted $\overrightarrow x_1$ and $\overrightarrow x_2$. A datum from the $j$th level of $\overrightarrow x_1$ is denoted $x_{1[j]}$, and analogously for the second factor. The predicted value is a baseline plus a deflection due to the level of factor 1 plus a deflection due to the level of factor 2 plus a residual deflection due to the interaction of factors:
>
> \begin{align*}
> \mu & = \beta_0 + \overrightarrow \beta_1 \cdot \overrightarrow x_1 + \overrightarrow \beta_2 \cdot \overrightarrow x_2 + \overrightarrow \beta_{1 \times 2} \cdot \overrightarrow x_{1 \times 2} \\
> & = \beta_0 + \sum_j \beta_{1[j]} x_{1[j]} + \sum_k \beta_{2[k]} x_{2[k]} + \sum_{j, k} \beta_{1 \times 2[j, k]} x_{1 \times 2[j, k]}
> \end{align*}
>
> The deflections within factors and within the interaction are constrained to sum to zero:
>
> \begin{align*}
> \sum_j \beta_{1[j]} = 0                                &&& \text{and} && \sum_k \beta_{2[k]} = 0 \;\;\; \text{and} \\
> \sum_j \beta_{1 \times 2[j, k]} = 0 \text{ for all } k &&& \text{and} && \sum_k \beta_{1 \times 2[j, k]} = 0 \text{ for all } j
> \end{align*}
>
> ([these equations] are repetitions of Equations 15.9 and 15.10, p. 434). The actual data are assumed to be randomly distributed around the predicted value. (pp. 584--585)

### Interaction.

> An important concept of models with multiple predictors is interaction. Interaction means that the effect of a predictor depends on the level of another predictor. A little more technically, interaction is what is left over after the main effects of the factors are added: interaction is the nonadditive influence of the factors. (p. 585)

Here are the data necessary for our version of Figure 20.1, which displays an interaction of two 2-level factors.


```r
library(tidyverse)

grand_mean            <- 5
deflection_1          <- 1.8
deflection_2          <- 0.2
nonadditive_component <- -1

(
  d <-
  tibble(x1 = rep(c(-1, 1), each = 2),
         x2 = rep(c(1, -1), times = 2)) %>% 
  mutate(mu_additive = grand_mean + (x1 * deflection_1) + (x2 * deflection_2)) %>% 
  mutate(mu_multiplicative = mu_additive + (x1 * x2 * nonadditive_component),
         # we'll need this to accommodate `position = "dodge"` within `geom_col()`
         x1_offset         = x1 + x2 * -.45,
         # we'll need this for the fill
         x2_c              = factor(x2, levels = c(1, -1)))
)
```

```
## # A tibble: 4 x 6
##      x1    x2 mu_additive mu_multiplicative x1_offset x2_c 
##   <dbl> <dbl>       <dbl>             <dbl>     <dbl> <fct>
## 1    -1     1         3.4               4.4     -1.45 1    
## 2    -1    -1         3                 2       -0.55 -1   
## 3     1     1         7                 6        0.55 1    
## 4     1    -1         6.6               7.6      1.45 -1
```

There's enough going on with the lines, arrows, and titles across the three panels that to my mind it seems easier to make three distinct plots and them join them at the end with syntax from the **patchwork** package. But enough of the settings are common among the panels that it also makes sense to keep from repeating that part of the code. So we'll take a three-step solution. For the first step, we'll make the baseline or foundational plot, which we'll call `p`.

Before we make `p`, let's talk color and theme. For this chapter, we'll carry forward our practice from [Chapter 19][Metric Predicted Variable with One Nominal Predictor] and take our color palette from the [**palettetown** package](https://CRAN.R-project.org/package=palettetown) [@R-palettetown]. Our color palette will be #15, which is based on [Beedrill](https://www.pokemon.com/us/pokedex/beedrill).


```r
library(palettetown)

scales::show_col(pokepal(pokemon = 15))
```

<img src="20_files/figure-gfm/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" />

```r
bd <- pokepal(pokemon = 15)

bd
```

```
##  [1] "#181818" "#D8C8F0" "#F8F8F8" "#B8A8C0" "#606060" "#E8A030" "#A090A8" "#F8C848" "#885000"
## [10] "#980008" "#E8E0F8" "#F8F0A0" "#F89068" "#D01830"
```

Also like in the last chapter, our overall plot theme will be based on the default `theme_grey()` with a good number of adjustments. This time, it will have more of a [`theme_black()`](https://jonlefcheck.net/2013/03/11/black-theme-for-ggplot2-2/) vibe. 


```r
theme_set(
  theme_grey() +
    theme(text = element_text(color = bd[3]),
          axis.text = element_text(color = bd[3]),
          axis.ticks = element_line(color = bd[3]),
          legend.background = element_blank(),
          legend.box.background = element_blank(),
          legend.key = element_rect(fill = bd[1]),
          panel.background = element_rect(fill = bd[1], color = bd[3]),
          panel.grid = element_blank(),
          plot.background = element_rect(fill = bd[1], color = bd[1]),
          strip.background = element_rect(fill = alpha(bd[5], 1/3), color = alpha(bd[5], 1/3)),
          strip.text = element_text(color = bd[3]))
)
```

Okay, it's time to make `p`, the baseline or foundational plot for our Figure 20.1.


```r
p <-
  d %>% 
  ggplot(aes(x = x1, y = mu_multiplicative)) +
  geom_col(aes(fill = x2_c),
           position = "dodge") +
  scale_fill_manual(NULL, values = bd[c(11, 6)], labels = c("x2[1]", "x2[2]")) +
  scale_x_continuous(breaks = c(-1, 1), labels = c("x1[1]", "x1[2]")) +
  scale_y_continuous(expression(mu), breaks = seq(from = 0, to = 10, by = 2), 
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(ylim = c(0, 10)) +
  theme(axis.ticks.x = element_blank(),
        legend.position = c(.17, .875))

p
```

<img src="20_files/figure-gfm/unnamed-chunk-5-1.png" width="264" style="display: block; margin: auto;" />

Now we have `p`, we'll add panel-specific elements to it, which we'll save as individual objects, `p1`, `p2`, and `p3`. That's step 2. Then for step 3, we'll bring them all together with **patchwork**.


```r
# deflection from additive
p1 <-
  p +
  geom_segment(aes(x = x1_offset, xend = x1_offset,
                   y = mu_additive, yend = mu_multiplicative),
               size = 1.25, color = bd[5],
               arrow = arrow(length = unit(.275, "cm"))) +
  geom_line(aes(x = x1_offset, y = mu_additive, group = x2),
            linetype = 2, color = bd[5]) +
  geom_line(aes(x = x1_offset, y = mu_additive, group = x1),
            linetype = 2, color = bd[5]) +
  coord_cartesian(ylim = c(0, 10)) +
  ggtitle("Deflection from additive")
  
# effect of x1 depends on x2
p2 <-
  p +
  geom_segment(aes(x = x1_offset, xend = x1_offset,
                   y = mu_additive, yend = mu_multiplicative),
               size = .5, color = bd[5],
               arrow = arrow(length = unit(.15, "cm"))) +
  geom_line(aes(x = x1_offset, y = mu_additive, group = x2),
            linetype = 2, color = bd[5]) +
  geom_line(aes(x = x1_offset, y = mu_multiplicative, group = x2),
            size = 1.25, color = bd[5]) +
  ggtitle("Effect of x1 depends on x2")

# effect of x2 depends on x1
p3 <-
  p +
  geom_segment(aes(x = x1_offset, xend = x1_offset,
                   y = mu_additive, yend = mu_multiplicative),
               size = .5, color = bd[5],
               arrow = arrow(length = unit(.15, "cm"))) +
  geom_line(aes(x = x1_offset, y = mu_multiplicative, group = x1),
            size = 1.25, color = bd[5]) +
  geom_line(aes(x = x1_offset, y = mu_additive, group = x1),
            linetype = 2, color = bd[5]) +
  ggtitle("Effect of x2 depends on x1")

library(patchwork)

p1 + p2 + p3
```

<img src="20_files/figure-gfm/unnamed-chunk-6-1.png" width="792" style="display: block; margin: auto;" />

And in case it's not clear, "the average deflection from baseline due to a predictor... is called the main effect of the predictor. The main effects of the predictors correspond to the dashed lines in the left panel of Figure 20.1" (p. 587). And further

> The left panel of Figure 20.1 highlights the interaction as the nonadditive component, emphasized by the heavy vertical arrows that mark the departure from additivity. The middle panel of Figure 20.1 highlights the interaction by emphasizing that the effect of $x_1$ depends on the level of $x_2$. The heavy lines mark the effect of $x_1$, that is, the changes from level 1 of $x_1$ to level 2 of $x_1$. Notice that the heavy lines have different slopes: The heavy line for level 1 of $x_2$ has a shallower slope than the heavy line for level 2 of $x_2$. The right panel of Figure 20.1 highlights the interaction by emphasizing that the effect of $x_2$ depends on the level of $x_1$. (p. 587)

### Traditional ANOVA.

> As was explained in [Section 19.2][Traditional analysis of variance] (p. 556), the terminology, "analysis of variance," comes from a decomposition of overall data variance into within-group variance and between-group variance... The Bayesian approach is not ANOVA, but is analogous to ANOVA. Traditional ANOVA makes decisions about equality of groups (i.e., null hypotheses) on the basis of $p$ values using a null hypothesis that assumes (i) the data are normally distributed within groups, and (ii) the standard deviation of the data within each group is the same for all groups. The second assumption is sometimes called "homogeneity of variance." The entrenched precedent of ANOVA is why basic models of grouped data make those assumptions, and why the basic models presented in this chapter will also make those assumptions. Later in the chapter, those constraints will be relaxed. (pp. 587--588)

## Hierarchical Bayesian approach 

"Our goal is to estimate the main and interaction deflections, and other parameters, based on the observed data" (p. 588).

Figure 20.2 will provides a generic model diagram of how this can work.


```r
# bracket
p1 <-
  tibble(x = .99,
         y = .5,
         label = "{_}") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, hjust = 1, color = bd[2], family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

##  plain arrow
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")
p2 <-
  tibble(x    = .68,
         y    = 1,
         xend = .68,
         yend = .25) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()
  
# normal density
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p4 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("0", "sigma[beta][1]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p5 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("0", "sigma[beta][2]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p6 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 0.67), y = .6,
           hjust = c(.5, 0),
           label = c("0", "sigma[beta][1%*%2]"), 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# four annotated arrows
p7 <-
  tibble(x    = c(.05, .34, .64, .945),
         y    = c(1, 1, 1, 1),
         xend = c(.05, .18, .45, .74),
         yend = c(0, 0, 0, 0)) %>%
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = c(.025, .23, .30, .52, .585, .81, .91), y = .5,
           label = c("'~'", "'~'", "italic(j)", "'~'", "italic(k)", "'~'", "italic(jk)"),
           size = c(10, 10, 7, 10, 7, 10, 7), 
           color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p8 <-
  tibble(x = .5,
         y = .25,
         label = "beta[0]+sum()[italic(j)]*beta[1]['['*italic(j)*']']*italic(x)[1]['['*italic(j)*']'](italic(i))+sum()[italic(k)]*beta[2]['['*italic(k)*']']*italic(x)[2]['['*italic(k)*']'](italic(i))+sum()[italic(jk)]*beta[1%*%2]['['*italic(jk)*']']*italic(x)[1%*%2]['['*italic(jk)*']'](italic(i))") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(hjust = .5, size = 7, color = bd[3], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# half-normal density
p9 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma]", 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# the final normal density
p10 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("mu[italic(i)]", "sigma[italic(y)]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# an annotated arrow
p11 <-
  tibble(x     = .4,
         y     = .5,
         label = "'='") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, color = bd[3], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = .1, 
               arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()

# another annotated arrow
p12 <-
  tibble(x     = .49,
         y     = .55,
         label = "'~'") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, color = bd[3], parse = T, family = "Times") +
  geom_segment(x = .79, xend = .4,
               y = 1, yend = .2, 
               arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()

# the final annotated arrow
p13 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), 
            color = bd[3], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()

# some text
p14 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = bd[3], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 1, l = 6, r = 7),
  area(t = 1, b = 1, l = 10, r = 11),
  area(t = 1, b = 1, l = 14, r = 15),
  area(t = 3, b = 4, l = 1, r = 3),
  area(t = 3, b = 4, l = 5, r = 7),
  area(t = 3, b = 4, l = 9, r = 11),
  area(t = 3, b = 4, l = 13, r = 15),
  area(t = 2, b = 3, l = 6, r = 7),
  area(t = 2, b = 3, l = 10, r = 11),
  area(t = 2, b = 3, l = 14, r = 15),
  area(t = 6, b = 7, l = 1, r = 15),
  area(t = 5, b = 6, l = 1, r = 15),
  area(t = 9, b = 10, l = 10, r = 12),
  area(t = 12, b = 13, l = 7, r = 9),
  area(t = 8, b = 12, l = 7, r = 9),
  area(t = 11, b = 12, l = 7, r = 12),
  area(t = 14, b = 14, l = 7, r = 9),
  area(t = 15, b = 15, l = 7, r = 9)
)

# combine and plot!
(p1 + p1 + p1 + p3 + p4 + p5 + p6 + p2 + p2 + p2 + p8 + p7 + p9 + p10 + p11 + p12 + p13 + p14) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

<img src="20_files/figure-gfm/unnamed-chunk-7-1.png" width="768" style="display: block; margin: auto;" />

Wow that plot has a lot of working parts! ðŸ˜®

### Implementation in ~~JAGS~~ brms.

Below is how to implement the model based on the code from Kruschke's `Jags-Ymet-Xnom2fac-MnormalHom.R` and `Jags-Ymet-Xnom2fac-MnormalHom-Example.R` scripts. With **brms**, we'll need to specify the `stanvars`.


```r
mean_y <- mean(my_data$y)
sd_y   <- sd(my_data$y)

omega <- sd_y / 2
sigma <- 2 * sd_y

s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)

stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta")
```

And before that, of course, make sure you've defined the `gamma_a_b_from_omega_sigma()` function. E.g.,


```r
gamma_a_b_from_omega_sigma <- function(mode, sd) {
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate <- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape <- 1 + mode * rate
  return(list(shape = shape, rate = rate))
}
```

With the preparatory work done, now all we'd need to do is run the `brm()` code.


```r
fit <-
  brm(data = my_data,
      family = gaussian,
      y ~ 1 + (1 | factor_1) + (1 | factor_2) + (1 | factor_1:factor_2),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      stanvars = stanvars)
```

If you have reason to use different priors for the random effects, you can always specify multiple lines of `class = sd`, each with the appropriate `coef` argument.

The big new element is multiple `(|)` parts in the `formula`. In this simple model type, we're only working random intercepts, in this case with two factors and their interaction. The `formula` above presumes the interaction is not itself coded within the data. But consider the case you have data including a term for the interaction of the two lower-level factors, called `interaction`. In that case, you'd have that last part of the `formula` read `(1 | interaction)`, instead.

### Example: It's only money.

Load the salary data [^3].


```r
my_data <- read_csv("data.R/Salary.csv")

glimpse(my_data)
```

```
## Rows: 1,080
## Columns: 6
## $ Org     <chr> "PL", "MUTH", "ENG", "CMLT", "LGED", "MGMT", "INFO", "CRIN", "CRIN", "PSY", "SOC"â€¦
## $ OrgName <chr> "Philosophy", "Music Theory", "English", "Comparative Literature", "Language Educâ€¦
## $ Cla     <chr> "PC", "PC", "PC", "PC", "PT", "PR", "PT", "PR", "PR", "PR", "PR", "PR", "PR", "PCâ€¦
## $ Pos     <chr> "FT2", "FT2", "FT2", "FT2", "FT3", "NDW", "FT3", "FT1", "NDW", "FT1", "FT1", "FT1â€¦
## $ ClaPos  <chr> "PC.FT2", "PC.FT2", "PC.FT2", "PC.FT2", "PT.FT3", "PR.NDW", "PT.FT3", "PR.FT1", "â€¦
## $ Salary  <dbl> 72395, 61017, 82370, 68805, 63796, 219600, 98814, 107745, 114275, 173302, 117240,â€¦
```

We'll follow Kruschke's example on page 593 and modify the `Pos` variable a bit.


```r
my_data <-
  my_data %>% 
  mutate(Pos = factor(Pos,
                      levels = c("FT3", "FT2", "FT1", "NDW", "DST") ,
                      ordered = T,
                      labels = c("Assis", "Assoc", "Full", "Endow", "Disting")))
```

With 1080 cases, two factors, and a criterion, these data are a little too unwieldy to look at the individual case level. But if we're tricky on how we aggregate, we can get a good sense of their structure with a `geom_tile()` plot. Here our strategy is to aggregate by our two factors, `Pos` and `Org`. Since our criterion is `Salary`, we'll compute the mean value of the cases within each unique paring, encoded as `m_salary`. Also, we'll get a sense of how many cases there are within each factor pairing with `n`.


```r
my_data %>% 
  group_by(Pos, Org) %>% 
  summarise(m_salary = mean(Salary),
            n        = n()) %>% 
  ungroup() %>% 
  mutate(Org = fct_reorder(Org, m_salary),
         Pos = fct_reorder(Pos, m_salary)) %>% 
  
  ggplot(aes(x = Org, y = Pos, fill = m_salary, label = n)) +
  geom_tile() +
  geom_text(size = 2.75) +
  # everything below this is really just aesthetic flourish
  scale_fill_gradient(low = bd[9], high = bd[12],
                      breaks = c(55e3, 15e4, 26e4), 
                      labels = c("$55K", "$150K", "$260K")) +
  scale_x_discrete("Org", expand = c(0, 0)) +
  scale_y_discrete("Pos", expand = c(0, 0)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0),
        axis.text.y = element_text(hjust = 0),
        axis.ticks = element_blank(),
        legend.position = "top")
```

<img src="20_files/figure-gfm/unnamed-chunk-13-1.png" width="864" style="display: block; margin: auto;" />

Hopefully it's clear that each cell is a unique pairing of `Org` and `Pos`. The cells are color coded by the mean `Salary`. The numbers in the cells give the $n$ cases they represent. When there's no data for a unique combination of `Org` and `Pos`, the cells are left light gray and blank.

Load **brms**.


```r
library(brms)
```

Define our `stanvars`.


```r
mean_y <- mean(my_data$Salary)
sd_y   <- sd(my_data$Salary)

omega <- sd_y / 2
sigma <- 2 * sd_y

s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)

stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta")
```

Now fit the model.


```r
fit20.1 <-
  brm(data = my_data,
      family = gaussian,
      Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 20,
      control = list(adapt_delta = 0.999,
                     max_treedepth = 13),
      stanvars = stanvars,
      file = "fits/fit20.01")
```

The chains look fine.


```r
bayesplot::color_scheme_set(scheme = bd[c(13, 7:9, 11:12)])

plot(fit20.1)
```

<img src="20_files/figure-gfm/unnamed-chunk-16-1.png" width="768" style="display: block; margin: auto;" />

Here's the model summary.


```r
print(fit20.1)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org) 
##    Data: my_data (Number of observations: 1080) 
## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;
##          total post-warmup samples = 8000
## 
## Group-Level Effects: 
## ~Org (Number of levels: 60) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept) 30648.04   3042.28 25442.24 37414.75 1.00     2059     3368
## 
## ~Pos (Number of levels: 5) 
##               Estimate Est.Error l-95% CI  u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept) 56119.95  24966.14 26767.66 120147.98 1.00     3423     5330
## 
## ~Pos:Org (Number of levels: 216) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)  9697.15   1203.43  7393.97 12147.09 1.00     2523     4289
## 
## Population-Level Effects: 
##            Estimate Est.Error l-95% CI  u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept 126255.52  27552.64 69727.11 181336.74 1.00     3225     4886
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma 17975.97    436.80 17152.78 18853.66 1.00     7103     5660
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

This was a difficult model to fit with **brms**. Stan does well when the criteria are on or close to a standardized metric and these `Salary` data are a far cry from that. Tuning `adapt_delta` and `max_treedepth` went a long way to help the model out.

Okay, let's get ready for our version of Figure 20.3. First, we'll use `tidybayes::add_fitted_draws()` to help organize the necessary posterior draws.


```r
library(tidybayes)

# how many draws would you like?
n_draw <- 20

# wrangle
f <-
  my_data %>% 
  distinct(Pos) %>% 
  expand(Pos, 
         Org = c("BFIN", "CHEM", "PSY", "ENG")) %>% 
  add_fitted_draws(fit20.1, n = n_draw, seed = 20,
                   allow_new_levels = T,
                   dpar = c("mu", "sigma")) %>% 
  mutate(ll = qnorm(.025, mean = mu, sd = sigma),
         ul = qnorm(.975, mean = mu, sd = sigma)) %>% 
  mutate(Salary = map2(ll, ul, seq, length.out = 100)) %>% 
  unnest(Salary) %>% 
  mutate(density = dnorm(Salary, mu, sigma)) %>% 
  group_by(.draw) %>% 
  mutate(density = density / max(density)) %>% 
  mutate(Org = factor(Org, levels = c("BFIN", "CHEM", "PSY", "ENG")))

glimpse(f)
```

```
## Rows: 40,000
## Columns: 13
## Groups: .draw [20]
## $ Pos        <ord> Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, Aâ€¦
## $ Org        <fct> BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, â€¦
## $ .row       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â€¦
## $ .chain     <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NAâ€¦
## $ .iteration <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NAâ€¦
## $ .draw      <int> 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127â€¦
## $ .value     <dbl> 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1â€¦
## $ mu         <dbl> 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1â€¦
## $ sigma      <dbl> 18226.54, 18226.54, 18226.54, 18226.54, 18226.54, 18226.54, 18226.54, 18226.54â€¦
## $ ll         <dbl> 145963.7, 145963.7, 145963.7, 145963.7, 145963.7, 145963.7, 145963.7, 145963.7â€¦
## $ ul         <dbl> 217410.4, 217410.4, 217410.4, 217410.4, 217410.4, 217410.4, 217410.4, 217410.4â€¦
## $ Salary     <dbl> 145963.7, 146685.4, 147407.1, 148128.8, 148850.5, 149572.1, 150293.8, 151015.5â€¦
## $ density    <dbl> 0.1465288, 0.1582290, 0.1705958, 0.1836410, 0.1973740, 0.2118018, 0.2269281, 0â€¦
```

We're ready to plot.


```r
library(ggridges)

f %>% 
  ggplot(aes(x = Salary, y = Pos)) +
  geom_vline(xintercept = fixef(fit20.1)[, 1], color = bd[5]) +
  geom_ridgeline(aes(height = density, group = interaction(Pos, .draw),
                     color = Pos),
                 fill = NA, show.legend = F,
                 size = 1/4, scale = 3/4) +
  geom_jitter(data = my_data %>% 
                filter(Org %in% c("BFIN", "CHEM", "PSY", "ENG")) %>% 
                mutate(Org = factor(Org, levels = c("BFIN", "CHEM", "PSY", "ENG"))),
              height = .025, alpha = 1/2, size = 2/3, color = bd[11]) +
  scale_color_manual(values = bd[c(14, 13, 8, 12, 3)]) +
  scale_x_continuous(breaks = seq(from = 0, to = 300000, length.out = 4),
                     labels = c("$0", "$100K", "200K", "$300K")) +
  coord_cartesian(xlim = c(0, 35e4),
                  ylim = c(1.25, 5.5)) +
  labs(title = "Data with Posterior Predictive Distributions", 
       subtitle = "The white vertical line is the model-implied grand mean.",
       y = "Pos") +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank()) +
  facet_wrap(~Org, ncol = 2)
```

<img src="20_files/figure-gfm/unnamed-chunk-19-1.png" width="768" style="display: block; margin: auto;" />

The **brms** package doesn't have a convenience function that returns output quite like what Kruschke displayed in his Table 20.2. But we can get close. The `posterior_summary()` will return posterior means, $SD$s, and percentile-based 95% intervals for all model parameters. Due to space concerns, I'll just show the first ten lines.


```r
posterior_summary(fit20.1)[1:10,]
```

```
##                         Estimate  Est.Error       Q2.5      Q97.5
## b_Intercept           126255.523 27552.6419  69727.105 181336.736
## sd_Org__Intercept      30648.036  3042.2779  25442.242  37414.752
## sd_Pos__Intercept      56119.953 24966.1418  26767.655 120147.980
## sd_Pos:Org__Intercept   9697.154  1203.4267   7393.966  12147.087
## sigma                  17975.971   436.8002  17152.785  18853.664
## r_Org[ACTG,Intercept]  80254.691  7533.7446  65447.882  95019.103
## r_Org[AFRO,Intercept] -14880.629  9964.0507 -34291.306   4570.176
## r_Org[AMST,Intercept] -16510.435 10044.9806 -36225.773   3517.428
## r_Org[ANTH,Intercept] -18933.854  7700.2653 -34071.502  -4130.609
## r_Org[APHS,Intercept]   2678.886  7948.2368 -13079.343  18192.566
```

I'm not aware of a simple way to extract the effective samples for all parameters within a **brms** fit object. There is no `effsamples()` function. However, we do have `neff_ratio()`, which returns a named vector of the ratios. For space constraints, here we look at the first ten values.


```r
neff_ratio(fit20.1)[1:10]
```

```
##           b_Intercept     sd_Org__Intercept     sd_Pos__Intercept sd_Pos:Org__Intercept 
##             0.4175883             0.2579337             0.4780342             0.3140237 
##                 sigma r_Org[ACTG,Intercept] r_Org[AFRO,Intercept] r_Org[AMST,Intercept] 
##             0.8754821             0.3452653             0.5259244             0.5291086 
## r_Org[ANTH,Intercept] r_Org[APHS,Intercept] 
##             0.3124535             0.3494390
```

The `brms::neff_ratio()` function returns ratios of the effective samples over the total number of post-warmup iterations. So if we know the `neff_ratio()` values and the number of post-warmup iterations, the 'Eff.Sample' values are just a little algebra away. A quick solution is to look at the 'total post-warmup samples' line at the top of our `print()` output. Another way is to extract that information from our `brm()` fit object. I'm not aware of a way to do that directly, but we can extract the iter value (i.e., `fit20.1$fit@sim$iter`), the warmup value (i.e., `fit20.1$fit@sim$warmup`), and the number of chains (i.e., `fit20.1$fit@sim$chains`). With those values in hand, simple algebra will return the `total post-warmup samples` value. E.g.,


```r
(n_iter <- (fit20.1$fit@sim$iter - fit20.1$fit@sim$warmup) * fit20.1$fit@sim$chains)
```

```
## [1] 8000
```

And now we have `n_iter`, we can calculate the 'Eff.Sample' values.


```r
neff_ratio(fit20.1) %>% 
  data.frame() %>% 
  rownames_to_column() %>% 
  set_names("parameter", "neff_ratio") %>% 
  mutate(eff_sample = (neff_ratio * n_iter) %>% round(digits = 0)) %>% 
  select(-neff_ratio) %>% 
  arrange(parameter) %>% 
  head()
```

```
##               parameter eff_sample
## 1           b_Intercept       3341
## 2                  lp__       1799
## 3 r_Org[ACTG,Intercept]       2762
## 4 r_Org[AFRO,Intercept]       4207
## 5 r_Org[AMST,Intercept]       4233
## 6 r_Org[ANTH,Intercept]       2500
```

If we're careful, we can use `left_join()` to bind together the posterior summaries and their effective samples like this.


```r
post <- posterior_samples(fit20.1)

# extract the means and medians
post %>% 
  gather() %>% 
  arrange(key) %>% 
  group_by(key) %>% 
  summarise(Mean   = mean(value),
            Median = median(value)) %>% 
  # join to them the mode and HDIs
  left_join(
    post %>% 
      gather() %>% 
      group_by(key) %>% 
      mode_hdi(value) %>% 
      select(key:.upper) %>% 
      rename(Mode     = value,
             HDI_low  = .lower,
             HDI_high = .upper),
    by = "key"
    ) %>% 
  # now finally join the effective samples
  left_join(
    neff_ratio(fit20.1) %>% 
      as.data.frame() %>% 
      rownames_to_column() %>% 
      set_names("key", "neff_ratio") %>% 
      mutate(eff_sample = (neff_ratio * n_iter)) %>% 
      select(-neff_ratio) %>% 
      arrange(key),
    by = "key"
    ) %>% 
  # here we just reformat a bit
  mutate_if(is.double, round, digits = 0) %>% 
  rename(Parameter = key) %>% 
  
  head(n = 10)
```

```
## # A tibble: 10 x 7
##    Parameter               Mean Median   Mode HDI_low HDI_high eff_sample
##    <chr>                  <dbl>  <dbl>  <dbl>   <dbl>    <dbl>      <dbl>
##  1 b_Intercept           126256 126790 127983   68383   179760       3341
##  2 lp__                  -12530 -12529 -12528  -12563   -12498       1799
##  3 r_Org[ACTG,Intercept]  80255  80375  81556   65313    94857       2762
##  4 r_Org[AFRO,Intercept] -14881 -14899 -14384  -34416     4442       4207
##  5 r_Org[AMST,Intercept] -16510 -16410 -15106  -37139     2364       4233
##  6 r_Org[ANTH,Intercept] -18934 -18831 -17860  -33410    -3506       2500
##  7 r_Org[APHS,Intercept]   2679   2834   3435  -13173    18070       2796
##  8 r_Org[AST,Intercept]     -15   -122   -259  -17091    18328       3591
##  9 r_Org[BEPP,Intercept]  47848  47723  46539   30509    65360       3251
## 10 r_Org[BFIN,Intercept] 106924 106987 107801   92028   121476       3071
```

Our code returned the summaries for all model parameters, including the log posterior (i.e., `lp__`). Since that's way too much output to display, here, we just looked at the first 10. If you wanted to subset the parameters, you could just `filter()` by the `Parameter` column. And if this kind of a table was really important, to you, you might even use the code above as inspiration for a custom function.

As Kruschke then pointed out, "individual salaries vary tremendously around the predicted cell mean" (p. 594), which you can quantify using $\sigma_y$. Here it is using `posterior_summary()`.


```r
posterior_summary(fit20.1)["sigma", ]
```

```
##   Estimate  Est.Error       Q2.5      Q97.5 
## 17975.9712   436.8002 17152.7849 18853.6636
```

And we can get a better sense of the distribution with a dot plot.


```r
post %>%
  ggplot(aes(x = sigma, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95, 
                    shape = 23, stroke = 1/4, point_size = 3, slab_size = 1/4,
                    slab_fill = bd[6], slab_color = bd[1], color = bd[2], point_color = bd[1], point_fill = bd[2],
                    quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(sigma[y]))
```

<img src="20_files/figure-gfm/unnamed-chunk-26-1.png" width="384" style="display: block; margin: auto;" />

As Kruschke pointed out, this parameter is held constant across all subgroups. That is, the subgroups are homogeneous with respect to their variances. We'll relax this constraint later on.

Before we move on to the next section, look above at how many arguments we fiddled with to configure `stat_dotsinterval()`. Given how many more dot plots we have looming in our not-too-distant future, we might go ahead and save these settings as a new function. We'll call it `stat_beedrill()`.


```r
stat_beedrill <- function(quantiles = 100, ...) {
  
  stat_dotsinterval(point_interval = mode_hdi, .width = .95, 
                    shape = 23, stroke = 1/4, 
                    point_size = 3, slab_size = 1/4,
                    slab_fill = bd[6], slab_color = bd[1], 
                    color = bd[2], 
                    point_color = bd[1], point_fill = bd[2],
                    quantiles = quantiles,
                    ...)
}
```

### Main effect contrasts.

> In applications with multiple levels of the factors, it is virtually always the case that we are interested in comparing particular levels with each other.... These sorts of comparisons, which involve levels of a single factor and collapse across the other factor(s), are called main effect comparisons or contrasts.(p. 595)

The `fitted()` function provides a versatile framework for contrasts among the main effects. Here's the first contrast.


```r
# define the new data
nd <- 
  tibble(Pos = c("Assis", "Assoc"),
         Org = "mu")

# feed the new data into `fitted()`
f <-
  fitted(fit20.1,
         newdata = nd,
         summary = F,
         allow_new_levels = T) %>% 
  as_tibble() %>% 
  set_names("Assis", "Assoc") %>% 
  mutate(`Assoc vs Assis` = Assoc - Assis)

# plot
f %>% 
  ggplot(aes(x = `Assoc vs Assis`, y = 0)) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Assoc vs Assis",
       x = "Difference")
```

<img src="20_files/figure-gfm/unnamed-chunk-28-1.png" width="288" style="display: block; margin: auto;" />

In case you were curious, here are the summary statistics.


```r
f %>% 
  mode_hdi(`Assoc vs Assis`) %>% 
  select(`Assoc vs Assis`:.upper) %>% 
  mutate_if(is.double, round, digits = 0)
```

```
## # A tibble: 1 x 3
##   `Assoc vs Assis` .lower .upper
##              <dbl>  <dbl>  <dbl>
## 1            12858 -13969  39896
```

Now make the next two contrasts.


```r
nd <- 
  tibble(Org = c("BFIN", "CHEM", "ENG", "PSY"),
         Pos = "mu")

f <-
  fitted(fit20.1,
         newdata = nd,
         summary = F,
         allow_new_levels = T) %>% 
  as_tibble() %>% 
  set_names("BFIN", "CHEM", "ENG", "PSY") %>% 
  transmute(`CHEM vs PSY`     = CHEM - PSY,
            `BFIN vs other 3` = BFIN - (CHEM + ENG + PSY) / 3)

# plot
f %>% 
  gather() %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  facet_wrap(~key, scales = "free")
```

<img src="20_files/figure-gfm/unnamed-chunk-30-1.png" width="576" style="display: block; margin: auto;" />

And here are their numeric summaries.


```r
f %>% 
  gather(contrast, mode) %>% 
  group_by(contrast) %>% 
  mode_hdi(mode) %>% 
  select(contrast:.upper) %>% 
  mutate_if(is.double, round, digits = 0)
```

```
## # A tibble: 2 x 4
##   contrast          mode .lower .upper
##   <chr>            <dbl>  <dbl>  <dbl>
## 1 BFIN vs other 3 106775  79070 132215
## 2 CHEM vs PSY      11245 -18329  43468
```

For further discussion on marginal contrasts in **brms**, see the discussion in [issue #552]((https://github.com/paul-buerkner/brms/issues/552)) in the **brms** GitHub repo and [this discussion](https://discourse.mc-stan.org/t/how-do-i-get-marginal-effects-for-categorical-variables-to-condition-on-an-average-rather-than-a-category/5323) in the Stan user forums.

### Interaction contrasts and simple effects.

If we'd like to make the simple effects and interaction contrasts like Kruschke displayed in Figure 20.5 within our **tidyverse**/**brms** paradigm, it'll be simplest to just redefine our `nd` data and use `fitted()`, again.


```r
# define our new data
nd <-
  crossing(Pos = c("Assis", "Full"),
           Org = c("CHEM", "PSY")) 

# we need to update our col_names
brief_col_names <-
  crossing(Pos = c("Assis", "Full"),
           Org = c("CHEM", "PSY")) %>% 
  unite(key, Pos, Org) %>% 
  pull()

# get the draws with `fitted()`
f1 <-
  fitted(fit20.1, 
         newdata = nd,
         summary = F) %>% 
  # wrangle
  as_tibble() %>% 
  set_names(brief_col_names) %>% 
  mutate(`Full - Assis @ PSY`  = Full_PSY - Assis_PSY,
         `Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM) %>% 
  mutate(`Full.v.Assis (x) CHEM.v.PSY` = `Full - Assis @ CHEM` - `Full - Assis @ PSY`)

# what have we done?
head(f1)
```

```
## # A tibble: 6 x 7
##   Assis_CHEM Assis_PSY Full_CHEM Full_PSY `Full - Assis @ Pâ€¦ `Full - Assis @ Câ€¦ `Full.v.Assis (x) Câ€¦
##        <dbl>     <dbl>     <dbl>    <dbl>              <dbl>              <dbl>                <dbl>
## 1     85196.    87625.   156697.  113805.             26180.             71501.               45322.
## 2     90696.    84075.   156326.  118037.             33963.             65630.               31667.
## 3     89715.    80157.   156637.  118550.             38393.             66922.               28529.
## 4     84612.    90458.   164782.  115114.             24656.             80170.               55515.
## 5     94628.    82272.   161350.  112559.             30287.             66722.               36435.
## 6     84340.    86217.   147959.  118240.             32024.             63619.               31596.
```

It'll take just a tiny bit more wrangling before we're ready to plot.


```r
text <-
  tibble(key   = "Full - Assis @ PSY",
         value = 15500,
         y     = .95,
         label = "ROPE") %>% 
  mutate(key = factor(key, levels = c("Full - Assis @ PSY", "Full - Assis @ CHEM", "Full.v.Assis (x) CHEM.v.PSY")))

f1 %>% 
  select(-contains("_")) %>% 
  gather() %>% 
  mutate(key = factor(key, levels = c("Full - Assis @ PSY", "Full - Assis @ CHEM", "Full.v.Assis (x) CHEM.v.PSY"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  # for kicks and giggles we'll throw in the ROPE
  geom_rect(xmin = -1e3, xmax = 1e3,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = bd[7]) +
  stat_beedrill() +
  geom_text(data = text,
            aes(y = y, label = label),
            color = bd[7], size = 5) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  facet_wrap(~key, scales = "free_y")
```

<img src="20_files/figure-gfm/unnamed-chunk-33-1.png" width="768" style="display: block; margin: auto;" />

If it was really important that the labels in the $x$-axes were different, like they are in Kruschke's Figure 20.5, you could always make the three plots separately and then bind them together with **patchwork** syntax.

Though he didn't show the results, on page 598 Kruschke mentioned a few other contrasts we might consider. The example entailed comparing the differences within `BFIN` to the average of the other three. Let's walk it out.


```r
# define our new data
nd <-
  crossing(Pos = c("Assis", "Full"),
           Org = c("BFIN", "CHEM", "ENG", "PSY"))

# we need to update our `brief_col_names`
brief_col_names <-
  crossing(Pos = c("Assis", "Full"),
           Org = c("BFIN", "CHEM", "ENG", "PSY")) %>% 
  unite(key, Pos, Org) %>% 
  pull()

# get the draws with `fitted()`
f2 <-
  fitted(fit20.1, 
         newdata = nd,
         summary = F) %>% 
  # wrangle
  as_tibble() %>% 
  set_names(brief_col_names) %>% 
  mutate(`Full - Assis @ BFIN` = Full_BFIN - Assis_BFIN,
         `Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM,
         `Full - Assis @ ENG`  = Full_ENG - Assis_ENG,
         `Full - Assis @ PSY`  = Full_PSY - Assis_PSY) %>% 
  mutate(`Full.v.Assis (x) BFIN.v.the rest` = `Full - Assis @ BFIN` - (`Full - Assis @ CHEM` + `Full - Assis @ ENG` + `Full - Assis @ PSY`) / 3)

# what have we done?
glimpse(f2)
```

```
## Rows: 8,000
## Columns: 13
## $ Assis_BFIN                         <dbl> 204013.3, 194167.9, 190908.6, 204986.8, 206590.5, 1990â€¦
## $ Assis_CHEM                         <dbl> 85196.23, 90695.72, 89715.38, 84612.13, 94628.19, 8433â€¦
## $ Assis_ENG                          <dbl> 61319.08, 66098.68, 61094.59, 70835.14, 65593.89, 6069â€¦
## $ Assis_PSY                          <dbl> 87625.17, 84074.65, 80157.10, 90458.46, 82272.07, 8621â€¦
## $ Full_BFIN                          <dbl> 234638.3, 243410.2, 239923.8, 227318.5, 248931.7, 2354â€¦
## $ Full_CHEM                          <dbl> 156697.5, 156325.8, 156637.2, 164782.3, 161350.2, 1479â€¦
## $ Full_ENG                           <dbl> 110973.1, 106921.4, 107845.9, 105359.4, 104676.9, 1198â€¦
## $ Full_PSY                           <dbl> 113804.9, 118037.4, 118549.9, 115114.0, 112559.1, 1182â€¦
## $ `Full - Assis @ BFIN`              <dbl> 30625.00, 49242.23, 49015.15, 22331.71, 42341.18, 3635â€¦
## $ `Full - Assis @ CHEM`              <dbl> 71501.25, 65630.09, 66921.85, 80170.22, 66722.06, 6361â€¦
## $ `Full - Assis @ ENG`               <dbl> 49654.02, 40822.74, 46751.28, 34524.27, 39083.05, 5912â€¦
## $ `Full - Assis @ PSY`               <dbl> 26179.68, 33962.77, 38392.82, 24655.50, 30287.04, 3202â€¦
## $ `Full.v.Assis (x) BFIN.v.the rest` <dbl> -18486.6456, 2437.0316, -1673.4980, -24118.2823, -3022â€¦
```

Now plot.


```r
f2 %>% 
  select(-contains("_")) %>% 
  gather() %>% 
  
  ggplot(aes(x = value, y = 0)) +
  geom_rect(xmin = -1e3, xmax = 1e3,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = bd[7]) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  facet_wrap(~key, scales = "free_y", ncol = 3)
```

<img src="20_files/figure-gfm/unnamed-chunk-35-1.png" width="768" style="display: block; margin: auto;" />

So while the overall pay averages for those in `BFIN` were larger than those in the other three departments, the differences between full and associate professors within `BFIN` wasnâ€™t substantially different from the differences within the other three departments. To be sure, the interquartile range of that last difference distribution fell below both zero and the ROPE, but there's still a lot of spread in the rest of the distribution.

#### Interaction effects: High uncertainty and shrinkage.

"It is important to realize that the estimates of interaction contrasts are typically much more uncertain than the estimates of simple effects or main effects" (p. 598).

If we start with our `fitted()` object `f1`, we can wrangle a bit, compute the HDIs with `tidybayes::mode_hdi()` and then use simple subtraction to compute the interval range for each difference.


```r
f1 %>% 
  select(-contains("_")) %>% 
  gather() %>% 
  mutate(key = factor(key, levels = c("Full - Assis @ PSY", "Full - Assis @ CHEM", "Full.v.Assis (x) CHEM.v.PSY"))) %>% 
  group_by(key) %>% 
  mode_hdi(value) %>% 
  select(key:.upper) %>% 
  mutate(`interval range` = .upper - .lower)
```

```
## # A tibble: 3 x 5
##   key                          value .lower .upper `interval range`
##   <fct>                        <dbl>  <dbl>  <dbl>            <dbl>
## 1 Full - Assis @ PSY          32732. 17575. 45755.           28181.
## 2 Full - Assis @ CHEM         68192. 55265. 83841.           28577.
## 3 Full.v.Assis (x) CHEM.v.PSY 37203. 17582. 57824.           40242.
```

Just like Kruschke pointed out in the text, the interval for the interaction estimate was quite larger than the intervals for the simple contrasts.

> This large uncertainty of an interaction contrast is caused by the fact that it involves at least four sources of uncertainty (i.e., at least four groups of data), unlike its component simple effects which each involve only half of those sources of uncertainty. In general, interaction contrasts require a lot of data to estimate accurately. (p. 598)

Gelman has blogged on this, a bit (e.g., [*You need 16 times the sample size to estimate an interaction than to estimate a main effect*](https://andrewgelman.com/2018/03/15/need-16-times-sample-size-estimate-interaction-estimate-main-effect/)).

There is also shrinkage.

> The interaction contrasts also can experience notable shrinkage from the hierarchical model. In the present application, for example, there are 300 interaction deflections (5 levels of seniority times 60 departments) that are assumed to come from a higher- level distribution that has an estimated standard deviation, denoted $\sigma_{\beta 1 \times 2}$ in Figure 20.2. Chances are that most of the 300 interaction deflections will be small, and therefore the estimated standard deviation of the interaction deflections will be small, and therefore the estimated deflections themselves will be shrunken toward zero. This shrinkage is inherently neither good nor bad; it is simply the correct consequence of the model assumptions. The shrinkage can be good insofar as it mitigates false alarms about interactions, but the shrinkage can be bad if it inappropriately obscures meaningful interactions. (p. 598)

Here's that $\sigma_{\beta 1 \times 2}$.


```r
post %>% 
  ggplot(aes(x = `sd_Pos:Org__Intercept`, y = 0)) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL)
```

<img src="20_files/figure-gfm/unnamed-chunk-37-1.png" width="360" style="display: block; margin: auto;" />

## Rescaling can change interactions, homogeneity, and normality

> When interpreting interactions, it can be important to consider the scale on which the data are measured. This is because an interaction means non-additive effects when measured on the current scale. If the data are nonlinearly transformed to a different scale, then the non-additivity can also change. (p. 599)

Here is Kruschke's initial example of a possible interaction effect of sex and political party with respect to wages.


```r
d <-
  tibble(monetary_units = c(10, 12, 15, 18),
         politics       = rep(c("democrat", "republican"), each = 2),
         sex            = rep(c("women", "men"), times = 2)) %>% 
  mutate(sex_number = if_else(sex == "women", 1, 2),
         politics   = factor(politics, levels = c("republican", "democrat")))

d %>% 
  ggplot(aes(x = sex_number, y = monetary_units, color = politics)) +
  geom_line(size = 3) +
  scale_color_manual(NULL, values = bd[8:7]) +
  scale_x_continuous("sex", breaks = 1:2, labels = c("women", "men")) +
  coord_cartesian(ylim = c(0, 20)) +
  theme(legend.position = c(.2, .15))
```

<img src="20_files/figure-gfm/unnamed-chunk-38-1.png" width="312" style="display: block; margin: auto;" />

Because the pay discrepancy between men and women is not equal between Democrats and Republicans, in this example, it can be tempting to claim there is a subtle interaction. Not necessarily so.


```r
tibble(politics      = c("democrat", "republican"),
       female_salary = c(10, 15)) %>% 
  mutate(male_salary = 1.2 * female_salary)
```

```
## # A tibble: 2 x 3
##   politics   female_salary male_salary
##   <chr>              <dbl>       <dbl>
## 1 democrat              10          12
## 2 republican            15          18
```

If we take female salary as the baseline and then add 20% to it for the men, the salary difference between Republican men and women will be larger than that between Democratic men and women. Even though the rate increase from women to men was the same, the increase in absolute value was greater within Republicans because Republican women made more than Democratic women.

Look what happens to our original plot when we transform `monetary_units` with `log10()`.


```r
d %>% 
  ggplot(aes(x = sex_number, y = log10(monetary_units), color = politics)) +
  geom_line(size = 3) +
  scale_color_manual(NULL, values = bd[8:7]) +
  scale_x_continuous("sex", breaks = 1:2, labels = c("women", "men")) +
  theme(legend.position = c(.2, .4))
```

<img src="20_files/figure-gfm/unnamed-chunk-40-1.png" width="312" style="display: block; margin: auto;" />

"Equal ratios are transformed to equal distances by a logarithmic transformation" (p. 599).

We can get a better sense of this with our version of Figure 20.6. Probably the easiest way to inset the offset labels will be with help from the `geom_dl()` function from the [**directlabels** package](https://CRAN.R-project.org/package=directlabels) [@R-directlabels].


```r
library(directlabels)

# define the data
d <-
  crossing(int = c("Nonâˆ’crossover Interaction", "Crossover Interaction"),
           x1  = factor(1:2),
           x2  = factor(1:2)) %>% 
  mutate(y = c(6, 2, 15, 48, 2, 6, 15, 48)) %>% 
  mutate(ly = log(y))

# save the subplots
p1 <-
  d %>% 
  filter(int == "Nonâˆ’crossover Interaction") %>% 
  ggplot(aes(x = x1, y = y, group = x2, label = x2)) +
  geom_line(aes(color = x2),
            size = 1) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[8:7]) +
  scale_x_discrete(NULL, breaks = NULL, expand = expansion(mult = 0.1)) +
  labs(subtitle = "Nonâˆ’crossover Interaction") +
  theme(legend.position = c(.15, .8),
        legend.key.size = unit(1/3, "cm"))

p2 <-
  d %>% 
  filter(int == "Crossover Interaction") %>% 
  ggplot(aes(x = x1, y = y, group = x2, label = x2)) +
  geom_line(aes(color = x2),
            size = 1) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[8:7], breaks = NULL) +
  scale_x_discrete(NULL, breaks = NULL, expand = expansion(mult = 0.1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Crossover Interaction")

p3 <-
  d %>% 
  filter(int == "Crossover Interaction") %>% 
  ggplot(aes(x = x2, y = y, group = x1, label = x1)) +
  geom_line(aes(color = x1),
            size = 2) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[12:11]) +
  scale_x_discrete(NULL, breaks = NULL, expand = expansion(mult = 0.1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Crossover Interaction") +
  theme(legend.position = c(.15, .8),
        legend.key.size = unit(1/3, "cm"))

p4 <-
  d %>% 
  filter(int == "Nonâˆ’crossover Interaction") %>% 
  ggplot(aes(x = x1, y = ly, group = x2, label = x2)) +
  geom_line(aes(color = x2),
            size = 1) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[8:7], breaks = NULL) +
  scale_x_discrete(expand = expansion(mult = 0.1)) +
  ylab(expression(log(y)))

p5 <-
  d %>% 
  filter(int == "Crossover Interaction") %>% 
  ggplot(aes(x = x1, y = ly, group = x2, label = x2)) +
  geom_line(aes(color = x2),
            size = 1) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[8:7], breaks = NULL) +
  scale_x_discrete(expand = expansion(mult = 0.1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ylab(expression(log(y)))

p6 <-
  d %>% 
  filter(int == "Crossover Interaction") %>% 
  ggplot(aes(x = x2, y = ly, group = x1, label = x1)) +
  geom_line(aes(color = x1),
            size = 2) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[12:11], breaks = NULL) +
  scale_x_discrete(expand = expansion(mult = 0.1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ylab(expression(log(y)))

# combine
p1 + p2 + p3 + p4 + p5 + p6 
```

<img src="20_files/figure-gfm/unnamed-chunk-41-1.png" width="720" style="display: block; margin: auto;" />

"The transformability from interaction to non-interaction is only possible for non-crossover interactions. This terminology, 'noncrossover,' is merely a description of the graph: The lines do not cross over each other and they have the same sign slope" (p. 601). The plots in the leftmost column are examples of non-crossover interactions. The plots in the center column are examples of crossover interactions.

Kruschke then pointed out that transforming data can have unexpected consequences for summary values, such as variances:

> Suppose one condition has data values of 100, 110, and 120, while a second condition has data values of 1100, 1110, and 1120. For both conditions, the variance is 66.7, so there is homogeneity of variance. When the data are logarithmically transformed, the variance of the first group becomes 1.05eâˆ’3, but the variance of the second group becomes two orders of magnitude smaller, namely 1.02eâˆ’5. In the transformed data there is not homogeneity of variance. (p. 601)

See for yourself.


```r
tibble(x = c(100, 110, 120, 1100, 1110, 1120),
       y = rep(letters[1:2], each = 3)) %>% 
  group_by(y) %>% 
  summarise(variance_of_x     = var(x),
            variance_of_log_x = var(log(x)))
```

```
## # A tibble: 2 x 3
##   y     variance_of_x variance_of_log_x
##   <chr>         <dbl>             <dbl>
## 1 a               100         0.00832  
## 2 b               100         0.0000812
```

Though it looks like the numbers Kruschke reported in the text are off, his overall point stands. While we had homogeneity of variance for `x`, the variance is heterogeneous when working with `log(x)`.

## Heterogeneous variances and robustness against outliers

As we will see in just a moment, our approach to the variant of this model with heterogeneous variances and robustness against outliers will differ slightly from the one Kruschke presented in the text. The two are the same in spirit, but ours differs in how we model $\sigma_{[jk](i)}$. We'll get a sense of that difference in our version of the hierarchical model diagram of Figure 20.7.


```r
# bracket
p1 <-
  tibble(x     = .99,
         y     = .5,
         label = "{_}") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, hjust = 1, color = bd[2], family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

#  plain arrow
p2 <-
  tibble(x    = .73,
         y    = 1,
         xend = .68,
         yend = .25) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()
  
# normal density
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p4 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("0", "sigma[beta][1]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p5 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("0", "sigma[beta][2]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p6 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 0.75), y = .6,
           hjust = c(.5, 0),
           label = c("0", "sigma[beta][1%*%2]"), 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# four annotated arrows
p7 <-
  tibble(x    = c(.05, .34, .64, .945),
         y    = c(1, 1, 1, 1),
         xend = c(.05, .18, .45, .75),
         yend = c(0, 0, 0, 0)) %>%
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = c(.03, .23, .30, .52, .585, .82, .90), y = .5,
           label = c("'~'", "'~'", "italic(j)", "'~'", "italic(k)", "'~'", "italic(jk)"),
           size = c(10, 10, 7, 10, 7, 10, 7), 
           color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p8 <-
  tibble(x = .5,
         y = .25,
         label = "beta[0]+sum()[italic(j)]*beta[1]['['*italic(j)*']']*italic(x)[1]['['*italic(j)*']'](italic(i))+sum()[italic(k)]*beta[2]['['*italic(k)*']']*italic(x)[2]['['*italic(k)*']'](italic(i))+sum()[italic(jk)]*beta[1%*%2]['['*italic(jk)*']']*italic(x)[1%*%2]['['*italic(jk)*']'](italic(i))") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(hjust = .5, size = 7, color = bd[3], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# normal density
p9 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("italic(M)[mu[sigma]]", "italic(S)[mu[sigma]]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# half-normal density
p10 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma[sigma]]", 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# exponential density
p11 <-
  tibble(x = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dexp(x, 2) / max(dexp(x, 2))))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "exp",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(K)",
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# normal density
p12 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dnorm(x)) / max(dnorm(x)))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.2), y = .6,
           hjust = c(.5, 0),
           label = c("mu[sigma]", "sigma[sigma]"), 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# student-t density
p13 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, ymin = 0, ymax = (dt(x, 3) / max(dt(x, 3))))) +
  geom_ribbon(fill = bd[6], size = 0) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "student t",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(-1.4, 0), y = .6,
           label = c("nu", "mu[italic(i)]"),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# two annotated arrows
p14 <-
  tibble(x    = c(.18, .82),
         y    = c(1, 1),
         xend = c(.46, .66),
         yend = c(.28, .28)) %>%
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = c(.24, .69), y = .62,
           label = "'~'",
           size = 10, color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# log sigma
p15 <-
  tibble(x = .65,
         y = .6,
         label = "log(sigma['['*italic(jk)*']('*italic(i)*')'])") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(hjust = 0, size = 7, color = bd[3], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) +
  ylim(0, 1) +
  theme_void()

# two annotated arrows
p16 <-
  tibble(x    = c(.18, .18), # .2142857
         y    = c(1, .45),
         xend = c(.18, .67),
         yend = c(.75, .14)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = c(.13, .18, .26), y = c(.92, .64, .22),
           label = c("'~'", "nu*minute+1", "'='"),
           size = c(10, 7, 10), color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# one annotated arrow
p17 <-
  tibble(x    = .5,
         y    = 1,
         xend = .5,
         yend = .03) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = .4, y = .5,
           label = "'='",
           size = 10, color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# another annotated arrow
p18 <-
  tibble(x    = .87,
         y    = 1,
         xend = .43,
         yend = .2) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = c(.56, .7), y = .5,
           label = c("'~'", "italic(jk)"),
           size = c(10, 7), color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# the final annotated arrow
p19 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>%

  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7),
            color = bd[3], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()

# some text
p20 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>%

  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = bd[3], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 1, l = 6, r = 7),
  area(t = 1, b = 1, l = 10, r = 11),
  area(t = 1, b = 1, l = 14, r = 15),
  area(t = 3, b = 4, l = 1, r = 3),
  area(t = 3, b = 4, l = 5, r = 7),
  area(t = 3, b = 4, l = 9, r = 11),
  area(t = 3, b = 4, l = 13, r = 15),
  area(t = 2, b = 3, l = 6, r = 7),
  area(t = 2, b = 3, l = 10, r = 11),
  area(t = 2, b = 3, l = 14, r = 15),
  area(t = 6, b = 7, l = 1, r = 15),
  area(t = 5, b = 6, l = 1, r = 15),
  area(t = 9, b = 10, l = 9, r = 11),
  area(t = 9, b = 10, l = 13, r = 15),
  area(t = 12, b = 13, l = 1, r = 3),
  area(t = 12, b = 13, l = 11, r = 13),
  area(t = 16, b = 17, l = 5, r = 7),
  area(t = 11, b = 12, l = 9, r = 15),
  area(t = 16, b = 17, l = 5, r = 10),
  area(t = 14, b = 16, l = 1, r = 7),
  area(t = 8, b = 16, l = 5, r = 7),
  area(t = 14, b = 16, l = 5, r = 13),
  area(t = 18, b = 18, l = 5, r = 7),
  area(t = 19, b = 19, l = 5, r = 7)
)

# combine and plot!
(p1 + p1 + p1 + p3 + p4 + p5 + p6 + p2 + p2 + p2 + p8 + p7 + p9 + p10 + p11 + p12 + p13 + p14 + p15 + p16 + p17 + p18 + p19 + p20) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

<img src="20_files/figure-gfm/unnamed-chunk-43-1.png" width="768" style="display: block; margin: auto;" />

Wow that's a lot! If you compare our diagram with the one in the text, you'll see the $\nu$ and $\mu$ structures are the same. If you look down toward the bottom of the diagram, the first big difference is that we're modeling the log of $\sigma_{[jk](i)}$, which is the typical **brms** strategy for avoiding negative values when modeling a $\sigma$ parameter. Then when you look up and to the right, you'll see that we're modeling $\log(\sigma_{[jk](i)})$ with a conventional hierarchical Gaussian structure. Keep this in mind when we get into our **brms** code, below.

Before we fit the robust hierarchical variances model, we need to define our `stanvars`.


```r
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta") +
  stanvar(1/29,      name = "one_over_twentynine")
```

Recall that to fit a robust hierarchical variances model, we need to wrap our two formulas within the `bf()` function. âš ï¸ **Warning**: this one took several hours fit. âš ï¸




```r
fit20.2 <-
  brm(data = my_data,
      family = student,
      bf(Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org), 
         sigma  ~ 1 + (1 | Pos:Org)),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(normal(log(sd_y), 1),     class = Intercept, dpar = sigma),
                prior(gamma(alpha, beta), class = sd),
                prior(normal(0, 1),       class = sd, dpar = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      seed = 20,
      control = list(adapt_delta = 0.9995,
                     max_treedepth = 15),
      stanvars = stanvars,
      file = "fits/fit20.02")
```

Behold the summary.


```r
print(fit20.2)
```

```
##  Family: student 
##   Links: mu = identity; sigma = log; nu = identity 
## Formula: Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org) 
##          sigma ~ 1 + (1 | Pos:Org)
##    Data: my_data (Number of observations: 1080) 
## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;
##          total post-warmup samples = 12000
## 
## Group-Level Effects: 
## ~Org (Number of levels: 60) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept) 31447.99   3029.60 26167.92 38068.59 1.00     2752     5125
## 
## ~Pos (Number of levels: 5) 
##               Estimate Est.Error l-95% CI  u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept) 50657.30  23555.77 23446.66 110351.14 1.00     4707     6450
## 
## ~Pos:Org (Number of levels: 216) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)        5303.89    862.72  3741.71  7154.75 1.00     2618     4524
## sd(sigma_Intercept)     0.97      0.07     0.84     1.12 1.00     3422     5842
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI  u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept       122225.83  24590.96 72456.12 172261.44 1.00     3407     5453
## sigma_Intercept      9.11      0.09     8.93      9.29 1.00     3125     5806
## 
## Family Specific Parameters: 
##    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## nu     6.96      4.60     3.71    15.51 1.00     5352     6040
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

This time we'll just feed the results of the wrangling code right into the plotting code for our version of the top panels of Figure 20.8.


```r
# how many draws would you like?
n_draw <- 20

# wrangle
my_data %>% 
  distinct(Pos) %>% 
  expand(Pos, 
         Org = c("BFIN", "CHEM", "PSY", "ENG")) %>% 
  add_fitted_draws(fit20.2, n = n_draw, seed = 20,
                   allow_new_levels = T,
                   dpar = c("mu", "sigma", "nu")) %>% 
  mutate(ll = qt(.025, df = nu),
         ul = qt(.975, df = nu)) %>% 
  mutate(Salary = map2(ll, ul, seq, length.out = 100)) %>% 
  unnest(Salary) %>%
  mutate(density = dt(Salary, nu)) %>% 
  # notice the conversion
  mutate(Salary = mu + Salary * sigma) %>% 
  group_by(.draw) %>% 
  mutate(density = density / max(density)) %>% 
  mutate(Org = factor(Org, levels = c("BFIN", "CHEM", "PSY", "ENG"))) %>%
  
  # plot
  ggplot(aes(x = Salary, y = Pos)) +
  geom_vline(xintercept = fixef(fit20.1)[, 1], color = bd[5]) +
  geom_ridgeline(aes(height = density, group = interaction(Pos, .draw), color = Pos),
                 fill = NA, show.legend = F,
                 size = 1/4, scale = 3/4) +
  geom_jitter(data = my_data %>% 
                filter(Org %in% c("BFIN", "CHEM", "PSY", "ENG")) %>% 
                mutate(Org = factor(Org, levels = c("BFIN", "CHEM", "PSY", "ENG"))),
              height = .025, alpha = 1/2, size = 2/3, color = bd[11]) +
  scale_color_manual(values = bd[c(14, 13, 8, 12, 3)]) +
  scale_x_continuous(breaks = seq(from = 0, to = 300000, length.out = 4),
                     labels = c("$0", "$100K", "200K", "$300K")) +
  coord_cartesian(xlim = c(0, 35e4),
                  ylim = c(1.25, 5.5)) +
  labs(title = "Data with Posterior Predictive Distributions", 
       subtitle = "The white vertical line is the model-implied grand mean.",
       y = "Pos") +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank()) +
  facet_wrap(~Org, ncol = 2)
```

<img src="20_files/figure-gfm/unnamed-chunk-47-1.png" width="768" style="display: block; margin: auto;" />

Our results for the bottom panel of Figure 20.8 will differ substantially from Kruschke's. Recall that Kurschke modeled $\sigma_{[j, k](i)}$ with a hierarchical gamma distribution and using the $\omega + \sigma$ parameterization. We, however, modeled our hierarchical $\log (\sigma)$ with the typical normal distribution. As such, we have posteriors for $\sigma_\mu$ and $\sigma_\sigma$.


```r
# wrangle
posterior_samples(fit20.2) %>% 
  transmute(Normality              = log10(nu), 
            `Mean of Cell Sigma's` = exp(b_sigma_Intercept), 
            `SD of Cell Sigma's`   = exp(`sd_Pos:Org__sigma_Intercept`)) %>%
  gather() %>% 
  mutate(key = factor(key, levels = c("Normality", "Mean of Cell Sigma's", "SD of Cell Sigma's"))) %>% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("param. value") +
  facet_wrap(~key, scales = "free", ncol = 3)
```

<img src="20_files/figure-gfm/unnamed-chunk-48-1.png" width="768" style="display: block; margin: auto;" />

Though our $\sigma_\mu$ is on a similar metric to Kruschke's $\sigma_\omega$, our $\sigma_\sigma$ is just fundamentally different from his. So it goes. If you think I'm in error, here, [share your insights](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues). Even though our hierarchical $\sigma$ parameters look different from Kruschke's, it turns the contrast distributions are quite similar. Here's the necessary wrangling to make our version for Figure 20.9.


```r
# define our new data
nd <-
  crossing(Pos = c("Assis", "Full"),
           Org = c("CHEM", "PSY"))

# we need to update our col_names
brief_col_names <-
  crossing(Pos = c("Assis", "Full"),
           Org = c("CHEM", "PSY")) %>% 
  unite(key, Pos, Org) %>% 
  pull()

# get the draws with `fitted()`
f <-
  fitted(fit20.2, 
         newdata = nd,
         summary = F) %>% 
  # wrangle
  as_tibble() %>% 
  set_names(brief_col_names) %>% 
  transmute(`Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM,
            `Full - Assis @ PSY`  = Full_PSY - Assis_PSY) %>% 
  mutate(`Full.v.Assis (x) CHEM.v.PSY` = `Full - Assis @ CHEM` - `Full - Assis @ PSY`) %>% 
  gather() %>% 
  mutate(key = factor(key, levels = c("Full - Assis @ PSY", "Full - Assis @ CHEM", "Full.v.Assis (x) CHEM.v.PSY")))

# what have we done?
head(f)
```

```
## # A tibble: 6 x 2
##   key                  value
##   <fct>                <dbl>
## 1 Full - Assis @ CHEM 48941.
## 2 Full - Assis @ CHEM 48521.
## 3 Full - Assis @ CHEM 50871.
## 4 Full - Assis @ CHEM 49690.
## 5 Full - Assis @ CHEM 50048.
## 6 Full - Assis @ CHEM 42843.
```

Now plot.


```r
f %>% 
  ggplot(aes(x = value, y = 0)) +
  geom_rect(xmin = -1e3, xmax = 1e3,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = bd[7]) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  facet_wrap(~key, scales = "free", ncol = 3)
```

<img src="20_files/figure-gfm/unnamed-chunk-50-1.png" width="768" style="display: block; margin: auto;" />

See? Our contrast distributions are really close those in the text. Here are the numeric estimates.


```r
f %>% 
  group_by(key) %>% 
  mode_hdi(value) %>% 
  select(key:.upper) %>% 
  mutate_if(is.double, round, digits = 0)
```

```
## # A tibble: 3 x 4
##   key                         value .lower .upper
##   <fct>                       <dbl>  <dbl>  <dbl>
## 1 Full - Assis @ PSY          33451  22559  43609
## 2 Full - Assis @ CHEM         53226  40070  67693
## 3 Full.v.Assis (x) CHEM.v.PSY 18579   3367  38073
```

In the second half of the middle paragraph on page 605, Kruschke contrasted the $\sigma_{\beta 1 \times 2}$ parameter in the two models (i.e., our `fit20.1` and `fit20.2`). Recall that in the **brms** output, these are termed `sd_Pos:Org__Intercept`. Here are the comparisons from our **brms** models.


```r
posterior_summary(fit20.1)["sd_Pos:Org__Intercept", ]
```

```
##  Estimate Est.Error      Q2.5     Q97.5 
##  9697.154  1203.427  7393.966 12147.087
```

```r
posterior_summary(fit20.2)["sd_Pos:Org__Intercept", ]
```

```
##  Estimate Est.Error      Q2.5     Q97.5 
## 5303.8945  862.7234 3741.7096 7154.7545
```

They are similar to the values in the text. And recall, of course, the `brms::posterior_summary()` function returns posterior means. If you really wanted the posterior modes, like Kruschke reported in the text, you'll have to work a little harder.


```r
bind_rows(posterior_samples(fit20.1) %>% select(`sd_Pos:Org__Intercept`),
          posterior_samples(fit20.2) %>% select(`sd_Pos:Org__Intercept`)) %>% 
  mutate(fit = rep(c("fit20.1", "fit20.2"), each = n() / 2)) %>% 
  group_by(fit) %>% 
  mode_hdi(`sd_Pos:Org__Intercept`) %>% 
  select(fit:.upper) %>% 
  mutate_if(is.double, round, digits = 0)
```

```
## # A tibble: 3 x 4
##   fit     `sd_Pos:Org__Intercept` .lower .upper
##   <chr>                     <dbl>  <dbl>  <dbl>
## 1 fit20.1                    9664   4137   6622
## 2 fit20.1                    9664   6927  12114
## 3 fit20.2                    5100   3599   7003
```

The curious might even look at those in a plot.


```r
bind_rows(posterior_samples(fit20.1) %>% select(`sd_Pos:Org__Intercept`),
          posterior_samples(fit20.2) %>% select(`sd_Pos:Org__Intercept`)) %>% 
  mutate(fit = rep(c("fit20.1", "fit20.2"), each = n() / 2)) %>% 
  
  ggplot(aes(x = `sd_Pos:Org__Intercept`, y = fit)) +
  stat_beedrill() +
  labs(x = expression(sigma[beta][1%*%2]),
       y = NULL) +
  coord_cartesian(ylim = c(1.5, NA))
```

<img src="20_files/figure-gfm/unnamed-chunk-54-1.png" width="576" style="display: block; margin: auto;" />

"Which model is a better description of the data?... In principle, an intrepid programmer could do a Bayesian model comparison..." (pp. 605--606). We could also examine iformation criteria, like the LOO.


```r
fit20.1 <- add_criterion(fit20.1, "loo")
fit20.2 <- add_criterion(fit20.2, "loo")
```

*Sigh*. Both models had high `pareto_k` values, suggesting there were outliers relative to what was expected by their likelihoods. Just a little further in the text, Kruschke gives us hints why this might be so:

> Moreover, both models assume that the data within cells are distributed symmetrically above and below their central tendency, either as a normal distribution or a $t$-distribution. The data instead seem to be skewed toward larger values, especially for advanced seniorities. (p. 606)

Here's the current LOO difference.


```r
loo_compare(fit20.1, fit20.2) %>% print(simplify = F)
```

```
##         elpd_diff se_diff  elpd_loo se_elpd_loo p_loo    se_p_loo looic    se_looic
## fit20.2      0.0       0.0 -11766.9     41.5       314.8     10.9  23533.7     83.1
## fit20.1   -426.8      34.1 -12193.7     42.8       139.5     11.7  24387.3     85.6
```

For a more robust model comparison, we might follow the `kfold()` suggestion in the warning. However, be warned: The `kfold()` function entails refitting the model `K` times. `fit20.1` took about an hour to fit and `fit20.2` took several more. Proceed with caution.

But really, "we might want to create a model that describes the data within each cell as a skewed distribution such as a Weibull" (p. 606). Yes, **brms** can handle Weibull regression (e.g., [here](https://discourse.mc-stan.org/t/fitting-time-to-event-data-with-weibull-hazard-using-brm-function/4638)).



## Within-subject designs

> When every subject contributes many measurements to every cell, then the model of the situation is a straight-forward extension of the models we have already considered. We merely add "subject" as another nominal predictor in the model, with each individual subject being a level of the predictor. If there is one predictor other than subject, the model becomes
>
> $$ y = \beta_0 + \overrightarrow \beta_1 \overrightarrow x_1 + \overrightarrow \beta_S \overrightarrow x_S + \overrightarrow \beta_{1 \times S} \overrightarrow x_{1 \times S} $$
>
> This is exactly the two-predictor model we have already considered, with the second predictor being subject. When there are two predictors other than subject, the model becomes
> 
> \begin{align*}
> y = & \; \beta_0 & \text{baseline} \\
> & + \overrightarrow \beta_1 \overrightarrow x_1 + \overrightarrow \beta_2 \overrightarrow x_2 + \overrightarrow \beta_S \overrightarrow x_S  & \text{main effects} \\
> & + \overrightarrow \beta_{1 \times 2} \overrightarrow x_{1 \times 2} + \overrightarrow \beta_{1 \times S} \overrightarrow x_{1 \times S} + \overrightarrow \beta_{2 \times S} \overrightarrow x_{2 \times S} & \text{two-way interactions} \\
> & + \overrightarrow \beta_{1 \times 2 \times S} \overrightarrow x_{1 \times 2 \times S} & \text{three-way interactions}
> \end{align*}
> 
> This model includes all the two-way interactions of the factors, plus the three-way interaction. (p. 607)

In situations in which subjects only contribute one observation per condition/cell, we simplify the model to

\begin{align*}
y = & \; \beta_0 \\
& + \overrightarrow \beta_1 \overrightarrow x_1 + \overrightarrow \beta_2 \overrightarrow x_2 + \overrightarrow \beta_{1 \times 2} \overrightarrow x_{1 \times 2} \\
& + \overrightarrow \beta_S \overrightarrow x_S
\end{align*}

"In other words, we assume a main effect of subject, but no interaction of subject with other predictors. In this model, the subject effect (deflection) is constant across treatments, and the treatment effects (deflections) are constant across subjects" (p. 608).

### Why use a within-subject design? And why not?

Kruschke opined "the primary reason to use a within-subject design is that you can achieve greater precision in the estimates of the effects than in a between-subject design" (p. 608). Well, to that I counterpoint: "No one goes to the circus to see the average dog jump through the hoop significantly oftener than untrained does raised under the same circumstances" [@skinnerCaseHistoryScientific1956, p. 228]. And it's unlikely youâ€™ll make a skillful jumper of your dog without repeated trials. There's also the related issue that between- and within-person processes aren't necessarily the same. For an introduction to the issue, see Hamaker's [-@hamakerWhyResearchersShould2012] chapter, [*Why researchers should think "within-person": A paradigmatic rationale*](https://www.researchgate.net/publication/266896375_Why_researchers_should_think_within-person_A_paradigmatic_rationale), or the paper from @bolgerCausalProcessesPsychology2019, [*Causal processes in psychology are heterogeneous*](https://www.researchgate.net/profile/Niall_Bolger/publication/332358948_Causal_processes_in_psychology_are_heterogeneous/links/5cd9b471a6fdccc9ddaa7879/Causal-processes-in-psychology-are-heterogeneous.pdf).

But we digress.

Here's the 4-subject response time data.


```r
(
  d <-
  tibble(response_time = c(300, 320, 350, 370, 400, 420, 450, 470),
         subject       = rep(1:4, each = 2),
         hand          = rep(c("dominant", "nondominant"), times = 4))
)
```

```
## # A tibble: 8 x 3
##   response_time subject hand       
##           <dbl>   <int> <chr>      
## 1           300       1 dominant   
## 2           320       1 nondominant
## 3           350       2 dominant   
## 4           370       2 nondominant
## 5           400       3 dominant   
## 6           420       3 nondominant
## 7           450       4 dominant   
## 8           470       4 nondominant
```

"For every subject, the difference between dominant and nondominant hands is exactly 20 ms, but there are big differences across subjects in overall response times" (p. 608). Here's what that looks like.


```r
d %>% 
  mutate(subject = factor(subject)) %>% 
  
  ggplot(aes(x = response_time, y = subject)) +
  geom_line(aes(group = subject), 
            color = bd[3], linetype = 3) +
  geom_point(aes(color = hand), size = 3) +
  scale_color_manual(values = bd[8:9])
```

<img src="20_files/figure-gfm/unnamed-chunk-59-1.png" width="576" style="display: block; margin: auto;" />

Here there is more variability between subjects than within them, which you'd never detect without a within-subject design including multiple subjects.

### Split-plot design.

"Split-plot experiments were invented by @fisherStatisticalMethodsResearch1925 (p. 610)."

Kruschke then wrote this to set the stage for the next subsection:

> Consider an agricultural experiment investigating the productivity of different soil tilling methods and different fertilizers. It is relatively easy to provide all the farmers with the several different fertilizers. But it might be relatively difficult to provide all farmers with all the machinery for several different tilling methods. Therefore, any particular farmer will use a single (randomly assigned) tilling method on his whole plot, and tilling methods will differ between whole plots. Each farmer will split his field into subplots and apply all the fertilizers to different (randomly assigned) split plots, and fertilizers will differ across split plots within whole plots. This type of experiment inspires the name, split-plot design. The generic experiment-design term for the farmer's field is "block." Then, the factor that varies within every field is called the within-block factor and the factor that varies between fields is called the between-block factor. Notice also that each split plot yields a single measurement (in this case the productivity measured in bushels per acre), not multiple measurements. (p. 610)

#### Example: Knee high by the fourth of July.

Load the agronomy data.


```r
my_data <- read_csv("data.R/SplitPlotAgriData.csv")

glimpse(my_data)
```

```
## Rows: 99
## Columns: 4
## $ Field <dbl> 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10â€¦
## $ Till  <chr> "Chisel", "Chisel", "Chisel", "Chisel", "Chisel", "Chisel", "Chisel", "Chisel", "Châ€¦
## $ Fert  <chr> "Broad", "Deep", "Surface", "Broad", "Deep", "Surface", "Broad", "Deep", "Surface",â€¦
## $ Yield <dbl> 119, 130, 123, 135, 148, 134, 140, 146, 142, 126, 132, 131, 128, 141, 153, 117, 130â€¦
```

We might use `geom_tile()` to visualize the data like this.


```r
my_data %>% 
  mutate(Fert = str_c("Fert: ", Fert)) %>% 
  
  ggplot(aes(x = Till, y = Field)) +
  geom_tile(aes(fill = Yield)) +
  scale_fill_gradient(low = bd[1], high = bd[12]) +
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  theme(panel.background = element_rect(fill = bd[7])) +
  facet_wrap(~Fert)
```

<img src="20_files/figure-gfm/unnamed-chunk-61-1.png" width="576" style="display: block; margin: auto;" />

As Kruschke pointed out in the text, notice how each `Field` has only one level of `Till`, but three levels of `Fert`.

#### The descriptive model.

> In the classical ANOVA-style model for a split-plot design, the overall variance is conceptually decomposed into five components: the main effect of the between-subjects factor, the main effect of the within-subjects factor, the interaction of the two factors, the effect of subject within levels of the between-subject factor, and the interaction of subject with the within-subject factor. Unfortunately, because there is only a single datum per cell, the five components exactly match the data, which is to say that there are as many parameters as there are data points. (If every subject contributed multiple data points to every cell then the five-component model could be used.) Because there is no residual noise within cells, the classical approach is to treat the final component as noise, that is, treat the interaction of subject with the within-subject factor as noise. That component is not included in the model (at least, not distinct from noise). We will do the same for the descriptive model in our Bayesian analysis. (p. 612)

#### Implementation in ~~JAGS~~ brms.

Define the `stanvars`.


```r
mean_y <- mean(my_data$Yield)
sd_y   <- sd(my_data$Yield)

omega <- sd_y / 2
sigma <- 2 * sd_y

s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)

stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta")
```

Here's how to fit the model with `brm()`.


```r
fit20.3 <-
  brm(data = my_data,
      family = gaussian,
      Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Field) + (1 | Till:Fert),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 20,
      control = list(adapt_delta = 0.99,
                     max_treedepth = 12),
      stanvars = stanvars,
      file = "fits/fit20.03")
```

#### Results.

Check the summary.


```r
print(fit20.3)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Field) + (1 | Till:Fert) 
##    Data: my_data (Number of observations: 99) 
## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;
##          total post-warmup samples = 8000
## 
## Group-Level Effects: 
## ~Fert (Number of levels: 3) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)    15.36     13.52     1.35    51.01 1.00     2495     2714
## 
## ~Field (Number of levels: 33) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)    11.74      1.72     8.87    15.60 1.00     1639     3004
## 
## ~Till (Number of levels: 3) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)    15.48     13.84     1.25    53.53 1.00     2087     2781
## 
## ~Till:Fert (Number of levels: 9) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     8.79      4.13     3.69    19.67 1.00     1901     3365
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   140.44     15.87   107.70   174.18 1.00     2632     3057
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     5.69      0.53     4.75     6.83 1.00     3583     5148
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

We might compare the $\sigma$ posteriors with a plot.


```r
posterior_samples(fit20.3) %>% 
  pivot_longer(c(sigma, starts_with("sd"))) %>% 
    
  ggplot(aes(x = value, y = name)) +
  stat_beedrill(point_color = bd[2]) +
  labs(x = NULL,
       y = NULL) +
  coord_cartesian(xlim = c(0, 50),
                  ylim = c(1.5, NA)) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())
```

```
## Warning: Duplicated aesthetics after name standardisation: point_colour
```

<img src="20_files/figure-gfm/unnamed-chunk-64-1.png" width="768" style="display: block; margin: auto;" />

Now we have the results of `fit20.3`, we are ready to make our version if Figure 20.10. Note that how within `add_fitted_draws()`, we used the `re_formula` argument to average over the random effects of `Field` (i.e., we left `(1 | Field)` out of the formula). That's our equivalent to when Kruschke wrote "The predictive normal distributions are plotted with means at $\beta_0 + \beta_B + \beta_W + \beta_{B \times W}$ (collapsed across $\beta_S$) and with standard deviation $\sigma$" (pp. 614--615).


```r
# wrangle
my_data %>% 
  distinct(Till, Fert) %>% 
  add_fitted_draws(fit20.3, n = 20,
                   re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert),
                   dpar = c("mu", "sigma")) %>% 
  mutate(ll = qnorm(.025, mean = mu, sd = sigma),
         ul = qnorm(.975, mean = mu, sd = sigma)) %>% 
  mutate(Yield = map2(ll, ul, seq, length.out = 100)) %>% 
  unnest(Yield) %>% 
  mutate(density = dnorm(Yield, mu, sigma)) %>% 
  group_by(.draw) %>% 
  mutate(density = density / max(density)) %>% 
  
  # plot
  ggplot(aes(x = Yield, y = Fert)) +
  geom_path(data = my_data,
            aes(group = Field %>% as.factor()),
            size = 1/4, color = bd[5]) +
  geom_ridgeline(aes(height = -density, group = interaction(Fert, .draw), color = Fert),
                 fill = NA, size = 1/3, scale = 3/4,
                 min_height = NA) +
  geom_jitter(data = my_data,
              height = .025, alpha = 1/2, color = bd[5]) +
  scale_color_manual(values = bd[c(9, 6, 12)]) +
  scale_x_continuous(breaks = seq(from = 100, to = 180, by = 20)) +
  coord_flip(xlim = c(90, 190),
             ylim = c(0.5, 2.75)) +
  ggtitle("Data with Posterior Predictive Distribution") +
  theme(axis.ticks.x = element_blank(),
        legend.position = "none") +
  facet_wrap(~Till)
```

<img src="20_files/figure-gfm/unnamed-chunk-65-1.png" width="576" style="display: block; margin: auto;" />

Now let's make our Figure 20.11 contrasts.


```r
col_names <-
  my_data %>% 
  distinct(Till, Fert) %>% 
  unite(key, Till, Fert) %>% 
  pull()

nd <-
  my_data %>% 
  distinct(Till, Fert)

fitted(fit20.3,
       newdata = nd,
       summary = F,
       re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert)) %>% 
  as_tibble() %>% 
  set_names(col_names) %>% 
  transmute(
    `Moldbrd vs Ridge` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 3) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3),
            
    `Moldbrd.Ridge vs Chisel` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface + Ridge_Broad + Ridge_Deep + Ridge_Surface) / 6) - ((Chisel_Broad + Chisel_Deep + Chisel_Surface) / 3),
    
    `Deep.Surface vs Broad` = ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6) - ((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3),
    
    `Chisel.Moldbrd.v.Ridge (x) Broad.v.Deep.Surface` = (((Chisel_Broad + Chisel_Deep + Chisel_Surface + Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 6) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3)) - (((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3) - ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6))
    ) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("Moldbrd vs Ridge", "Moldbrd.Ridge vs Chisel", "Deep.Surface vs Broad", "Chisel.Moldbrd.v.Ridge (x) Broad.v.Deep.Surface"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  geom_rect(xmin = -5, xmax = 5,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = bd[7]) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  theme(strip.text = element_text(size = 6)) +
  facet_wrap(~name, scales = "free", ncol = 4)
```

<img src="20_files/figure-gfm/unnamed-chunk-66-1.png" width="768" style="display: block; margin: auto;" />

As far as I can tell, it appears that our contrasts indicate our variance parameter for `Till` ended up larger than Kruschkeâ€™s.

Kruschke then posed a model "with field/subject coding suppressed, hence no lines connecting data from the same field/subject" (p. 616). Here's how to fit that model.


```r
fit20.4 <-
  brm(data = my_data,
      family = gaussian,
      Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 20,
      control = list(adapt_delta = 0.999,
                     max_treedepth = 12),
      stanvars = stanvars,
      file = "fits/fit20.04")
```

Behold the summary.


```r
print(fit20.4)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert) 
##    Data: my_data (Number of observations: 99) 
## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;
##          total post-warmup samples = 8000
## 
## Group-Level Effects: 
## ~Fert (Number of levels: 3) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)    14.89     12.84     1.35    49.45 1.00     2960     3323
## 
## ~Till (Number of levels: 3) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)    16.27     13.75     1.77    52.94 1.00     2855     2968
## 
## ~Till:Fert (Number of levels: 9) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     7.66      4.56     1.28    18.44 1.00     2064     2814
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   140.64     16.03   108.18   174.17 1.00     3718     3685
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    12.71      0.96    11.01    14.76 1.00     8073     5309
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

Look at how much larger the posterior is for $\sigma_y$ in this model compared to `fit20.3`.


```r
posterior_summary(fit20.3)["sigma", ]
```

```
##  Estimate Est.Error      Q2.5     Q97.5 
## 5.6934670 0.5314513 4.7521209 6.8286296
```

```r
posterior_summary(fit20.4)["sigma", ]
```

```
##   Estimate  Est.Error       Q2.5      Q97.5 
## 12.7086864  0.9606523 11.0091522 14.7612638
```

Here's the top portion of Figure 20.12.


```r
# wrangle
my_data %>% 
  distinct(Till, Fert) %>% 
  add_fitted_draws(fit20.4, n = 20,
                   re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert),
                   dpar = c("mu", "sigma")) %>% 
  mutate(ll = qnorm(.025, mean = mu, sd = sigma),
         ul = qnorm(.975, mean = mu, sd = sigma)) %>% 
  mutate(Yield = map2(ll, ul, seq, length.out = 200)) %>% 
  unnest(Yield) %>% 
  mutate(density = dnorm(Yield, mu, sigma)) %>% 
  group_by(.draw) %>% 
  mutate(density = density / max(density)) %>% 
  
  # plot
  ggplot(aes(x = Yield, y = Fert)) +
  geom_ridgeline(aes(height = -density, group = interaction(Fert, .draw), color = Fert),
                 fill = NA, size = 1/3, scale = 3/4,
                 min_height = NA) +
  geom_jitter(data = my_data,
              height = .025, alpha = 1/2, color = bd[5]) +
  scale_color_manual(values = bd[c(9, 6, 12)]) +
  scale_x_continuous(breaks = seq(from = 100, to = 180, by = 20)) +
  coord_flip(xlim = c(90, 190),
             ylim = c(0.5, 2.75)) +
  ggtitle("Data with Posterior Predictive Distribution") +
  theme(axis.ticks.x = element_blank(),
        legend.position = "none") +
  facet_wrap(~Till)
```

<img src="20_files/figure-gfm/unnamed-chunk-69-1.png" width="576" style="display: block; margin: auto;" />

Now the make the plots for the contrast distributions.


```r
fitted(fit20.4,
       newdata = nd,
       summary = F,
       re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert)) %>% 
  as_tibble() %>% 
  set_names(col_names) %>% 
  transmute(
    `Moldbrd vs Ridge` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 3) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3),
            
    `Moldbrd.Ridge vs Chisel` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface + Ridge_Broad + Ridge_Deep + Ridge_Surface) / 6) - ((Chisel_Broad + Chisel_Deep + Chisel_Surface) / 3),
    
    `Deep.Surface vs Broad` = ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6) - ((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3),
    
    `Chisel.Moldbrd.v.Ridge (x) Broad.v.Deep.Surface` = (((Chisel_Broad + Chisel_Deep + Chisel_Surface + Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 6) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3)) - (((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3) - ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6))
    ) %>% 
  gather() %>% 
  mutate(key = factor(key, levels = c("Moldbrd vs Ridge", "Moldbrd.Ridge vs Chisel", "Deep.Surface vs Broad", "Chisel.Moldbrd.v.Ridge (x) Broad.v.Deep.Surface"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  geom_rect(xmin = -5, xmax = 5,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = bd[7]) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  theme(strip.text = element_text(size = 6)) +
  facet_wrap(~key, scales = "free", ncol = 4)
```

<img src="20_files/figure-gfm/unnamed-chunk-70-1.png" width="768" style="display: block; margin: auto;" />

Though not identical, these were closer to those Kruschke displayed in the text.

#### Model comparison approach.

Like we covered in Chapter 10, I'm not aware that Stan/**brms** will allow for $\delta$ factor-inclusion parameters the way JAGS allows. However, if you'd like to compare models with different parameters, you can always use information criteria.


```r
fit20.3 <- add_criterion(fit20.3, "loo")
fit20.4 <- add_criterion(fit20.4, "loo")
```

Executing that yielded the following warning message:

> Found 4 observations with a pareto_k > 0.7 in model 'fit20.3'. It is recommended to set 'reloo = TRUE' in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model 4 times to compute the ELPDs for the problematic observations directly.

Let's follow the diagnostic warnings to make sure the model comparison is robust to the outlying values.


```r
fit20.3 <- add_criterion(fit20.3, "loo", reloo = T)
```

We're finally ready for the LOO comparison.


```r
loo_compare(fit20.3, fit20.4) %>% 
  print(simplify = F)
```

```
##         elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic
## fit20.3    0.0       0.0  -335.3      8.2        34.8    4.5    670.5   16.4  
## fit20.4  -61.1       7.9  -396.4      5.7         8.7    1.0    792.7   11.4
```

All good. Based on the LOO values, we should prefer the fuller model. This, of course, should be no surprise. The posterior for $\sigma_\text{Field}$ was a far cry from zero.


```r
posterior_summary(fit20.3)["sd_Field__Intercept", ]
```

```
##  Estimate Est.Error      Q2.5     Q97.5 
## 11.741891  1.723704  8.868938 15.604229
```

Kruschke ended this chapter by mentioning Bayes' factors:

> Bayes' factor approaches to hypothesis tests in ANOVA were presented by @rouderDefaultBayesFactors2012 and Wetzels, Grasman, and Wagenmakers [-@wetzelsDefaultBayesianHypothesis2012]. Morey and Rouder's BayesFactor package for R is available at the Web site [http://bayesfactorpcl.r-forge.r-project.org/](http://bayesfactorpcl.r-forge.r-project.org/). (p. 618)

If you wanna go that route, you're on your own.

## Session info {-}


```r
sessionInfo()
```

```
## R version 3.6.3 (2020-02-29)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Catalina 10.15.3
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] directlabels_2020.1.31 ggridges_0.5.2         tidybayes_2.1.1        brms_2.13.5           
##  [5] Rcpp_1.0.5             patchwork_1.0.1.9000   palettetown_0.1.1      forcats_0.5.0         
##  [9] stringr_1.4.0          dplyr_1.0.1            purrr_0.3.4            readr_1.3.1           
## [13] tidyr_1.1.1            tibble_3.0.3           ggplot2_3.3.2          tidyverse_1.3.0       
## 
## loaded via a namespace (and not attached):
##   [1] TH.data_1.0-10       colorspace_1.4-1     ellipsis_0.3.1       rsconnect_0.8.16    
##   [5] estimability_1.3     markdown_1.1         base64enc_0.1-3      fs_1.4.1            
##   [9] rstudioapi_0.11      farver_2.0.3         rstan_2.19.3         svUnit_1.0.3        
##  [13] DT_0.13              fansi_0.4.1          mvtnorm_1.1-0        lubridate_1.7.8     
##  [17] xml2_1.3.1           codetools_0.2-16     splines_3.6.3        bridgesampling_1.0-0
##  [21] knitr_1.28           shinythemes_1.1.2    bayesplot_1.7.1      jsonlite_1.7.0      
##  [25] broom_0.5.5          dbplyr_1.4.2         ggdist_2.1.1         shiny_1.5.0         
##  [29] compiler_3.6.3       httr_1.4.1           emmeans_1.4.5        backports_1.1.9     
##  [33] assertthat_0.2.1     Matrix_1.2-18        fastmap_1.0.1        cli_2.0.2           
##  [37] later_1.1.0.1        prettyunits_1.1.1    htmltools_0.5.0      tools_3.6.3         
##  [41] igraph_1.2.5         coda_0.19-3          gtable_0.3.0         glue_1.4.2          
##  [45] reshape2_1.4.4       cellranger_1.1.0     vctrs_0.3.4          nlme_3.1-144        
##  [49] crosstalk_1.1.0.1    xfun_0.13            ps_1.3.4             rvest_0.3.5         
##  [53] mime_0.9             miniUI_0.1.1.1       lifecycle_0.2.0      gtools_3.8.2        
##  [57] MASS_7.3-51.5        zoo_1.8-7            scales_1.1.1         colourpicker_1.0    
##  [61] hms_0.5.3            promises_1.1.1       Brobdingnag_1.2-6    sandwich_2.5-1      
##  [65] parallel_3.6.3       inline_0.3.15        shinystan_2.5.0      yaml_2.2.1          
##  [69] gridExtra_2.3        StanHeaders_2.21.0-1 loo_2.3.1            stringi_1.4.6       
##  [73] dygraphs_1.1.1.6     pkgbuild_1.1.0       rlang_0.4.7          pkgconfig_2.0.3     
##  [77] matrixStats_0.56.0   HDInterval_0.2.0     evaluate_0.14        lattice_0.20-38     
##  [81] rstantools_2.1.1     htmlwidgets_1.5.1    labeling_0.3         tidyselect_1.1.0    
##  [85] processx_3.4.4       plyr_1.8.6           magrittr_1.5         bookdown_0.18       
##  [89] R6_2.4.1             generics_0.0.2       multcomp_1.4-13      DBI_1.1.0           
##  [93] pillar_1.4.6         haven_2.2.0          withr_2.2.0          xts_0.12-0          
##  [97] survival_3.1-12      abind_1.4-5          modelr_0.1.6         crayon_1.3.4        
## [101] arrayhelpers_1.1-0   utf8_1.1.4           rmarkdown_2.1        emo_0.0.0.9000      
## [105] grid_3.6.3           readxl_1.3.1         callr_3.4.4          threejs_0.3.3       
## [109] reprex_0.3.0         digest_0.6.25        xtable_1.8-4         httpuv_1.5.4        
## [113] stats4_3.6.3         munsell_0.5.0        quadprog_1.5-8       shinyjs_1.1
```





## Footnote {-}

[^3]: It's not quite clear where these data come from. At the top of page 591, Kruschke wrote "in this section, we will be looking at some real-world salaries." Further down on the same page, Kruschke added: "The data are annual salaries of 1,080 tenure-track professors at a large-enrollment, small-city, midwestern-American, research-oriented, state university. (Salaries at big- city and private universities tend to be higher, while salaries at liberal-arts colleges and teaching-oriented state universities tend to be lower.) The data span 60 academic departments that had at least seven members. The data also include the professorâ€™s seniority." He did not provide a reference.

