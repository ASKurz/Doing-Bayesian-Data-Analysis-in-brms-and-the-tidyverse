[["index.html", "Doing Bayesian Data Analysis in brms and the tidyverse version 1.0.1 What and why R setup We have updates Thank-you’s are in order License and citation", " Doing Bayesian Data Analysis in brms and the tidyverse version 1.0.1 A Solomon Kurz 2023-01-06 What and why Kruschke began his text with “This book explains how to actually do Bayesian data analysis, by real people (like you), for realistic data (like yours).” In the same way, this project is designed to help those real people do Bayesian data analysis. My contribution is converting Kruschke’s JAGS and Stan code for use in Bürkner’s brms package (Bürkner, 2017, 2018, 2022e), which makes it easier to fit Bayesian regression models in R (R Core Team, 2021) using Hamiltonian Monte Carlo. I also prefer plotting and data wrangling with the packages from the tidyverse (Wickham et al., 2019; Wickham, 2021). So we’ll be using those methods, too. This ebook is not meant to stand alone. It’s a supplement to the second edition of Kruschke’s (2015) Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Please give the source material some love. R setup To get the full benefit from this ebook, you’ll need some software. Happily, everything will be free (provided you have access to a decent personal computer and an good internet connection). First, you’ll need to install R, which you can learn about at https://cran.r-project.org/. Though not necessary, your R experience might be more enjoyable if done through the free RStudio interface, which you can learn about at https://rstudio.com/products/rstudio/. Once you have installed R, execute the following to install the bulk of the add-on packages. This will probably take a few minutes to finish. Go make yourself a coffee. packages &lt;- c(&quot;bayesplot&quot;, &quot;brms&quot;, &quot;coda&quot;, &quot;cowplot&quot;, &quot;cubelyr&quot;, &quot;devtools&quot;, &quot;fishualize&quot;, &quot;GGally&quot;, &quot;ggdist&quot;, &quot;ggExtra&quot;, &quot;ggforce&quot;, &quot;ggmcmc&quot;, &quot;ggridges&quot;, &quot;ggthemes&quot;, &quot;janitor&quot;, &quot;lisa&quot;, &quot;loo&quot;, &quot;palettetown&quot;, &quot;patchwork&quot;, &quot;psych&quot;, &quot;remotes&quot;, &quot;rstan&quot;, &quot;santoku&quot;, &quot;scico&quot;, &quot;tidybayes&quot;, &quot;tidyverse&quot;) install.packages(packages, dependencies = T) A few of the other packages are not officially available via the Comprehensive R Archive Network (CRAN; https://cran.r-project.org/). You can download them directly from GitHub by executing the following. remotes::install_github(&quot;clauswilke/colorblindr&quot;) devtools::install_github(&quot;dill/beyonce&quot;) devtools::install_github(&quot;ropenscilabs/ochRe&quot;) It’s possible you’ll have problems installing some of these packages. Here are some likely suspects and where you can find help: for difficulties installing brms, go to https://github.com/paul-buerkner/brms#how-do-i-install-brms or search around in the brms section of the Stan forums; and for difficulties installing rstan, go to https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started. We have updates For a brief rundown of the version history, we have: Version 0.1.0. I released the 0.1.0 version of this project in February 17, 2020. It was the first [fairly] complete draft including material from all the chapters in Kruschke’s text. The supermajority of Kruschke’s JAGS and Stan models were fit brms 2.11.5. The results were saved in the fits folder on GitHub and most of the results are quite comparable to those in the original text. We also reproduced most of the data-related figures and tables and little subpoints and examples sprinkled throughout Kruschke’s prose. Version 0.2.0. The 0.2.0 update came in May 19, 2020. Noteworthy changes included: reproducing the simulation necessary for Figure 7.3 (see GitHub issue #14) with help from Cardy Moten III (@cmoten); with guidance from Bjørn Peare Bartholdy (@bbartholdy), Mladen Jovanović (@mladenjovanovic), Cory Whitney (@CWWhitney), and Brenton M. Wiernik (@bwiernik), we improved in-text citations and reference sections using BibTex (BibTeX, 2020), Better BibTeX (Heyns, 2020), and zotero (Roy Rosenzweig Center for History and New Media, 2020); the plot resolution increased with fig.retina = 2.5; and small code, hyperlink, and typo corrections. Version 0.3.0. The 0.3.0 update came in September 22, 2020. Noteworthy changes included: adding the Kruschke-style model diagrams throughout the text (e.g., Figure 8.5); adding chapter-specific plotting schemes with help from the cowplot package (Wilke, 2020), Wilke’s (2019) Fundamentals of data visualization, and many other great color-scheme packages; an overhaul to the plotting workflow in Section 6.4.1; and updating all model fits with brms version 2.13.5. Version 0.4.0. The 0.4.0 update came in May 6, 2021. Noteworthy changes included: using the Metropolis algorithm to fit the bivariate Bernoulli model for Figure 7.6 (Section 7.4.3), thanks to help from Omid Ghasemi; corrections to mistakes around the lag() and lead() functions in Section 7.5.2; an added bonus section clarifying the pooled standard deviation for standardized mean differences (Section 16.3.0.1); refining the custom stat_wilke() plotting function in Chapter 18; an overhaul of the bonus section covering effect sizes (Section 19.6); refining/correcting the threshold workflow for univariable logistic regression models (Chapter 21); fixing the divergent transitions issue for the robust logistic regression model by adding boundaries on the prior (Section 21.3); corrections to a few incorrectly computed effect sizes in Chapter 23; the addition of a new bonus section (Section 22.3.3.1.1) highlighting the benefits of the intercepts-only softmax model; expansions to the material on censored data (Section 25.4) and the addition of a brief introduction to truncated data (Section 25.4.4); and updating all HMC fits to the current version of brms (2.15.0). Version 1.0.0. The big 1.0.0 update came in May 4, 2022. This was the first full draft in the sense that it contained brms versions of all of Kruschke’s JAGS and Stan models, excluding examples not currently possible with the brms paradigm (e.g., Section 10.3.2). Noteworthy changes included: two new solutions for the conditional logistic models of Chapter 22 thanks to the generous efforts by Henrik Singmann and Mattan Ben-Shachar; replacing the depreciated posterior_samples() function with the new posterior::as_draws_df()-based workflow; adding a new solution for the multivariate Bernoulli model with different trial numbers via the resp_subset() function in Section 7.4.4.1; improving the efficiency of the intercept-only Bernoulli models with the new lb and ub arguments for priors of class = Intercept (e.g., fit7.1a, fit7.1b, and fit7.2 in Chapter 7); updating all model fits with brms version 2.17.0; and various minor code, hyperlink, and typo corrections. Version 1.0.1. Welcome to version 1.0.1! In this minor update, I have fixed the broken links to my professional website, https://solomonkurz.netlify.app/, and corrected a couple typos. Some minor issues remain. There are some minor improvements I’d like to add in future versions. Most importantly, a few simulations, figures, and models are beyond my current skill set. I’ve opened separate GitHub issues for the most important ones and they are as follows: the effective-sample-size simulations in Section 7.5.2 and the corresponding plots in Figures 7.13 and 7.14 (issue #15), several of the simulations in Sections 11.1.4, 11.3.1, and 11.3.2 and their corresponding figures (issues #16, #17, #18, and #19), the stopping-rule simulations in Section 13.3.2 and their corresponding figures (issue #20), the data necessary to properly reproduce the HMC proposal schematic presented in Section 14.1 and Figures 14.1 through 14.3 (issue #21), and If you know how to conquer any of these unresolved challenges, I’d love to hear all about it. In addition, please feel free to open a new GitHub issue if you find any flaws in the other sections of the ebook. Thank-you’s are in order Before we enter the primary text, I’d like to thank the following for their helpful contributions: Bjørn Peare Bartholdy (@bbartholdy), David Baumeister (@xdavebx), Mattan Ben-Shachar (@mattansb), Paul-Christian Bürkner (@paul-buerkner), flachboard (@flachboard), Andrew Gelman (@andrewgelman), Omid Ghasemi (@OmidGhasemi21), Mladen Jovanović (@mladenjovanovic), Matthew Kay (@mjskay), TJ Mahr (@tjmahr), Cardy Moten III (@cmoten), Lukas Neugebauer (@LukasNeugebauer), Demetri Pananos (@Dpananos), Peter Ralph (@petrelharp), Henrik Singmann (@singmann), Aki Vehtari (@avehtari), Matti Vuorre (@mvuorre), Cory Whitney (@CWWhitney), and Brenton M. Wiernik (@bwiernik). License and citation This book is licensed under the Creative Commons Zero v1.0 Universal license. You can learn the details, here. In short, you can use my work. Just please give me the appropriate credit the same way you would for any other scholarly resource. Here’s the citation information: @book{kurzDoingBayesianDataAnalysis2022, title = {Doing {{Bayesian}} data analysis in brms and the tidyverse}, author = {Kurz, A. Solomon}, year = {2023}, month = {1}, edition = {Version 1.0.1}, url = {https://bookdown.org/content/3686/} } References BibTeX. (2020). http://www.bibtex.org/ Bürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01 Bürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms. The R Journal, 10(1), 395–411. https://doi.org/10.32614/RJ-2018-017 Bürkner, P.-C. (2022e). brms: Bayesian regression models using ’Stan’. https://CRAN.R-project.org/package=brms Heyns, E. (2020). Better BibTeX for zotero. https://retorque.re/zotero-better-bibtex/ Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ R Core Team. (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Roy Rosenzweig Center for History and New Media. (2020). Zotero. https://www.zotero.org/ Wickham, H. (2021). tidyverse: Easily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 Wilke, C. O. (2019). Fundamentals of data visualization. https://clauswilke.com/dataviz/ Wilke, C. O. (2020). cowplot: Streamlined plot theme and plot annotations for ggplot2 [Manual]. https://wilkelab.org/cowplot/ "],["whats-in-this-book-read-this-first.html", "1 What’s in This Book (Read This First!) 1.1 Real people can read this book 1.2 What’s in this book 1.3 What’s new in the second edition 1.4 Gimme feedback (be polite) 1.5 Thank you! Session info", " 1 What’s in This Book (Read This First!) 1.1 Real people can read this book Kruschke began his (2015, p. 1) text with “This book explains how to actually do Bayesian data analysis, by real people (like you), for realistic data (like yours).” Agreed. Similarly, this project is designed to help those real people do Bayesian data analysis. While I’m at it, I may as well explicate my assumptions about you. If you’re looking at this project, I’m guessing you’re a graduate student, a post-graduate academic or researcher of some sort. Which means I’m presuming you have at least a 101-level foundation in statistics. In his text, it seems like Kruschke presumed his readers would have a good foundation in calculus, too. I make no such presumption. But if your stats 101 chops are rusty, check out Legler and Roback’s free (2019) bookdown text, Broadening your statistical horizons or Navarro’s free (2019) text, Learning statistics with R: A tutorial for psychology students and other beginners. I presume a basic working fluency in R and a vague idea about what the tidyverse is. Kruschke does some R warm-up in Chapter 3, and I follow suit. But if you’re totally new to R, you might also consider starting with Peng’s (2020) R programming for data science. The best introduction to the tidyvese-style of data analysis I’ve found is Grolemund and Wickham’s (2017) R for data science. If you prefer learning by video, Navarro has some nice introductory playlists here. 1.2 What’s in this book This ebook is not meant to stand alone. It’s a supplement to the second edition of Kruschke’s (2015) Doing Bayesian data analysis. I follow the structure of his text, chapter by chapter, translating his analyses into brms and tidyverse code. However, many of the sections in the text are composed entirely of equations and prose, leaving us nothing to translate. When we run into those, the corresponding sections in this ebook may be missing. Also beware the content herein will depart at times from Kruschke’s source material. Bayesian data analysis with HMC is an active area of development in terms of both statistical methods and software implementation. There will also be times when my thoughts and preferences on Bayesian data analysis diverge a bit from Kruschke’s. In those places of divergence, I will often provide references and explanations. I use a handful of formatting conventions gleaned from R4DS and Xie, Allaire, and Grolemund’s (2022) R markdown: The definitive guide. For example: I put R and R packages (e.g., brms) in boldface. R code blocks and their output appear in a gray background. E.g., 2 + 2 ## [1] 4 Did you notice how there were two strips of gray background, there? The first one designated the actual code. The second one was the output of that code, and the output of a code block often begins with ##. Functions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., brm()). When I want to make explicit what packages a given function comes from, I insert the double-colon operator :: between the package name and the function (e.g., tidyr::pivot_longer()). R objects, such as data or function arguments, are in typewriter font atop a gray background (e.g., d or size = 2). Hyperlinks are denoted by their typical blue-colored font. 1.3 What’s new in the second edition There’s nothing new from my end. I have only translated the edition of Kruschke’s text. If you’d like to review the version history of this ebook, go here or here. 1.4 Gimme feedback (be polite) I am not a statistician and I have no formal background in computer science. I finished my PhD in clinical psychology in 2018. During my graduate training I developed an unexpected interest in applied statistics and programming. I became an R user in 2015 and started learning about Bayesian statistics around 2013. There is still so much to learn, so my apologies for when my code appears dated or inelegant. There will also be occasions in which I’m not yet sure how to reproduce models or plots in the text. Which is all to say, suggestions on how to improve my code are welcome. My preference is to do so by opening a GitHub issue. You can find more guidelines on contributing to this ebook in the GitHub CONTRIBUTING section. If you’d like to learn more about me, you can find my website at https://solomonkurz.netlify.com. 1.5 Thank you! While in grad school, I benefited tremendously from free online content. This ebook and other projects like it (see here and here) are my attempts to pay it forward. As soon as you’ve gained a little proficiency, do consider doing to same. I addition to great texts like Kruschke’s, I’d like to point out a few other important resources that have allowed me to release an ebook like this: Jenny Bryan’s Happy Git and GitHub for the useR (Bryan et al., 2020) is the reference that finally got me working on GitHub. Again and again, I return to Grolemund and Wickham’s (2017) R for data science to learn about the tidyverse way of coding. Yihui Xie’s (2022) bookdown: Authoring books and technical documents with R Markdown is the primary source from which I learned how to make online books like this. While you’re at it, also check out Xie, Allaire, and Grolemund’s (2022) R Markdown: The definitive guide to learn more about how you can mix your R code with well-formatted prose. If you haven’t already, bookmark these resources and share them with your friends. With time and practice, these resources might help you write books and other online projects with R. Session info At the end of every chapter, I use the sessionInfo() function to help make my results more reproducible. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.28 digest_0.6.30 R6_2.5.1 jsonlite_1.8.3 ## [5] magrittr_2.0.3 evaluate_0.18 stringi_1.7.8 cachem_1.0.6 ## [9] rlang_1.0.6 cli_3.5.0 rstudioapi_0.13 jquerylib_0.1.4 ## [13] bslib_0.4.0 rmarkdown_2.16 tools_4.2.0 stringr_1.4.1 ## [17] xfun_0.35 fastmap_1.1.0 compiler_4.2.0 htmltools_0.5.3 ## [21] knitr_1.40 sass_0.4.2 References Bryan, J., the STAT 545 TAs, &amp; Hester, J. (2020). Happy Git and GitHub for the useR. https://happygitwithr.com Grolemund, G., &amp; Wickham, H. (2017). R for data science. O’Reilly. https://r4ds.had.co.nz Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Legler, J., &amp; Roback, P. (2019). Broadening your statistical horizons: Generalized linear models and multilevel models. https://bookdown.org/roback/bookdown-bysh/ Navarro, D. (2019). Learning statistics with R. https://learningstatisticswithr.com Peng, R. D. (2020). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/ Xie, Y. (2022). Bookdown: Authoring books and technical documents with R Markdown. Chapman and Hall/CRC. https://bookdown.org/yihui/bookdown/ Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2022). R Markdown: The definitive guide. Chapman and Hall/CRC. https://bookdown.org/yihui/rmarkdown/ "],["introduction-credibility-models-and-parameters.html", "2 Introduction: Credibility, Models, and Parameters 2.1 Bayesian inference is reallocation of credibility across possibilities 2.2 Possibilities are parameter values in descriptive models 2.3 The steps of Bayesian data analysis Session info", " 2 Introduction: Credibility, Models, and Parameters The goal of this chapter is to introduce the conceptual framework of Bayesian data analysis. Bayesian data analysis has two foundational ideas. The first idea is that Bayesian inference is reallocation of credibility across possibilities. The second foundational idea is that the possibilities, over which we allocate credibility, are parameter values in meaningful mathematical models. (Kruschke, 2015, p. 15) 2.1 Bayesian inference is reallocation of credibility across possibilities The first step toward making Figure 2.1 is putting together a data object. To help with that, we’ll open up the tidyverse. library(tidyverse) d &lt;- crossing(iteration = 1:3, stage = factor(c(&quot;Prior&quot;, &quot;Posterior&quot;), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) %&gt;% expand(nesting(iteration, stage), Possibilities = LETTERS[1:4]) %&gt;% mutate(Credibility = c(rep(.25, times = 4), 0, rep(1/3, times = 3), 0, rep(1/3, times = 3), rep(c(0, .5), each = 2), rep(c(0, .5), each = 2), rep(0, times = 3), 1)) When making data with many repetitions in the rows, it’s good to have the tidyr::expand() function up your sleeve. Go here to learn more. We can take a look at the top few rows of the data with the head() function. head(d) ## # A tibble: 6 × 4 ## iteration stage Possibilities Credibility ## &lt;int&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Prior A 0.25 ## 2 1 Prior B 0.25 ## 3 1 Prior C 0.25 ## 4 1 Prior D 0.25 ## 5 1 Posterior A 0 ## 6 1 Posterior B 0.333 Before we attempt Figure 2.1, we’ll need two supplemental data frames. The first one, text, will supply the coordinates for the annotation in the plot. The second, arrow, will supply the coordinates for the arrows. text &lt;- tibble(Possibilities = &quot;B&quot;, Credibility = .75, label = str_c(LETTERS[1:3], &quot; is\\nimpossible&quot;), iteration = 1:3, stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) arrow &lt;- tibble(Possibilities = LETTERS[1:3], iteration = 1:3) %&gt;% expand(nesting(Possibilities, iteration), Credibility = c(0.6, 0.01)) %&gt;% mutate(stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) Now we’re ready to code our version of Figure 2.1. d %&gt;% ggplot(aes(x = Possibilities, y = Credibility)) + geom_col(color = &quot;grey30&quot;, fill = &quot;grey30&quot;) + # annotation in the bottom row geom_text(data = text, aes(label = label)) + # arrows in the bottom row geom_line(data = arrow, arrow = arrow(length = unit(0.30, &quot;cm&quot;), ends = &quot;first&quot;, type = &quot;closed&quot;)) + facet_grid(stage ~ iteration) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank(), strip.text.x = element_blank()) We will take a similar approach to make our version of Figure 2.2. But this time, we’ll define our supplemental data sets directly in geom_text() and geom_line(). It’s good to have both methods up your sleeve. Also notice how we simply fed our primary data set directly into ggplot() without saving it, either. # primary data crossing(stage = factor(c(&quot;Prior&quot;, &quot;Posterior&quot;), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)), Possibilities = LETTERS[1:4]) %&gt;% mutate(Credibility = c(rep(0.25, times = 4), rep(0, times = 3), 1)) %&gt;% # plot! ggplot(aes(x = Possibilities, y = Credibility)) + geom_col(color = &quot;grey30&quot;, fill = &quot;grey30&quot;) + # annotation in the bottom panel geom_text(data = tibble( Possibilities = &quot;B&quot;, Credibility = .8, label = &quot;D is\\nresponsible&quot;, stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ), aes(label = label) ) + # the arrow geom_line(data = tibble( Possibilities = LETTERS[c(4, 4)], Credibility = c(.25, .99), stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ), arrow = arrow(length = unit(0.30, &quot;cm&quot;), ends = &quot;last&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + facet_wrap(~ stage, ncol = 1) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) 2.1.1 Data are noisy and inferences are probabilistic. Now on to Figure 2.3. I’m pretty sure the curves in the plot are Gaussian, which we’ll make with the dnorm() function. After a little trial and error, their standard deviations look to be 1.2. However, it’s tricky placing those curves in along with the probabilities, because the probabilities for the four discrete sizes (i.e., 1 through 4) are in a different metric than the Gaussian density curves. Since the probability metric for the four discrete sizes are the primary metric of the plot, we need to rescale the curves using a little algebra, which we do in the data code below. After that, the code for the plot is relatively simple. # data tibble(mu = 1:4, p = .25) %&gt;% expand(nesting(mu, p), x = seq(from = -2, to = 6, by = .1)) %&gt;% mutate(density = dnorm(x, mean = mu, sd = 1.2)) %&gt;% mutate(d_max = max(density)) %&gt;% mutate(rescale = p / d_max) %&gt;% mutate(density = density * rescale) %&gt;% # plot! ggplot(aes(x = x)) + geom_col(data = . %&gt;% distinct(mu, p), aes(x = mu, y = p), fill = &quot;grey67&quot;, width = 1/3) + geom_line(aes(y = density, group = mu)) + scale_x_continuous(breaks = 1:4) + scale_y_continuous(breaks = 0:5 / 5) + coord_cartesian(xlim = c(0, 5), ylim = c(0, 1)) + labs(title = &quot;Prior&quot;, x = &quot;Possibilities&quot;, y = &quot;Credibility&quot;) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) We can use the same basic method to make the bottom panel of Figure 2.3. Kruschke gave the relative probability values on page 21. Notice that they sum perfectly to 1. The only other notable change from the previous plot is our addition of a geom_point() section, the data in which we defined on the fly. tibble(mu = 1:4, p = c(.11, .56, .31, .02)) %&gt;% expand(nesting(mu, p), x = seq(from = -2, to = 6, by = .1)) %&gt;% mutate(density = dnorm(x, mean = mu, sd = 1.2)) %&gt;% mutate(d_max = max(density)) %&gt;% mutate(rescale = p / d_max) %&gt;% mutate(density = density * rescale) %&gt;% # plot! ggplot() + geom_col(data = . %&gt;% distinct(mu, p), aes(x = mu, y = p), fill = &quot;grey67&quot;, width = 1/3) + geom_line(aes(x = x, y = density, group = mu)) + geom_point(data = tibble(x = c(1.75, 2.25, 2.75), y = 0), aes(x = x, y = y), size = 3, color = &quot;grey33&quot;, alpha = 3/4) + scale_x_continuous(breaks = 1:4) + scale_y_continuous(breaks = 0:5 / 5) + coord_cartesian(xlim = c(0, 5), ylim = c(0, 1)) + labs(title = &quot;Posterior&quot;, x = &quot;Possibilities&quot;, y = &quot;Credibility&quot;) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) In summary, the essence of Bayesian inference is reallocation of credibility across possibilities. The distribution of credibility initially reflects prior knowledge about the possibilities, which can be quite vague. Then new data are observed, and the credibility is re-allocated. Possibilities that are consistent with the data garner more credibility, while possibilities that are not consistent with the data lose credibility. Bayesian analysis is the mathematics of re-allocating credibility in a logically coherent and precise way. (p. 22) 2.2 Possibilities are parameter values in descriptive models “A key step in Bayesian analysis is defining the set of possibilities over which credibility is allocated. This is not a trivial step, because there might always be possibilities beyond the ones we include in the initial set” (p. 22, emphasis added). In the last section, we used the dnorm() function to make curves following the normal distribution (a.k.a. the Gaussian distribution). Here we’ll do that again, but also use the rnorm() function to simulate actual data from that same normal distribution. Behold Figure 2.4.a. # set the seed to make the simulation reproducible set.seed(2) # simulate the data with `rnorm()` d &lt;- tibble(x = rnorm(2000, mean = 10, sd = 5)) # plot! ggplot(data = d, aes(x = x)) + geom_histogram(aes(y = ..density..), binwidth = 1, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/10) + geom_line(data = tibble(x = seq(from = -6, to = 26, by = .01)), aes(x = x, y = dnorm(x, mean = 10, sd = 5)), color = &quot;grey33&quot;) + coord_cartesian(xlim = c(-5, 25)) + labs(subtitle = &quot;The candidate normal distribution\\nhas a mean of 10 and SD of 5.&quot;, x = &quot;Data Values&quot;, y = &quot;Data Probability&quot;) + theme(panel.grid = element_blank()) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. Did you notice how we made the data for the density curve within geom_line()? That’s one way to do it, and in our next plot we’ll take a more elegant approach with the stat_function() function. Here’s our Figure 2.4.b. ggplot(data = d, aes(x = x)) + geom_histogram(aes(y = ..density..), binwidth = 1, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + stat_function(fun = dnorm, n = 101, args = list(mean = 8, sd = 6), color = &quot;grey33&quot;, linetype = 2) + coord_cartesian(xlim = c(-5, 25)) + labs(subtitle = &quot;The candidate normal distribution\\nhas a mean of 8 and SD of 6.&quot;, x = &quot;Data Values&quot;, y = &quot;Data Probability&quot;) + theme(panel.grid = element_blank()) 2.3 The steps of Bayesian data analysis In general, Bayesian analysis of data follows these steps: Identify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors? Define a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis. Specify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists. Use Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step). Check that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a “posterior predictive check”). If not, then consider a different descriptive model. Perhaps the best way to explain these steps is with a realistic example of Bayesian data analysis. The discussion that follows is abbreviated for purposes of this introductory chapter, with many technical details suppressed. (p. 25) I will show you a few more details than Kruschke did in the text. But just has he did, we’ll cover this workflow in much more detail in the chapters to come. In order to recreate Figure 2.5, we need to generate the data and fit a model to those data. In his HtWtDataDenerator.R script, Kruschke provided the code for a function that will generate height/weight data of the kind in his text. Here is the code in full: HtWtDataGenerator &lt;- function(nSubj, rndsd = NULL, maleProb = 0.50) { # Random height, weight generator for males and females. Uses parameters from # Brainard, J. &amp; Burmaster, D. E. (1992). Bivariate distributions for height and # weight of men and women in the United States. Risk Analysis, 12(2), 267-275. # Kruschke, J. K. (2011). Doing Bayesian data analysis: # A Tutorial with R and BUGS. Academic Press / Elsevier. # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition: # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier. # require(MASS) # Specify parameters of multivariate normal (MVN) distributions. # Men: HtMmu &lt;- 69.18 HtMsd &lt;- 2.87 lnWtMmu &lt;- 5.14 lnWtMsd &lt;- 0.17 Mrho &lt;- 0.42 Mmean &lt;- c(HtMmu, lnWtMmu) Msigma &lt;- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd, Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2) # Women cluster 1: HtFmu1 &lt;- 63.11 HtFsd1 &lt;- 2.76 lnWtFmu1 &lt;- 5.06 lnWtFsd1 &lt;- 0.24 Frho1 &lt;- 0.41 prop1 &lt;- 0.46 Fmean1 &lt;- c(HtFmu1, lnWtFmu1) Fsigma1 &lt;- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1, Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2) # Women cluster 2: HtFmu2 &lt;- 64.36 HtFsd2 &lt;- 2.49 lnWtFmu2 &lt;- 4.86 lnWtFsd2 &lt;- 0.14 Frho2 &lt;- 0.44 prop2 &lt;- 1 - prop1 Fmean2 &lt;- c(HtFmu2, lnWtFmu2) Fsigma2 &lt;- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2, Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2) # Randomly generate data values from those MVN distributions. if (!is.null(rndsd)) {set.seed(rndsd)} datamatrix &lt;- matrix(0, nrow = nSubj, ncol = 3) colnames(datamatrix) &lt;- c(&quot;male&quot;, &quot;height&quot;, &quot;weight&quot;) maleval &lt;- 1; femaleval &lt;- 0 # arbitrary coding values for (i in 1:nSubj) { # Flip coin to decide sex sex &lt;- sample(c(maleval, femaleval), size = 1, replace = TRUE, prob = c(maleProb, 1 - maleProb)) if (sex == maleval) {datum = MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)} if (sex == femaleval) { Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2)) if (Fclust == 1) {datum = MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)} if (Fclust == 2) {datum = MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)} } datamatrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1)) } return(datamatrix) } # end function Now we have the HtWtDataGenerator() function, all we need to do is determine how many values to generate and how probable we want the values to be based on those from men. These are controlled by the nSubj and maleProb parameters. # set your seed to make the data generation reproducible set.seed(2) d &lt;- HtWtDataGenerator(nSubj = 57, maleProb = .5) %&gt;% as_tibble() d %&gt;% head() ## # A tibble: 6 × 3 ## male height weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 62.6 109. ## 2 0 63.3 154. ## 3 1 71.8 155 ## 4 0 67.9 146. ## 5 0 64.4 135. ## 6 0 66.8 119 We’re about ready for the model, which we will fit it with the Hamiltonian Monte Carlo (HMC) method via the brms package. We’ll introduce brms more fully in Chapter 8 and you can go here for an introduction, too. In the meantime, let’s just load the package and see what happens. library(brms) The traditional use of diffuse and noninformative priors is discouraged with HMC, as is the uniform distribution for sigma. Instead, we’ll use weakly-regularizing priors for the intercept and slope and a half Cauchy with a fairly large scale parameter for \\(\\sigma\\). fit2.1 &lt;- brm(data = d, family = gaussian, weight ~ 1 + height, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = b), prior(cauchy(0, 10), class = sigma)), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 2, file = &quot;fits/fit02.01&quot;) If you wanted a quick model summary, you could execute print(fit1). Again, we’ll walk through that and other diagnostics in greater detail starting in Chapter 8. For now, here’s how we might make Figure 2.5.a. # extract the posterior draws draws &lt;- as_draws_df(fit2.1) # this will subset the output n_lines &lt;- 150 # plot! draws %&gt;% slice(1:n_lines) %&gt;% ggplot() + geom_abline(aes(intercept = b_Intercept, slope = b_height, group = .draw), color = &quot;grey50&quot;, size = 1/4, alpha = .3) + geom_point(data = d, aes(x = height, y = weight), shape = 1) + # the `eval(substitute(paste()))` trick came from: https://www.r-bloggers.com/value-of-an-r-object-in-an-expression/ labs(subtitle = eval(substitute(paste(&quot;Data with&quot;, n_lines, &quot;credible regression lines&quot;))), x = &quot;Height in inches&quot;, y = &quot;Weight in pounds&quot;) + coord_cartesian(xlim = c(55, 80), ylim = c(50, 250)) + theme(panel.grid = element_blank()) For Figure 2.5.b., we’ll mark off the mode and 95% highest density interval (HDI) with help from the handy stat_histinterval() function, provided by the tidybayes package (Kay, 2020). library(tidybayes) draws %&gt;% ggplot(aes(x = b_height, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .2, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 8)) + labs(title = &quot;The posterior distribution&quot;, subtitle = &quot;The mode and 95% HPD intervals are\\nthe dot and horizontal line at the bottom.&quot;, x = expression(beta[1]~(slope))) + theme(panel.grid = element_blank()) To make Figure 2.6, we use the brms::predict() function, which we’ll cover more fully in the pages to come. nd &lt;- tibble(height = seq(from = 53, to = 81, length.out = 20)) predict(fit2.1, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = height)) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), color = &quot;grey67&quot;, shape = 20) + geom_point(data = d, aes(y = weight), alpha = 2/3) + labs(subtitle = &quot;Data with the percentile-based 95% intervals and\\nthe means of the posterior predictions&quot;, x = &quot;Height in inches&quot;, y = &quot;Weight in inches&quot;) + theme(panel.grid = element_blank()) The posterior predictions might be easier to depict with a ribbon and line, instead. nd &lt;- tibble(height = seq(from = 53, to = 81, length.out = 30)) predict(fit2.1, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = height)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey75&quot;) + geom_line(aes(y = Estimate), color = &quot;grey92&quot;) + geom_point(data = d, aes(y = weight), alpha = 2/3) + labs(subtitle = &quot;Data with the percentile-based 95% intervals and\\nthe means of the posterior predictions&quot;, x = &quot;Height in inches&quot;, y = &quot;Weight in inches&quot;) + theme(panel.grid = element_blank()) “We have seen the five steps of Bayesian analysis in a fairly realistic example. This book explains how to do this sort of analysis for many different applications and types of descriptive models” (p. 30). Are you intrigued and excited? You should be. Welcome to Bayes, friends! Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_3.0.2 brms_2.18.0 Rcpp_1.0.9 forcats_0.5.1 ## [5] stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 ## [9] tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 ## [4] igraph_1.3.4 svUnit_1.0.6 splines_4.2.0 ## [7] crosstalk_1.2.0 TH.data_1.1-1 rstantools_2.2.0 ## [10] inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 ## [16] googlesheets4_1.0.1 tzdb_0.3.0 modelr_0.1.8 ## [19] RcppParallel_5.1.5 matrixStats_0.62.0 xts_0.12.1 ## [22] sandwich_3.0-2 prettyunits_1.1.1 colorspace_2.0-3 ## [25] rvest_1.0.2 ggdist_3.2.0 haven_2.5.1 ## [28] xfun_0.35 callr_3.7.3 crayon_1.5.2 ## [31] jsonlite_1.8.3 lme4_1.1-31 survival_3.4-0 ## [34] zoo_1.8-10 glue_1.6.2 gtable_0.3.1 ## [37] gargle_1.2.0 emmeans_1.8.0 distributional_0.3.1 ## [40] pkgbuild_1.3.1 rstan_2.21.7 abind_1.4-5 ## [43] scales_1.2.1 mvtnorm_1.1-3 DBI_1.1.3 ## [46] miniUI_0.1.1.1 xtable_1.8-4 HDInterval_0.2.2 ## [49] stats4_4.2.0 StanHeaders_2.21.0-7 DT_0.24 ## [52] htmlwidgets_1.5.4 httr_1.4.4 threejs_0.3.3 ## [55] arrayhelpers_1.1-0 posterior_1.3.1 ellipsis_0.3.2 ## [58] pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 ## [61] sass_0.4.2 dbplyr_2.2.1 utf8_1.2.2 ## [64] tidyselect_1.1.2 labeling_0.4.2 rlang_1.0.6 ## [67] reshape2_1.4.4 later_1.3.0 munsell_0.5.0 ## [70] cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [73] cli_3.5.0 generics_0.1.3 broom_1.0.1 ## [76] ggridges_0.5.3 evaluate_0.18 fastmap_1.1.0 ## [79] processx_3.8.0 knitr_1.40 fs_1.5.2 ## [82] nlme_3.1-159 mime_0.12 projpred_2.2.1 ## [85] xml2_1.3.3 compiler_4.2.0 bayesplot_1.9.0 ## [88] shinythemes_1.2.0 rstudioapi_0.13 gamm4_0.2-6 ## [91] reprex_2.0.2 bslib_0.4.0 stringi_1.7.8 ## [94] highr_0.9 ps_1.7.2 Brobdingnag_1.2-8 ## [97] lattice_0.20-45 Matrix_1.4-1 nloptr_2.0.3 ## [100] markdown_1.1 shinyjs_2.1.0 tensorA_0.36.2 ## [103] vctrs_0.5.1 pillar_1.8.1 lifecycle_1.0.3 ## [106] jquerylib_0.1.4 bridgesampling_1.1-2 estimability_1.4.1 ## [109] httpuv_1.6.5 R6_2.5.1 bookdown_0.28 ## [112] promises_1.2.0.1 gridExtra_2.3 codetools_0.2-18 ## [115] boot_1.3-28 colourpicker_1.1.1 MASS_7.3-58.1 ## [118] gtools_3.9.3 assertthat_0.2.1 withr_2.5.0 ## [121] shinystan_2.6.0 multcomp_1.4-20 mgcv_1.8-40 ## [124] parallel_4.2.0 hms_1.1.1 grid_4.2.0 ## [127] minqa_1.2.5 coda_0.19-4 rmarkdown_2.16 ## [130] googledrive_2.0.0 shiny_1.7.2 lubridate_1.8.0 ## [133] base64enc_0.1-3 dygraphs_1.1.1.6 References Kay, M. (2020). tidybayes: Tidy data and ’geoms’ for Bayesian models. https://mjskay.github.io/tidybayes/ Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ "],["the-r-programming-language.html", "3 The R Programming Language 3.1 Get the software 3.2 A simple example of R in action 3.3 Basic commands and operators in R 3.4 Variable types 3.5 Loading and saving data 3.6 Some utility functions 3.7 Programming in R 3.8 Graphical plots: Opening and saving 3.9 Conclusion brms-related needs Session info Footnote", " 3 The R Programming Language The material in this chapter is rather dull reading because it basically amounts to a list (although a carefully scaffolded list) of basic commands in R along with illustrative examples. After reading the first few pages and nodding off, you may be tempted to skip ahead, and I wouldn’t blame you. But much of the material in this chapter is crucial, and all of it will eventually be useful, so you should at least skim it all so you know where to return when the topics arise later. (Kruschke, 2015, p. 35) Most, but not all, of this part of my project will mirror what’s in the text. However, I do add tidyverse-oriented content, such as a small walk through of plotting with the ggplot2 package (Wickham, 2016; Wickham et al., 2021). 3.1 Get the software The first step to following along with this ebook is to install R on your computer. Go to https://cran.r-project.org/ and follow the instructions, from there. If you get confused, there are any number of brief video tutorials available to lead you through the steps. Just use a search term like “install R.” If you’re new to R or just curious about its origins, check out Chapter 2 of Peng (2020), History and overview of R. One of the great features about R, which might seem odd or confusing if you’re more used to working with propriety software like SPSS, is a lot of the functionality comes from the add-on packages users develop to make R easier to use. Peng briefly discusses these features in Section 2.7, Design of the R system. I make use of a variety of add-on packages in this project. You can install them all by executing this code block. packages &lt;- c(&quot;bayesplot&quot;, &quot;brms&quot;, &quot;coda&quot;, &quot;cowplot&quot;, &quot;cubelyr&quot;, &quot;devtools&quot;, &quot;fishualize&quot;, &quot;GGally&quot;, &quot;ggdist&quot;, &quot;ggExtra&quot;, &quot;ggforce&quot;, &quot;ggmcmc&quot;, &quot;ggridges&quot;, &quot;ggthemes&quot;, &quot;janitor&quot;, &quot;lisa&quot;, &quot;loo&quot;, &quot;palettetown&quot;, &quot;patchwork&quot;, &quot;psych&quot;, &quot;remotes&quot;, &quot;rstan&quot;, &quot;santoku&quot;, &quot;scico&quot;, &quot;tidybayes&quot;, &quot;tidyverse&quot;) install.packages(packages, dependencies = T) remotes::install_github(&quot;clauswilke/colorblindr&quot;) devtools::install_github(&quot;dill/beyonce&quot;) devtools::install_github(&quot;ropenscilabs/ochRe&quot;) 3.1.1 A look at RStudio. The R programming language comes with its own basic user interface that is adequate for modest applications. But larger applications become unwieldy in the basic R user interface, and therefore it helps to install a more sophisticated R-friendly editor. There are a number of useful editors available, many of which are free, and they are constantly evolving. At the time of this writing, I recommend RStudio, which can be obtained from [https://rstudio.com/] (p. 35). I completely agree. R programming is easier with RStudio. However, I should point out that there are other user interfaces available. You can find several alternatives listed here or here. 3.2 A simple example of R in action Basic arithmetic is straightforward in R. 2 + 3 ## [1] 5 Much like Kruschke did in his text, I denote my programming prompts in typewriter font atop a gray background, like this: 2 + 3. Anyway, algebra is simple in R, too. x &lt;- 2 x + x ## [1] 4 I don’t tend to save lists of commands in text files. Rather, I almost exclusively work within R Notebook files, which I discuss more fully in Section 3.7. As far as sourcing, I never use the source() approach Kruschke discussed in page 36. I’m not opposed to it. It’s just not my style. Anyway, behold Figure 3.1. library(tidyverse) d &lt;- tibble(x = seq(from = -2, to = 2, by = .1)) %&gt;% mutate(y = x^2) ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) + theme(panel.grid = element_blank()) If you’re new to the tidyverse and/or making figures with ggplot2, it’s worthwhile to walk that code out. With the first line, library(tidyverse), we opened up the core packages within the tidyverse, which are: ggplot2 (Wickham, 2016; Wickham et al., 2021), dplyr (Wickham et al., 2020), tidyr (Wickham &amp; Henry, 2020), readr (Wickham et al., 2018), purrr (Henry &amp; Wickham, 2020), tibble (Müller &amp; Wickham, 2020), stringr (Wickham, 2019), and forcats (Wickham, 2020b). With the few lines, d &lt;- tibble(x = seq(from = -2, to = 2, by = .1)) %&gt;% mutate(y = x^2) we made our tibble. In R, data frames are one of the primary types of data objects (see Section 3.4.4, below). We’ll make extensive use of data frames in this project. Tibbles are a particular type of data frame, which you might learn more about in the tibbles section of Grolemund and Wickham’s (2017) R4DS. With those first two lines, we determined what the name of our tibble would be, d, and made the first column, x. Note the %&gt;% operator at the end of the second line. In prose, we call that the pipe. As explained in Section 5.6.1 of R4DS, “a good way to pronounce %&gt;% when reading code is ‘then.’” So in words, the those first two lines indicate “Make an object, d, which is a tibble with a variable, x, defined by the seq() function, then…” In the portion after then (i.e., the %&gt;%), we changed d. The dplyr::mutate() function let us add another variable, y, which is a function of our first variable, x. With the next 4 lines of code, we made our plot. When plotting with ggplot2, the first line is always with the ggplot() function. This is where you typically tell ggplot2 what data object you’re using–which must be a data frame or tibble–and what variables you want on your axes. The interesting thing about ggplot2 is that the code is modular. So if we only coded the ggplot() portion, we’d get: ggplot(data = d, aes(x = x, y = y)) Although ggplot2 knows which variables to put on which axes, it has no idea how we’d like to express the data. The result is an empty coordinate system. The next line of code is the main event. With geom_line() we told ggplot2 to connect the data points with a line. With the color argument, we made that line \"skyblue\". [Here’s a great list of the named colors available in ggplot2.] Also, notice the + operator at the end of the ggplot() function. With ggplot2, you add functions by placing the + operator on the right of the end of one function, which will then append the next function. ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) Personally, I’m not a fan of gridlines. They occasionally have their place and I do use them from time to time. But on the whole, I prefer to omit them from my plots. The final theme() function allowed me to do so. ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) + theme(panel.grid = element_blank()) Chapter 3 of R4DS is a great introduction to plotting with ggplot2. If you want to dive deeper, see the references at the bottom of this page. And of course, you might read up in Wickham’s (2016) ggplot2: Elegant graphics for data analysis. 3.2.1 Get the programs used with this book. This subtitle has a double meaning, here. Yes, you should probably get Kruschke’s scripts from the book’s website, https://sites.google.com/site/doingbayesiandataanalysis/. You may have noticed this already, but unlike in Kruschke’s text, I will usually show all my code. Indeed, the purpose of my project is to make coding these kinds of models and visualizations easier. But if you’re ever curious, you can always find my script files in their naked form at https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse. For example, the raw file for this very chapter is at https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/blob/master/03.Rmd. Later in this subsection, Kruschke mentioned working directories. If you don’t know what your current working directory is, just execute getwd(). I’ll have more to say on this topic later on when I make my pitch for RStudio projects in Section 3.7.2. 3.3 Basic commands and operators in R In addition to the resource link Kruschke provided in the text, Grolemund and Wickham’s R4DS is an excellent general introduction to the kinds of R functions you’ll want to succeed with your data analysis. Other than that, I’ve learned the most when I had a specific data problem to solve and then sought out the specific code/techniques required to solve it. If already have your own data or can get your hands on some sexy data, learn these techniques by playing around with them. This isn’t the time to worry about rigor, preregistration, or all of that. This is time to play. 3.3.1 Getting help in R. As with plot() you can learn more about the ggplot() function with ?. ?ggplot help.start() can be nice, too. help.start() ??geom_line() can help us learn more about the geom_line() function. ??geom_line() Quite frankly, a bit part of becoming a successful R user is learning how to get help, online. In addition to the methods, above, type in the name of your function of interest in your favorite web browser. For example, If I wanted to learn more about the geom_line() function, I’d literally do a web search for “geom_line()”. In my case, the first search results when doing so was https://ggplot2.tidyverse.org/reference/geom_path.html, which is a nicely-formatted official reference page put out by the ggplot2 team. 3.3.2 Arithmetic and logical operators. With arithmetic, the order of operations is: power first, then multiplication, then addition. 1 + 2 * 3^2 ## [1] 19 With parentheses, you can force addition before multiplication. (1 + 2) * 3^2 ## [1] 27 Operations inside parentheses get done before power operations. (1 + 2 * 3)^2 ## [1] 49 One can nest parentheses. ((1 + 2) * 3)^2 ## [1] 81 ?Syntax We can use R to perform a variety of logical tests, such as negation. !TRUE ## [1] FALSE We can do conjunction. TRUE &amp; FALSE ## [1] FALSE And we can do disjunction. TRUE | FALSE ## [1] TRUE Conjunction has precedence over disjunction. TRUE | TRUE &amp; FALSE ## [1] TRUE However, with parentheses we can force disjunction first. (TRUE | TRUE) &amp; FALSE ## [1] FALSE 3.3.3 Assignment, relational operators, and tests of equality. In contrast to Kruschke’s preference, I will use the arrow operator, &lt;-, to assign values to named variables1. x = 1 x &lt;- 1 Yep, this ain’t normal math. (x = 1) ## [1] 1 (x = x + 1) ## [1] 2 Here we use == to test for equality. (x = 2) ## [1] 2 x == 2 ## [1] TRUE Using !=, we can check whether the value of x is NOT equal to 3. x != 3 ## [1] TRUE We can use &lt; to check whether the value of x is less than 3. x &lt; 3 ## [1] TRUE Similarly, we can use &gt; to check whether the value of x is greater than 3. x &gt; 3 ## [1] FALSE This normal use of the &lt;- operator x &lt;- 3 is not the same as x &lt; - 3 ## [1] FALSE The limited precision of a computer’s memory can lead to odd results. x &lt;- 0.5 - 0.3 y &lt;- 0.3 - 0.1 Although mathematically TRUE, this is FALSE for limited precision. x == y ## [1] FALSE However, they are equal up to the precision of a computer. all.equal(x, y) ## [1] TRUE 3.4 Variable types If you’d like to learn more about the differences among vectors, matrices, lists, data frames and so on, you might check out Roger Peng’s (2020) R Programming for data science, Chapter 4. 3.4.1 Vector. “A vector is simply an ordered list of elements of the same type” (p. 42). 3.4.1.1 The combine function. The combine function is c(), which makes vectors. Here we’ll first make an unnamed vector. Then we’ll sve that vector as x. c(2.718, 3.14, 1.414) ## [1] 2.718 3.140 1.414 x &lt;- c(2.718, 3.14, 1.414) You’ll note the equivalence. x == c(2.718, 3.14, 1.414) ## [1] TRUE TRUE TRUE This leads to the next subsection. 3.4.1.2 Component-by-component vector operations. We can multiply two vectors, component by component. c(1, 2, 3) * c(7, 6, 5) ## [1] 7 12 15 If you have a sole number, a scaler, you can multiply an entire vector by it like: 2 * c(1, 2, 3) ## [1] 2 4 6 which is a more compact way to perform this. c(2, 2, 2) * c(1, 2, 3) ## [1] 2 4 6 The same sensibilities hold for other operations, such as addition. 2 + c(1, 2, 3) ## [1] 3 4 5 3.4.1.3 The colon operator and sequence function. The colon operator, :, is a handy way to make integer sequences. Here we use it to serially list the inters from 4 to 7. 4:7 ## [1] 4 5 6 7 The colon operator has precedence over addition. 2 + 3:6 ## [1] 5 6 7 8 Parentheses override default precedence. (2 + 3):6 ## [1] 5 6 The power operator has precedence over the colon operator. 1:3^2 ## [1] 1 2 3 4 5 6 7 8 9 And parentheses override default precedence. (1:3)^2 ## [1] 1 4 9 The seq() function is quite handy. If you don’t specify the length of the output, it will figure that out the logical consequence of the other arguments. seq(from = 0, to = 3, by = 0.5) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 This sequence won’t exceed to = 3. seq(from = 0, to = 3, by = 0.5001) ## [1] 0.0000 0.5001 1.0002 1.5003 2.0004 2.5005 In each of the following examples, we’ll omit one of the core seq() arguments: from, to, by, and length.out. Here we do not define the end point. seq(from = 0, by = 0.5, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 This time we fail to define the increment. seq(from = 0, to = 3, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 And this time we omit a starting point. seq(to = 3, by = 0.5, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 In this ebook, I will always explicitly name my arguments within seq(). 3.4.1.4 The replicate function. We’ll define our pre-replication vector with the &lt;- operator. abc &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) With the times argument, we repeat the vector as a unit with the rep() function. rep(abc, times = 2) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; But if we mix the times argument with c(), we can repeat individual components of abc differently. rep(abc, times = c(4, 2, 1)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; With the each argument, we repeat the individual components of abc one at a time. rep(abc, each = 2) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; And you can even combine each and length, repeating each element until the length requirement has been fulfilled. rep(abc, each = 2, length = 10) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; You can also combine each and times. rep(abc, each = 2, times = 3) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; I tend to do things like the above as two separate steps. One way to do so is by nesting one rep() function within another. rep(rep(abc, each = 2), times = 3) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; As Kruschke points out, this can look confusing. rep(abc, each = 2, times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; But breaking the results up into two steps might be easier to understand, rep(rep(abc, each = 2), times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; And especially earlier in my R career, it helped quite a bit to break operation sequences like this up by saving and assessing the intermediary steps. step_1 &lt;- rep(abc, each = 2) step_1 ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; rep(step_1, times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; 3.4.1.5 Getting at elements of a vector. Behold our exemplar vector, x. x &lt;- c(2.718, 3.14, 1.414, 47405) The straightforward way to extract the second and fourth elements is with a combination of brackets, [], and c(). x[c(2, 4)] ## [1] 3.14 47405.00 Or you might use reverse logic and omit the first and third elements. x[c(-1, -3 )] ## [1] 3.14 47405.00 It’s handy to know that T is a stand in for TRUE and F is a stand in for FALSE. You’ll probably notice I use the abbreviations most of the time. x[c(F, T, F, T)] ## [1] 3.14 47405.00 The names() function makes it easy to name the components of a vector. names(x) &lt;- c(&quot;e&quot;, &quot;pi&quot;, &quot;sqrt2&quot;, &quot;zipcode&quot;) x ## e pi sqrt2 zipcode ## 2.718 3.140 1.414 47405.000 Now we can call the components with their names. x[c(&quot;pi&quot;, &quot;zipcode&quot;)] ## pi zipcode ## 3.14 47405.00 Once we start working with summaries from our Bayesian models, we’ll use this trick a lot. Here’s Kruschke’s review: # define a vector x &lt;- c(2.718, 3.14, 1.414, 47405) # name the components names(x) &lt;- c(&quot;e&quot;, &quot;pi&quot;, &quot;sqrt2&quot;, &quot;zipcode&quot;) # you can indicate which elements you&#39;d like to include x[c(2, 4)] ## pi zipcode ## 3.14 47405.00 # you can decide which to exclude x[c(-1, -3)] ## pi zipcode ## 3.14 47405.00 # or you can use logical tests x[c(F, T, F, T)] ## pi zipcode ## 3.14 47405.00 # and you can use the names themselves x[c(&quot;pi&quot;, &quot;zipcode&quot;)] ## pi zipcode ## 3.14 47405.00 3.4.2 Factor. “Factors are a type of vector in R for which the elements are categorical values that could also be ordered. The values are stored internally as integers with labeled levels” (p. 46, emphasis in the original). Here are our five-person socioeconomic status data. x &lt;- c(&quot;high&quot;, &quot;medium&quot;, &quot;low&quot;, &quot;high&quot;, &quot;medium&quot;) x ## [1] &quot;high&quot; &quot;medium&quot; &quot;low&quot; &quot;high&quot; &quot;medium&quot; The factor() function turns them into a factor, which will return the levels when called. xf &lt;- factor(x) xf ## [1] high medium low high medium ## Levels: high low medium Here are the factor levels as numerals. as.numeric(xf) ## [1] 1 3 2 1 3 With the levels and ordered arguments, we can order the factor elements. xfo &lt;- factor(x, levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), ordered = T) xfo ## [1] high medium low high medium ## Levels: low &lt; medium &lt; high Now “high” is a larger integer. as.numeric(xfo) ## [1] 3 2 1 3 2 We’ve already specified xf. xf ## [1] high medium low high medium ## Levels: high low medium And we know how it’s been coded numerically. as.numeric(xf) ## [1] 1 3 2 1 3 We can have levels and labels. xfol &lt;- factor(x, levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), ordered = T, labels = c(&quot;Bottom SES&quot;, &quot;Middle SES&quot;, &quot;Top SES&quot;)) xfol ## [1] Top SES Middle SES Bottom SES Top SES Middle SES ## Levels: Bottom SES &lt; Middle SES &lt; Top SES Factors can come in very handy when modeling with certain kinds of categorical variables, as in Chapter 22, or when arranging elements within a plot. 3.4.3 Matrix and array. Kruschke uses these more often than I do. I’m more of a vector and data frame kinda guy. Even so, here’s an example of a matrix. matrix(1:6, ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 We can get the same thing using nrow. matrix(1:6, nrow = 2) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 Note how the numbers got ordered by rows within each column? We can specify them to be ordered across columns, first. matrix(1:6, nrow = 2, byrow = T) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 We can name the dimensions. I’m not completely consistent, but I generally follow The tidyverse style guide (Wickham, 2020d) for naming my R objects and their elements. From Section 2.1, we read Variable and function names should use only lowercase letters, numbers, and _. Use underscores (_) (so called snake case) to separate words within a name. By those sensibilities, we’ll name our rows and columns like this. matrix(1:6, nrow = 2, dimnames = list(TheRowDimName = c(&quot;row_1_name&quot;, &quot;row_2_name&quot;), TheColDimName = c(&quot;col_1_name&quot;, &quot;col_2_name&quot;, &quot;col_3_name&quot;))) ## TheColDimName ## TheRowDimName col_1_name col_2_name col_3_name ## row_1_name 1 3 5 ## row_2_name 2 4 6 You’ve also probably noticed that I “always put a space after a comma, never before, just like in regular English,” as well as “put a space before and after = when naming arguments in function calls.” IMO, this makes code easier to read. You do you. We’ll name our matrix x. x &lt;- matrix(1:6, nrow = 2, dimnames = list(TheRowDimName = c(&quot;row_1_name&quot;, &quot;row_2_name&quot;), TheColDimName = c(&quot;col_1_name&quot;, &quot;col_2_name&quot;, &quot;col_3_name&quot;))) Since there are 2 dimensions, we’ll subset with two dimensions. Numerical indices work. x[2, 3] ## [1] 6 Row and column names work, too. Just make sure to use quotation marks, \"\", for those. x[&quot;row_2_name&quot;, &quot;col_3_name&quot;] ## [1] 6 Here we specify the range of columns to include. x[2, 1:3] ## col_1_name col_2_name col_3_name ## 2 4 6 Leaving that argument blank returns them all. x[2, ] ## col_1_name col_2_name col_3_name ## 2 4 6 And leaving the row index blank returns all row values within the specified column(s). x[, 3] ## row_1_name row_2_name ## 5 6 Mind your commas! This produces the second row, returned as a vector. x[2, ] ## col_1_name col_2_name col_3_name ## 2 4 6 This returns both rows of the 2nd column. x[, 2] ## row_1_name row_2_name ## 3 4 Leaving out the comma will return the numbered element. x[2] ## [1] 2 It’ll be important in your brms career to have a sense of 3-dimensional arrays. Several brms convenience functions often return them (e.g., ranef() in multilevel models). a &lt;- array(1:24, dim = c(3, 4, 2), # 3 rows, 4 columns, 2 layers dimnames = list(RowDimName = c(&quot;r1&quot;, &quot;r2&quot;, &quot;r3&quot;), ColDimName = c(&quot;c1&quot;, &quot;c2&quot;, &quot;c3&quot;, &quot;c4&quot;), LayDimName = c(&quot;l1&quot;, &quot;l2&quot;))) a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 Since these have 3 dimensions, you have to use 3-dimensional indexing. As with 2-dimensional objects, leaving the indices for a dimension blank will return all elements within that dimension. For example, this code returns all columns of r3 and l2, as a vector. a[&quot;r3&quot;, , &quot;l2&quot;] ## c1 c2 c3 c4 ## 15 18 21 24 And this code returns all layers of r3 and c4, as a vector. a[&quot;r3&quot;, &quot;c4&quot;, ] ## l1 l2 ## 12 24 This whole topic of subsetting–whether from matrices and arrays, or from data frames and tibbles–can be confusing. For more practice and clarification, you might check out Peng’s Chapter 9, Subsetting R objects. 3.4.4 List and data frame. “The list structure is a generic vector in which components can be of different types, and named” (p. 51). Here’s my_list. my_list &lt;- list(&quot;a&quot; = 1:3, &quot;b&quot; = matrix(1:6, nrow = 2), &quot;c&quot; = &quot;Hello, world.&quot;) my_list ## $a ## [1] 1 2 3 ## ## $b ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## $c ## [1] &quot;Hello, world.&quot; To return the contents of the a portion of my_list, just execute this. my_list$a ## [1] 1 2 3 We can index further within a. my_list$a[2] ## [1] 2 To return the contents of the first item in our list with the double bracket, [[]], do: my_list[[1]] ## [1] 1 2 3 You can index further to return only the second element of the first list item. my_list[[1]][2] ## [1] 2 But double brackets, [][], are no good, here. my_list[1][2] ## $&lt;NA&gt; ## NULL To learn more, Jenny Bryan has a great talk discussing the role of lists within data wrangling. There’s also this classic pic from Hadley Wickham: Figure 3.1: Indexing lists in #rstats. Inspired by the Residence Inn But here’s a data frame. d &lt;- data.frame(integers = 1:3, number_names = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) d ## integers number_names ## 1 1 one ## 2 2 two ## 3 3 three With data frames, we can continue indexing with the $ operator. d$number_names ## [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; We can also use the double bracket. d[[2]] ## [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; Notice how the single bracket with no comma indexes columns rather than rows. d[2] ## number_names ## 1 one ## 2 two ## 3 three But adding the comma returns the factor-level information when indexing columns. d[, 2] ## [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; It works a touch differently when indexing by row. d[2, ] ## integers number_names ## 2 2 two Let’s try with a tibble, instead. t &lt;- tibble(integers = 1:3, number_names = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) t ## # A tibble: 3 × 2 ## integers number_names ## &lt;int&gt; &lt;chr&gt; ## 1 1 one ## 2 2 two ## 3 3 three One difference is that tibbles default to assigning text columns as character strings rather than factors. Another difference occurs when printing large data frames versus large tibbles. Tibbles yield more compact glimpses. For more, check out R4DS Chapter 10. It’s also worthwhile pointing out that within the tidyverse, you can pull out a specific column with the select() function. Here we select number_names. t %&gt;% select(number_names) ## # A tibble: 3 × 1 ## number_names ## &lt;chr&gt; ## 1 one ## 2 two ## 3 three Go here learn more about select(). 3.5 Loading and saving data 3.5.1 The read.csv read_csv() and read.table read_table() functions. Although read.csv() is the default CSV reader in R, the read_csv() function from the readr package (i.e., one of the core tidyverse packages) is a new alternative. In comparison to base R’s read.csv(), readr::read_csv() is faster and returns tibbles (as opposed to data frames with read.csv()). The same general points hold for base R’s read.table() versus readr::read_table(). Using Kruschke’s HGN.csv example, we’d load the CSV with read_csv() like this. hgn &lt;- read_csv(&quot;data.R/HGN.csv&quot;) Note again that read_csv() defaults to returning columns with character information as characters, not factors. hgn$Hair ## [1] &quot;black&quot; &quot;brown&quot; &quot;blond&quot; &quot;black&quot; &quot;black&quot; &quot;red&quot; &quot;brown&quot; See? As a character variable, Hair no longer has factor level information. But if you knew you wanted to treat Hair as a factor, you could easily convert it with dplyr::mutate(). hgn &lt;- hgn %&gt;% mutate(Hair = factor(Hair)) hgn$Hair ## [1] black brown blond black black red brown ## Levels: black blond brown red And here’s a tidyverse way to reorder the levels for the Hair factor. hgn &lt;- hgn %&gt;% mutate(Hair = factor(Hair, levels = c(&quot;red&quot;, &quot;blond&quot;, &quot;brown&quot;, &quot;black&quot;))) hgn$Hair ## [1] black brown blond black black red brown ## Levels: red blond brown black as.numeric(hgn$Hair) ## [1] 4 3 2 4 4 1 3 Since we imported hgn with read_csv(), the Name column is already a character vector, which we can verify with the str() function. hgn$Name %&gt;% str() ## chr [1:7] &quot;Alex&quot; &quot;Betty&quot; &quot;Carla&quot; &quot;Diane&quot; &quot;Edward&quot; &quot;Frank&quot; &quot;Gabrielle&quot; Note how using as.vector() did nothing in our case. Name was already a character vector. hgn$Name %&gt;% as.vector() %&gt;% str() ## chr [1:7] &quot;Alex&quot; &quot;Betty&quot; &quot;Carla&quot; &quot;Diane&quot; &quot;Edward&quot; &quot;Frank&quot; &quot;Gabrielle&quot; The Group column was imported as composed of integers. hgn$Group %&gt;% str() ## num [1:7] 1 1 1 2 2 2 2 Switching Group to a factor is easy enough. hgn &lt;- hgn %&gt;% mutate(Group = factor(Group)) hgn$Group ## [1] 1 1 1 2 2 2 2 ## Levels: 1 2 3.5.2 Saving data from R. The readr package has a write_csv() function, too. The arguments are as follows: write_csv(x, file, na = \"NA\", append = FALSE, col_names = !append, quote_escape = \"double\", eol = \"\\n\"). Learn more by executing ?write_csv. Saving hgn in your working directory is as easy as: write_csv(hgn, &quot;hgn.csv&quot;) You could also use save(). save(hgn, file = &quot;hgn.Rdata&quot; ) Once we start fitting Bayesian models, this method will be an important way to save the results of those models. The load() function is simple. load(&quot;hgn.Rdata&quot; ) The ls() function works very much the same way as the more verbosely-named objects() function. ls() ## [1] &quot;a&quot; &quot;abc&quot; &quot;d&quot; &quot;hgn&quot; &quot;my_list&quot; &quot;step_1&quot; &quot;t&quot; ## [8] &quot;x&quot; &quot;xf&quot; &quot;xfo&quot; &quot;xfol&quot; &quot;y&quot; 3.6 Some utility functions “A function is a process that takes some input, called the arguments, and produces some output, called the value” (p. 56, emphasis in the original). # this is a more compact way to replicate 100 1&#39;s, 200 2&#39;s, and 300 3&#39;s x &lt;- rep(1:3, times = c(100, 200, 300)) summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 2.500 2.333 3.000 3.000 We can use the pipe to convert and then summarize x. x %&gt;% factor() %&gt;% summary() ## 1 2 3 ## 100 200 300 The head() and tail() functions are quite useful. head(x) ## [1] 1 1 1 1 1 1 tail(x) ## [1] 3 3 3 3 3 3 I used head() a lot. Within the tidyverse, the slice() function serves a similar role. In order to use slice(), we’ll want to convert x, which is just a vector of integers, into a data frame. Then we’ll use slice() to return a subset of the rows. x &lt;- x %&gt;% data.frame() x %&gt;% slice(1:6) ## . ## 1 1 ## 2 1 ## 3 1 ## 4 1 ## 5 1 ## 6 1 So that was analogous to what we accomplished with head(). Here’s the analogue to tail(). x %&gt;% slice(595:600) ## . ## 1 3 ## 2 3 ## 3 3 ## 4 3 ## 5 3 ## 6 3 The downside of that code was we had to do the math to determine that \\(600 - 6 = 595\\) in order to get the last six rows, as returned by tail(). A more general approach is to use n(), which will return the total number of rows in the tibble. x %&gt;% slice((n() - 6):n()) ## . ## 1 3 ## 2 3 ## 3 3 ## 4 3 ## 5 3 ## 6 3 ## 7 3 To unpack (n() - 6):n(), because n() = 600, (n() - 6) = 600 - 6 = 595. Therefore (n() - 6):n() was equivalent to having coded 595:600. Instead of having to do the math ourselves, n() did it for us. It’s often easier to just go with head() or tail(). But the advantage of this more general approach is that it allows one take more complicated slices of the data, such as returning the first three and last three rows. x %&gt;% slice(c(1:3, (n() - 3):n())) ## . ## 1 1 ## 2 1 ## 3 1 ## 4 3 ## 5 3 ## 6 3 ## 7 3 We’ve already used the handy str() function a bit. It’s also nice to know that tidyverse::glimpse() performs a similar function. x %&gt;% str() ## &#39;data.frame&#39;: 600 obs. of 1 variable: ## $ .: int 1 1 1 1 1 1 1 1 1 1 ... x %&gt;% glimpse() ## Rows: 600 ## Columns: 1 ## $ . &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… Within the tidyverse, we’d use group_by() and then summarize() as alternatives to the base R aggregate() function. With group_by() we group the observations first by Hair and then by Gender within Hair. After that, we summarize the groups by taking the median() values of their Number. hgn %&gt;% group_by(Hair, Gender) %&gt;% summarize(median = median(Number)) ## # A tibble: 5 × 3 ## # Groups: Hair [4] ## Hair Gender median ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 red M 7 ## 2 blond F 3 ## 3 brown F 7 ## 4 black F 7 ## 5 black M 1.5 One of the nice things about this workflow is that the code reads somewhat like how we’d explain what we were doing. We, in effect, told R to Take hgn, then group the data by Hair and Gender within Hair, and then summarize() those groups by their median() Number values. There’s also the nice quality that we don’t have to continually tell R where the data are coming from the way the aggregate() function required Kruschke to prefix each of his variables with HGNdf$. We also didn’t have to explicitly rename the output columns the way Kruschke had to. I’m not aware that our group_by() %&gt;% summarize() workflow has a formula format the way aggregate() does. To count how many levels we had in a grouping factor, we’d use the n() function in summarize(). hgn %&gt;% group_by(Hair, Gender) %&gt;% summarize(n = n()) ## # A tibble: 5 × 3 ## # Groups: Hair [4] ## Hair Gender n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 red M 1 ## 2 blond F 1 ## 3 brown F 2 ## 4 black F 1 ## 5 black M 2 Alternatively, we could switch out the group_by() and summarize() lines with count(). hgn %&gt;% count(Hair, Gender) ## # A tibble: 5 × 3 ## Hair Gender n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 red M 1 ## 2 blond F 1 ## 3 brown F 2 ## 4 black F 1 ## 5 black M 2 We could then use spread() to convert that output to a format similar to Kruschke’s table of counts. hgn %&gt;% count(Hair, Gender) %&gt;% spread(key = Hair, value = n) ## # A tibble: 2 × 5 ## Gender red blond brown black ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 F NA 1 2 1 ## 2 M 1 NA NA 2 With this method, the NAs are stand-ins for 0’s. a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 apply() is part of a family of functions that offer a wide array of uses. You can learn more about the apply() family here or here. apply(a, MARGIN = c(2, 3), FUN = sum) ## LayDimName ## ColDimName l1 l2 ## c1 6 42 ## c2 15 51 ## c3 24 60 ## c4 33 69 Here’s a. a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 The reshape2 package (Wickham, 2007, 2020c) was a precursor to the tidyr package (i.e., one of the core tidyverse packages). The reshape2::melt() function is a quick way to transform the 3-dimensional a matrix into a tidy data frame. a %&gt;% reshape2::melt() ## RowDimName ColDimName LayDimName value ## 1 r1 c1 l1 1 ## 2 r2 c1 l1 2 ## 3 r3 c1 l1 3 ## 4 r1 c2 l1 4 ## 5 r2 c2 l1 5 ## 6 r3 c2 l1 6 ## 7 r1 c3 l1 7 ## 8 r2 c3 l1 8 ## 9 r3 c3 l1 9 ## 10 r1 c4 l1 10 ## 11 r2 c4 l1 11 ## 12 r3 c4 l1 12 ## 13 r1 c1 l2 13 ## 14 r2 c1 l2 14 ## 15 r3 c1 l2 15 ## 16 r1 c2 l2 16 ## 17 r2 c2 l2 17 ## 18 r3 c2 l2 18 ## 19 r1 c3 l2 19 ## 20 r2 c3 l2 20 ## 21 r3 c3 l2 21 ## 22 r1 c4 l2 22 ## 23 r2 c4 l2 23 ## 24 r3 c4 l2 24 We have an alternative if you wanted to stay within the tidyverse. To my knowledge, the fastest way to make the transformation is to first use as.tbl_cube() and follow that up with as_tibble(). The as.tbl_cube() function will convert the a matrix into a tbl_cube. We will use the met_name argument to determine the name of the measure assessed in the data. Since the default is for as.tbl_cube() to name the measure name as ., it seemed value was a more descriptive choice. We’ll then use the as_tibble() function to convert our tbl_cube object into a tidy tibble. library(cubelyr) a %&gt;% as.tbl_cube(met_name = &quot;value&quot;) %&gt;% as_tibble() ## # A tibble: 24 × 4 ## RowDimName ColDimName LayDimName value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 r1 c1 l1 1 ## 2 r2 c1 l1 2 ## 3 r3 c1 l1 3 ## 4 r1 c2 l1 4 ## 5 r2 c2 l1 5 ## 6 r3 c2 l1 6 ## 7 r1 c3 l1 7 ## 8 r2 c3 l1 8 ## 9 r3 c3 l1 9 ## 10 r1 c4 l1 10 ## # … with 14 more rows Notice how the first three columns are returned as characters instead of factors. If you really wanted those to be factors, you could always follow up the code with mutate_if(is.character, as.factor). Also notice that we gained access to the as.tbl_cube() function by loading the cubelyr package (Wickham, 2020a). Though as.tbl_cube() was available in earlier versions of dplyr (one of the core packages in the tidyverse), it was removed in the version 1.0.0 update and is now available via cubelyr. 3.7 Programming in R It’s worthy to note that this project was done with R Markdown, which is an alternative to an R script. As Grolemund and Wickham point out R Markdown integrates a number of R packages and external tools. This means that help is, by-and-large, not available through ?. Instead, as you work through this chapter, and use R Markdown in the future, keep these resources close to hand: R Markdown Cheat Sheet: Help &gt; Cheatsheets &gt; R Markdown Cheat Sheet, R Markdown Reference Guide: Help &gt; Cheatsheets &gt; R Markdown Reference Guide. Both cheatsheets are also available at https://rstudio.com/resources/cheatsheets/. I also strongly recommend checking out R Notebooks, which is a kind of R Markdown document but with a few bells a whistles that make it more useful for working scientists. You can learn more about it here and here. And for a more comprehensive overview, check out Xie, Allaire, and Grolemund’s (2022) R markdown: The definitive guide. 3.7.1 Variable names in R. Kruschke prefers to use camelBack notation for his variable and function names. Though I initially hated it, I largely use snake_case. It seems easier to read_prose_in_snake_case than it is to readProseInCamelBack. To each their own. 3.7.2 Running a program. I do most of my data analyses using RStudio projects. In short, RStudio projects provide a handy way to organize your files within a given project, be it an analysis for a scientific paper, an ebook, or even a personal website. See R4DS Chapter 8 for a nice overview on working directories within the context of an RStudio project. I didn’t really get it, at first. But after using RStudio projects for a couple days, I was hooked. I bet you will be, too. 3.7.3 Programming a function. Here’s our simple a_sq_plus_b function. a_sq_plus_b &lt;- function(a, b = 1) { c &lt;- a^2 + b return(c) } If you explicitly denote your arguments, everything works fine. a_sq_plus_b(a = 3, b = 2) ## [1] 11 Keep things explicit and you can switch up the order of the arguments. a_sq_plus_b(b = 2, a = 3) ## [1] 11 But here’s what happens when you are less explicit. # this a_sq_plus_b(3, 2) ## [1] 11 # is not the same as this a_sq_plus_b(2, 3) ## [1] 7 Since we gave b a default value, we can be really lazy. a_sq_plus_b(a = 2) ## [1] 5 But we can’t be lazy with a. This a_sq_plus_b(b = 1) yielded this warning on my computer: “Error in a_sq_plus_b(b = 1) : argument”a” is missing, with no default”. If we’re completely lazy, a_sq_plus_b() presumes our sole input value is for the a argument and it uses the default value of 1 for b. a_sq_plus_b(2) ## [1] 5 The lesson is important because it’s good practice to familiarize yourself with the defaults of the functions you use in statistics and data analysis, more generally. If you haven’t done it much, before, programming your own functions can feel like a superpower. It’s a good idea to get comfortable with the basics. I’ve found it really helps when I’m working on models for real-world scientific projects. For more practice, Check out R4DS, Chapter 19, or R programming for data science, Chapter 14. 3.7.4 Conditions and loops. Here’s our starting point for if() and else(). if(x &lt;= 3) { # if x is less than or equal to 3 show(&quot;small&quot;) # display the word &quot;small&quot; } else { # otherwise show(&quot;big&quot;) # display the word &quot;big&quot; } # end of ’else’ clause This returns an error: Error in if (x &lt;= 3) { : the condition has length &gt; 1. Further: if (x &lt;= 3) {show(&quot;small&quot;)} else {show(&quot;big&quot;)} On my computer, it returned this message: “the condition has length &gt; 1 and only the first element will be used[1]”small” Error: unexpected ‘else’ in “else”“. Here we use the loop. for (count_down in 5:1) { show(count_down) } ## [1] 5 ## [1] 4 ## [1] 3 ## [1] 2 ## [1] 1 for (note in c(&quot;do&quot;, &quot;re&quot;, &quot;mi&quot;)) { show(note) } ## [1] &quot;do&quot; ## [1] &quot;re&quot; ## [1] &quot;mi&quot; It’s also useful to understand how to use the ifelse() function within the context of a data frame. Recall how x is a data frame. x &lt;- tibble(x = 1:5) x ## # A tibble: 5 × 1 ## x ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 We can use the mutate() function to make a new variable, size, which is itself a function of the original variable, x. We’ll use the ifelse() function to return “small” if x &lt;= 3, but to return “big” otherwise. x %&gt;% mutate(size = ifelse(x &lt;= 3, &quot;small&quot;, &quot;big&quot;)) ## # A tibble: 5 × 2 ## x size ## &lt;int&gt; &lt;chr&gt; ## 1 1 small ## 2 2 small ## 3 3 small ## 4 4 big ## 5 5 big You should also know there’s a dplyr alternative, called if_else(). It works quite similarly, but is stricter about type consistency. If you ever get into a situation where you need to do many ifelse() statements or a many-layered ifelse() statement, you might check out dplyr::case_when(). We’ll get some practice with case_when() in the next chapter. 3.7.5 Measuring processing time. This will be nontrivial to consider in your Bayesian career. Here’s the loop. start_time &lt;- proc.time() y &lt;- vector(mode = &quot;numeric&quot;, length = 1.0E6) for (i in 1:1.0E6) {y[i] &lt;- log(i)} stop_time &lt;- proc.time() elapsed_time_loop &lt;- stop_time - start_time show(elapsed_time_loop) ## user system elapsed ## 0.055 0.004 0.058 Now we use a vector. start_time &lt;- proc.time() y &lt;- log(1:1.0E6) stop_time &lt;- proc.time() elapsed_time_vector &lt;- stop_time - start_time show(elapsed_time_vector) ## user system elapsed ## 0.011 0.004 0.014 Here we compare the two times. elapsed_time_vector[1] / elapsed_time_loop[1] ## user.self ## 0.2 For my computer, the vectorized approach took about 20% the time the loop approach did. When using R, avoid loops for vectorized approaches whenever possible. As an alternative, I tend to just use Sys.time() when I’m doing analyses like these. I’m not going to walk them out, here. But as we go along, you might notice I sometimes use functions from the purrr::map() family in places where Kruschke used loops. I think they’re pretty great. For more on loops and purrr::map() alternatives, check out R4DS Chapter 21. 3.7.6 Debugging. This should be no surprise by now, but in addition to Kruschke’s good advice, I also recommend checking out R4DS. I reference it often. 3.8 Graphical plots: Opening and saving For making and saving plots with ggplot2, I recommend reviewing R4DS Chapters 3 and 28 or Wickham’s ggplot2: Elegant graphics for data analysis. 3.9 Conclusion brms-related needs Given its central role in this ebook, we’d be remiss not to mention a bit about using brms. The main website for brms is the GitHub repository at https://github.com/paul-buerkner/brms. The official page on CRAN, https://CRAN.R-project.org/package=brms is also handy in that it contains a nice list of vignettes and other documents to help you get started. You can find a few video lectures of Paul Bürkner discussing brms, such as here, here, and here. Finally, make sure to keep an eye on all the hot brms discussions in the brms section of the Stan forums, https://discourse.mc-stan.org/c/interfaces/brms/36. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] cubelyr_1.0.1 forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 ## [5] purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 tibble_3.1.8 ## [9] ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.9 lubridate_1.8.0 assertthat_0.2.1 ## [4] digest_0.6.30 utf8_1.2.2 plyr_1.8.7 ## [7] R6_2.5.1 cellranger_1.1.0 backports_1.4.1 ## [10] reprex_2.0.2 evaluate_0.18 httr_1.4.4 ## [13] highr_0.9 pillar_1.8.1 rlang_1.0.6 ## [16] googlesheets4_1.0.1 readxl_1.4.1 rstudioapi_0.13 ## [19] jquerylib_0.1.4 rmarkdown_2.16 labeling_0.4.2 ## [22] googledrive_2.0.0 bit_4.0.4 munsell_0.5.0 ## [25] broom_1.0.1 compiler_4.2.0 modelr_0.1.8 ## [28] xfun_0.35 pkgconfig_2.0.3 htmltools_0.5.3 ## [31] tidyselect_1.1.2 bookdown_0.28 fansi_1.0.3 ## [34] crayon_1.5.2 tzdb_0.3.0 dbplyr_2.2.1 ## [37] withr_2.5.0 grid_4.2.0 jsonlite_1.8.3 ## [40] gtable_0.3.1 lifecycle_1.0.3 DBI_1.1.3 ## [43] magrittr_2.0.3 scales_1.2.1 vroom_1.5.7 ## [46] cli_3.5.0 stringi_1.7.8 cachem_1.0.6 ## [49] reshape2_1.4.4 farver_2.1.1 fs_1.5.2 ## [52] xml2_1.3.3 bslib_0.4.0 ellipsis_0.3.2 ## [55] generics_0.1.3 vctrs_0.5.1 tools_4.2.0 ## [58] bit64_4.0.5 glue_1.6.2 hms_1.1.1 ## [61] parallel_4.2.0 fastmap_1.1.0 colorspace_2.0-3 ## [64] gargle_1.2.0 rvest_1.0.2 knitr_1.40 ## [67] haven_2.5.1 sass_0.4.2 Footnote References Grolemund, G., &amp; Wickham, H. (2017). R for data science. O’Reilly. https://r4ds.had.co.nz Henry, L., &amp; Wickham, H. (2020). purrr: Functional programming tools. https://CRAN.R-project.org/package=purrr Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Müller, K., &amp; Wickham, H. (2020). tibble: Simple data frames. https://CRAN.R-project.org/package=tibble Peng, R. D. (2020). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/ Wickham, H. (2007). Reshaping data with the reshape package. Journal of Statistical Software, 21(12), 1–20. https://doi.org/10.18637/jss.v021.i12 Wickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2-book.org/ Wickham, H. (2019). stringr: Simple, consistent wrappers for common string operations. https://CRAN.R-project.org/package=stringr Wickham, H. (2020a). cubelyr: A data cube ’dplyr’ backend. https://CRAN.R-project.org/package=cubelyr Wickham, H. (2020b). forcats: Tools for working with categorical variables (factors). https://CRAN.R-project.org/package=forcats Wickham, H. (2020c). reshape2: Flexibly reshape data: A reboot of the reshape package. https://CRAN.R-project.org/package=reshape2 Wickham, H. (2020d). The tidyverse style guide. https://style.tidyverse.org/ Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H., &amp; Dunnington, D. (2021). ggplot2: Create elegant data visualisations using the grammar of graphics. https://CRAN.R-project.org/package=ggplot2 Wickham, H., François, R., Henry, L., &amp; Müller, K. (2020). dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr Wickham, H., &amp; Henry, L. (2020). tidyr: Tidy messy data. https://CRAN.R-project.org/package=tidyr Wickham, H., Hester, J., &amp; Francois, R. (2018). readr: Read rectangular text data. https://CRAN.R-project.org/package=readr Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2022). R Markdown: The definitive guide. Chapman and Hall/CRC. https://bookdown.org/yihui/rmarkdown/ The whole = versus &lt;- controversy can spark some strong opinions within the R community. If you’re curious about the historical roots for &lt;-, check out Colon Fay’s nice blog post, Why do we use arrow as an assignment operator?.↩︎ "],["what-is-this-stuff-called-probability.html", "4 What is This Stuff Called Probability? 4.1 The set of all possible events 4.2 Probability: Outside or inside the head 4.3 Probability distributions 4.4 Two-way distributions Session info", " 4 What is This Stuff Called Probability? Inferential statistical techniques assign precise measures to our uncertainty about possibilities. Uncertainty is measured in terms of probability, and therefore we must establish the properties of probability before we can make inferences about it. This chapter introduces the basic ideas of probability. (Kruschke, 2015, p. 71, emphasis in the original) 4.1 The set of all possible events This snip from page 72 is important (emphasis in the original): Whenever we ask about how likely an outcome is, we always ask with a set of possible outcomes in mind. This set exhausts all possible outcomes, and the outcomes are all mutually exclusive. This set is called the sample space. 4.2 Probability: Outside or inside the head It’s worthwhile to quote this section in full. Sometimes we talk about probabilities of outcomes that are “out there” in the world. The face of a flipped coin is such an outcome: We can observe the flip, and the probability of coming up heads can be estimated by observing several flips. But sometimes we talk about probabilities of things that are not so clearly “out there,” and instead are just possible beliefs “inside the head.” Our belief about the fairness of a coin is an example of something inside the head. The coin may have an intrinsic physical bias, but now I am referring to our belief about the bias. Our beliefs refer to a space of mutually exclusive and exhaustive possibilities. It might be strange to say that we randomly sample from our beliefs, like we randomly sample from a sack of coins. Nevertheless, the mathematical properties of probabilities outside the head and beliefs inside the head are the same in their essentials, as we will see. (pp. 73–74, emphasis in the original) 4.2.1 Outside the head: Long-run relative frequency. For events outside the head, it’s intuitive to think of probability as being the long-run relative frequency of each possible outcome… We can determine the long-run relative frequency by two different ways. One way is to approximate it by actually sampling from the space many times and tallying the number of times each event happens. A second way is by deriving it mathematically. These two methods are now explored in turn. (p. 74) 4.2.1.1 Simulating a long-run relative frequency. Before we try coding the simulation, we’ll first load the tidyverse. library(tidyverse) Now run the simulation. n &lt;- 500 # specify the total number of flips p_heads &lt;- 0.5 # specify underlying probability of heads # Kruschke reported this was the seed he used at the top of page 94 set.seed(47405) # here we use that seed to flip a coin n times and compute the running proportion of heads at each flip. # we generate a random sample of n flips (heads = 1, tails = 0) d &lt;- tibble(flip_sequence = sample(x = c(0, 1), prob = c(1 - p_heads, p_heads), size = n, replace = T)) %&gt;% mutate(n = 1:n, r = cumsum(flip_sequence)) %&gt;% mutate(run_prop = r / n) end_prop &lt;- d %&gt;% select(run_prop) %&gt;% slice(n()) %&gt;% round(digits = 3) %&gt;% pull() Now we’re ready to make Figure 4.1. d %&gt;% filter(n &lt; 1000) %&gt;% # this step cuts down on the time it takes to make the plot ggplot(aes(x = n, y = run_prop)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_line(color = &quot;grey50&quot;) + geom_point(color = &quot;grey50&quot;, alpha = 1/4) + scale_x_log10(breaks = c(1, 2, 5, 10, 20, 50, 200, 500)) + coord_cartesian(xlim = c(1, 500), ylim = c(0, 1)) + labs(title = &quot;Running proportion of heads&quot;, subtitle = paste(&quot;Our end proportion =&quot;, end_prop), x = &quot;Flip number&quot;, y = &quot;Proportion of heads&quot;) + theme(panel.grid = element_blank()) 4.2.1.2 Deriving a long-run relative frequency. Sometimes, when the situation is simple enough mathematically, we can derive the exact long-run relative frequency. The case of the fair coin is one such simple situation. The sample space of the coin consists of two possible outcomes, head and tail. By the assumption of fairness, we know that each outcome is equally likely. Therefore, the long-run relative frequency of heads should be exactly one out of two, i.e., \\(1/2\\), and the long-run relative frequency of tails should also be exactly \\(1/2\\). (p. 76) 4.2.2 Inside the head: Subjective belief. To specify our subjective beliefs, we have to specify how likely we think each possible outcome is. It can be hard to pin down mushy intuitive beliefs. In the next section, we explore one way to “calibrate” subjective beliefs, and in the subsequent section we discuss ways to mathematically describe degrees of belief. (p. 76) 4.2.3 Probabilities assign numbers to possibilities. In general, a probability, whether it’s outside the head or inside the head, is just a way of assigning numbers to a set of mutually exclusive possibilities. The numbers, called “probabilities,” merely need to satisfy three properties (Kolmogorov &amp; Bharucha-Reid, 1956): A probability value must be nonnegative (i.e., zero or positive). The sum of the probabilities across all events in the entire sample space must be \\(1.0\\) (i.e., one of the events in the space must happen, otherwise the space does not exhaust all possibilities). For any two mutually exclusive events, the probability that one or the other occurs is the sum of their individual probabilities. For example, the probability that a fair six-sided die comes up \\(3\\)-dots or \\(4\\)-dots is \\(1/6 + 1/6 = 2/6\\). Any assignment of numbers to events that respects those three properties will also have all the properties of probabilities that we will discuss below. (pp. 77–78, emphasis in the original) 4.3 Probability distributions “A probability distribution is simply a list of all possible outcomes and their corresponding probabilities” (p. 78, emphasis in the original) 4.3.1 Discrete distributions: Probability mass. When the sample space consists of discrete outcomes, then we can talk about the probability of each distinct outcome. For example, the sample space of a flipped coin has two discrete outcomes, and we talk about the probability of head or tail… For continuous outcome spaces, we can discretize the space into a finite set of mutually exclusive and exhaustive “bins.” (p. 78, emphasis in the original) In order to recreate Figure 4.2, we need to generate the heights data with Kruschke’s HtWtDataGenerator() function. You can find the original code in Kruschke’s HtWtDataGenerator.R script, and we first used this function in Section 2.3. Once again, here’s how to make the function. HtWtDataGenerator &lt;- function(n_subj, rndsd = NULL, male_prob = 0.50) { # Random height, weight generator for males and females. Uses parameters from # Brainard, J. &amp; Burmaster, D. E. (1992). Bivariate distributions for height and # weight of men and women in the United States. Risk Analysis, 12(2), 267-275. # Kruschke, J. K. (2011). Doing Bayesian data analysis: # A Tutorial with R and BUGS. Academic Press / Elsevier. # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition: # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier. # require(MASS) # Specify parameters of multivariate normal (MVN) distributions. # Men: HtMmu &lt;- 69.18 HtMsd &lt;- 2.87 lnWtMmu &lt;- 5.14 lnWtMsd &lt;- 0.17 Mrho &lt;- 0.42 Mmean &lt;- c(HtMmu, lnWtMmu) Msigma &lt;- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd, Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2) # Women cluster 1: HtFmu1 &lt;- 63.11 HtFsd1 &lt;- 2.76 lnWtFmu1 &lt;- 5.06 lnWtFsd1 &lt;- 0.24 Frho1 &lt;- 0.41 prop1 &lt;- 0.46 Fmean1 &lt;- c(HtFmu1, lnWtFmu1) Fsigma1 &lt;- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1, Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2) # Women cluster 2: HtFmu2 &lt;- 64.36 HtFsd2 &lt;- 2.49 lnWtFmu2 &lt;- 4.86 lnWtFsd2 &lt;- 0.14 Frho2 &lt;- 0.44 prop2 &lt;- 1 - prop1 Fmean2 &lt;- c(HtFmu2, lnWtFmu2) Fsigma2 &lt;- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2, Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2) # Randomly generate data values from those MVN distributions. if (!is.null(rndsd)) {set.seed(rndsd)} data_matrix &lt;- matrix(0, nrow = n_subj, ncol = 3) colnames(data_matrix) &lt;- c(&quot;male&quot;, &quot;height&quot;, &quot;weight&quot;) maleval &lt;- 1; femaleval &lt;- 0 # arbitrary coding values for (i in 1:n_subj) { # Flip coin to decide sex sex = sample(c(maleval, femaleval), size = 1, replace = TRUE, prob = c(male_prob, 1 - male_prob)) if (sex == maleval) {datum &lt;- MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)} if (sex == femaleval) { Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2)) if (Fclust == 1) {datum &lt;- MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)} if (Fclust == 2) {datum &lt;- MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)} } data_matrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1)) } return(data_matrix) } # end function Now we have the HtWtDataGenerator() function, all we need to do is determine how many values are generated and how probable we want the values to be based on those from men. These are controlled by the n_subj and male_prob parameters. set.seed(4) d &lt;- HtWtDataGenerator(n_subj = 10000, male_prob = .5) %&gt;% data.frame() %&gt;% mutate(person = 1:n()) d %&gt;% head() ## male height weight person ## 1 1 76.0 221.5 1 ## 2 0 59.5 190.0 2 ## 3 0 60.2 117.9 3 ## 4 1 64.1 137.7 4 ## 5 1 69.3 147.6 5 ## 6 1 69.1 165.9 6 For Figure 4.2, we’ll discretize the continuous height values into bins with the case_when() function, which you can learn more about from hrbrmstr’s blog post, Making a case for case_when. d_bin &lt;- d %&gt;% mutate(bin = case_when( height &lt; 51 ~ 51, between(height, 51, 53) ~ 53, between(height, 53, 55) ~ 55, between(height, 55, 57) ~ 57, between(height, 57, 59) ~ 59, between(height, 59, 61) ~ 61, between(height, 61, 63) ~ 63, between(height, 63, 65) ~ 65, between(height, 65, 67) ~ 67, between(height, 67, 69) ~ 69, between(height, 69, 71) ~ 71, between(height, 71, 73) ~ 73, between(height, 73, 75) ~ 75, between(height, 75, 77) ~ 77, between(height, 77, 79) ~ 79, between(height, 79, 81) ~ 71, between(height, 81, 83) ~ 83, height &gt; 83 ~ 85) ) %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% mutate(height = bin - 1) d %&gt;% ggplot(aes(x = height, y = person)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 51, to = 83, by = 2), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Person #&quot;) + theme(panel.grid = element_blank()) Because we’re simulating and we don’t know what seed number Kruschke used for his plot, ours will differ a little from his. But the overall pattern is the same. One way to make a version of the histogram in Kruschke’s Figure 4.2.b would be to input the d data directly into ggplot() and set x = height and y = stat(density) within the aes(). Then you could set binwidth = 2 within geom_histogram() to make the bins within the histogram perform like the bins in the plot above. However, since we have already discretized the height values into bins in our d_bin data, it might make more sense to plot those bins with geom_col(). The only other step we need is to manually compute the density values using the formula Kruschke showed in Figure 4.2.b. Here’ how: d_bin %&gt;% # density is the probability mass divided by the bin width mutate(density = (n / sum(n)) / 2) %&gt;% ggplot(aes(x = height, y = density, fill = bin == 65)) + geom_col() + scale_fill_manual(values = c(&quot;gray67&quot;, &quot;gray30&quot;), breaks = NULL) + scale_y_continuous(&quot;Probability density&quot;, breaks = c(0, 0.04, 0.08)) + xlab(&quot;Height (inches)&quot;) + coord_cartesian(xlim = c(51, 83)) + theme(panel.grid = element_blank()) In the text, Kruschke singled out the bin for the values between 63 and 65 with an arrow. In our plot, we highlighted that bin with shading, instead. Here’s how we computed the exact density value for that bin. d_bin %&gt;% mutate(density = (n / sum(n)) / 2) %&gt;% filter(bin == 65) %&gt;% select(n, density) ## # A tibble: 1 × 2 ## n density ## &lt;int&gt; &lt;dbl&gt; ## 1 1728 0.0864 Due to sampling variation, our density value is a little different from the one in the text. Our data binning approach for Figure 4.2.c will be a little different than what we did, above. Here we’ll make our bins with the round() function. d_bin &lt;- d %&gt;% mutate(bin = round(height, digits = 0)) %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% mutate(height = bin - 0.5) d %&gt;% ggplot(aes(x = height, y = person)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 51, to = 83, by = 1), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n, angle = 90), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Person #&quot;) + theme(panel.grid = element_blank()) However, our method for Figure 4.2.d will be like what we did, before. d_bin %&gt;% # density is the probability mass divided by the bin width mutate(density = (n / sum(n)) / 1) %&gt;% ggplot(aes(x = height, y = density, fill = bin == 64)) + geom_col() + scale_fill_manual(values = c(&quot;gray67&quot;, &quot;gray30&quot;), breaks = NULL) + scale_y_continuous(&quot;Probability density&quot;, breaks = c(0, 0.04, 0.08)) + xlab(&quot;Height (inches)&quot;) + coord_cartesian(xlim = c(51, 83)) + theme(panel.grid = element_blank()) Here’s the hand-computed density value for the focal bin. d_bin %&gt;% mutate(density = (n / sum(n)) / 1) %&gt;% filter(bin == 64) %&gt;% select(n, density) ## # A tibble: 1 × 2 ## n density ## &lt;int&gt; &lt;dbl&gt; ## 1 942 0.0942 The probability of a discrete outcome, such as the probability of falling into an interval on a continuous scale, is referred to as a probability mass. Loosely speaking, the term “mass” refers the amount of stuff in an object. When the stuff is probability and the object is an interval of a scale, then the mass is the proportion of the outcomes in the interval. (p. 80, emphasis in the original) 4.3.2 Continuous distributions: Rendezvous with density. If you think carefully about a continuous outcome space, you realize that it becomes problematic to talk about the probability of a specific value on the continuum, as opposed to an interval on the continuum… Therefore, what we will do is make the intervals infinitesimally narrow, and instead of talking about the infinitesimal probability mass of each infinitesimal interval, we will talk about the ratio of the probability mass to the interval width. That ratio is called the probability density. Loosely speaking, density is the amount of stuff per unit of space it takes up. Because we are measuring amount of stuff by its mass, then density is the mass divided by the amount space it occupies. (p. 80, emphasis in the original) To make Figure 4.3, we’ll need new data. set.seed(4) d &lt;- tibble(height = rnorm(1e4, mean = 84, sd = .1)) %&gt;% mutate(door = 1:n()) d %&gt;% head() ## # A tibble: 6 × 2 ## height door ## &lt;dbl&gt; &lt;int&gt; ## 1 84.0 1 ## 2 83.9 2 ## 3 84.1 3 ## 4 84.1 4 ## 5 84.2 5 ## 6 84.1 6 To make the bins for our version of Figure 4.3.a, we could use the case_when() approach from above. However, that would require some tedious code. Happily, we have an alternative in the santoku package (Hugh-Jones, 2020), which I learned about with help from the great Mara Averick, Tyson Barrett, and Omar Wasow. We can use the santoku::chop() function to discretize our height values. Here we’ll walk through the first part. library(santoku) d_bin &lt;- d %&gt;% mutate(bin = chop(height, breaks = seq(from = 83.6, to = 84.4, length.out = 32), # label the boundaries with 3 decimal places, separated by a dash labels = lbl_dash(fmt = &quot;%.3f&quot;)) head(d_bin) ## # A tibble: 6 × 3 ## height door bin ## &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; ## 1 84.0 1 84.013—84.039 ## 2 83.9 2 83.935—83.961 ## 3 84.1 3 84.065—84.090 ## 4 84.1 4 84.039—84.065 ## 5 84.2 5 84.142—84.168 ## 6 84.1 6 84.065—84.090 With this format, the lower-limit for each level of bin is the left side of the dash and the upper-limit is on the right. Though the cut points are precise to many decimal places, the lbl_dash(fmt = \"%.3f\") part of the code rounded the numbers to three decimal places in the bin labels. The width of each bin is just a bit over 0.0258. (84.4 - 83.6) / (32 - 1) ## [1] 0.02580645 Now to make use of the d_bin data in a plot, we’ll have to summarize and separate the values from the bin names to compute the midway points. Here’s one way how. d_bin &lt;- d_bin %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% separate(bin, c(&quot;min&quot;, &quot;max&quot;), sep = &quot;—&quot;, remove = F, convert = T) %&gt;% mutate(height = (min + max) / 2) head(d_bin) ## # A tibble: 6 × 5 ## bin min max n height ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 83.626—83.652 83.6 83.7 4 83.6 ## 2 83.652—83.677 83.7 83.7 6 83.7 ## 3 83.677—83.703 83.7 83.7 7 83.7 ## 4 83.703—83.729 83.7 83.7 16 83.7 ## 5 83.729—83.755 83.7 83.8 37 83.7 ## 6 83.755—83.781 83.8 83.8 76 83.8 Now we plot. d %&gt;% ggplot(aes(x = height, y = door)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 83.6, to = 84.4, length.out = 32), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n, angle = 90), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Door #&quot;) + theme(panel.grid = element_blank()) The only tricky thing about Figure 4.3.b is getting the denominator in the density equation correct. d_bin %&gt;% # density is the probability mass divided by the bin width mutate(density = (n / sum(n)) / ((84.4 - 83.6) / (32 - 1))) %&gt;% ggplot(aes(x = height, y = density, fill = bin == &quot;83.910—83.935&quot;)) + geom_col() + scale_fill_manual(values = c(&quot;gray67&quot;, &quot;gray30&quot;), breaks = NULL) + scale_y_continuous(&quot;Probability density&quot;, breaks = 0:4) + xlab(&quot;Height (inches)&quot;) + coord_cartesian(xlim = c(83.6, 84.4)) + theme(panel.grid = element_blank()) Here’s the density value for that focal bin. d_bin %&gt;% # density is the probability mass divided by the bin width mutate(density = (n / sum(n)) / ((84.4 - 83.6) / (32 - 1))) %&gt;% filter(bin == &quot;83.910—83.935&quot;) %&gt;% select(bin, n, density) ## # A tibble: 1 × 3 ## bin n density ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 83.910—83.935 781 3.03 As Kruschke remarked: “There is nothing mysterious about probability densities larger than 1.0; it means merely that there is a high concentration of probability mass relative to the scale” (p. 82). 4.3.2.1 Properties of probability density functions. In general, for any continuous value that is split up into intervals, the sum of the probability masses of the intervals must be \\(1\\), because, by definition of making a measurement, some value of the measurement scale must occur. (p. 82) 4.3.2.2 The normal probability density function. “Perhaps the most famous probability density function is the normal distribution, also known as the Gaussian distribution” (p. 83). We’ll use dnorm() again to make our version of Figure 4.4. tibble(x = seq(from = -.8, to = .8, by = .02)) %&gt;% mutate(p = dnorm(x, mean = 0, sd = .2)) %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = p), color = &quot;grey50&quot;, size = 1.25) + geom_linerange(aes(ymin = 0, ymax = p), size = 1/3) + labs(title = &quot;Normal probability density&quot;, subtitle = expression(paste(mu, &quot; = 0 and &quot;, sigma, &quot; = 0.2&quot;)), y = &quot;p(x)&quot;) + coord_cartesian(xlim = c(-.61, .61)) + theme(panel.grid = element_blank()) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. The equation for the normal probability density follows the form \\[ p(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left ( - \\frac{1}{2} \\left [ \\frac{x - \\mu}{\\sigma}^2 \\right ] \\right ), \\] where \\(\\mu\\) governs the mean and \\(\\sigma\\) governs the standard deviation. 4.3.3 Mean and variance of a distribution. The mean of a probability distribution is also called the expected value, which follows the form \\[E[x] = \\sum_x p(x) x\\] when \\(x\\) is discrete. For continuous \\(x\\) values, the formula is \\[E[x] = \\int \\text d x \\; p(x) x.\\] The variance is defined as the mean squared deviation from the mean, \\[\\text{var}_x = \\int \\text d x \\; p(x) (x - E[x])^2.\\] If you take the square root of the variance, you get the standard deviation. 4.3.4 Highest density interval (HDI). The HDI indicates which points of a distribution are most credible, and which cover most of the distribution. Thus, the HDI summarizes the distribution by specifying an interval that spans most of the distribution, say \\(95\\%\\) of it, such that every point inside the interval has higher credibility than any point outside the interval. (p. 87) In Chapter 10 (p. 294), Kruschke briefly mentioned his HDIofICDF() function, the code for which you can find in his DBDA2E-utilities.R file. It’s a handy function which we’ll put to use from time to time. Here’s a mild reworking of his code. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { # Arguments: # `name` is R&#39;s name for the inverse cumulative density function # of the distribution. # `width` is the desired mass of the HDI region. # `tol` is passed to R&#39;s optimize function. # Return value: # Highest density iterval (HDI) limits in a vector. # Example of use: For determining HDI of a beta(30, 12) distribution, type # `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)` # Notice that the parameters of the `name` must be explicitly stated; # e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work. # Adapted and corrected from Greg Snow&#39;s TeachingDemos package. incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } We already know from the text, and perhaps from prior experience, what the 95% HDI is for the unit normal. But it’s nice to be able to confirm that with a function. h &lt;- hdi_of_icdf(name = qnorm, mean = 0, sd = 1) h ## [1] -1.959964 1.959964 Now we’ve saved those values in h, we can use then to make our version of Figure 4.5.a. tibble(x = seq(from = -3.5, to = 3.5, by = .05)) %&gt;% mutate(d = dnorm(x, mean = 0, sd = 1)) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = &quot;grey75&quot;) + geom_area(data = . %&gt;% filter(x &gt;= h[1] &amp; x &lt;= h[2]), fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1] + .02, h[2] - .02), d = c(.059, .059)), arrow = arrow(length = unit(.2, &quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(geom = &quot;text&quot;, x = 0, y = .09, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + xlim(-3.1, 3.1) + ylab(&quot;p(x)&quot;) + theme(panel.grid = element_blank()) As far as I could tell, Figure 4.5.b is of a beta distribution, which Kruschke covered in greater detail starting in Chapter 6. I got the shape1 and shape2 values from playing around. If you have a more principled approach, do share. But anyway, we can use our hdi_of_icdf() function to ge the correct values. h &lt;- hdi_of_icdf(name = qbeta, shape1 = 15, shape2 = 4) h ## [1] 0.6103498 0.9507510 Let’s put those h values to work. tibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% mutate(d = dbeta(x, shape1 = 15, shape2 = 4)) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = &quot;grey75&quot;) + geom_area(data = . %&gt;% filter(x &gt;= h[1] &amp; x &lt;= h[2]), fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1] + .01, h[2] - .002), d = c(.75, .75)), arrow = arrow(length = unit(.2, &quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(geom = &quot;text&quot;, x = .8, y = 1.1, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + xlim(.4, 1) + ylab(&quot;p(x)&quot;) + theme(panel.grid = element_blank()) Figure 4.5.c was also a lot of trial and error. It seemed the easiest way to reproduce the shape was to mash two Gaussians together. After playing around with rnorm(), I ended up with this. set.seed(4) d &lt;- tibble(x = c(rnorm(6e5, mean = 1.50, sd = .5), rnorm(4e5, mean = 4.75, sd = .5))) glimpse(d) ## Rows: 1,000,000 ## Columns: 1 ## $ x &lt;dbl&gt; 1.6083774, 1.2287537, 1.9455723, 1.7979903, 2.3178090, 1.8446377, 0.… As you’ll see, it’s not exactly right. But it’s close enough to give you a sense of what’s going on. But anyway, since we’re working with simulated data rather than an analytic solution, we’ll want to use one of the powerful convenience functions from the tidybayes package. library(tidybayes) Kay’s tidybayes package provides a family of functions for generating point summaries and intervals from draws in a tidy format. These functions follow the naming scheme [median|mean|mode]_[qi|hdi], for example, median_qi(), mean_qi(), mode_hdi(), and so on. The first name (before the _) indicates the type of point summary, and the second name indicates the type of interval. qi yields a quantile interval (a.k.a. equi-tailed interval, central interval, or percentile interval) and hdi yields a highest (posterior) density interval. (Kay, 2021, “Point summaries and intervals”) Here we’ll use mode_hdi() to compute the HDIs and put them in a tibble. We’ll be using a lot of mode_hdi() in this project. h &lt;- d %&gt;% mode_hdi() h ## # A tibble: 2 × 6 ## x .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1.50 0.468 2.54 0.95 mode hdi ## 2 1.50 3.81 5.68 0.95 mode hdi Usually, mode_hdi() will return a tibble with just one row. But in this case, since we had a bimodal distribution, it returned two rows—one for each of the two distinct regions. Oh, and in case it wasn’t clear, that first column x is the measure of central tendency—the mode, in this case. Though I acknowledge, it’s a little odd to speak of central tendency in a bimodal distribution. Again, this won’t happen much. In order to fill the bimodal density with the split HDIs, you need to use the density() function to transform the d data to a tibble with the values for the \\(x\\)-axis in an x vector and the corresponding density values in a y vector. dens &lt;- d$x %&gt;% density() %&gt;% with(tibble(x, y)) head(dens) ## # A tibble: 6 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.11 0.0000000503 ## 2 -1.09 0.0000000822 ## 3 -1.08 0.000000131 ## 4 -1.06 0.000000201 ## 5 -1.04 0.000000304 ## 6 -1.03 0.000000449 We’re finally ready to plot. Forgive me. It’s a monster. ggplot(data = dens, aes(x = x, y = y)) + geom_area(fill = &quot;grey75&quot;) + # note the use of `pull()`, which extracts the values, rather than return a tibble geom_area(data = dens %&gt;% filter(x &gt; h[1, 2] %&gt;% pull() &amp; x &lt; h[1, 3] %&gt;% pull()), fill = &quot;grey50&quot;) + geom_area(data = dens %&gt;% filter(x &gt; h[2, 2] %&gt;% pull() &amp; x &lt; h[2, 3] %&gt;% pull()), fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1, 2] %&gt;% pull(), h[1, 3] %&gt;% pull()), y = c(.06, .06)), arrow = arrow(length = unit(.2,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + geom_line(data = tibble(x = c(h[2, 2] %&gt;% pull(), h[2, 3] %&gt;% pull()), y = c(.06, .06)), arrow = arrow(length = unit(.2,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(geom = &quot;text&quot;, x = c(1.5, 4.75), y = .1, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + scale_x_continuous(breaks = 0:6, limits = c(0, 6.3)) + scale_y_continuous(&quot;p(x)&quot;, breaks = c(0, .1, .2, .3, .4, .5)) + theme(panel.grid = element_blank()) When the distribution refers to credibility of values, then the width of the HDI is another way of measuring uncertainty of beliefs. If the HDI is wide, then beliefs are uncertain. If the HDI is narrow, then beliefs are relatively certain. (p. 89) 4.4 Two-way distributions In the note below Table 4.1, Kruschke indicated the data came from Snee (1974), Graphical display of two-way contingency tables. Kruschke has those data saved as the HairEyeColor.csv file. d &lt;- read_csv(&quot;data.R/HairEyeColor.csv&quot;) glimpse(d) ## Rows: 16 ## Columns: 3 ## $ Hair &lt;chr&gt; &quot;Black&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Blond&quot;, &quot;Blond&quot;, &quot;Blond&quot;, &quot;… ## $ Eye &lt;chr&gt; &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green&quot;, &quot;Hazel&quot;, &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green&quot;, &quot;Ha… ## $ Count &lt;dbl&gt; 20, 68, 5, 15, 94, 7, 16, 10, 84, 119, 29, 54, 17, 26, 14, 14 We’ll need to transform Hair and Eye a bit to ensure our output matches the order in Table 4.1. d &lt;- d %&gt;% mutate(Hair = if_else(Hair == &quot;Brown&quot;, &quot;Brunette&quot;, Hair) %&gt;% factor(., levels = c(&quot;Black&quot;, &quot;Brunette&quot;, &quot;Red&quot;, &quot;Blond&quot;)), Eye = factor(Eye, levels = c(&quot;Brown&quot;, &quot;Blue&quot;, &quot;Hazel&quot;, &quot;Green&quot;))) Here we’ll use the tabyl() and adorn_totals() functions from the janitor package (Firke, 2020) to help make the table of proportions by Eye and Hair. library(janitor) d &lt;- d %&gt;% uncount(weights = Count, .remove = F) %&gt;% tabyl(Eye, Hair) %&gt;% adorn_totals(c(&quot;row&quot;, &quot;col&quot;)) %&gt;% data.frame() %&gt;% mutate_if(is.double, ~ . / 592) d %&gt;% mutate_if(is.double, round, digits = 2) ## Eye Black Brunette Red Blond Total ## 1 Brown 0.11 0.20 0.04 0.01 0.37 ## 2 Blue 0.03 0.14 0.03 0.16 0.36 ## 3 Hazel 0.03 0.09 0.02 0.02 0.16 ## 4 Green 0.01 0.05 0.02 0.03 0.11 ## 5 Total 0.18 0.48 0.12 0.21 1.00 4.4.1 Conditional probability. We often want to know the probability of one outcome, given that we know another outcome is true. For example, suppose I sample a person at random from the population referred to in Table 4.1. Suppose I tell you that this person has blue eyes. Conditional on that information, what is the probability that the person has blond hair (or any other particular hair color)? It is intuitively clear how to compute the answer: We see from the blue-eye row of Table 4.1 that the total (i.e., marginal) amount of blue-eyed people is \\(0.36\\), and that \\(0.16\\) of the population has blue eyes and blond hair. (p. 91) Kruschke then showed how to compute such conditional probabilities by hand in Table 4.2. Here’s a slightly reformatted version of that information. d %&gt;% filter(Eye == &quot;Blue&quot;) %&gt;% pivot_longer(Black:Blond, names_to = &quot;Hair&quot;, values_to = &quot;proportion&quot;) %&gt;% rename(`p(Eyes = &quot;Blue&quot;)` = Total) %&gt;% mutate(`conditional probability` = proportion / `p(Eyes = &quot;Blue&quot;)`) %&gt;% select(Eye, Hair, `p(Eyes = &quot;Blue&quot;)`, proportion, `conditional probability`) ## # A tibble: 4 × 5 ## Eye Hair `p(Eyes = &quot;Blue&quot;)` proportion `conditional probability` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Blue Black 0.363 0.0338 0.0930 ## 2 Blue Brunette 0.363 0.142 0.391 ## 3 Blue Red 0.363 0.0287 0.0791 ## 4 Blue Blond 0.363 0.159 0.437 The only reason our values differ from those in Table 4.2 is because Kruschke rounded. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] janitor_2.1.0 tidybayes_3.0.2 forcats_0.5.1 stringr_1.4.1 ## [5] dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 ## [9] tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] fs_1.5.2 lubridate_1.8.0 bit64_4.0.5 ## [4] httr_1.4.4 tensorA_0.36.2 tools_4.2.0 ## [7] backports_1.4.1 bslib_0.4.0 utf8_1.2.2 ## [10] R6_2.5.1 DBI_1.1.3 colorspace_2.0-3 ## [13] ggdist_3.2.0 withr_2.5.0 tidyselect_1.1.2 ## [16] bit_4.0.4 compiler_4.2.0 cli_3.5.0 ## [19] rvest_1.0.2 pacman_0.5.1 HDInterval_0.2.2 ## [22] arrayhelpers_1.1-0 xml2_1.3.3 labeling_0.4.2 ## [25] bookdown_0.28 posterior_1.3.1 sass_0.4.2 ## [28] scales_1.2.1 checkmate_2.1.0 digest_0.6.30 ## [31] rmarkdown_2.16 pkgconfig_2.0.3 htmltools_0.5.3 ## [34] dbplyr_2.2.1 fastmap_1.1.0 highr_0.9 ## [37] rlang_1.0.6 readxl_1.4.1 rstudioapi_0.13 ## [40] jquerylib_0.1.4 farver_2.1.1 generics_0.1.3 ## [43] svUnit_1.0.6 jsonlite_1.8.3 vroom_1.5.7 ## [46] googlesheets4_1.0.1 distributional_0.3.1 magrittr_2.0.3 ## [49] Rcpp_1.0.9 munsell_0.5.0 fansi_1.0.3 ## [52] abind_1.4-5 lifecycle_1.0.3 stringi_1.7.8 ## [55] snakecase_0.11.0 MASS_7.3-58.1 grid_4.2.0 ## [58] parallel_4.2.0 crayon_1.5.2 lattice_0.20-45 ## [61] haven_2.5.1 hms_1.1.1 knitr_1.40 ## [64] pillar_1.8.1 reprex_2.0.2 glue_1.6.2 ## [67] evaluate_0.18 santoku_0.8.0 modelr_0.1.8 ## [70] vctrs_0.5.1 tzdb_0.3.0 cellranger_1.1.0 ## [73] gtable_0.3.1 assertthat_0.2.1 cachem_1.0.6 ## [76] xfun_0.35 broom_1.0.1 coda_0.19-4 ## [79] googledrive_2.0.0 gargle_1.2.0 ellipsis_0.3.2 References Firke, S. (2020). janitor: Simple tools for examining and cleaning dirty data. https://CRAN.R-project.org/package=janitor Hugh-Jones, D. (2020). santoku: A versatile cutting tool. https://CRAN.R-project.org/package=santoku Kay, M. (2021). Extracting and visualizing tidy draws from brms models. https://mjskay.github.io/tidybayes/articles/tidy-brms.html Kolmogorov, A. N., &amp; Bharucha-Reid, A. T. (1956). Foundations of the theory of probability: Second English Edition. Chelsea Publishing Company. https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations.pdf Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Snee, R. D. (1974). Graphical display of two-way contingency tables. The American Statistician, 28(1), 9–12. https://doi.org/10.1080/00031305.1974.10479053 "],["bayes-rule.html", "5 Bayes’ Rule 5.1 Bayes’ rule 5.2 Applied to parameters and data 5.3 Complete examples: Estimating bias in a coin 5.4 Why Bayesian inference can be difficult Session info", " 5 Bayes’ Rule “Bayes’ rule is merely the mathematical relation between the prior allocation of credibility and the posterior reallocation of credibility conditional on data” (Kruschke, 2015, pp. 99–100). 5.1 Bayes’ rule Thomas Bayes (1702–1761) was a mathematician and Presbyterian minister in England. His famous theorem was published posthumously in 1763, thanks to the extensive editorial efforts of his friend, Richard Price (Bayes, 1763). The simple rule has vast ramifications for statistical inference, and therefore as long as his name is attached to the rule, we’ll continue to see his name in textbooks. But Bayes himself probably was not fully aware of these ramifications, and many historians argue that it is Bayes’ successor, Pierre-Simon Laplace (1749–1827), whose name should really label this type of analysis, because it was Laplace who independently rediscovered and extensively developed the methods (e.g., Dale, 2012; McGrayne, 2011). (p. 100) I do recommend checking out McGrayne’s book It’s an easy and entertaining read. For a sneak preview, why not listen to her discuss the main themes she covered in the book? 5.1.1 Derived from definitions of conditional probability. With Equations 5.5 and 5.6, Kruschke gave us Bayes’ rule in terms of \\(c\\) and \\(r\\). Equation 5.5 was \\[p(c|r) = \\frac{p(r|c) \\; p(c)}{p(r)}.\\] Since \\(p(r) = \\sum_{c^*}p(r|c^*)p(c^*)\\), we can re-express that as Equation 5.6: \\[p(c|r) = \\frac{p(r|c) \\; p(c)}{\\sum_{c^*}p(r|c^*) \\; p(c^*)},\\] where \\(c^*\\) “in the denominator is a variable that takes on all possible values” of \\(c\\) (p. 101). 5.2 Applied to parameters and data Here we get those equations re-expressed in the terms data analysts tend to think with, parameters (i.e., \\(\\theta\\)) and data (i.e., \\(D\\)): \\[\\begin{align*} p(\\theta|D) &amp; = \\frac{p(D|\\theta) \\; p(\\theta)}{p(D)} \\;\\; \\text{and since} \\\\ p(D) &amp; = \\sum\\limits_{\\theta^*}p(D|\\theta^*) \\; p(\\theta^*), \\;\\; \\text{it&#39;s also the case that} \\\\ p(\\theta|D) &amp; = \\frac{p(D|\\theta) \\; p(\\theta)}{\\sum\\limits_{\\theta^*}p(D|\\theta^*) \\; p(\\theta^*)}. \\end{align*}\\] As in the previous section where we spoke in terms of \\(r\\) and \\(c\\), our updated \\(\\theta^*\\) notation is meant to indicate all possible values of \\(\\theta\\). For practice, it’s worth repeating how Kruschke broke this down with Equation 5.7, \\[ \\underbrace{p(\\theta|D)}_\\text{posterior} \\; = \\; \\underbrace{p(D|\\theta)}_\\text{likelihood} \\;\\; \\underbrace{p(\\theta)}_\\text{prior} \\; / \\; \\underbrace{p(D)}_\\text{evidence}. \\] The “prior,” \\(p(\\theta)\\), is the credibility of the \\(\\theta\\) values without the data \\(D\\). The “posterior,” \\(p(\\theta|D)\\), is the credibility of \\(\\theta\\) values with the data \\(D\\) taken into account. The “likelihood,” \\(p(D|\\theta)\\), is the probability that the data could be generated by the model with parameter value \\(\\theta\\). The “evidence” for the model, \\(p(D)\\), is the overall probability of the data according to the model, determined by averaging across all possible parameter values weighted by the strength of belief in those parameter values. (pp. 106–107) And don’t forget, “evidence” is short for “marginal likelihood,” which is the term we’ll use in some of our code, below. 5.3 Complete examples: Estimating bias in a coin As we begin to work with Kruschke’s coin example, we should clarify that when [Kruschke refered] to the “bias” in a coin, [he] sometimes [referred] to its underlying probability of coming up heads. Thus, when a coin is fair, it has a “bias” of \\(\\textit{0.5}\\). Other times, [Kruschke used] the term “bias” in its colloquial sense of a departure from fairness, as in “head biased” or “tail biased.” Although [Kruschke tried] to be clear about which meaning is intended, there will be times that you will have to rely on context to determine whether “bias” means the probability of heads or departure from fairness. (p. 108, emphasis in the original) In this ebook, I will generally avoid Kruschke’s idiosyncratic use of the term “bias.” Though be warned: it may pop up from time to time. As the the coin example at hand, here’s a way to expres Kruschke’s initial prior distribution in a data frame and then make Figure 5.1.a. library(tidyverse) # make the data frame for the prior d &lt;- tibble(theta = seq(from = 0, to = 1, by = .1), prior = c(0:5, 4:0) * 0.04) # plot d %&gt;% ggplot(aes(x = theta, y = prior)) + geom_col(width = .025, color = &quot;grey50&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Prior&quot;, y = expression(p(theta))) + theme(panel.grid = element_blank()) If you were curious, it is indeed the case that those prior values sum to 1. d %&gt;% summarise(s = sum(prior)) ## # A tibble: 1 × 1 ## s ## &lt;dbl&gt; ## 1 1 On page 109, Kruschke proposed the Bernoulli distribution for coin-tossing data, which he defined in Equation 5.10 as \\[p(y | \\theta) = \\theta^y (1 - \\theta)^{(1 - y)}.\\] We can express it as a function in R like this. bernoulli &lt;- function(theta, y) { return(theta^y * (1 - theta)^(1 - y)) } To get a sense of how it works, consider a single coin flip which comes up. In this example we wanted heads, which means the trial was a success. In the bernoulli() function, we’ll refer to that single successful trial by setting y = 1. We can then compute the likelihood of different values of \\(\\theta\\) by inserting those values one at a time or in bulk into the theta argument. Here we’ll look at a sequene of 11 candidate \\(\\theta\\) values, which we’ll save in a vector called theta_sequence. theta_sequence &lt;- seq(from = 0, to = 1, by = .1) bernoulli(theta = theta_sequence, y = 1) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Those values are the probability of the data (y = 1) given different values of theta (theta = theta_sequence). In terms of Bayes’ rule from the previous section, we call that the likelihood \\(p(D|\\theta)\\). Here’s how we might compute the likelihood within the context of our d data frame. d &lt;- d %&gt;% mutate(likelihood = bernoulli(theta = theta, y = 1)) d ## # A tibble: 11 × 3 ## theta prior likelihood ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 ## 2 0.1 0.04 0.1 ## 3 0.2 0.08 0.2 ## 4 0.3 0.12 0.3 ## 5 0.4 0.16 0.4 ## 6 0.5 0.2 0.5 ## 7 0.6 0.16 0.6 ## 8 0.7 0.12 0.7 ## 9 0.8 0.08 0.8 ## 10 0.9 0.04 0.9 ## 11 1 0 1 Now our d data contains information about the likelihood, we can use those results to make the middle panel of Figure 5.1. d %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_col(width = .025, color = &quot;grey50&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Likelihood&quot;, y = expression(p(D*&#39;|&#39;*theta))) + theme(panel.grid = element_blank()) In order to compute \\(p(D)\\) (i.e., the evidence or the marginal likelihood), we’ll need to multiply our respective prior and likelihood values for each point in our theta sequence and then sum all that up. That sum will be our marginal likelihood. Then we can compute the posterior \\(p(\\theta | D)\\) by dividing the product of the prior and the likelihood by the marginal likelihood and make Figure 5.1.c. # compute d &lt;- d %&gt;% mutate(marginal_likelihood = sum(prior * likelihood)) %&gt;% mutate(posterior = (prior * likelihood) / marginal_likelihood) # plot d %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_col(width = .025, color = &quot;grey50&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Posterior&quot;, y = expression(p(theta*&#39;|&#39;*D))) + theme(panel.grid = element_blank()) The posterior is a compromise between the prior distribution and the likelihood function. Sometimes this is loosely stated as a compromise between the prior and the data. The compromise favors the prior to the extent that the prior distribution is sharply peaked and the data are few. The compromise favors the likelihood function (i.e., the data) to the extent that the prior distribution is flat and the data are many. (p. 112) 5.3.1 Influence of sample size on the posterior. Our warmup in the last section was limited in that we computed the posterior based on a single data point y = 1. In order to follow along with this section, we’re going to have to update our Bernoulli likelihood function so it can accommodate more than a single trial. In anticipation of Chapter 6, we’ll call our more general function the bernoulli_likelihood(). bernoulli_likelihood &lt;- function(theta, data) { # `theta` = success probability parameter ranging from 0 to 1 # `data` = the vector of data (i.e., a series of 0s and 1s) n &lt;- length(data) return(theta^sum(data) * (1 - theta)^(n - sum(data))) } Now we can compute the likelihood for a range of theta values given a vector of coin-flip data. To practice, we’ll make a vector of \\(N = 4\\) coin flips (three tails and one head) called small_data, and compute the likelihood for those data given our theta_sequence from above. # define the data small_data &lt;- rep(0:1, times = c(3, 1)) # compute the likelihood over a range of theta values bernoulli_likelihood(theta = theta_sequence, data = small_data) ## [1] 0.0000 0.0729 0.1024 0.1029 0.0864 0.0625 0.0384 0.0189 0.0064 0.0009 ## [11] 0.0000 To make Figure 5.2, we’ll move away from the coarse 11-point theta_sequence to a denser 1,001-point sequence of \\(\\theta\\) values. Here’s the work required to make our version of the left portion of Figure 5.2. # make the primary data set d &lt;- tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = small_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) # wrangle d %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% pivot_longer(-theta) %&gt;% mutate(name = factor(name, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% # plot ggplot(aes(x = theta, y = value)) + geom_area(fill = &quot;grey67&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + ylab(&quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;, ncol = 1) As Kruschke remarked on page 112, the mode of the posterior is at \\(\\theta = .4\\). d %&gt;% filter(Posterior == max(Posterior)) %&gt;% select(theta, Posterior) ## # A tibble: 1 × 2 ## theta Posterior ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.4 0.00237 We just follow the same overall procedure to make the right portion of Figure 5.2. The only difference is how we switch from small_data to the \\(N = 40\\) large_data. large_data &lt;- rep(0:1, times = c(30, 10)) # compute d &lt;- d %&gt;% mutate(Likelihood = bernoulli_likelihood(theta = theta, data = large_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) # wrangle d %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% pivot_longer(-theta) %&gt;% mutate(name = factor(name, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% # plot ggplot(aes(x = theta, y = value)) + geom_area(fill = &quot;grey67&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + ylab(&quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;, ncol = 1) Now the mode of the posterior is lower at \\(\\theta = .268\\). d %&gt;% filter(Posterior == max(Posterior)) %&gt;% select(theta, Posterior) ## # A tibble: 1 × 2 ## theta Posterior ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.268 0.00586 With just an \\(N = 40\\), the likelihood already dominated the posterior. But this is also a function of our fairly gentle prior. “In general, the more data we have, the more precise is the estimate of the parameter(s) in the model. Larger sample sizes yield greater precision or certainty of estimation” (p. 113). 5.3.2 Influence of prior on the posterior. It’s not immediately obvious how Kruschke made his prior distributions for Figure 5.3. However, hidden away in his BernGridExample.R file he indicated that to get the distribution for the left side of Figure 5.3, you simply raise the prior from the left of Figure 5.2 to the 0.1 power. small_data &lt;- rep(0:1, times = c(3, 1)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% # here&#39;s the important line of code mutate(Prior = Prior^0.1 / sum(Prior^0.1), Likelihood = bernoulli_likelihood(theta = theta, data = small_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% pivot_longer(-theta) %&gt;% mutate(name = factor(name, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, y = value)) + geom_area(fill = &quot;grey67&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + ylab(&quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;, ncol = 1) With a vague flat prior and a small data set, the posterior well tend to look a lot like the likelihood. With the right side of Figure 5.3, we consider a larger data set and a stronger prior. Also, note our tricky Prior code. large_data &lt;- rep(0:1, times = c(30, 10)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% # here&#39;s the important line of code mutate(Prior = (Prior / sum(Prior))^10, Likelihood = bernoulli_likelihood(theta = theta, data = large_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% pivot_longer(-theta) %&gt;% mutate(name = factor(name, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, y = value)) + geom_area(fill = &quot;grey67&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + ylab(&quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;, ncol = 1) Bayesian inference is intuitively rational: With a strongly informed prior that uses a lot of previous data to put high credibility over a narrow range of parameter values, it takes a lot of novel contrary data to budge beliefs away from the prior. But with a weakly informed prior that spreads credibility over a wide range of parameter values, it takes relatively little data to shift the peak of the posterior distribution toward the data (although the posterior will be relatively wide and uncertain). (p. 114) 5.4 Why Bayesian inference can be difficult Determining the posterior distribution directly from Bayes’ rule involves computing the evidence (a.k.a. marginal likelihood) in Equations 5.8 and 5.9. In the usual case of continuous parameters, the integral in Equation 5.9 can be impossible to solve analytically. Historically, the difficulty of the integration was addressed by restricting models to relatively simple likelihood functions with corresponding formulas for prior distributions, called conjugate priors, that “played nice” with the likelihood function to produce a tractable integral. (p. 115, emphasis in the original) However, the simple model + conjugate prior approach has its limitations. As we’ll see, we often want to fit complex models without shackling ourselves with conjugate priors—which can be quite a pain to work with. Happily, another kind of approximation involves randomly sampling a large number of representative combinations of parameter values from the posterior distribution. In recent decades, many such algorithms have been developed, generally referred to as Markov chain Monte Carlo (MCMC) methods. What makes these methods so useful is that they can generate representative parameter-value combinations from the posterior distribution of complex models without computing the integral in Bayes’ rule. It is the development of these MCMC methods that has allowed Bayesian statistical methods to gain practical use. (pp. 115–116, emphasis in the original) Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 ## [5] readr_2.1.2 tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 ## [9] tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] lubridate_1.8.0 assertthat_0.2.1 digest_0.6.30 ## [4] utf8_1.2.2 R6_2.5.1 cellranger_1.1.0 ## [7] backports_1.4.1 reprex_2.0.2 evaluate_0.18 ## [10] httr_1.4.4 highr_0.9 pillar_1.8.1 ## [13] rlang_1.0.6 googlesheets4_1.0.1 readxl_1.4.1 ## [16] rstudioapi_0.13 jquerylib_0.1.4 rmarkdown_2.16 ## [19] labeling_0.4.2 googledrive_2.0.0 munsell_0.5.0 ## [22] broom_1.0.1 compiler_4.2.0 modelr_0.1.8 ## [25] xfun_0.35 pkgconfig_2.0.3 htmltools_0.5.3 ## [28] tidyselect_1.1.2 bookdown_0.28 fansi_1.0.3 ## [31] crayon_1.5.2 tzdb_0.3.0 dbplyr_2.2.1 ## [34] withr_2.5.0 grid_4.2.0 jsonlite_1.8.3 ## [37] gtable_0.3.1 lifecycle_1.0.3 DBI_1.1.3 ## [40] magrittr_2.0.3 scales_1.2.1 cli_3.5.0 ## [43] stringi_1.7.8 cachem_1.0.6 farver_2.1.1 ## [46] fs_1.5.2 xml2_1.3.3 bslib_0.4.0 ## [49] ellipsis_0.3.2 generics_0.1.3 vctrs_0.5.1 ## [52] tools_4.2.0 glue_1.6.2 hms_1.1.1 ## [55] fastmap_1.1.0 colorspace_2.0-3 gargle_1.2.0 ## [58] rvest_1.0.2 knitr_1.40 haven_2.5.1 ## [61] sass_0.4.2 References Bayes, T. (1763). LII. An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, FRS communicated by Mr. Price, in a letter to John Canton, AMFR S. Philosophical Transactions of the Royal Society of London, 53, 370–418. https://royalsocietypublishing.org/doi/pdf/10.1098/rstl.1763.0053 Dale, A. I. (2012). A history of inverse probability: From Thomas Bayes to Karl Pearson. Springer Science &amp; Business Media. https://www.springer.com/gp/book/9780387988078 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ McGrayne, S. B. (2011). The theory that would not die: How Bayes’ rule cracked the enigma code, hunted down Russian submarines, &amp; emerged triumphant from two centuries of controversy. Yale University Press. https://yalebooks.yale.edu/book/9780300188226/theory-would-not-die "],["inferring-a-binomial-probability-via-exact-mathematical-analysis.html", "6 Inferring a Binomial Probability via Exact Mathematical Analysis 6.1 The likelihood function: The Bernoulli distribution 6.2 A description of credibilities: The beta distribution 6.3 The posterior beta 6.4 Examples 6.5 Summary Session info", " 6 Inferring a Binomial Probability via Exact Mathematical Analysis This chapter presents an example of how to do Bayesian inference using pure analytical mathematics without any approximations. Ultimately, we will not use the pure analytical approach for complex applications, but this chapter is important for two reasons. First, the relatively simple mathematics in this chapter nicely reveal the underlying concepts of Bayesian inference on a continuous parameter. The simple formulas show how the continuous allocation of credibility changes systematically as data accumulate. The examples provide an important conceptual foundation for subsequent approximation methods, because the examples give you a clear sense of what is being approximated. Second, the distributions introduced in this chapter, especially the beta distribution, will be used repeatedly in subsequent chapters. (Kruschke, 2015, p. 123, emphasis added) 6.1 The likelihood function: The Bernoulli distribution If we denote a set of possible outcomes as \\(\\{y_i\\}\\), Kruschke’s Bernoulli likelihood function for a set of \\(N\\) trials follows the form \\[p(\\{y_i\\} | \\theta) = \\theta^z \\cdot (1 - \\theta) ^ {N - z},\\] where \\(z\\) is the number of 1’s in the data (i.e., heads in a series of coin flips) and the sole parameter \\(\\theta\\) is the probability a given observation will be a 1. Otherwise put, the Bernoulli function gives us \\(p(y_i = 1 | \\theta)\\). If you follow that equation closely, here is how we might express it in R. bernoulli_likelihood &lt;- function(theta, data) { # `theta` = success probability parameter ranging from 0 to 1 # `data` = the vector of data (i.e., a series of 0s and 1s) n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } This will come in handy in just a bit. 6.2 A description of credibilities: The beta distribution In this chapter, we use purely mathematical analysis, with no numerical approximation, to derive the mathematical form of the posterior credibilities of parameter values. To do this, we need a mathematical description of the prior allocation of credibilities… In principle, we could use any probability density function supported on the interval \\([0, 1]\\). When we intend to apply Bayes’ rule (Equation 5.7, p. 106), however, there are two desiderata for mathematical tractability. First, it would be convenient if the product of \\(p(y | \\theta)\\) and \\(p(\\theta)\\), which is in the numerator of Bayes’ rule, results in a function of the same form as \\(p(\\theta)\\)… Second, we desire the denominator of Bayes’ rule (Equation 5.9, p. 107), namely \\(\\int \\text d \\theta\\ p(y | \\theta) p(\\theta)\\), to be solvable analytically. This quality also depends on how the form of the function \\(p(\\theta)\\) relates to the form of the function \\(p(y | \\theta)\\). When the forms of \\(p(y | \\theta)\\) and \\(p(\\theta)\\) combine so that the posterior distribution has the same form as the prior distribution, then \\(p(\\theta)\\) is called a conjugate prior for \\(p(y | \\theta)\\). (p. 127 emphasis in the original) When we want a conjugate prior for \\(\\theta\\) of the Bernoulli likelihood, the beta distribution is a handy choice. Beta has two parameters, \\(a\\) and \\(b\\) (also sometimes called \\(\\alpha\\) and \\(\\beta\\)), and the density is defined as \\[\\begin{align*} p(\\theta | a, b) &amp; = \\operatorname{Beta} (\\theta | a, b) \\\\ &amp; = \\frac{\\theta^{(a - 1)} \\; (1 - \\theta)^{(b - 1)}}{B(a, b)}, \\end{align*}\\] where \\(B(a, b)\\) is a normalizing constant, keeping the results in a probability metric, and \\(B(\\cdot)\\) is the Beta function. Kruschke then clarified that the beta distribution and the Beta function are not the same. In R, we use the beta density with the dbeta() function, whereas we use the Beta function with beta(). In this project, we’ll primarily use dbeta(). But to give a sense, notice that when given the same input for \\(a\\) and \\(b\\), the two functions return very different values. theta &lt;- .5 a &lt;- 3 b &lt;- 3 dbeta(theta, a, b) ## [1] 1.875 beta(a, b) ## [1] 0.03333333 The \\(a\\) and \\(b\\) parameters are also called shape parameters. And indeed, if we look at the parameters of the dbeta() function in R, we’ll see that \\(a\\) is called shape1 and \\(b\\) is called shape2. print(dbeta) ## function (x, shape1, shape2, ncp = 0, log = FALSE) ## { ## if (missing(ncp)) ## .Call(C_dbeta, x, shape1, shape2, log) ## else .Call(C_dnbeta, x, shape1, shape2, ncp, log) ## } ## &lt;bytecode: 0x7f991940b6e0&gt; ## &lt;environment: namespace:stats&gt; You can learn more about the dbeta() function here. Before we make Figure 6.1, we’ll need some data. library(tidyverse) length &lt;- 1e2 d &lt;- crossing(shape1 = c(.1, 1:4), shape2 = c(.1, 1:4)) %&gt;% expand(nesting(shape1, shape2), x = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(a = str_c(&quot;a = &quot;, shape1), b = str_c(&quot;b = &quot;, shape2)) %&gt;% mutate(density = dbeta(x, shape1 = shape1, shape2 = shape2)) head(d) ## # A tibble: 6 × 6 ## shape1 shape2 x a b density ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0.1 0.1 0 a = 0.1 b = 0.1 Inf ## 2 0.1 0.1 0.0101 a = 0.1 b = 0.1 3.20 ## 3 0.1 0.1 0.0202 a = 0.1 b = 0.1 1.73 ## 4 0.1 0.1 0.0303 a = 0.1 b = 0.1 1.21 ## 5 0.1 0.1 0.0404 a = 0.1 b = 0.1 0.945 ## 6 0.1 0.1 0.0505 a = 0.1 b = 0.1 0.781 Now we’re ready for our Figure 6.1. d %&gt;% ggplot(aes(x = x, y = density)) + geom_area(fill = &quot;grey50&quot;) + scale_x_continuous(expression(theta), breaks = c(0, .5, 1)) + coord_cartesian(ylim = c(0, 3)) + labs(title = &quot;Examples of the beta distribution&quot;, y = expression(p(theta*&quot;|&quot;*a*&quot;, &quot;*b))) + theme(panel.grid = element_blank()) + facet_grid(b ~ a) Notice that as \\(a\\) gets bigger (left to right across columns of Figure 6.1), the bulk of the distribution moves rightward over higher values of \\(\\theta\\), but as \\(b\\) gets bigger (top to bottom across rows of Figure 6.1), the bulk of the distribution moves leftward over lower values of \\(\\theta\\). Notice that as \\(a\\) and \\(b\\) get bigger together, the beta distribution gets narrower. (p. 127). We have a lot of practice with the beta distribution waiting for us in the chapters to come. If you like informal tutorials, you might also check out Karin Knudson’s nice blog post, Beta distributions, Dirichlet distributions and Dirichlet processes. 6.2.1 Specifying a beta prior. Though the \\(a\\) and \\(b\\) parameters in the beta distribution might initially seem confusing, they have a nice connection to the kinds of binary coin-flipping-type data we explores with the Bernoulli distribution in the last chapter. As Kruschke wrote: “You can think of \\(a\\) and \\(b\\) in the prior as if they were previously observed data, in which there were \\(a\\) heads and \\(b\\) tails in a total of \\(n = a + b\\) flips” (pp. 127–128). For example, a popular default minimally-informative prior would be \\(\\operatorname{Beta}(1, 1)\\), which is like the sum of two coin flips of one tails and one heads. That produces a uniform distribution like so: d %&gt;% filter(shape1 == 1 &amp; shape2 == 1) %&gt;% ggplot(aes(x = x, y = density)) + geom_area(fill = &quot;grey50&quot;) + scale_x_continuous(expression(theta), breaks = c(0, .5, 1)) + coord_cartesian(ylim = c(0, 3)) + labs(title = &quot;Beta(1, 1)&quot;, y = expression(p(theta*&quot;|&quot;*a*&quot;, &quot;*b))) + theme(panel.grid = element_blank()) We read further: It is useful to know the central tendency and spread of the beta distribution expressed in terms of \\(a\\) and \\(b\\). It turns out that the mean of the \\(\\operatorname{beta}(\\theta | a, b)\\) distribution is \\(\\mu = a / (a + b)\\) and the mode is \\(\\omega = (a − 1) / (a + b − 2)\\) for \\(a &gt; 1\\) and \\(b &gt; 1\\) (\\(\\mu\\) is Greek letter mu and \\(\\omega\\) is Greek letter omega). (p. 129) In the case of our \\(\\operatorname{Beta}(1, 1)\\), the mean is then \\(1 / (1 + 1) = 0.5\\) and the mode is undefined (which hopefully makes sense given the uniform distribution). Kruschke continued: The spread of the beta distribution is related to the “concentration” \\(\\kappa = a + b\\) (\\(\\kappa\\) is Greek letter kappa). You can see from Figure 6.1 that as \\(\\kappa = a + b\\) gets larger, the beta distribution gets narrower or more concentrated. (p. 129) In the case of our \\(\\operatorname{Beta}(1, 1)\\), the concentration is \\(1 + 1 = 2\\), which is also like the hypothetical sample size we’ve been using, \\(n = 2\\). If we further explore Kruschke’s formulas, we learn you can specify a beta distribution in terms of \\(\\omega\\) and \\(\\kappa\\) by \\[\\operatorname{Beta} \\big (\\alpha = \\omega (\\kappa - 2) + 1, \\beta = (1 - \\omega) \\cdot (\\kappa - 2) + 1 \\big ),\\] as long as \\(\\kappa &gt; 2\\). Kruschke further clarified: The value we choose for the prior \\(\\kappa\\) can be thought of this way: It is the number of new flips of the coin that we would need to make us teeter between the new data and the prior belief about \\(\\mu\\). If we would only need a few new flips to sway our beliefs, then our prior beliefs should be represented by a small \\(\\kappa\\). If we would need a large number of new flips to sway us away from our prior beliefs about \\(\\mu\\), then our prior beliefs are worth a very large \\(\\kappa\\). (p. 129) He went on to clarify why we might prefer the mode to the mean when discussing the central tendency of a beta distribution. The mode can be more intuitive than the mean, especially for skewed distributions, because the mode is where the distribution reaches its tallest height, which is easy to visualize. The mean in a skewed distribution is somewhere away from the mode, in the direction of the longer tail. (pp. 129–130) Figure 6.2 helped contrast the mean and mode for beta. We’ll use the same process from Figure 6.1 and create the data, first. d &lt;- tibble(shape1 = c(5.6, 17.6, 5, 17), shape2 = c(1.4, 4.4, 2, 5)) %&gt;% mutate(a = str_c(&quot;a = &quot;, shape1), b = str_c(&quot;b = &quot;, shape2), kappa = rep(c(&quot;kappa==7&quot;, &quot;kappa==22&quot;), times = 2), mu_omega = rep(c(&quot;mu==0.8&quot;, &quot;omega==0.8&quot;), each = 2)) %&gt;% mutate(kappa = factor(kappa, levels = c(&quot;kappa==7&quot;, &quot;kappa==22&quot;)), label = str_c(a, &quot;, &quot;, b)) %&gt;% expand(nesting(shape1, shape2, a, b, label, kappa, mu_omega), x = seq(from = 0, to = 1, length.out = 1e3)) %&gt;% mutate(density = dbeta(x, shape1 = shape1, shape2 = shape2)) head(d) ## # A tibble: 6 × 9 ## shape1 shape2 a b label kappa mu_omega x density ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 2 a = 5 b = 2 a = 5, b = 2 kappa==7 omega==0.8 0 0 ## 2 5 2 a = 5 b = 2 a = 5, b = 2 kappa==7 omega==0.8 0.00100 3.01e-11 ## 3 5 2 a = 5 b = 2 a = 5, b = 2 kappa==7 omega==0.8 0.00200 4.81e-10 ## 4 5 2 a = 5 b = 2 a = 5, b = 2 kappa==7 omega==0.8 0.00300 2.43e- 9 ## 5 5 2 a = 5 b = 2 a = 5, b = 2 kappa==7 omega==0.8 0.00400 7.68e- 9 ## 6 5 2 a = 5 b = 2 a = 5, b = 2 kappa==7 omega==0.8 0.00501 1.87e- 8 Here’s Figure 6.2. d %&gt;% ggplot(aes(x = x)) + geom_area(aes(y = density), fill = &quot;grey50&quot;) + geom_vline(xintercept = .8, color = &quot;grey92&quot;, linetype = 2) + geom_text(data = . %&gt;% group_by(label) %&gt;% slice(1), aes(x = .025, y = 4.75, label = label), hjust = 0, size = 3) + scale_x_continuous(expression(theta), breaks = c(0, .8, 1)) + ylab(expression(p(theta*&quot;|&quot;*a*&quot;, &quot;*b))) + coord_cartesian(ylim = c(0, 5)) + theme(panel.grid = element_blank()) + facet_grid(mu_omega ~ kappa, labeller = label_parsed) It’s also possible to define the beta distribution in terms of the mean \\(\\mu\\) and standard deviation \\(\\sigma\\). In this case, \\[ \\begin{align*} \\alpha &amp; = \\mu \\left ( \\frac{\\mu(1 - \\mu)}{\\sigma^2} - 1\\right), \\text{and} \\\\ \\beta &amp; = (1 - \\mu) \\left ( \\frac{\\mu(1 - \\mu)}{\\sigma^2} - 1\\right). \\end{align*} \\] In lines 264 to 290 in his DBDA2E-utilities.R file, Kruschke provided a series of betaABfrom...() functions that will allow us to compute the \\(a\\) and \\(b\\) parameters from measures of central tendency (i.e., mean and mode) and of spread (i.e., \\(\\kappa\\) and \\(\\sigma\\)). Here are those bits of his code. # Shape parameters from central tendency and scale: betaABfromMeanKappa &lt;- function(mean, kappa) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (kappa &lt;= 0) stop(&quot;kappa must be &gt; 0&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } betaABfromModeKappa &lt;- function(mode, kappa) { if (mode &lt;= 0 | mode &gt;= 1) stop(&quot;must have 0 &lt; mode &lt; 1&quot;) if (kappa &lt;= 2) stop(&quot;kappa must be &gt; 2 for mode parameterization&quot;) a &lt;- mode * (kappa - 2) + 1 b &lt;- (1.0 - mode) * (kappa - 2) + 1 return(list(a = a, b = b)) } betaABfromMeanSD &lt;- function(mean, sd) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) kappa &lt;- mean * (1 - mean)/sd^2 - 1 if (kappa &lt;= 0) stop(&quot;invalid combination of mean and sd&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } You can use them like so. betaABfromMeanKappa(mean = .25, kappa = 4) ## $a ## [1] 1 ## ## $b ## [1] 3 betaABfromModeKappa(mode = .25, kappa = 4) ## $a ## [1] 1.5 ## ## $b ## [1] 2.5 betaABfromMeanSD(mean = .5, sd = .1) ## $a ## [1] 12 ## ## $b ## [1] 12 You can also save the results as an object, which can then be indexed by parameter using the base-R $ syntax. beta_param &lt;- betaABfromModeKappa(mode = .25, kappa = 4) beta_param$a ## [1] 1.5 beta_param$b ## [1] 2.5 We’ll find this trick quite handy in the sections to come. 6.3 The posterior beta I’m not going to reproduce all of Formula 6.8. But this a fine opportunity to re-express Bayes’ rule in terms of \\(z\\) and \\(N\\), \\[p(\\theta | z, N) = \\frac{p(z, N | \\theta) \\; p(\\theta)}{p(z, N)}.\\] A key insight from the equations Kruschke worked through this section is: “If the prior distribution is \\(\\operatorname{beta}(a, b)\\), and the data have \\(z\\) heads in \\(N\\) flips, then the posterior distribution is \\(\\operatorname{beta}(\\theta | z + a, N - z + b)\\)” (p. 132). 6.3.1 Posterior is compromise of prior and likelihood. You might wonder how Kruschke computed the HDI values for Figure 6.3. Remember our hdi_of_icdf() function from back in Chapter 4? Yep, that’s how. Here’s that code, again. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { # Arguments: # `name` is R&#39;s name for the inverse cumulative density function # of the distribution. # `width` is the desired mass of the HDI region. # `tol` is passed to R&#39;s optimize function. # Return value: # Highest density iterval (HDI) limits in a vector. # Example of use: For determining HDI of a beta(30, 12) distribution, type # `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)` # Notice that the parameters of the `name` must be explicitly stated; # e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work. # Adapted and corrected from Greg Snow&#39;s TeachingDemos package. incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Recall it’s based off of the HDIofICDF() function from Kruschke’s DBDA2E-utilities.R file. I’ve altered Kruschke’s formatting a little bit, but the guts of the code are unchanged. Our hdi_of_icdf() function will take the name of an “inverse cumulative density function” and its parameters and then return an HDI range, as defined by the width parameter. Since the prior at the top of Figure 6.3 is \\(\\operatorname{Beta}(5, 5)\\), we can use hdi_of_icdf() to calculate the HDI like so. hdi_of_icdf(name = qbeta, shape1 = 5, shape2 = 5, width = .95) ## [1] 0.2120085 0.7879915 Here they are for the posterior distribution at the bottom of the figure. hdi_of_icdf(name = qbeta, shape1 = 6, shape2 = 14) ## [1] 0.1142339 0.4967144 Note that since we set width = .95 as the default, we can leave it out if we want to stick with the conventional 95% intervals. Here are the mean calculations from the last paragraph on page 134. n &lt;- 10 z &lt;- 1 a &lt;- 5 b &lt;- 5 (prior_mean &lt;- a / (a + b)) ## [1] 0.5 (proportion_heads &lt;- z / n) ## [1] 0.1 (posterior_mean &lt;- (z + a) / (n + a + b)) ## [1] 0.3 In order to make the plots for Figure 6.3, we’ll want to compute the prior, likelihood, and posterior density values across a densely-packed range of \\(\\theta\\) values. trial_data &lt;- c(rep(0, 9), 1) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 100)) %&gt;% mutate(`Prior (beta)` = dbeta(theta, shape1 = a, shape2 = b), `Likelihood (Bernoulli)` = bernoulli_likelihood(theta = theta, data = trial_data), `Posterior (beta)` = dbeta(theta, shape1 = 6, shape2 = 14)) glimpse(d) ## Rows: 100 ## Columns: 4 ## $ theta &lt;dbl&gt; 0.00000000, 0.01010101, 0.02020202, 0.0303030… ## $ `Prior (beta)` &lt;dbl&gt; 0.000000e+00, 6.297429e-06, 9.670878e-05, 4.6… ## $ `Likelihood (Bernoulli)` &lt;dbl&gt; 0.000000000, 0.009218977, 0.016812166, 0.0229… ## $ `Posterior (beta)` &lt;dbl&gt; 0.000000e+00, 1.500163e-05, 4.201284e-04, 2.7… To make things easier on ourselves, we’ll also make two additional data objects to annotate the plots with lines and text. # save the levels levels &lt;- c(&quot;Prior (beta)&quot;, &quot;Likelihood (Bernoulli)&quot;, &quot;Posterior (beta)&quot;) # the data for the in-plot lines line &lt;- tibble(theta = c(.212 + .008, .788 - .008, .114 + .004, .497 - .005), value = rep(c(.51, .66), each = 2), xintercept = c(.212, .788, .114, .497), name = rep(c(&quot;Prior (beta)&quot;, &quot;Posterior (beta)&quot;), each = 2)) %&gt;% mutate(name = factor(name, levels = levels)) # the data for the annotation text &lt;- tibble(theta = c(.5, .3), value = c(.8, 1.125), label = &quot;95% HDI&quot;, name = c(&quot;Prior (beta)&quot;, &quot;Posterior (beta)&quot;)) %&gt;% mutate(name = factor(name, levels = levels)) Finally, here’s our Figure 6.3. library(cowplot) d %&gt;% pivot_longer(-theta) %&gt;% mutate(name = factor(name, levels = levels)) %&gt;% ggplot(aes(x = theta, y = value, )) + # densities geom_area(fill = &quot;steelblue&quot;) + # dashed vertical lines geom_vline(data = line, aes(xintercept = xintercept), linetype = 2, color = &quot;white&quot;) + # arrows geom_line(data = line, arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;white&quot;) + # text geom_text(data = text, aes(label = label), color = &quot;white&quot;) + labs(x = expression(theta), y = NULL) + facet_wrap(~ name, scales = &quot;free_y&quot;, ncol = 1) + theme_cowplot() Note how we loaded the cowplot package (Wilke, 2020). We played around a bit with plotting conventions in the previous chapters. From this chapter onward we’ll explore plotting conventions in a more deliberate fashion. One quick way to alter the look and feel of a plot is by altering its theme, and the cowplot package includes several theme options. In this chapter, we’ll focus on making simple and conventional-looking plots with the theme_cowplot() function. 6.4 Examples 6.4.1 Prior knowledge expressed as a beta distribution. If you flip an unaltered freshly-minted coin 20 times and end up with 17 heads, 85% of those trials are heads. 100 * (17 / 20) ## [1] 85 In the first paragraph of this section, Kruschke suggested we consider a beta prior with a mode of \\(\\omega = .5\\) and an effective sample size \\(\\kappa = 500\\). Why? Because even in the face of 17 heads out of 20 flips, our default prior assumption should still be that freshly-minted coins are fair. To compute the \\(a\\) and \\(b\\) parameters that correspond to \\(\\omega = .5\\) and \\(\\kappa = 500\\), we might use Kruschke’s betaABfromModeKappa() function. betaABfromModeKappa(mode = .5, kappa = 500) ## $a ## [1] 250 ## ## $b ## [1] 250 Confusingly, Kruschke switched from \\(\\operatorname{Beta(250, 250)}\\) in the prose to \\(\\operatorname{Beta(100, 100)}\\) in Figure 6.4.a, which he acknowledged in his Corrigenda. We’ll stick with \\(\\operatorname{Beta(100, 100)}\\), which corresponds to \\(\\omega = .5\\) and \\(\\kappa = 200\\). betaABfromModeKappa(mode = .5, kappa = 200) ## $a ## [1] 100 ## ## $b ## [1] 100 Here’s how to use those values and some of the equations from above to make the data necessary for the left column of Figure 6.4. # define the prior beta_param &lt;- betaABfromModeKappa(mode = .5, kappa = 200) # compute the corresponding HDIs prior_hdi &lt;- hdi_of_icdf(name = qbeta, shape1 = beta_param$a, shape2 = beta_param$b, width = .95) # define the data n &lt;- 20 z &lt;- 17 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) # compute the HDIs for the posterior post_hdi &lt;- hdi_of_icdf(name = qbeta, shape1 = z + beta_param$a, shape2 = n - z + beta_param$b, width = .95) # use the above to compute the prior, the likelihood, and the posterior # densities using the grid approximation approach d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %&gt;% mutate(prior = dbeta(theta, shape1 = beta_param$a, shape2 = beta_param$b), likelihood = bernoulli_likelihood(theta = theta, data = trial_data), posterior = dbeta(theta, shape1 = z + beta_param$a, shape2 = n - z + beta_param$b)) # what have we done? glimpse(d) ## Rows: 1,000 ## Columns: 4 ## $ theta &lt;dbl&gt; 0.000000000, 0.001001001, 0.002002002, 0.003003003, 0.00400… ## $ prior &lt;dbl&gt; 0.000000e+00, 4.526977e-237, 2.598214e-207, 6.376221e-190, … ## $ likelihood &lt;dbl&gt; 0.000000e+00, 1.014103e-51, 1.325213e-46, 1.301756e-43, 1.7… ## $ posterior &lt;dbl&gt; 0.000000e+00, 3.226730e-282, 2.420099e-247, 5.833988e-227, … We’re finally ready to plot the prior, the likelihood, and the posterior for the left column of Figure 6.4. ## Figure 6.4, left column # prior d %&gt;% ggplot(aes(x = theta, y = prior)) + geom_area(fill = &quot;steelblue&quot;, alpha = 1/2) + geom_area(data = . %&gt;% filter(theta &gt; prior_hdi[1] &amp; theta &lt; prior_hdi[2]), fill = &quot;steelblue&quot;) + geom_segment(x = prior_hdi[1] + .005, xend = prior_hdi[2] - .005, y = 1.8, yend = 1.8, arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;white&quot;) + annotate(geom = &quot;text&quot;, x = .5, y = 3.5, label = &quot;95% HDI&quot;) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*100*&quot;, &quot;*100))) + coord_cartesian(ylim = c(0, 12)) + theme_cowplot() # likelihood d %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_area(fill = &quot;steelblue&quot;) + labs(title = &quot;Likelihood (Bernoulli)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) + theme_cowplot() # posterior d %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_area(fill = &quot;steelblue&quot;, alpha = 1/2) + geom_area(data = . %&gt;% filter(theta &gt; post_hdi[1] &amp; theta &lt; post_hdi[2]), fill = &quot;steelblue&quot;) + geom_segment(x = post_hdi[1] + .005, xend = post_hdi[2] - .005, y = 2, yend = 2, arrow = arrow(length = unit(.15, &quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;white&quot;) + annotate(geom = &quot;text&quot;, x = .532, y = 3.5, label = &quot;95% HDI&quot;) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*117*&quot;, &quot;*103))) + coord_cartesian(ylim = c(0, 12)) + theme_cowplot() Here are the exact HDI values for the prior and posterior densities. prior_hdi ## [1] 0.4309509 0.5690491 post_hdi ## [1] 0.4660154 0.5974595 If you double back to page 129 in the text, you’ll see Kruschke defined the mode of a beta density as \\[\\omega_\\text{beta} = (a - 1) / (a + b - 2)\\] whenever \\(a &gt; 1\\) and \\(b &gt; 1\\). Thus we can compute the modes for our prior and posterior densities like this. (beta_param$a - 1) / (beta_param$a + beta_param$b - 2) ## [1] 0.5 (z + beta_param$a - 1) / (z + beta_param$a + n - z + beta_param$b - 2) ## [1] 0.5321101 For the next example, we consider the probability a professional basketball player will make free a throw. We have the same likelihood based on 17 successes our of 20 trials, but this time our prior is based on \\(\\omega = .75\\) and \\(\\kappa = 25\\). Here we update those values and our d data for the plot. # update the beta parameters for the prior beta_param &lt;- betaABfromModeKappa(mode = .75, kappa = 25) # update the HDIs prior_hdi &lt;- hdi_of_icdf(name = qbeta, shape1 = beta_param$a, shape2 = beta_param$b, width = .95) post_hdi &lt;- hdi_of_icdf(name = qbeta, shape1 = z + beta_param$a, shape2 = n - z + beta_param$b, width = .95) # update the data d &lt;- d %&gt;% mutate(prior = dbeta(theta, shape1 = beta_param$a, shape2 = beta_param$b), posterior = dbeta(theta, shape1 = z + beta_param$a, shape2 = n - z + beta_param$b)) With our updated values in hand, we’re ready to make our versions of the middle column of Figure 6.4. ## plot Figure 6.4, middle column! # prior d %&gt;% ggplot(aes(x = theta, y = prior)) + geom_area(fill = &quot;steelblue&quot;, alpha = 1/2) + geom_area(data = . %&gt;% filter(theta &gt; prior_hdi[1] &amp; theta &lt; prior_hdi[2]), fill = &quot;steelblue&quot;) + geom_segment(x = prior_hdi[1] + .005, xend = prior_hdi[2] - .005, y = 0.75, yend = 0.75, arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;white&quot;) + annotate(geom = &quot;text&quot;, x = .75, y = 1.5, label = &quot;95% HDI&quot;, color = &quot;white&quot;) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*18.25*&quot;, &quot;*6.75))) + coord_cartesian(ylim = c(0, 7)) + theme_cowplot() # likelihood, which is the same as the last time d %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_area(fill = &quot;steelblue&quot;) + labs(title = &quot;Likelihood (Bernoulli)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) + theme_cowplot() # posterior d %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_area(fill = &quot;steelblue&quot;, alpha = 1/2) + geom_area(data = . %&gt;% filter(theta &gt; post_hdi[1] &amp; theta &lt; post_hdi[2]), fill = &quot;steelblue&quot;) + geom_segment(x = post_hdi[1] + .005, xend = post_hdi[2] - .005, y = 1, yend = 1, arrow = arrow(length = unit(.15, &quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;white&quot;) + annotate(geom = &quot;text&quot;, x = .797, y = 2, label = &quot;95% HDI&quot;, color = &quot;white&quot;) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*35.25*&quot;, &quot;*9.75))) + coord_cartesian(ylim = c(0, 7)) + theme_cowplot() Here are the exact HDI values for the prior and posterior densities. prior_hdi ## [1] 0.5581935 0.8915815 post_hdi ## [1] 0.6629078 0.8966491 Here are the the modes for our prior and posterior densities. (beta_param$a - 1) / (beta_param$a + beta_param$b - 2) ## [1] 0.75 (z + beta_param$a - 1) / (z + beta_param$a + n - z + beta_param$b - 2) ## [1] 0.7965116 For our final example, we consider the tendency of a newly discovered substance on a distant planet to be blue versus green. Just as in the previous two examples, we discover 17 out of 20 trials come up positive (i.e., blue). This time we have a noncommittal uniform prior, \\(\\operatorname{Beta}(1, 1)\\). Here’s how to plot the results, as shown in the right column of Figure 6.4. # update beta_param beta_param$a &lt;- 1 beta_param$b &lt;- 1 # update the HDIs prior_hdi &lt;- hdi_of_icdf(name = qbeta, shape1 = beta_param$a, shape2 = beta_param$b, width = .95) post_hdi &lt;- hdi_of_icdf(name = qbeta, shape1 = z + beta_param$a, shape2 = n - z + beta_param$b, width = .95) # update the data d &lt;- d %&gt;% mutate(prior = dbeta(theta, shape1 = beta_param$a, shape2 = beta_param$b), posterior = dbeta(theta, shape1 = z + beta_param$a, shape2 = n - z + beta_param$b)) ## plot Figure 6.4, rightmost column! # prior d %&gt;% ggplot(aes(x = theta, y = prior)) + geom_area(fill = &quot;steelblue&quot;) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*1*&quot;, &quot;*1))) + coord_cartesian(ylim = c(0, 5)) + theme_cowplot() # likelihood, which is the same as the last two examples d %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_area(fill = &quot;steelblue&quot;) + labs(title = &quot;Likelihood (Bernoulli)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) + theme_cowplot() # posterior d %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_area(fill = &quot;steelblue&quot;, alpha = 1/2) + geom_area(data = . %&gt;% filter(theta &gt; post_hdi[1] &amp; theta &lt; post_hdi[2]), fill = &quot;steelblue&quot;) + geom_segment(x = post_hdi[1] + .005, xend = post_hdi[2] - .005, y = 0.8, yend = 0.8, arrow = arrow(length = unit(.15, &quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;white&quot;) + annotate(geom = &quot;text&quot;, x = (post_hdi[1] + post_hdi[2]) / 2, y = 1.5, label = &quot;95% HDI&quot;, color = &quot;white&quot;) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*18*&quot;, &quot;*4))) + coord_cartesian(ylim = c(0, 5)) + theme_cowplot() Here are the exact HDI values for the posterior density. post_hdi ## [1] 0.6599474 0.9591231 Because both the \\(a\\) and \\(b\\) parameters for our beta prior are 1, we can’t use the formula from above to compute the mode. I hope this makes sense if you look back at the plot. The density for \\(\\operatorname{Beta}(1, 1)\\) is uniform and has no mode. We can, at least, compute the mode for the posterior. (z + beta_param$a - 1) / (z + beta_param$a + n - z + beta_param$b - 2) ## [1] 0.85 6.4.2 Prior knowledge that cannot be expressed as a beta distribution. The beauty of using a beta distribution to express prior knowledge is that the posterior distribution is again exactly a beta distribution, and therefore, no matter how much data we include, we always have an exact representation of the posterior distribution and a simple way of computing it. But not all prior knowledge can be expressed by a beta distribution, because the beta distribution can only be in the forms illustrated by Figure 6.1. If the prior knowledge cannot be expressed as a beta distribution, then we must use a different method to derive the posterior. In particular, we might revert to grid approximation as was explained in Section 5.5 (p. 116). For such a small section in the text, the underlying code is a bit of a beast. For kicks, we’ll practice two ways. First we’ll follow the code Kruschke used in the text. Our second attempt will be in a more tidyverse sort of way. 6.4.2.1 Figure 6.5 in Kruschke style. # Fine teeth for Theta theta &lt;- seq(0, 1, length = 1000) # Two triangular peaks on a small non-zero floor p_theta &lt;- c(rep(1, 200), seq(1, 100, length = 50), seq(100, 1, length = 50), rep(1, 200)) %&gt;% rep(., times = 2) # Make p_theta sum to 1.0 p_theta &lt;- p_theta / sum(p_theta) Here’s Kruschke’s BernGrid() code in all its glory. BernGrid = function( Theta , pTheta , Data , plotType=c(&quot;Points&quot;,&quot;Bars&quot;)[2] , showCentTend=c(&quot;Mean&quot;,&quot;Mode&quot;,&quot;None&quot;)[3] , showHDI=c(TRUE,FALSE)[2] , HDImass=0.95 , showpD=c(TRUE,FALSE)[2] , nToPlot=length(Theta) ) { # Theta is vector of values between 0 and 1. # pTheta is prior probability mass at each value of Theta # Data is vector of 0&#39;s and 1&#39;s. # Check for input errors: if ( any( Theta &gt; 1 | Theta &lt; 0 ) ) { stop(&quot;Theta values must be between 0 and 1&quot;) } if ( any( pTheta &lt; 0 ) ) { stop(&quot;pTheta values must be non-negative&quot;) } if ( !isTRUE(all.equal( sum(pTheta) , 1.0 )) ) { stop(&quot;pTheta values must sum to 1.0&quot;) } if ( !all( Data == 1 | Data == 0 ) ) { stop(&quot;Data values must be 0 or 1&quot;) } # Create summary values of Data z = sum( Data ) # number of 1&#39;s in Data N = length( Data ) # Compute the Bernoulli likelihood at each value of Theta: pDataGivenTheta = Theta^z * (1-Theta)^(N-z) # Compute the evidence and the posterior via Bayes&#39; rule: pData = sum( pDataGivenTheta * pTheta ) pThetaGivenData = pDataGivenTheta * pTheta / pData # Plot the results. layout( matrix( c( 1,2,3 ) ,nrow=3 ,ncol=1 ,byrow=FALSE ) ) # 3x1 panels par( mar=c(3,3,1,0) , mgp=c(2,0.7,0) , mai=c(0.5,0.5,0.3,0.1) ) # margins cexAxis = 1.33 cexLab = 1.75 # convert plotType to notation used by plot: if ( plotType==&quot;Points&quot; ) { plotType=&quot;p&quot; } if ( plotType==&quot;Bars&quot; ) { plotType=&quot;h&quot; } dotsize = 5 # how big to make the plotted dots barsize = 5 # how wide to make the bar lines # If the comb has a zillion teeth, it&#39;s too many to plot, so plot only a # thinned out subset of the teeth. nteeth = length(Theta) if ( nteeth &gt; nToPlot ) { thinIdx = round( seq( 1, nteeth , length=nteeth ) ) } else { thinIdx = 1:nteeth } # Plot the prior. yLim = c(0,1.1*max(c(pTheta,pThetaGivenData))) plot( Theta[thinIdx] , pTheta[thinIdx] , type=plotType , pch=&quot;.&quot; , cex=dotsize , lwd=barsize , xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis , xlab=bquote(theta) , ylab=bquote(p(theta)) , cex.lab=cexLab , main=&quot;Prior&quot; , cex.main=1.5 , col=&quot;skyblue&quot; ) if ( showCentTend != &quot;None&quot; ) { if ( showCentTend == &quot;Mean&quot; ) { meanTheta = sum( Theta * pTheta ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mean=&quot; * .(signif(meanTheta,3)) ) , cex=2.0 , adj=textadj ) } if ( showCentTend == &quot;Mode&quot; ) { modeTheta = Theta[ which.max( pTheta ) ] if ( modeTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mode=&quot; * .(signif(modeTheta,3)) ) , cex=2.0 , adj=textadj ) } } # Mark the highest density interval. HDI points are not thinned in the plot. if ( showHDI ) { HDIinfo = HDIofGrid( pTheta , credMass=HDImass ) points( Theta[ HDIinfo$indices ] , rep( HDIinfo$height , length( HDIinfo$indices ) ) , pch=&quot;-&quot; , cex=1.0 ) text( mean( Theta[ HDIinfo$indices ] ) , HDIinfo$height , bquote( .(100*signif(HDIinfo$mass,3)) * &quot;% HDI&quot; ) , adj=c(0.5,-1.5) , cex=1.5 ) # Mark the left and right ends of the waterline. # Find indices at ends of sub-intervals: inLim = HDIinfo$indices[1] # first point for ( idx in 2:(length(HDIinfo$indices)-1) ) { if ( ( HDIinfo$indices[idx] != HDIinfo$indices[idx-1]+1 ) | # jumps on left, OR ( HDIinfo$indices[idx] != HDIinfo$indices[idx+1]-1 ) ) { # jumps on right inLim = c(inLim,HDIinfo$indices[idx]) # include idx } } inLim = c(inLim,HDIinfo$indices[length(HDIinfo$indices)]) # last point # Mark vertical lines at ends of sub-intervals: for ( idx in inLim ) { lines( c(Theta[idx],Theta[idx]) , c(-0.5,HDIinfo$height) , type=&quot;l&quot; , lty=2 , lwd=1.5 ) text( Theta[idx] , HDIinfo$height , bquote(.(round(Theta[idx],3))) , adj=c(0.5,-0.1) , cex=1.2 ) } } # Plot the likelihood: p(Data|Theta) plot( Theta[thinIdx] , pDataGivenTheta[thinIdx] , type=plotType , pch=&quot;.&quot; , cex=dotsize , lwd=barsize , xlim=c(0,1) , ylim=c(0,1.1*max(pDataGivenTheta)) , cex.axis=cexAxis , xlab=bquote(theta) , ylab=bquote( &quot;p(D|&quot; * theta * &quot;)&quot; ) , cex.lab=cexLab , main=&quot;Likelihood&quot; , cex.main=1.5 , col=&quot;skyblue&quot; ) if ( z &gt; .5*N ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx ,1.0*max(pDataGivenTheta) ,cex=2.0 ,bquote( &quot;Data: z=&quot; * .(z) * &quot;,N=&quot; * .(N) ) ,adj=textadj ) if ( showCentTend != &quot;None&quot; ) { if ( showCentTend == &quot;Mean&quot; ) { meanTheta = sum( Theta * pDataGivenTheta ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , 0.7*max(pDataGivenTheta) , bquote( &quot;mean=&quot; * .(signif(meanTheta,3)) ) , cex=2.0 , adj=textadj ) } if ( showCentTend == &quot;Mode&quot; ) { modeTheta = Theta[ which.max( pDataGivenTheta ) ] if ( modeTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , 0.7*max(pDataGivenTheta) , bquote( &quot;mode=&quot; * .(signif(modeTheta,3)) ) , cex=2.0 , adj=textadj ) } } # Plot the posterior. yLim = c(0,1.1*max(c(pTheta,pThetaGivenData))) plot( Theta[thinIdx] , pThetaGivenData[thinIdx] , type=plotType , pch=&quot;.&quot; , cex=dotsize , lwd=barsize , xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis , xlab=bquote(theta) , ylab=bquote( &quot;p(&quot; * theta * &quot;|D)&quot; ) , cex.lab=cexLab , main=&quot;Posterior&quot; , cex.main=1.5 , col=&quot;skyblue&quot; ) if ( showCentTend != &quot;None&quot; ) { if ( showCentTend == &quot;Mean&quot; ) { meanTheta = sum( Theta * pThetaGivenData ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mean=&quot; * .(signif(meanTheta,3)) ) , cex=2.0 , adj=textadj ) } if ( showCentTend == &quot;Mode&quot; ) { modeTheta = Theta[ which.max( pThetaGivenData ) ] if ( modeTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mode=&quot; * .(signif(modeTheta,3)) ) , cex=2.0 , adj=textadj ) } } # Plot marginal likelihood pData: if ( showpD ) { meanTheta = sum( Theta * pThetaGivenData ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , 0.75*max(pThetaGivenData) , cex=2.0 , bquote( &quot;p(D)=&quot; * .(signif(pData,3)) ) ,adj=textadj ) } # Mark the highest density interval. HDI points are not thinned in the plot. if ( showHDI ) { HDIinfo = HDIofGrid( pThetaGivenData , credMass=HDImass ) points( Theta[ HDIinfo$indices ] , rep( HDIinfo$height , length( HDIinfo$indices ) ) , pch=&quot;-&quot; , cex=1.0 ) text( mean( Theta[ HDIinfo$indices ] ) , HDIinfo$height , bquote( .(100*signif(HDIinfo$mass,3)) * &quot;% HDI&quot; ) , adj=c(0.5,-1.5) , cex=1.5 ) # Mark the left and right ends of the waterline. # Find indices at ends of sub-intervals: inLim = HDIinfo$indices[1] # first point for ( idx in 2:(length(HDIinfo$indices)-1) ) { if ( ( HDIinfo$indices[idx] != HDIinfo$indices[idx-1]+1 ) | # jumps on left, OR ( HDIinfo$indices[idx] != HDIinfo$indices[idx+1]-1 ) ) { # jumps on right inLim = c(inLim,HDIinfo$indices[idx]) # include idx } } inLim = c(inLim,HDIinfo$indices[length(HDIinfo$indices)]) # last point # Mark vertical lines at ends of sub-intervals: for ( idx in inLim ) { lines( c(Theta[idx],Theta[idx]) , c(-0.5,HDIinfo$height) , type=&quot;l&quot; , lty=2 , lwd=1.5 ) text( Theta[idx] , HDIinfo$height , bquote(.(round(Theta[idx],3))) , adj=c(0.5,-0.1) , cex=1.2 ) } } # return( pThetaGivenData ) } # end of function You plot using Kruschke’s method, like so. Data &lt;- c(rep(0, 13), rep(1, 14)) BernGrid(theta, p_theta, Data, plotType = &quot;Bars&quot;, showCentTend = &quot;None&quot;, showHDI = FALSE, showpD = FALSE) The method works fine, but I’m not a fan. It’s clear Kruschke put a lot of thought into the BernGrid() function. However, its inner workings are too opaque, for me, which leads to our next section… 6.4.2.2 Figure 6.5 in tidyverse style. Here we’ll be plotting with ggplot2. But let’s first get the data into a tibble. # we need these to compute the likelihood n &lt;- 27 z &lt;- 14 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) # (i.e., Data) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000), # (i.e., Theta) Prior = c(rep(1, 200), # (i.e., pTheta) seq(1, 100, length = 50), seq(100, 1, length = 50), rep(1, 200)) %&gt;% rep(., times = 2)) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, # (i.e., pDataGivenTheta) data = trial_data)) %&gt;% mutate(evidence = sum(Likelihood * Prior)) %&gt;% # (i.e., pData) mutate(Posterior = Likelihood * Prior / evidence) # (i.e., pThetaGivenData) glimpse(d) ## Rows: 1,000 ## Columns: 5 ## $ theta &lt;dbl&gt; 0.000000000, 0.001001001, 0.002002002, 0.003003003, 0.00400… ## $ Prior &lt;dbl&gt; 9.174312e-05, 9.174312e-05, 9.174312e-05, 9.174312e-05, 9.1… ## $ Likelihood &lt;dbl&gt; 0.000000e+00, 1.000988e-42, 1.618784e-38, 4.664454e-36, 2.5… ## $ evidence &lt;dbl&gt; 3.546798e-10, 3.546798e-10, 3.546798e-10, 3.546798e-10, 3.5… ## $ Posterior &lt;dbl&gt; 0.000000e+00, 2.589202e-37, 4.187221e-33, 1.206529e-30, 6.6… With our nice tibble in hand, we’ll plot the prior, likelihood, and posterior one at a time. # prior (p1 &lt;- d %&gt;% ggplot(aes(x = theta, y = Prior)) + geom_area(fill = &quot;steelblue&quot;) + labs(title = &quot;Prior&quot;, x = expression(theta), y = expression(p(theta))) + theme_cowplot() ) # likelihood (p2 &lt;- d %&gt;% ggplot(aes(x = theta, y = Likelihood)) + geom_area(fill = &quot;steelblue&quot;) + labs(title = &quot;Likelihood&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) + theme_cowplot() ) # posterior (p3 &lt;- d %&gt;% ggplot(aes(x = theta, y = Posterior)) + geom_area(fill = &quot;steelblue&quot;) + labs(title = &quot;Posterior&quot;, x = expression(theta), y = expression(p(theta*&quot;|&quot;*D))) + theme_cowplot() ) Note how we saved each the plots as objects. There are many ways to combine multiple ggplots, such as stacking them one atop another like they’re presented in Figure 6.5. One of the earliest methods I learned was the good old multiplot() function. For a long time I relied on grid.arrange() from the gridExtra package (Auguie, 2017). But it’s hard to beat the elegant syntax from Thomas Lin Pedersen’s (2020) patchwork package. library(patchwork) p1 / p2 / p3 We could have taken this same approach to combine all our subplots from the three columns and three rows of Figure 6.4. You can learn more about how to use patchwork this way here. We’ll have many more opportunities to practice as we progress through the chapters. 6.5 Summary The main point of this chapter was to demonstrate how Bayesian inference works when Bayes’ rule can be solved analytically, using mathematics alone, without numerical approximation… Unfortunately, there are two severe limitations with this approach… Thus, although it is interesting and educational to see how Bayes’ rule can be solved analytically, we will have to abandon exact mathematical solutions when doing complex applications. We will instead use Markov chain Monte Carlo (MCMC) methods. (p. 139) And if you’re using this ebook, I imagine that’s exactly what you’re looking for. We want to use the power of a particular kind of MCMC, Hamiltonian Monte Carlo, through the interface of the brms package. Get excited. It’s coming. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.1.2 cowplot_1.1.1 forcats_0.5.1 stringr_1.4.1 ## [5] dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 ## [9] tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] lubridate_1.8.0 assertthat_0.2.1 digest_0.6.30 ## [4] utf8_1.2.2 R6_2.5.1 cellranger_1.1.0 ## [7] backports_1.4.1 reprex_2.0.2 evaluate_0.18 ## [10] httr_1.4.4 highr_0.9 pillar_1.8.1 ## [13] rlang_1.0.6 googlesheets4_1.0.1 readxl_1.4.1 ## [16] rstudioapi_0.13 jquerylib_0.1.4 rmarkdown_2.16 ## [19] labeling_0.4.2 googledrive_2.0.0 munsell_0.5.0 ## [22] broom_1.0.1 compiler_4.2.0 modelr_0.1.8 ## [25] xfun_0.35 pkgconfig_2.0.3 htmltools_0.5.3 ## [28] tidyselect_1.1.2 bookdown_0.28 fansi_1.0.3 ## [31] crayon_1.5.2 tzdb_0.3.0 dbplyr_2.2.1 ## [34] withr_2.5.0 grid_4.2.0 jsonlite_1.8.3 ## [37] gtable_0.3.1 lifecycle_1.0.3 DBI_1.1.3 ## [40] magrittr_2.0.3 scales_1.2.1 cli_3.5.0 ## [43] stringi_1.7.8 cachem_1.0.6 farver_2.1.1 ## [46] fs_1.5.2 xml2_1.3.3 bslib_0.4.0 ## [49] ellipsis_0.3.2 generics_0.1.3 vctrs_0.5.1 ## [52] tools_4.2.0 glue_1.6.2 hms_1.1.1 ## [55] fastmap_1.1.0 colorspace_2.0-3 gargle_1.2.0 ## [58] rvest_1.0.2 knitr_1.40 haven_2.5.1 ## [61] sass_0.4.2 References Auguie, B. (2017). gridExtra: Miscellaneous functions for \"grid\" graphics. https://CRAN.R-project.org/package=gridExtra Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Pedersen, Thomas Lin. (2020). patchwork: The composer of plots. https://CRAN.R-project.org/package=patchwork Wilke, C. O. (2020). cowplot: Streamlined plot theme and plot annotations for ggplot2 [Manual]. https://wilkelab.org/cowplot/ "],["markov-chain-monte-carlo.html", "7 Markov Chain Monte Carlo 7.1 Approximating a distribution with a large sample 7.2 A simple case of the Metropolis algorithm 7.3 The Metropolis algorithm more generally 7.4 Toward Gibbs sampling: Estimating two coin biases 7.5 MCMC representativeness, accuracy, and efficiency 7.6 Summary Session info Footnote", " 7 Markov Chain Monte Carlo This chapter introduces the methods we will use for producing accurate approximations to Bayesian posterior distributions for realistic applications. The class of methods is called Markov chain Monte Carlo (MCMC), for reasons that will be explained later in the chapter. It is MCMC algorithms and software, along with fast computer hardware, that allow us to do Bayesian data analysis for realistic applications that would have been effectively impossible \\(30\\) years ago. (Kruschke, 2015, p. 144) Statistician David Draper covered some of the history of MCMC in his lecture, Bayesian Statistical Reasoning. 7.1 Approximating a distribution with a large sample The concept of representing a distribution by a large representative sample is foundational for the approach we take to Bayesian analysis of complex models. The idea is applied intuitively and routinely in everyday life and in science. For example, polls and surveys are founded on this concept: By randomly sampling a subset of people from a population, we estimate the underlying tendencies in the entire population. The larger the sample, the better the estimation. What is new in the present application is that the population from which we are sampling is a mathematically defined distribution, such as a posterior probability distribution. (p. 145) Like in Chapters 4 and 6, we need to define the hdi_of_icdf() function. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Our hdi_of_icdf() function will compute the analytic 95% highest density intervals (HDIs) for the distribution under consideration in Figure 7.1, \\(\\operatorname{Beta}(\\theta | 15, 7)\\). h &lt;- hdi_of_icdf(name = qbeta, shape1 = 15, shape2 = 7) h ## [1] 0.4907001 0.8639305 Using an equation from Chapter 6, \\(\\omega = (a − 1) / (a + b − 2)\\), we can compute the corresponding mode. (omega &lt;- (15 - 1) / (15 + 7 - 2)) ## [1] 0.7 To get the density in the upper left panel of Figure 7.1, we’ll make use of the dbeta() function and of our h[1:2] and omega values. library(tidyverse) library(cowplot) tibble(theta = seq(from = 0, to = 1, length.out = 100)) %&gt;% mutate(density = dbeta(theta, shape1 = 15, shape2 = 7)) %&gt;% ggplot() + geom_area(aes(x = theta, y = density), fill = &quot;steelblue&quot;) + geom_segment(aes(x = h[1], xend = h[2], y = 0, yend = 0), size = .75) + geom_point(aes(x = omega, y = 0), size = 1.5, shape = 19) + annotate(geom = &quot;text&quot;, x = .675, y = .4, label = &quot;95% HDI&quot;, color = &quot;white&quot;) + scale_x_continuous(expression(theta), breaks = c(0, h, omega, 1), labels = c(&quot;0&quot;, h %&gt;% round(2), omega, &quot;1&quot;)) + ggtitle(&quot;Exact distribution&quot;) + ylab(expression(p(theta))) + theme_cowplot() Note how we’re continuing to use theme_cowplot(), which we introduced in the last chapter. The remaining panels in Figure 7.1 require we simulate the data. set.seed(7) d &lt;- tibble(n = c(500, 5000, 50000)) %&gt;% mutate(theta = map(n, ~rbeta(., shape1 = 15, shape2 = 7))) %&gt;% unnest(theta) %&gt;% mutate(key = str_c(&quot;Sample N = &quot;, n)) head(d) ## # A tibble: 6 × 3 ## n theta key ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 500 0.806 Sample N = 500 ## 2 500 0.756 Sample N = 500 ## 3 500 0.727 Sample N = 500 ## 4 500 0.784 Sample N = 500 ## 5 500 0.782 Sample N = 500 ## 6 500 0.590 Sample N = 500 With the data in hand, we’re ready to plot the remaining panels for Figure 7.1. This time, we’ll use the handy stat_pointinterval() function from the tidybayes package to mark off the mode and 95% HDIs. library(tidybayes) d %&gt;% ggplot(aes(x = theta, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, breaks = 30, fill = &quot;steelblue&quot;) + scale_x_continuous(expression(theta), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme_cowplot() + facet_wrap(~ key, ncol = 3, scales = &quot;free&quot;) If we want the exact values for the mode and 95% HDIs, we can use the tidybayes::mode_hdi() function. d %&gt;% group_by(key) %&gt;% mode_hdi(theta) ## # A tibble: 3 × 7 ## key theta .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Sample N = 500 0.695 0.511 0.868 0.95 mode hdi ## 2 Sample N = 5000 0.688 0.497 0.870 0.95 mode hdi ## 3 Sample N = 50000 0.710 0.490 0.863 0.95 mode hdi If you wanted a better sense of the phenomena, you could do a simulation. We’ll make a custom simulation function to compute the modes from many random draws from our \\(\\operatorname{Beta}(\\theta | 15, 7)\\) distribution, with varying \\(N\\) values. my_mode_simulation &lt;- function(seed) { set.seed(seed) tibble(n = c(500, 5000, 50000)) %&gt;% mutate(theta = map(n, ~rbeta(., shape1 = 15, shape2 = 7))) %&gt;% unnest(theta) %&gt;% mutate(key = str_c(&quot;Sample N = &quot;, n)) %&gt;% group_by(key) %&gt;% mode_hdi(theta) } Here we put our my_mode_simulation() function to work. # we need an index of the values we set our seed with in our `my_mode_simulation()` function sim &lt;- tibble(seed = 1:1e3) %&gt;% group_by(seed) %&gt;% # inserting our subsamples mutate(modes = map(seed, my_mode_simulation)) %&gt;% # unnesting allows us to access our model results unnest(modes) sim %&gt;% ggplot(aes(x = theta, y = key)) + geom_vline(xintercept = .7, color = &quot;white&quot;) + stat_histinterval(.width = c(.5, .95), breaks = 20, fill = &quot;steelblue&quot;) + labs(title = expression(&quot;Variability of the mode for simulations of &quot;*beta(theta*&#39;|&#39;*15*&#39;, &#39;*7)*&quot;, the true mode of which is .7&quot;), subtitle = &quot;For each sample size, the dot is the median, the inner thick line is the percentile-based 50% interval,\\nand the outer thin line the percentile-based 95% interval. Although the central tendency\\napproximates the true value for all three conditions, the variability of the mode estimate is inversely\\nrelated to the sample size.&quot;, x = &quot;mode&quot;, y = NULL) + coord_cartesian(xlim = c(.6, .8), ylim = c(1.25, 3.5)) + theme_cowplot(font_size = 11.5) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) 7.2 A simple case of the Metropolis algorithm Our goal in Bayesian inference is to get an accurate representation of the posterior distribution. One way to do that is to sample a large number of representative points from the posterior. The question then becomes this: How can we sample a large number of representative values from a distribution? (p. 146). The answer, my friends, is MCMC. 7.2.1 A politician stumbles upon the Metropolis algorithm. I’m not going to walk out Kruschke’s politician example in any detail, here. But if we denote \\(P_\\text{proposed}\\) as the population of the proposed island and \\(P_\\text{current}\\) as the population of the current island, then \\[p_\\text{move} = \\frac{P_\\text{proposed}}{P_\\text{current}}.\\] “What’s amazing about this heuristic is that it works: In the long run, the probability that the politician is on any one of the islands exactly matches the relative population of the island” (p. 147)! 7.2.2 A random walk. The code below will allow us to reproduce Kruschke’s random walk. To give credit where it’s due, this is a mild amendment to the code from Chapter 8 of McElreath’s (2015) text, Statistical rethinking: A Bayesian course with examples in R and Stan. set.seed(7) num_days &lt;- 5e4 positions &lt;- rep(0, num_days) current &lt;- 4 for (i in 1:num_days) { # record current position positions[i] &lt;- current # flip coin to generate proposal proposal &lt;- current + sample(c(-1, 1), size = 1) # now make sure he loops around from 7 back to 1 if (proposal &lt; 1) proposal &lt;- 7 if (proposal &gt; 7) proposal &lt;- 1 # move? prob_accept_the_proposal &lt;- proposal/current current &lt;- ifelse(runif(1) &lt; prob_accept_the_proposal, proposal, current) } If you missed it, positions is the main product of our simulation. Here we’ll put positions in a tibble and reproduce the top portion of Figure 7.2. tibble(theta = positions) %&gt;% ggplot(aes(x = theta)) + geom_bar(fill = &quot;steelblue&quot;) + scale_x_continuous(expression(theta), breaks = 1:7) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + theme_cowplot() Did you notice that scale_y_continuous() line in the code? Claus Wilke, the author of the cowplot package, has a lot of thoughts on data visualization. He even wrote a (2019) book on it: Fundamentals of data visualization. In his (2020) Themes vignette, Wilke recommended against allowing for space between the bottoms of the bars in a bar plot and the \\(x\\)-axis line. The ggplot2 default is to allow for such a space. Here we followed Wilke and suppressed that space with expand = expansion(mult = c(0, 0.05)). You can learn more about the ggplot2::expansion() function here. Here’s the middle portion of Figure 7.2. tibble(t = 1:5e4, theta = positions) %&gt;% slice(1:500) %&gt;% ggplot(aes(x = theta, y = t)) + geom_path(size = 1/4, color = &quot;steelblue&quot;) + geom_point(size = 1/2, alpha = 1/2, color = &quot;steelblue&quot;) + scale_x_continuous(expression(theta), breaks = 1:7) + scale_y_log10(&quot;Time Step&quot;, breaks = c(1, 2, 5, 20, 100, 500)) + theme_cowplot() And now we make the bottom. tibble(x = 1:7, y = 1:7) %&gt;% ggplot(aes(x = x, y = y)) + geom_col(width = .2, fill = &quot;steelblue&quot;) + scale_x_continuous(expression(theta), breaks = 1:7) + scale_y_continuous(expression(p(theta)), expand = expansion(mult = c(0, 0.05))) + theme_cowplot() Notice that the sampled relative frequencies closely mimic the actual relative populations in the bottom panel! In fact, a sequence generated this way will converge, as the sequence gets longer, to an arbitrarily close approximation of the actual relative probabilities. (p. 149) 7.2.3 General properties of a random walk. The tajectory shown in Figure 7.2 is just one possible sequence of positions when the movement heuristic is applied. At each time step, the direction of the proposed move is random, and if the relative probability of the proposed position is less than that of the current position, then acceptance of the proposed move is also random. Because of the randomness, if the process were started over again, then the specific trajectory would almost certainly be different. Regardless of the specific trajectory, in the long run the relative frequency of visits mimics the target distribution. Figure 7.3 shows the probability of being in each position as a function of time. (p. 149) I was initially stumped on how to reproduce the simulation depicted in Figure 7.3. However, fellow enthusiast Cardy Moten III kindly shared a solution which was itself based on Kruschke’s blog post from 2012, Metropolis algorithm: Discrete position probabilities. Here’s a mild reworking of their solutions. First, we simulate. nslots &lt;- 7 p_target &lt;- 1:7 p_target &lt;- p_target / sum(p_target) # construct the transition matrix proposal_matrix &lt;- matrix(0, nrow = nslots, ncol = nslots) for(from_idx in 1:nslots) { for(to_idx in 1:nslots) { if(to_idx == from_idx - 1) {proposal_matrix[from_idx, to_idx] &lt;- 0.5} if(to_idx == from_idx + 1) {proposal_matrix[from_idx, to_idx] &lt;- 0.5} } } # construct the acceptance matrix acceptance_matrix &lt;- matrix(0, nrow = nslots, ncol = nslots) for(from_idx in 1:nslots) { for(to_idx in 1:nslots) { acceptance_matrix[from_idx, to_idx] &lt;- min(p_target[to_idx] / p_target[from_idx], 1) } } # compute the matrix of move probabilities move_matrix &lt;- proposal_matrix * acceptance_matrix # compute the transition matrix, including the probability of staying in place transition_matrix &lt;- move_matrix for (diag_idx in 1:nslots) { transition_matrix[diag_idx, diag_idx] = 1.0 - sum(move_matrix[diag_idx, ]) } # specify starting position vector: position_vec &lt;- rep(0, nslots) position_vec[round(nslots / 2)] &lt;- 1.0 p &lt;- list() data &lt;- tibble(position = 1:nslots, prob = position_vec) # loop through the requisite time indexes # update the data and transition vector for(time_idx in 1:99) { p[[time_idx]] &lt;- data # update the position vec position_vec &lt;- position_vec %*% transition_matrix # update the data data &lt;- NULL data &lt;- tibble(position = 1:nslots, prob = t(position_vec)) } Now we wrangle and plot. p %&gt;% as_tibble_col() %&gt;% mutate(facet = str_c(&quot;italic(t)==&quot;, 1:99)) %&gt;% slice(c(1:14, 99)) %&gt;% unnest(value) %&gt;% bind_rows( tibble(position = 1:nslots, prob = p_target, facet = &quot;target&quot;) ) %&gt;% mutate(facet = factor(facet, levels = c(str_c(&quot;italic(t)==&quot;, c(1:14, 99)), &quot;target&quot;))) %&gt;% # plot! ggplot(aes(x = position, y = prob, fill = facet == &quot;target&quot;)) + geom_col(width = .2) + scale_fill_manual(values = c(&quot;steelblue&quot;, &quot;goldenrod2&quot;), breaks = NULL) + scale_x_continuous(expression(theta), breaks = 1:7) + scale_y_continuous(expression(italic(p)(theta)), expand = expansion(mult = c(0, 0.05))) + theme_cowplot() + facet_wrap(~ facet, scales = &quot;free_y&quot;, labeller = label_parsed) 7.2.4 Why we care. Through the simple magic of the random walk procedure, we are able to do indirectly something we could not necessarily do directly: We can generate random samples from the target distribution. Moreover, we can generate those random samples from the target distribution even when the target distribution is not normalized. This technique is profoundly useful when the target distribution \\(P(\\theta)\\) is a posterior proportional to \\(p(D | \\theta) p(\\theta)\\). Merely by evaluating \\(p(D | \\theta) p(\\theta)\\), without normalizing it by \\(p(D)\\), we can generate random representative values from the posterior distribution. This result is wonderful because the method obviates direct computation of the evidence \\(p(D)\\), which, as you’ll recall, is one of the most difficult aspects of Bayesian inference. By using MCMC techniques, we can do Bayesian inference in rich and complex models. It has only been with the development of MCMC algorithms and software that Bayesian inference is applicable to complex data analysis, and it has only been with the production of fast and cheap computer hardware that Bayesian inference is accessible to a wide audience. (p. 152, emphasis in the original) 7.3 The Metropolis algorithm more generally “The procedure described in the previous section was just a special case of a more general procedure known as the Metropolis algorithm, named after the first author of a famous article (Metropolis et al., 1953)” (p. 156). Here’s how to generate a proposed jump from a zero-mean normal distribution with a standard deviation of 0.2. rnorm(1, mean = 0, sd = 0.2) ## [1] -0.1985524 To get a sense of what draws from rnorm() looks like in the long run, we might plot. mu &lt;- 0 sigma &lt;- 0.2 # how many proposals would you like? n &lt;- 500 set.seed(7) tibble(proposed_jump = rnorm(n, mean = mu, sd = sigma)) %&gt;% ggplot(aes(x = proposed_jump, y = 0)) + geom_jitter(width = 0, height = .1, size = 1/2, alpha = 1/2, color = &quot;steelblue&quot;) + # this is the idealized distribution stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), color = &quot;steelblue&quot;) + scale_x_continuous(breaks = seq(from = -0.6, to = 0.6, length.out = 7)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Jump proposals&quot;, subtitle = &quot;The blue line shows the data generating distribution.&quot;) + theme_cowplot() Anyway, having generated a proposed new position, the algorithm then decides whether or not to accept the proposal. The decision rule is exactly what was already specified in Equation 7.1. In detail, this is accomplished by computing the ratio \\(p_\\text{move} = P(\\theta_\\text{proposed}) / P(\\theta_\\text{current})\\). Then a random number from the uniform interval \\([0, 1]\\) is generated; in R, this can be accomplished with the command runif(1). If the random number is between \\(0\\) and pmove, then the move is accepted. (p. 157) We’ll see what that might look like in the next section. In the meantime, here’s how to use runif(). runif(1) ## [1] 0.2783186 Just for kicks, here’s what that looks like in bulk. # how many proposals would you like? n &lt;- 500 set.seed(7) tibble(draw = runif(n)) %&gt;% ggplot(aes(x = draw, y = 0)) + geom_jitter(width = 0, height = 1/4, size = 1/2, alpha = 1/2, color = &quot;steelblue&quot;) + stat_function(fun = dunif, color = &quot;steelblue&quot;) + scale_y_continuous(NULL, breaks = NULL, limits = c(-1/3, 5/3)) + labs(title = &quot;Uniform draws&quot;, subtitle = &quot;The blue line shows the data generating distribution.&quot;) + theme_cowplot() We do not see a concentration towards the mean, this time. The draws are uniformly distributed across the parameter space. 7.3.1 Metropolis algorithm applied to Bernoulli likelihood and beta prior. You can find Kruschke’s code in the BernMetrop.R file. I’m going to break it up a little. # specify the data, to be used in the likelihood function. my_data &lt;- c(rep(0, 6), rep(1, 14)) # define the Bernoulli likelihood function, p(D|theta). # the argument theta could be a vector, not just a scalar likelihood &lt;- function(theta, data) { z &lt;- sum(data) n &lt;- length(data) p_data_given_theta &lt;- theta^z * (1 - theta)^(n - z) # the theta values passed into this function are generated at random, # and therefore might be inadvertently greater than 1 or less than 0. # the likelihood for theta &gt; 1 or for theta &lt; 0 is zero p_data_given_theta[theta &gt; 1 | theta &lt; 0] &lt;- 0 return(p_data_given_theta) } # define the prior density function. prior_d &lt;- function(theta) { p_theta &lt;- dbeta(theta, 1, 1) # the theta values passed into this function are generated at random, # and therefore might be inadvertently greater than 1 or less than 0. # the prior for theta &gt; 1 or for theta &lt; 0 is zero p_theta[theta &gt; 1 | theta &lt; 0] = 0 return(p_theta) } # define the relative probability of the target distribution, # as a function of vector theta. for our application, this # target distribution is the unnormalized posterior distribution target_rel_prob &lt;- function(theta, data) { target_rel_prob &lt;- likelihood(theta, data) * prior_d(theta) return(target_rel_prob) } # specify the length of the trajectory, i.e., the number of jumps to try: traj_length &lt;- 50000 # this is just an arbitrary large number # initialize the vector that will store the results trajectory &lt;- rep(0, traj_length) # specify where to start the trajectory: trajectory[1] &lt;- 0.01 # another arbitrary value # specify the burn-in period burn_in &lt;- ceiling(0.0 * traj_length) # arbitrary number, less than `traj_length` # initialize accepted, rejected counters, just to monitor performance: n_accepted &lt;- 0 n_rejected &lt;- 0 That first part follows what Kruschke put in his script. I’m going to bundel the next large potion in a fucntion, my_metropolis() which will make it easier to plug the code into the purrr::map() function. my_metropolis &lt;- function(proposal_sd) { # now generate the random walk. the &#39;t&#39; index is time or trial in the walk. # specify seed to reproduce same random walk set.seed(47405) ## I&#39;m taking this section out and will replace it # # specify standard deviation of proposal distribution # proposal_sd &lt;- c(0.02, 0.2, 2.0)[2] ## end of the section I took out for (t in 1:(traj_length - 1)) { current_position &lt;- trajectory[t] # use the proposal distribution to generate a proposed jump proposed_jump &lt;- rnorm(1, mean = 0, sd = proposal_sd) # compute the probability of accepting the proposed jump prob_accept &lt;- min(1, target_rel_prob(current_position + proposed_jump, my_data) / target_rel_prob(current_position, my_data)) # generate a random uniform value from the interval [0, 1] to # decide whether or not to accept the proposed jump if (runif(1) &lt; prob_accept) { # accept the proposed jump trajectory[t + 1] &lt;- current_position + proposed_jump # increment the accepted counter, just to monitor performance if (t &gt; burn_in) {n_accepted &lt;- n_accepted + 1} } else { # reject the proposed jump, stay at current position trajectory[t + 1] &lt;- current_position # increment the rejected counter, just to monitor performance if (t &gt; burn_in) {n_rejected &lt;- n_rejected + 1} } } # extract the post-burn_in portion of the trajectory accepted_traj &lt;- trajectory[(burn_in + 1) : length(trajectory)] tibble(accepted_traj = accepted_traj, n_accepted = n_accepted, n_rejected = n_rejected) # end of Metropolis algorithm } Now we have my_metropolis(), we can run the analysis based on the three proposal_sd values, nesting the results in a tibble. d &lt;- tibble(proposal_sd = c(0.02, 0.2, 2.0)) %&gt;% mutate(accepted_traj = map(proposal_sd, my_metropolis)) %&gt;% unnest(accepted_traj) glimpse(d) ## Rows: 150,000 ## Columns: 4 ## $ proposal_sd &lt;dbl&gt; 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02… ## $ accepted_traj &lt;dbl&gt; 0.01000000, 0.01000000, 0.01000000, 0.01000000, 0.01149173, 0.02550380, 0.02… ## $ n_accepted &lt;dbl&gt; 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801,… ## $ n_rejected &lt;dbl&gt; 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198… Now we have d in hand, here’s the top portion of Figure 7.4. d &lt;- d %&gt;% mutate(proposal_sd = str_c(&quot;Proposal SD = &quot;, proposal_sd), iter = rep(1:50000, times = 3)) d %&gt;% ggplot(aes(x = accepted_traj, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = &quot;steelblue&quot;, slab_color = &quot;white&quot;, outline_bars = T, breaks = 40, normalize = &quot;panels&quot;) + scale_x_continuous(expression(theta), breaks = 0:5 * 0.2) + scale_y_continuous(NULL, breaks = NULL) + theme_cowplot() + panel_border() + facet_wrap(~ proposal_sd, ncol = 3) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. The modes are the points and the lines depict the 95% HDIs. Also, did you notice our use of the cowplot::panel_border() function? The settings from theme_cowplot() can make it difficult to differentiate among subplots when faceting. By throwing in a call to panel_border() after theme_cowplot(), we added in lightweight panel borders. Here’s the middle of Figure 7.4. d %&gt;% ggplot(aes(x = accepted_traj, y = iter)) + geom_path(size = 1/4, color = &quot;steelblue&quot;) + geom_point(size = 1/2, alpha = 1/2, color = &quot;steelblue&quot;) + scale_x_continuous(expression(theta), breaks = 0:5 * 0.2, limits = c(0, 1)) + scale_y_continuous(&quot;Step in Chain&quot;, limits = c(49900, 50000)) + ggtitle(&quot;End of Chain&quot;) + theme_cowplot() + panel_border() + facet_wrap(~ proposal_sd, ncol = 3) The bottom: d %&gt;% ggplot(aes(x = accepted_traj, y = iter)) + geom_path(size = 1/4, color = &quot;steelblue&quot;) + geom_point(size = 1/2, alpha = 1/2, color = &quot;steelblue&quot;) + scale_x_continuous(expression(theta), breaks = 0:5 * 0.2, limits = c(0, 1)) + scale_y_continuous(&quot;Step in Chain&quot;, limits = c(1, 100)) + ggtitle(&quot;End of Chain&quot;) + theme_cowplot() + panel_border() + facet_wrap(~ proposal_sd, ncol = 3) Regardless of the which proposal distribution in Figure 7.4 is used, the Metropolis algorithm will eventually produce an accurate representation of the posterior distribution, as is suggested by the histograms in the upper row of Figure 7.4. What differs is the efficiency of achieving a good approximation. (p. 160) Before we move on, you may have noticed the top row of Kruschke’s Figure 7.4 contains the estimates for the effective sample size (Eff.Sz.) for each chain, which Kruschke briefly mentioned on page 160. We’ll walk out effective sample sizes, later. But for now we can compute them with some of the helper functions from the posterior package (Bürkner et al., 2021). As it turns out, there are several kinds of effective sample size. With regards to the helper functions from posterior, I suspect the one closest to what Kruschke used in the text will be ess_mean(). Here’s how you might use ess_mean() to compute the Eff.Sz. estimate for each of our three chains. d %&gt;% select(proposal_sd, accepted_traj) %&gt;% group_by(proposal_sd) %&gt;% summarise(Eff.Sz. = posterior::ess_mean(accepted_traj)) ## # A tibble: 3 × 2 ## proposal_sd Eff.Sz. ## &lt;chr&gt; &lt;dbl&gt; ## 1 Proposal SD = 0.02 500. ## 2 Proposal SD = 0.2 11340. ## 3 Proposal SD = 2 2105. 7.3.2 Summary of Metropolis algorithm. The motivation for methods like the Metropolis algorithm is that they provide a high-resolution picture of the posterior distribution, even though in complex models we cannot explicitly solve the mathematical integral in Bayes’ rule. The idea is that we get a handle on the posterior distribution by generating a large sample of representative values. The larger the sample, the more accurate is our approximation. As emphasized previously, this is a sample of representative credible parameter values from the posterior distribution; it is not a resampling of data (there is a fixed data set). The cleverness of the method is that representative parameter values can be randomly sampled from complicated posterior distributions without solving the integral in Bayes’ rule, and by using only simple proposal distributions for which efficient random number generators already exist. (p. 161) 7.4 Toward Gibbs sampling: Estimating two coin biases “The Metropolis method is very useful, but it can be inefficient. Other methods can be more efficient in some situations” (p. 162). 7.4.1 Prior, likelihood and posterior for two biases. We are considering situations in which there are two underlying biases, namely \\(\\theta_1\\) and \\(\\theta_2\\), for the two coins. We are trying to determine what we should believe about these biases after we have observed some data from the two coins. Recall that [Kruschke used] the term “bias” as the name of the parameter \\(\\theta\\), and not to indicate that the value of \\(\\theta\\) deviates from \\(0.5\\)…. What we have to do next is specify a particular mathematical form for the prior distribution. We will work through the mathematics of a particular case for two reasons: First, it will allow us to explore graphical displays of two-dimensional parameter spaces, which will inform our intuitions about Bayes’ rule and sampling from the posterior distribution. Second, the mathematics will set the stage for a specific example of Gibbs sampling. Later in the book when we do applied Bayesian analysis, we will not be doing any of this sort of mathematics. We are doing the math now, for simple cases, to understand how the methods work so we can properly interpret their outputs in realistically complex cases. (pp. 163–165, emphasis in the original) 7.4.2 The posterior via exact formal analysis. The plots in the left column of Figure 7.5 are outside of my skill set. I believe they are referred to as wireframe plots and it’s my understanding that ggplot2 does not support wireframe plots at this time. However, I can reproduce versions of the right hand column. For our initial attempt for the upper right corner, we’ll simulate. set.seed(7) betas &lt;- tibble(theta_1 = rbeta(1e5, shape1 = 2, shape2 = 2), theta_2 = rbeta(1e5, shape1 = 2, shape2 = 2)) betas %&gt;% ggplot(aes(x = theta_1, y = theta_2)) + stat_density_2d() + labs(x = expression(theta[1]), y = expression(theta[2])) + coord_equal() + theme_cowplot() Instead of the contour lines, one might use color to depict the density variable. betas %&gt;% ggplot(aes(x = theta_1, y = theta_2, fill = stat(density))) + stat_density_2d(geom = &quot;raster&quot;, contour = F) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(theta[2])) + coord_equal() + theme_cowplot() ## Warning: `stat(density)` was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. Remember how we talked about suppressing the unsightly white space between the bottom of bar-plot bars and the \\(x\\)-axis? Well, look at all that unsightly white space between the axes and the boundaries of the parameter space in our bivariate Beta plot. We can further flex our expansion() skills to get rid of those in the next plot. Speaking of which, we might make a more precise version of that plot with the dbeta() function. This approach is also more in line with the title of this subsection: The posterior via exact formal analysis. theta_sequence &lt;- seq(from = 0, to = 1, by = .01) tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(prior_1 = dbeta(x = theta_1, shape1 = 2, shape2 = 2), prior_2 = dbeta(x = theta_2, shape1 = 2, shape2 = 2)) %&gt;% expand(nesting(theta_1, prior_1), nesting(theta_2, prior_2)) %&gt;% ggplot(aes(x = theta_1, y = theta_2, fill = prior_1 * prior_2)) + geom_tile() + scale_fill_viridis_c(&quot;joint prior density&quot;, option = &quot;A&quot;) + scale_x_continuous(expression(theta[1]), expand = expansion(mult = 0)) + scale_y_continuous(expression(theta[2]), expand = expansion(mult = 0)) + coord_equal() + theme_cowplot() Look at that–no more unsightly white space! We’ll need the bernoulli_likelihood() function from back in Chapter 6 for the middle right of Figure 7.5. bernoulli_likelihood &lt;- function(theta, data) { # theta = success probability parameter ranging from 0 to 1 # data = the vector of data (i.e., a series of 0&#39;s and 1&#39;s) n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } With our trusty bernoulli_likelihood() function in hand, we’re almost ready to compute and plot the likelihood. We just need to define our data. # set the parameters # coin 1 n1 &lt;- 8 z1 &lt;- 6 # coin 2 n2 &lt;- 7 z2 &lt;- 2 # use the parameters to make the data theta_1_data &lt;- rep(0:1, times = c(n1 - z1, z1)) theta_2_data &lt;- rep(0:1, times = c(n2 - z2, z2)) # take a look theta_1_data ## [1] 0 0 1 1 1 1 1 1 theta_2_data ## [1] 0 0 0 0 0 1 1 Note how these data sequences are of different sample sizes \\((N_1 = 8; N_2 = 7 )\\). Though it doesn’t matter much for the formal analysis approach, this will become very important when we fit the model with brms. But for right now, we’re finally ready to make a version of the middle right panel of Figure 7.5. tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_sequence, data = theta_1_data), likelihood_2 = bernoulli_likelihood(theta = theta_sequence, data = theta_2_data)) %&gt;% expand(nesting(theta_1, likelihood_1), nesting(theta_2, likelihood_2)) %&gt;% ggplot(aes(x = theta_1, y = theta_2, fill = likelihood_1 * likelihood_2)) + geom_tile() + scale_fill_viridis_c(&quot;joint likelihood&quot;, option = &quot;A&quot;) + scale_x_continuous(expression(theta[1]), expand = expansion(mult = 0)) + scale_y_continuous(expression(theta[2]), expand = expansion(mult = 0)) + coord_equal() + theme_cowplot() Here’s how to make the two-dimensional posterior in the lower right panel of Figure 7.5. # this is a redo from two plots up, but saved as `d_prior` d_prior &lt;- tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(prior_1 = dbeta(x = theta_1, shape1 = 2, shape2 = 2), prior_2 = dbeta(x = theta_2, shape1 = 2, shape2 = 2)) %&gt;% expand(nesting(theta_1, prior_1), nesting(theta_2, prior_2)) # this is a redo from one plot up, but saved as `d_likelihood` d_likelihood &lt;- tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_sequence, data = theta_1_data), likelihood_2 = bernoulli_likelihood(theta = theta_sequence, data = theta_2_data)) %&gt;% expand(nesting(theta_1, likelihood_1), nesting(theta_2, likelihood_2)) # here we combine `d_prior` and `d_likelihood` d_prior %&gt;% left_join(d_likelihood, by = c(&quot;theta_1&quot;, &quot;theta_2&quot;)) %&gt;% # we need the marginal likelihood, the denominator in Bayes&#39; rule mutate(marginal_likelihood = sum(prior_1 * prior_2 * likelihood_1 * likelihood_2)) %&gt;% # finally, the two-dimensional posterior mutate(posterior = (prior_1 * prior_2 * likelihood_1 * likelihood_2) / marginal_likelihood) %&gt;% # plot! ggplot(aes(x = theta_1, y = theta_2, fill = posterior)) + geom_tile() + scale_fill_viridis_c(expression(italic(p)(theta[1]*&#39;, &#39;*theta[2]*&#39;|&#39;*D)), option = &quot;A&quot;) + scale_x_continuous(expression(theta[1]), expand = expansion(mult = 0)) + scale_y_continuous(expression(theta[2]), expand = expansion(mult = 0)) + coord_equal() + theme_cowplot() That last plot, my friends, is a depiction of \\[p(\\theta_1, \\theta_2 | D) = \\frac{p(D | \\theta_1, \\theta_2) p(\\theta_1, \\theta_2)}{p(D)}.\\] 7.4.3 The posterior via the Metropolis algorithm. I initially skipped over this section because the purpose of this book is to explore Kruschke’s material with brms, which does not use the Metropolis algorithm (which really is primarily of historic interest, at this point). However, fellow enthusiast Omid Ghasemi worked it through and kindly shared his solution. The workflow, below, is based heavily on his, with a few small adjustments. To start off, we’ll refresh our two data sources and define a few custom functions. # we&#39;ve already defined these, but here they are again theta_1_data &lt;- rep(0:1, times = c(n1 - z1, z1)) theta_2_data &lt;- rep(0:1, times = c(n2 - z2, z2)) # define the bivariate Bernoulli likelihood bivariate_bernoulli_likelihood &lt;- function(theta1, data1, theta2, data2) { z1 &lt;- sum(data1) n1 &lt;- length(data1) z2 &lt;- sum(data2) n2 &lt;- length(data2) p_data_given_theta &lt;- (theta1^z1 * (1 - theta1)^(n1 - z1)) * (theta2^z2 * (1 - theta2)^(n2 - z2)) p_data_given_theta[theta1 &gt; 1 | theta1 &lt; 0] &lt;- 0 p_data_given_theta[theta2 &gt; 1 | theta2 &lt; 0] &lt;- 0 return(p_data_given_theta) } # we need to update the prior density function from above prior_d &lt;- function(theta1, theta2) { p_theta &lt;- dbeta(theta1, 1, 1) * dbeta(theta2, 1, 1) p_theta[theta1 &gt; 1 | theta1 &lt; 0] = 0 p_theta[theta2 &gt; 1 | theta2 &lt; 0] = 0 return(p_theta) } # we also need to update how we define the relative probability of the target distribution target_rel_prob &lt;- function(theta1, data1, theta2, data2) { l &lt;- bivariate_bernoulli_likelihood(theta1, data1, theta2, data2) p &lt;- prior_d(theta1, theta2) target_rel_prob &lt;- l * p return(target_rel_prob) } The next bit defines how we’ll apply the Metropolis algorithm to our bivariate Bernoulli data. Although the guts contain a lot of moving parts, there are only two parameters at the top level. The traj_length argument is set to 50,000, which will be our default number of MCMC draws. Of greater interest is the proposal_sd argument. From the text, we read: Recall that the Metropolis algorithm is a random walk through the parameter space that starts at some arbitrary point. We propose a jump to a new point in parameter space, with the proposed jump randomly generated from a proposal distribution from which it is easy to generate values. For our present purposes, the proposal distribution is a bivariate normal. (p. 168, emphasis in the original) For this exercise, the bivariate normal proposal distribution is centered at zero with an adjustable standard deviation. In the text, Kruschke compared the results for \\(\\operatorname{Normal}(0, 0.02)\\) and \\(\\operatorname{Normal}(0, 0.2)\\). For our my_bivariate_metropolis() function, the proposal_sd argument controls that \\(\\sigma\\) parameter. my_bivariate_metropolis &lt;- function(proposal_sd = 0.02, # specify the length of the trajectory (i.e., the number of jumps to try) traj_length = 50000) { # initialize the vector that will store the results trajectory1 &lt;- rep(0, traj_length) trajectory2 &lt;- rep(0, traj_length) # specify where to start the trajectory: trajectory1[1] &lt;- 0.5 # another arbitrary value trajectory2[1] &lt;- 0.5 # another arbitrary value # specify the burn-in period burn_in &lt;- ceiling(0.0 * traj_length) # arbitrary number, less than `traj_length` # initialize accepted, rejected counters, just to monitor performance: n_accepted &lt;- 0 n_rejected &lt;- 0 for (t in 1:(traj_length - 1)) { current_position1 &lt;- trajectory1[t] current_position2 &lt;- trajectory2[t] # use the proposal distribution to generate a proposed jump proposed_jump1 &lt;- rnorm(1, mean = 0, sd = proposal_sd) proposed_jump2 &lt;- rnorm(1, mean = 0, sd = proposal_sd) # compute the probability of accepting the proposed jump prob_accept &lt;- min(1, target_rel_prob(current_position1 + proposed_jump1, theta_1_data, current_position2 + proposed_jump2, theta_2_data) / target_rel_prob(current_position1, theta_1_data, current_position2, theta_2_data)) # generate a random uniform value from the interval [0, 1] to # decide whether or not to accept the proposed jump if (runif(1) &lt; prob_accept) { # accept the proposed jump trajectory1[t + 1] &lt;- current_position1 + proposed_jump1 trajectory2[t + 1] &lt;- current_position2 + proposed_jump2 # increment the accepted counter, just to monitor performance if (t &gt; burn_in) {n_accepted &lt;- n_accepted + 1} } else { # reject the proposed jump, stay at current position trajectory1[t + 1] &lt;- current_position1 trajectory2[t + 1] &lt;- current_position2 # increment the rejected counter, just to monitor performance if (t &gt; burn_in) {n_rejected &lt;- n_rejected + 1} } } # extract the post-burn_in portion of the trajectory accepted_traj1 &lt;- trajectory1[(burn_in + 1) : length(trajectory1)] accepted_traj2 &lt;- trajectory2[(burn_in + 1) : length(trajectory2)] # collect the results metrop_2d_data &lt;- tibble(iter = rep(1:traj_length), accepted_traj1 = accepted_traj1, accepted_traj2 = accepted_traj2, n_accepted = n_accepted, n_rejected = n_rejected) return(metrop_2d_data) } Now we’ve defined my_bivariate_metropolis() let’s apply it to our data with proposal_sd == 0.02 and proposal_sd == 0.2. We’ll save the results as mh. mh &lt;- tibble(proposal_sd = c(0.02, 0.2)) %&gt;% mutate(mh = map(proposal_sd, my_bivariate_metropolis)) %&gt;% unnest(mh) mh ## # A tibble: 100,000 × 6 ## proposal_sd iter accepted_traj1 accepted_traj2 n_accepted n_rejected ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.02 1 0.5 0.5 46401 3598 ## 2 0.02 2 0.535 0.486 46401 3598 ## 3 0.02 3 0.541 0.477 46401 3598 ## 4 0.02 4 0.507 0.495 46401 3598 ## 5 0.02 5 0.511 0.484 46401 3598 ## 6 0.02 6 0.522 0.500 46401 3598 ## 7 0.02 7 0.544 0.491 46401 3598 ## 8 0.02 8 0.572 0.473 46401 3598 ## 9 0.02 9 0.591 0.509 46401 3598 ## 10 0.02 10 0.579 0.513 46401 3598 ## # … with 99,990 more rows If you look at the top of Figure 7.6, you’ll see Kruschke summarized his results with the acceptance rate, \\(N_\\text{acc} / N_\\text{pro}\\). Here are ours. mh %&gt;% group_by(proposal_sd) %&gt;% slice(1) %&gt;% summarise(acceptance_rate = n_accepted / (n_accepted + n_rejected)) ## # A tibble: 2 × 2 ## proposal_sd acceptance_rate ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.02 0.928 ## 2 0.2 0.428 We can compute our effective sample sizes using the effectiveSize() function from the coda package (Plummer et al., 2006, 2020). library(coda) mh %&gt;% group_by(proposal_sd) %&gt;% summarise(ess_theta_1 = effectiveSize(accepted_traj1), ess_theta_2 = effectiveSize(accepted_traj2)) ## # A tibble: 2 × 3 ## proposal_sd ess_theta_1 ess_theta_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.02 188. 194. ## 2 0.2 6673. 5987. We really won’t use the coda package in this ebook beyond this chapter and the next. But do note it has a lot to offer and Kruschke used it a bit in his code. Recall we might also compute the effective sample size estimates with functions from posterior, like at the end of Section 7.3.1. Here are the results using ess_mean() and ess_bulk(). # posterior::ess_mean() mh %&gt;% group_by(proposal_sd) %&gt;% summarise(ess_theta_1 = posterior::ess_mean(accepted_traj1), ess_theta_2 = posterior::ess_mean(accepted_traj2)) ## # A tibble: 2 × 3 ## proposal_sd ess_theta_1 ess_theta_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.02 195. 164. ## 2 0.2 6566. 6073. # posterior::ess_bulk() mh %&gt;% group_by(proposal_sd) %&gt;% summarise(ess_theta_1 = posterior::ess_bulk(accepted_traj1), ess_theta_2 = posterior::ess_bulk(accepted_traj2)) ## # A tibble: 2 × 3 ## proposal_sd ess_theta_1 ess_theta_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.02 210. 172. ## 2 0.2 6782. 6206. Different functions from different packages yield different results. Exciting, eh? Welcome to applied statistics, friends. Anyway, now we make our version of Figure 7.6. mh %&gt;% filter(iter &lt; 1000) %&gt;% ggplot(aes(x = accepted_traj1, y = accepted_traj2)) + geom_path(size = 1/8, alpha = 1/2, color = &quot;steelblue&quot;) + geom_point(alpha = 1/4, color = &quot;steelblue&quot;) + scale_x_continuous(expression(theta[1]), breaks = 0:5 / 5, expand = c(0, 0), limits = c(0, 1)) + scale_y_continuous(expression(theta[2]), breaks = 0:5 / 5, expand = c(0, 0), limits = c(0, 1)) + coord_equal() + theme_cowplot() + panel_border() + theme(panel.spacing.x = unit(0.75, &quot;cm&quot;)) + facet_wrap(~ proposal_sd, labeller = label_both) In the limit of infinite random walks, the Metropolis algorithm yields arbitrarily accurate representations of the underlying posterior distribution. The left and right panels of Figure 7.6 would eventually converge to an identical and highly accurate approximation to the posterior distribution. But in the real world of finite random walks, we care about how efficiently the algorithm generates an accurate representative sample. We prefer to use the proposal distribution from the right panel of Figure 7.6 because it will, typically, produce a more accurate approximation of the posterior than the proposal distribution from left panel, for the same number of proposed jumps. (p. 170) 7.4.4 Gibbs Hamiltonian Monte Carlo sampling. Figure 7.7 is still out of my skill set. But let’s fit the model with our primary package, brms. First we need to load brms. library(brms) These, recall, are the data. theta_1_data ## [1] 0 0 1 1 1 1 1 1 theta_2_data ## [1] 0 0 0 0 0 1 1 Kruschke said he was starting us out simply. From a regression perspective, we are getting ready to fit an intercepts-only multivariate Bernoulli model, which isn’t the simplest of things to code into brms. Plus, this particular pair of data sets presents a complication we won’t usually have to contend with in this book: The data vectors are different lengths. Remember how we pointed that out in Section 7.4.2? The issue is that whereas brms has extensive multivariate capacities (Bürkner, 2022b), they’re usually designed for data with equal sample sizes (i.e., when the rows in the two columns of a data frame are of the same number). Since these are Bernoulli data, we have two options at our disposal: employ the resp_subset() helper function or fit an aggregated binomial2 model. Since each has its strengths and weaknesses, we’ll split this section up and fit the model both ways. 7.4.4.1 Uneven multivariate Benoulli via the resp_subset() approach. Though brms can receive data from a few different formats, our approach throughout this ebook will usually be with data frames or tibbles. Here’s how we might combine our two data vectors, theta_1_data and theta_2_data, into a single tibble called d. d &lt;- tibble(y1 = theta_1_data, y2 = c(theta_2_data, NA)) # what is this? d ## # A tibble: 8 × 2 ## y1 y2 ## &lt;int&gt; &lt;int&gt; ## 1 0 0 ## 2 0 0 ## 3 1 0 ## 4 1 0 ## 5 1 0 ## 6 1 1 ## 7 1 1 ## 8 1 NA Because the second data vector was one unit shorter than the first, we had to compensate by adding an eighth cell, which we coded as NA, the universal indicator for missing values within the R ecosystem. We’ll still need one more data column, though. Using the if_else() function, we will make a subset column with will be coded TRUE for all non-NA values in the y2 columns, and FALSE whenever is.na(y2). d &lt;- d %&gt;% mutate(subset = if_else(is.na(y2), FALSE, TRUE)) # what is this? d ## # A tibble: 8 × 3 ## y1 y2 subset ## &lt;int&gt; &lt;int&gt; &lt;lgl&gt; ## 1 0 0 TRUE ## 2 0 0 TRUE ## 3 1 0 TRUE ## 4 1 0 TRUE ## 5 1 0 TRUE ## 6 1 1 TRUE ## 7 1 1 TRUE ## 8 1 NA FALSE brms includes a handful of helper functions that let users incorporate additional information about the criterion variable(s) into the model. For a full listing of these helper functions, check out the addition-terms section of the brms reference manual (Bürkner, 2022d). Though we won’t be using a lot of these in this ebook, it just so turns out that two of them will come in handy for our multivariate Bernoulli data. In this case, we want the resp_subset() helper function, which, in practice, we will just call subset(). From the brms reference manual, we read: For multivariate models, subset may be used in the aterms part, to use different subsets of the data in different univariate models. For instance, if sub is a logical variable and y is the response of one of the univariate models, we may write y | subset(sub) ~ predictors so that y is predicted only for those observations for which sub evaluates to TRUE. (p. 42) In our case, the subset variable in the data tibble will be the logical variable for our criterion y2, which would leave us with the formula y2 | subset(subset) ~ 1. Note the use of the | operator, which is what you always use when adding information with a helper function of this kind. Since we will be using the data in all eight rows of the y1 column, that corresponding formula would just be y1 ~ 1. In both cases, the ~ 1 portions of the formulas indicates these are intercept-only models. There are not predictors for either variable. Next, we need to talk about how to combine these two formulas within the multivariate syntax. If you look through Bürkner’s (2022b) vignette, Estimating multivariate models with brms, you’ll see there are several ways to define a multivariate model with brms. In this case, I think it’ll be easiest to define each model as a separate object, which we’ll call model_1 and model_2. model_1 &lt;- bf(y1 ~ 1) model_2 &lt;- bf(y2 | subset(subset) ~ 1) Note how the formula syntax for each was wrapped within the bf() function. That’s shorthand for brmsformula(). If you wanted to, you could have defined these as model_1 &lt;- brmsformula(y1 ~ 1), and so on. Another issue we need to contend with is Kruschke’s \\(\\operatorname{Beta}(2, 2)\\) prior. By default, brms assumes an unbounded parameter space for the standard intercept priors. But we know that the beta distribution imposes boundaries within the range of \\([0, 1]\\). This is technically okay with a standard brms intercept prior, but it can lead to computational difficulties. When possible, it’s better to formally tell brms when you are using bounded priors. Here, then, is how we’ll define our \\(\\operatorname{Beta}(2, 2)\\) prior within the prior() function. prior(beta(2, 2), class = Intercept, lb = 0, ub = 1, resp = y1) We set the lower boundary with the lb argument and then set the upper boundary with the ub argument. Also notice our use of the resp argument, which told brms this prior was connected to the y1 criterion. For the other criterion, we’d set that to resp = y2. Okay, here’s how to put all of this together to fit the model. fit7.1a &lt;- brm(data = d, family = bernoulli(link = identity), model_1 + model_2, prior = c(prior(beta(2, 2), class = Intercept, lb = 0, ub = 1, resp = y1), prior(beta(2, 2), class = Intercept, lb = 0, ub = 1, resp = y2)), iter = 3000, warmup = 500, cores = 3, chains = 3, seed = 7, file = &quot;fits/fit07.01a&quot;) Notice how we combined our two model objects with the + operator (model_1 + model_2). Here is a summary of the results. print(fit7.1a) ## Family: MV(bernoulli, bernoulli) ## Links: mu = identity ## mu = identity ## Formula: y1 ~ 1 ## y2 | subset(subset) ~ 1 ## Data: d (Number of observations: 8) ## Draws: 3 chains, each with iter = 3000; warmup = 500; thin = 1; ## total post-warmup draws = 7500 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## y1_Intercept 0.67 0.13 0.40 0.88 1.00 5950 4467 ## y2_Intercept 0.36 0.14 0.12 0.65 1.00 4632 3445 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we’ll use the as_draws_df() function to collect out posterior draws and save them as a data frame, which we’ll name draws.a. draws.a &lt;- as_draws_df(fit7.1a) # what is this? head(draws.a) ## # A draws_df: 6 iterations, 1 chains, and 4 variables ## b_y1_Intercept b_y2_Intercept lprior lp__ ## 1 0.60 0.33 0.65 -11 ## 2 0.73 0.33 0.45 -11 ## 3 0.62 0.38 0.69 -11 ## 4 0.61 0.43 0.74 -11 ## 5 0.66 0.45 0.70 -11 ## 6 0.50 0.32 0.68 -12 ## # ... hidden reserved variables {&#39;.chain&#39;, &#39;.iteration&#39;, &#39;.draw&#39;} With draws.a in hand, we’re ready to make our version of Figure 7.8. To reduce the overplotting, we’re only looking at the first 500 post-warmup draws. p1 &lt;- draws.a %&gt;% filter(.draw &lt; 501) %&gt;% ggplot(aes(x = b_y1_Intercept, y = b_y2_Intercept)) + geom_point(alpha = 1/4, color = &quot;steelblue&quot;) + geom_path(size = 1/10, alpha = 1/2, color = &quot;steelblue&quot;) + scale_x_continuous(expression(theta[1]), breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) + scale_y_continuous(expression(theta[2]), breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) + labs(subtitle = &quot;fit7.1a (resp_subset() method)&quot;) + coord_equal() + theme_cowplot() p1 Finally, after all that terrible technical talk, our efforts payed off! I promise, it won’t always be this difficult to fit a model in brms. This is just one of those unfortunate cases where a textbook author’s seemingly simple example required non-default settings and approaches when applied to a software package that wasn’t the one they highlighted in their textbook. Now let’s try out the second approach. 7.4.4.2 Uneven multivariate Benoulli via the aggregated binomial approach. Instead of thinking of our data as two vectors of 0’s and 1’s, we can use the aggregate form to summarize them more compactly. d &lt;- tibble(n1 = n1, z1 = z1, n2 = n2, z2 = z2) # what does this look like? d ## # A tibble: 1 × 4 ## n1 z1 n2 z2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8 6 7 2 The resp_trials() function is the second helper that will let us incorporate additional information about the criterion variable(s) into the model. In actual practice, we’ll use it as trials(). The basic idea is that for the first coin, we have z1 == 6 heads out of n1 == 8 trials. We can express that as an intercept-only model as z1 | trials(n1) ~ 1. Thus, we’ll define our two submodels like this. model_1 &lt;- bf(z1 | trials(n1) ~ 1) model_2 &lt;- bf(z2 | trials(n2) ~ 1) Notice that when our data are in an aggregated format, we expressed both series of coin flips in one row of the data frame. Therefore, even though the first coin had more trials than the second coin (n1 &gt; n2), we don’t need to invoke the subset() helper. We just need to use trials() to tell brms how long each sequence of coin flips was. The other new thing is that instead of directly using the Bernoulli likelihood function, we’ll instead be setting family = binomial(link = \"identity\"). Recall that the Bernoulli function is a special case of the binomial function for which \\(N = 1\\), for which each data point is a discrete trial3. When each data point is an aggregate of multiple trials, we use the binomial likelihood, instead. Here’s how to fit the model. fit7.1b &lt;- brm(data = d, family = binomial(link = &quot;identity&quot;), model_1 + model_2, prior = c(prior(beta(2, 2), class = Intercept, lb = 0, ub = 1, resp = z1), prior(beta(2, 2), class = Intercept, lb = 0, ub = 1, resp = z2)), iter = 3000, warmup = 500, cores = 3, chains = 3, seed = 7, file = &quot;fits/fit07.01b&quot;) The model summary for our fit7.1b aggregated binomial approach is very similar to the one from our previous fit7.1a approach using the subset() helper. print(fit7.1b) ## Family: MV(binomial, binomial) ## Links: mu = identity ## mu = identity ## Formula: z1 | trials(n1) ~ 1 ## z2 | trials(n2) ~ 1 ## Data: d (Number of observations: 1) ## Draws: 3 chains, each with iter = 3000; warmup = 500; thin = 1; ## total post-warmup draws = 7500 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## z1_Intercept 0.67 0.13 0.40 0.89 1.00 5660 4952 ## z2_Intercept 0.36 0.14 0.12 0.65 1.00 6277 4453 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the updated version of Figure 7.8. draws.b &lt;- as_draws_df(fit7.1b) p2 &lt;- draws.b %&gt;% filter(.draw &lt; 501) %&gt;% ggplot(aes(x = b_z1_Intercept, y = b_z2_Intercept)) + geom_point(alpha = 1/4, color = &quot;steelblue&quot;) + geom_path(size = 1/10, alpha = 1/2, color = &quot;steelblue&quot;) + scale_x_continuous(expression(theta[1]), breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) + scale_y_continuous(expression(theta[2]), breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) + labs(subtitle = &quot;fit7.1b (aggregated binomial method)&quot;) + coord_equal() + theme_cowplot() # combine library(patchwork) p1 + p2 Just for kicks and giggles, we might also compare the two model types by plotting the marginal posterior densities. This will give is a better sense of how each of the marginal \\(\\theta\\) densities are shaped like the beta distribution. # combine the posterior samples from the two models draws &lt;- bind_rows( draws.a %&gt;% transmute(`theta[1]` = b_y1_Intercept, `theta[2]` = b_y2_Intercept), draws.b %&gt;% transmute(`theta[1]` = b_z1_Intercept, `theta[2]` = b_z2_Intercept) ) %&gt;% mutate(fit = rep(c(&quot;fit7.1a&quot;, &quot;fit7.1b&quot;), each = n() / 2)) # wrangle draws %&gt;% pivot_longer(-fit) %&gt;% # plot ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = &quot;steelblue&quot;) + scale_x_continuous(&quot;posterior&quot;, breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) + scale_y_continuous(NULL, breaks = NULL) + theme_cowplot() + theme(panel.spacing.x = unit(0.75, &quot;cm&quot;)) + panel_border() + facet_grid(fit ~ name, labeller = label_parsed) I hope this makes clear that the two estimation methods returned results that are within simulation variance of one another. Had we asked brms for a larger number of posterior draws from each, the results would have been even closer. 7.4.5 Is there a difference between biases? In his Figure 7.9, Kruschke compared the marginal posterior for \\(\\theta_1 - \\theta_2\\), as computed by two methods from the Metropolis algorithm and another two methods from the Gibbs sampler. Here we’ll focus, instead, on the two methods we explored using brms-based Hamiltonian Monte Carlo (HMC). draws %&gt;% mutate(dif = `theta[1]` - `theta[2]`, fit = if_else(fit == &quot;fit7.1a&quot;, &quot;fit7.1a (resp_subset() method)&quot;, &quot;fit7.1b (aggregated binomial method)&quot;)) %&gt;% ggplot(aes(x = dif, y = fit)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = &quot;steelblue2&quot;, slab_color = &quot;steelblue4&quot;, outline_bars = T, breaks = 40, normalize = &quot;panels&quot;) + geom_vline(xintercept = 0, linetype = 3) + labs(x = expression(theta[1]-theta[2]), y = NULL) + coord_cartesian(ylim = c(1.5, 2.4)) + theme_cowplot() ## Warning: Unknown or uninitialised column: `linewidth`. Here are the exact estimates of the mode and 95% HDIs for our difference distribution, \\(\\theta_1 - \\theta_2\\). draws %&gt;% mutate(dif = `theta[1]` - `theta[2]`) %&gt;% group_by(fit) %&gt;% mode_hdi(dif) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 2 × 7 ## fit dif .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 fit7.1a 0.328 -0.062 0.67 0.95 mode hdi ## 2 fit7.1b 0.36 -0.064 0.66 0.95 mode hdi I wouldn’t put too much emphasis on the seemingly large differences in the two modes. Among the three primary measures of central tendency, modes are particularly sensitive to things like sample variance. Here’s what happens if we compare the two methods with the mean, instead. draws %&gt;% mutate(dif = `theta[1]` - `theta[2]`) %&gt;% group_by(fit) %&gt;% summarise(mean_of_the_difference_score = mean(dif) %&gt;% round(digits = 3)) ## # A tibble: 2 × 2 ## fit mean_of_the_difference_score ## &lt;chr&gt; &lt;dbl&gt; ## 1 fit7.1a 0.303 ## 2 fit7.1b 0.305 Now the difference between the two methods seems trivial. 7.4.6 Terminology: MCMC. Any simulation that samples a lot of random values from a distribution is called a Monte Carlo simulation, named after the dice and spinners and shufflings of the famous casino locale. The appellation “Monte Carlo” is attributed (Eckhardt, 1987) to the mathematicians Stanislaw Ulam (1909–1984) and John von Neumann (1903–1957). (p. 177) In case you didn’t know, brms is a user-friendly interface for the Stan probabilistic programing language [Stan; Carpenter et al. (2017)] and Stan is named after Stanislaw Ulam. 7.5 MCMC representativeness, accuracy, and efficiency We have three main goals in generating an MCMC sample from the posterior distribution: The values in the chain must be representative of the posterior distribution. They should not be unduly influenced by the arbitrary initial value of the chain, and they should fully explore the range of the posterior distribution without getting stuck. The chain should be of sufficient size so that estimates are accurate and stable. In particular, the estimates of the central tendency (such as median or mode), and the limits of the \\(95\\%\\) HDI, should not be much different if the MCMC analysis is run again (using different seed states for the pseudorandom number generators). The chain should be generated efficiently, with as few steps as possible, so not to exceed our patience or computing power. (p. 178, emphasis in the original) 7.5.1 MCMC representativeness. Kruschke defined our new data in the note for Figure 7.10 (p. 179). z &lt;- 35 n &lt;- 50 d &lt;- tibble(y = rep(0:1, times = c(n - z, z))) Here we fit the model. Note how since we’re just univariate, it’s easy to switch back to directly modeling with the Bernoulli likelihood. fit7.2 &lt;- brm(data = d, family = bernoulli(link = identity), y ~ 1, prior(beta(2, 2), class = Intercept, lb = 0, ub = 1), iter = 10000, warmup = 500, cores = 3, chains = 3, seed = 7, file = &quot;fits/fit07.02&quot;) On page 179, Kruschke discussed burn-in steps within the Gibbs framework: The preliminary steps, during which the chain moves from its unrepresentative initial value to the modal region of the posterior, is called the burn-in period. For realistic applications, it is routine to apply a burn-in period of several hundred to several thousand steps. For each HMC chain, the first \\(n\\) iterations are called “warmups.” In this example, \\(n = 500\\) (i.e., warmup = 500). Within the Stan-HMC paradigm, warmups are somewhat analogous to but not synonymous with burn-in iterations as done by the Gibbs sampling in JAGS. But HMC warmups are like Gibbs burn-ins in that both are discarded and not used to describe the posterior. For more on warmup, check out McElreath’s lecture, starting here or, for more detail, the HMC Algorithm Parameters section (15.2) of the Stan reference manual, version 2.29 (Stan Development Team, 2022b). It appears that the upshot of all this is some of the packages in the Stan ecosystem don’t make it easy to extract the warmup values. For example, the brms::plot() function excludes them from the trace plot without the option to include them. plot(fit7.2, widths = c(2, 3)) Notice how the \\(x\\)-axis on the trace plot ranges from 0 to 9,500. Now recall that our model code included iter = 10000, warmup = 500. Those 9,500 iterations in the trace plot are excluding the first 500 warmup iterations. This code is a little janky, but if you really want those warmup iterations, you can extract them from the fit7.2 object like this. warmups &lt;- c(fit7.2$fit@sim$samples[[1]]$b_Intercept[1:500], fit7.2$fit@sim$samples[[2]]$b_Intercept[1:500], fit7.2$fit@sim$samples[[3]]$b_Intercept[1:500]) %&gt;% # since these come from lists, here we&#39;ll convert them to a data frame as.data.frame() %&gt;% rename(b_Intercept = &quot;.&quot;) %&gt;% # we&#39;ll need to recapture the iteration and chain information mutate(iter = rep(1:500, times = 3), chain = factor(rep(1:3, each = 500), levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;))) warmups %&gt;% head() ## b_Intercept iter chain ## 1 0.7625908 1 1 ## 2 0.7625908 2 1 ## 3 0.7625908 3 1 ## 4 0.7655583 4 1 ## 5 0.7444207 5 1 ## 6 0.6654870 6 1 The bayesplot package (Gabry et al., 2019; Gabry &amp; Mahr, 2022) makes it easier to reproduce some of the plots in Figure 7.10. library(bayesplot) We’ll reproduce the upper left panel with mcmc_trace(). mcmc_trace(warmups, pars = &quot;b_Intercept&quot;) As an alternative, we can also extract the warmup draws from a brm() fit with the ggmcmc package (Fernández i Marín, 2016, 2021). library(ggmcmc) The ggmcmc package has a variety of convenience functions for working with MCMC chains. The ggs() function extracts the posterior draws, including warmup, and arranges them in a tidy tibble. With those in hand, we can now make a trace plot with warmup draws. ggs(fit7.2) %&gt;% filter(Iteration &lt; 501 &amp; Parameter == &quot;b_Intercept&quot;) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x = Iteration, y = value, color = chain)) + geom_line() + scale_colour_brewer(direction = -1) + labs(title = &quot;My custom trace plots with warmups via ggmcmc::ggs()&quot;, x = NULL, y = NULL) + theme_cowplot(font_size = 12) You can make the same basic plot by pulling the posterior draws with as_draws_df(), as long as you include the inc_warmup = TRUE argument. Then you just have to be careful to filter() by the correct values in the .iteration meta-column. draws &lt;- as_draws_df(fit7.2, inc_warmup = T) draws %&gt;% filter(.iteration &lt;= 500) %&gt;% ggplot(aes(x = .iteration, y = b_Intercept, color = factor(.chain))) + geom_line() + scale_colour_brewer(&quot;chain&quot;, direction = -1) + labs(title = &quot;My custom trace plots with warmups via as_draws_df()&quot;, x = NULL, y = NULL) + theme_cowplot(font_size = 12) Anyway, it appears our HMC warmup draws found the posterior quite quickly. Here’s the autocorrelation plot. mcmc_acf(warmups, pars = &quot;b_Intercept&quot;, lags = 25) Our autocorrelation plots indicate substantially lower autocorrelations yielded by HMC as implemented by Stan than what Kruschke generated with the MH algorithm. This is one of the reasons folks using HMC tend to use fewer iterations than those using MH or Gibbs. If you were unhappy with the way mcmc_acf() defaults to faceting the plot by chain, you could always extract the data from the function and use them to make the plot the way you prefer. E.g., mcmc_acf(warmups)$data %&gt;% as_tibble() %&gt;% filter(Parameter == &quot;b_Intercept&quot;) %&gt;% ggplot(aes(x = Lag, y = AC, color = Chain %&gt;% as.factor())) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point(size = 2/3) + geom_line() + scale_colour_brewer(direction = -1) + ylab(&quot;Autocorrelation&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;) Here are the overlaid densities. mcmc_dens_overlay(warmups, pars = c(&quot;b_Intercept&quot;)) The densities aren’t great, but they still appear nicer than those in for the burn-in iterations in the text. With our warmups in their current state, I’m not aware how we might conveniently make a shrink factor plot, as seen in the lower left of Figure 7.10. So it goes… Figure 7.11 examined the post-burn-in iterations. We’ll follow suit with our post-warmup iterations. Note that for use with the bayesplot::mcmc_ functions, we’ll generally want our posterior draws in a data frame format. We an do this by extracting them with the brms::as_draws_df() function. draws &lt;- as_draws_df(fit7.2) draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_trace(pars = &quot;b_Intercept&quot;) The autocorrelation plots: draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_acf(pars = &quot;b_Intercept&quot;, lags = 40) As with the warmups, above, the post-warmup autocorrelation plots indicate substantially lower autocorrelations yielded by HMC as implemented by Stan than what Kruschke generated with the MH algorithm. This is one of the reasons folks using HMC tend to use fewer iterations than those using MH or Gibbs. Here are the overlaid densities. draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_dens_overlay(pars = &quot;b_Intercept&quot;) Now that we’re focusing on the post-warmup iterations, we can make a shrink factor plot. We’ll do so with the coda::gelman.plot() function. But you can’t just dump your brm() fit object into gelman.plot(). It’s the wrong object type. However, brms offers the as.mcmc() function which will convert brm() objects for use in functions from the coda package. fit7.2_c &lt;- as.mcmc(fit7.2) fit7.2_c %&gt;% glimpse() ## List of 3 ## $ : &#39;mcmc&#39; num [1:9500, 1:3] 0.64 0.596 0.702 0.693 0.675 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:3] &quot;b_Intercept&quot; &quot;lprior&quot; &quot;lp__&quot; ## ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1 ## $ : &#39;mcmc&#39; num [1:9500, 1:3] 0.757 0.73 0.54 0.578 0.583 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:3] &quot;b_Intercept&quot; &quot;lprior&quot; &quot;lp__&quot; ## ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1 ## $ : &#39;mcmc&#39; num [1:9500, 1:3] 0.67 0.677 0.677 0.631 0.631 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:3] &quot;b_Intercept&quot; &quot;lprior&quot; &quot;lp__&quot; ## ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1 ## - attr(*, &quot;class&quot;)= chr &quot;mcmc.list&quot; With our freshly-converted fit2_c object in hand, we’re ready to plot. gelman.plot(fit7.2_c[, &quot;b_Intercept&quot;, ]) Looks great. As Kruschke explained on page 181, that plot is based on the potential scale reduction factor, or \\(\\widehat R\\) as it’s typically referred to in the Stan ecosystem. Happily, brms reports the \\(\\widehat R\\) values for the major model parameters using print() or summary(). print(fit7.2) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: d (Number of observations: 50) ## Draws: 3 chains, each with iter = 10000; warmup = 500; thin = 1; ## total post-warmup draws = 28500 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.69 0.06 0.56 0.80 1.00 9810 11160 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Instead of a running value, you get a single statistic in the ‘Rhat’ column. On page 181, Kruschke discussed how his overlaid density plots include the HDIs, by chain. The convenience functions from brms and bayesplot don’t easily get us there. But we can get those easy enough with a little help tidybayes::stat_halfeye(). Note that for use with the various tidybayes functions, we’ll want the posterior draws in the format returned by the brms::as_draws_df() function, but this time without the warmups included. draws &lt;- as_draws_df(fit7.2) draws %&gt;% mutate(chain = factor(.chain)) %&gt;% ggplot(aes(x = b_Intercept, y = chain, fill = chain)) + stat_halfeye(point_interval = mode_hdi, .width = .95) + scale_fill_brewer(direction = -1) + scale_y_discrete(expand = expansion(mult = 0.035)) + theme_cowplot() + theme(legend.position = &quot;none&quot;) If you would like your chain-wise posterior densities to overlap, one way would be to play around with the height and alpha parameters within the stat_halfeye() function. draws %&gt;% mutate(chain = factor(.chain)) %&gt;% ggplot(aes(x = b_Intercept, y = chain, fill = chain)) + stat_halfeye(point_interval = mode_hdi, .width = .95, height = 9, alpha = 3/4) + scale_fill_brewer() + scale_y_discrete(expand = expansion(mult = 0.2)) + theme_cowplot() + theme(legend.position = &quot;none&quot;) 7.5.2 MCMC accuracy. We want measures of chain length and accuracy that take into account the clumpiness of the chain. And for that, we need a measure of clumpiness. We will measure clumpiness as autocorrelation, which is simply the correlation of the chain values with the chain values \\(k\\) steps ahead. There is a different autocorrelation for each choice of \\(k\\). (p. 182, emphasis in the original) We made a couple autocorrelation plots in the last section, but now it’s time to get a better sense of what they mean. Just a little further in the text, Kruschke wrote: “The number of steps between the chain and its superimposed copy is called the lag” (p. 182, emphasis in the original). In case it’s not clear, lag is a general term and can be applied to contexts outside of MCMC chains. You find it used sometimes in the longitudinal statistical literature, particularly for what are called timeseries models. Sadly, we won’t be fitting those in this book. If you’re curious, McElreath discussed them briefly in Chapter 16 of his (2020) text. We, however, will have to contend with a technical quirk within the tidyverse. The two dplyr functions relevant to lags are called lag() and lead(). Here’s a little example to see how they work. tibble(step = 1:5) %&gt;% mutate(lag = lag(step, n = 1), lead = lead(step, n = 1)) ## # A tibble: 5 × 3 ## step lag lead ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 NA 2 ## 2 2 1 3 ## 3 3 2 4 ## 4 4 3 5 ## 5 5 4 NA The original values are 1:5 in the step column. When you plug those into lag(n = 1), you get back the value from the previous row. The opposite happens when you plug step into lead(n = 1); there you get back the value from the next row. Returning to the block quote above, Kruschke wrote that autocorrelations are “the correlation of the chain values with the chain values \\(k\\) steps ahead” (p. 182, emphasis added). Within the context of the lag() and lead() functions, their n arguments are what Kruschke called \\(k\\), which is no big deal. Confusingly, though, since Kruschke wanted to focus on MCMC chains values that were “\\(k\\) steps ahead,” that means we’ll have to use the lead() function, not lag(). Please don’t fret about the semantics, here. Both Kruschke and the dplyr package are correct. We’re lagging. But in this specific case, we’ll be lagging our data with the lead() function. You can learn more about lag() and lead() here. On to the plot! If you read closely in the text (pp. 182–183), Kruschke didn’t elaborate on which model he was showcasing in Figure 7.12. We just get the vague explanation that the “upper panels show an MCMC chain of 70 steps, superimposed with the same chain translated a certain number of steps ahead” (p. 182). To make our lives simple, let’s just use the model we’ve been working with, fit7.2. Our sole parameter is the intercept which, as it turns out, will not be on the same scale as you see in the \\(y\\)-axis of Figure 7.12. So it goes… But anyways, our first step towards making our variant of the plot is to wrangle out posterior draws a bit. We’ll call the wrangled data frame lagged_draws. lagged_draws &lt;- draws %&gt;% filter(.chain == 1 &amp; .draw &lt; 71) %&gt;% select(b_Intercept, .draw) %&gt;% # sometimes the unlagged data are called lag_0 rename(lag_0 = b_Intercept) %&gt;% # lags for three different levels of k mutate(lag_1 = lead(lag_0, n = 1), lag_5 = lead(lag_0, n = 5), lag_10 = lead(lag_0, n = 10)) %&gt;% pivot_longer(-.draw, names_to = &quot;key&quot;) Now here’s our version of the top row. p1 &lt;- lagged_draws %&gt;% filter(key %in% c(&quot;lag_0&quot;, &quot;lag_1&quot;)) %&gt;% ggplot(aes(x = .draw, y = value, color = key)) + geom_point(aes(alpha = .draw == 50, shape = .draw == 50)) + geom_line(size = 1/3, alpha = 1/2) + annotate(geom = &quot;text&quot;, x = 46, y = c(.825, .79), label = c(&quot;Original&quot;, &quot;Lagged&quot;), color = c(&quot;black&quot;, &quot;steelblue&quot;)) + scale_color_manual(values = c(&quot;black&quot;, &quot;steelblue&quot;)) + scale_y_continuous(breaks = 6:8 / 10, limits = c(.53, .84)) + labs(title = &quot;Lag 1&quot;, x = &quot;Index 1:70&quot;, y = expression(theta)) p2 &lt;- lagged_draws %&gt;% filter(key %in% c(&quot;lag_0&quot;, &quot;lag_5&quot;)) %&gt;% ggplot(aes(x = .draw, y = value, color = key)) + geom_point(aes(alpha = .draw == 50, shape = .draw == 50)) + geom_line(size = 1/3, alpha = 1/2) + scale_color_manual(values = c(&quot;black&quot;, &quot;steelblue&quot;)) + scale_y_continuous(NULL, labels = NULL, breaks = 6:8 / 10, limits = c(.53, .84)) + labs(title = &quot;Lag 5&quot;, x = &quot;Index 1:70&quot;) p3 &lt;- lagged_draws %&gt;% filter(key %in% c(&quot;lag_0&quot;, &quot;lag_10&quot;)) %&gt;% ggplot(aes(x = .draw, y = value, color = key)) + geom_point(aes(alpha = .draw == 50, shape = .draw == 50)) + geom_line(size = 1/3, alpha = 1/2) + scale_color_manual(values = c(&quot;black&quot;, &quot;steelblue&quot;)) + scale_y_continuous(NULL, labels = NULL, breaks = 6:8 / 10, limits = c(.53, .84)) + labs(title = &quot;Lag 10&quot;, x = &quot;Index 1:70&quot;) # combine library(patchwork) (p1 + p2 + p3) &amp; scale_alpha_manual(values = c(1/2, 1)) &amp; scale_shape_manual(values = c(1, 19)) &amp; theme_cowplot() &amp; theme(legend.position = &quot;none&quot;) Here’s the middle row for Figure 7.12. p1 &lt;- lagged_draws %&gt;% pivot_wider(names_from = key, values_from = value) %&gt;% ggplot(aes(x = lag_1, y = lag_0)) + stat_smooth(method = &quot;lm&quot;) + geom_point(aes(alpha = .draw == 50, shape = .draw == 50)) p2 &lt;- lagged_draws %&gt;% pivot_wider(names_from = key, values_from = value) %&gt;% ggplot(aes(x = lag_5, y = lag_0)) + stat_smooth(method = &quot;lm&quot;) + geom_point(aes(alpha = .draw == 50, shape = .draw == 50)) + scale_y_continuous(NULL, labels = NULL) p3 &lt;- lagged_draws %&gt;% pivot_wider(names_from = key, values_from = value) %&gt;% ggplot(aes(x = lag_10, y = lag_0)) + stat_smooth(method = &quot;lm&quot;) + geom_point(aes(alpha = .draw == 50, shape = .draw == 50)) + scale_y_continuous(NULL, labels = NULL) # combine (p1 + p2 + p3) &amp; scale_alpha_manual(values = c(1/2, 1)) &amp; scale_shape_manual(values = c(1, 19)) &amp; theme_cowplot() &amp; theme(legend.position = &quot;none&quot;) For kicks and giggles, we used stat_smooth() to add an OLS regression line with its 95% frequentist confidence intervals to each plot. If you want the Pearson’s correlations among the lags, the lowerCor() function from the psych package (Revelle, 2022) can be handy. library(psych) lagged_draws %&gt;% pivot_wider(names_from = key, values_from = value) %&gt;% select(-.draw) %&gt;% lowerCor(digits = 3, use = &quot;pairwise.complete.obs&quot;) ## lag_0 lag_1 lag_5 lg_10 ## lag_0 1.000 ## lag_1 0.408 1.000 ## lag_5 0.202 0.012 1.000 ## lag_10 -0.121 -0.232 0.231 1.000 For our version of the bottom of Figure 7.12, we’ll use the bayesplot::mcmc_acf_bar() function to get the autocorrelation bar plot, by chain. draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_acf_bar(pars = &quot;b_Intercept&quot;, lags = 20) All three rows of our versions for Figure 7.12 indicate in their own way how much lower our autocorrelations were than the ones in the text. If you’re curious of the effective sample sizes for the parameters in your brms models, just look at the model summary using either summary() or print(). print(fit7.2) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: d (Number of observations: 50) ## Draws: 3 chains, each with iter = 10000; warmup = 500; thin = 1; ## total post-warmup draws = 28500 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.69 0.06 0.56 0.80 1.00 9810 11160 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Look at the last two columns in the Intercept summary. Earlier versions of brms had one column named Eff.Sample, which reported the effect sample size as discussed by Kruschke. Starting with version 2.10.0, brms now returns Bulk_ESS and Tail_ESS, instead. These originate from a (2021) paper by Stan-team all-stars Vehtari, Gelman, Simpson, Carpenter, and Bürkner. From their paper, we read: When reporting quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3, we show that convergence of Markov chains is not uniform across the parameter space, that is, convergence might be different in the bulk of the distribution (e.g., for the mean or median) than in the tails (e.g., for extreme quantiles). We propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as bulk-ESS), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (pp. 672–673) For more technical details, see the paper. The Bulk_ESS column in current versions of brms is what was previously referred to as Eff.Sample. This is what corresponds to what Kruschke meant when referring to effective sample size. Now rather than focusing solely on ‘the center of the’ posterior distribution’ as indexed by Bulk_ESS, we also gauge the effective sample size in the posterior intervals using Tail_ESS. Anyway, I’m not sure how to reproduce Kruschke’s MCMC ESS simulation studies. My confusion comes from at least two levels. If you read in the text, Kruschke described his simulation as based on “MCMC chains from the normal distribution” (p. 184). Though I do know how to initialize HMC chains for a model on data from the normal distribution, I have no idea how one would initialize chains from the standard normal distribution, itself. Second, if you view Kruschke’s simulation as based on a model which one could feasibly fit with brms, I don’t know how one would specify “an ESS of 10,000” for each iteration of the simulation. This is because brms is set up to fit models with a fixed number of iterations, for which the ESS values will vary. Kruschke’s simulation seems to be set in reverse. For more details on Kruschke’s simulation, you’ll just have to read through the text. Anyway, if you know how to fit such a simulation using brms, please share your code in my GitHub issue #15. If you’re interested in the Monte Carlo standard error (MCSE) for your brms parameters, the easiest way is to tack $fit onto your fit object. fit7.2$fit ## Inference for Stan model: 20e3ba6b38506cfef876fa747cb5168b. ## 3 chains, each with iter=10000; warmup=500; thin=1; ## post-warmup draws per chain=9500, total post-warmup draws=28500. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept 0.69 0.00 0.06 0.56 0.65 0.69 0.73 0.80 9894 1 ## lprior 0.23 0.00 0.12 -0.05 0.17 0.25 0.32 0.39 9423 1 ## lp__ -32.34 0.01 0.70 -34.32 -32.51 -32.08 -31.90 -31.85 10484 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Apr 20 09:33:05 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). This returns an rstan-like summary (Stan Development Team, 2022a). The ‘se_mean’ column is the MCSE. 7.5.3 MCMC efficiency. Kruschke wrote: “It is often the case in realistic applications that there is strong autocorrelation for some parameters, and therefore, an extremely long chain is required to achieve an adequate ESS or MCSE” (p. 187). As we’ll see, this is generally less of a problem for HMC than for MH or Gibbs. But it does still crop up, particularly in complicated models. As he wrote on the following page, “one sampling method that can be relatively efficient is Hamiltonian Monte Carlo.” Indeed. 7.6 Summary Let’s regain perspective on the forest of Bayesian inference after focusing on the trees of MCMC. Recall that the overarching goal of Bayesian analysis is identifying the credibility of parameter values in a descriptive model of data. Bayes’ rule provides an exact mathematical formulation for the posterior distribution on the parameter values. But the exact form requires evaluation of an integral that might be intractable for realistically complex models. Therefore, we approximate the posterior distribution, to arbitrarily high accuracy, using MCMC methods. Because of recent developments in MCMC algorithms, software that cleverly applies them in complex models, and hardware that runs them incredibly quickly, we can now use MCMC methods to analyze realistically complex models that would have been impossible only a few decades ago. (pp. 188–189) Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] psych_2.2.5 ggmcmc_1.5.1.1 bayesplot_1.9.0 patchwork_1.1.2 brms_2.18.0 Rcpp_1.0.9 ## [7] coda_0.19-4 tidybayes_3.0.2 cowplot_1.1.1 forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 ## [13] purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 igraph_1.3.4 ## [5] splines_4.2.0 svUnit_1.0.6 crosstalk_1.2.0 TH.data_1.1-1 ## [9] rstantools_2.2.0 inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 googlesheets4_1.0.1 ## [17] tzdb_0.3.0 modelr_0.1.8 RcppParallel_5.1.5 matrixStats_0.62.0 ## [21] sandwich_3.0-2 xts_0.12.1 prettyunits_1.1.1 colorspace_2.0-3 ## [25] rvest_1.0.2 ggdist_3.2.0 haven_2.5.1 xfun_0.35 ## [29] callr_3.7.3 crayon_1.5.2 jsonlite_1.8.3 lme4_1.1-31 ## [33] survival_3.4-0 zoo_1.8-10 glue_1.6.2 gtable_0.3.1 ## [37] gargle_1.2.0 emmeans_1.8.0 distributional_0.3.1 pkgbuild_1.3.1 ## [41] rstan_2.21.7 abind_1.4-5 scales_1.2.1 mvtnorm_1.1-3 ## [45] GGally_2.1.2 DBI_1.1.3 miniUI_0.1.1.1 viridisLite_0.4.1 ## [49] xtable_1.8-4 HDInterval_0.2.2 stats4_4.2.0 StanHeaders_2.21.0-7 ## [53] DT_0.24 htmlwidgets_1.5.4 httr_1.4.4 threejs_0.3.3 ## [57] RColorBrewer_1.1-3 arrayhelpers_1.1-0 posterior_1.3.1 ellipsis_0.3.2 ## [61] reshape_0.8.9 pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 ## [65] sass_0.4.2 dbplyr_2.2.1 utf8_1.2.2 tidyselect_1.1.2 ## [69] labeling_0.4.2 rlang_1.0.6 reshape2_1.4.4 later_1.3.0 ## [73] munsell_0.5.0 cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [77] cli_3.5.0 generics_0.1.3 broom_1.0.1 ggridges_0.5.3 ## [81] evaluate_0.18 fastmap_1.1.0 processx_3.8.0 knitr_1.40 ## [85] fs_1.5.2 nlme_3.1-159 projpred_2.2.1 mime_0.12 ## [89] xml2_1.3.3 compiler_4.2.0 shinythemes_1.2.0 rstudioapi_0.13 ## [93] gamm4_0.2-6 reprex_2.0.2 bslib_0.4.0 stringi_1.7.8 ## [97] highr_0.9 ps_1.7.2 Brobdingnag_1.2-8 lattice_0.20-45 ## [101] Matrix_1.4-1 nloptr_2.0.3 markdown_1.1 shinyjs_2.1.0 ## [105] tensorA_0.36.2 vctrs_0.5.1 pillar_1.8.1 lifecycle_1.0.3 ## [109] jquerylib_0.1.4 bridgesampling_1.1-2 estimability_1.4.1 httpuv_1.6.5 ## [113] R6_2.5.1 bookdown_0.28 promises_1.2.0.1 gridExtra_2.3 ## [117] codetools_0.2-18 boot_1.3-28 colourpicker_1.1.1 MASS_7.3-58.1 ## [121] gtools_3.9.3 assertthat_0.2.1 withr_2.5.0 mnormt_2.1.0 ## [125] shinystan_2.6.0 multcomp_1.4-20 mgcv_1.8-40 parallel_4.2.0 ## [129] hms_1.1.1 grid_4.2.0 minqa_1.2.5 rmarkdown_2.16 ## [133] googledrive_2.0.0 shiny_1.7.2 lubridate_1.8.0 base64enc_0.1-3 ## [137] dygraphs_1.1.1.6 Footnote References Bürkner, P.-C. (2022b). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html Bürkner, P.-C. (2022d). brms reference manual, Version 2.17.0. https://CRAN.R-project.org/package=brms/brms.pdf Bürkner, P.-C., Gabry, J., Kay, M., &amp; Vehtari, A. (2021). posterior: Tools for working with posterior distributions [Manual]. Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., &amp; Riddell, A. (2017). Stan: A probabilistic programming language. Journal of Statistical Software, 76(1). https://doi.org/10.18637/jss.v076.i01 Eckhardt, R. (1987). Stan Ulam, John von Neumann and the Monte Carlo method. Argonne, USA. https://library.sciencemadness.org/lanl1_a/lib-www/pubs/00326867.pdf Fernández i Marín, X. (2016). ggmcmc: Analysis of MCMC samples and Bayesian inference. Journal of Statistical Software, 70(9), 1–20. https://doi.org/10.18637/jss.v070.i09 Fernández i Marín, X. (2021). ggmcmc: Tools for analyzing MCMC simulations from Bayesian inference [Manual]. https://CRAN.R-project.org/package=ggmcmc Gabry, J., &amp; Mahr, T. (2022). bayesplot: Plotting for Bayesian models. https://CRAN.R-project.org/package=bayesplot Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., &amp; Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/ Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953). Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 21(6), 1087–1092. https://doi.org/10.1063/1.1699114 Plummer, M., Best, N., Cowles, K., &amp; Vines, K. (2006). CODA: Convergence diagnosis and output analysis for MCMC. R News, 6(1), 7–11. https://journal.r-project.org/archive/ Plummer, M., Best, N., Cowles, K., Vines, K., Sarkar, D., Bates, D., Almond, R., &amp; Magnusson, A. (2020). coda: Output analysis and diagnostics for MCMC [Manual]. https://CRAN.R-project.org/package=coda Revelle, W. (2022). psych: Procedures for psychological, psychometric, and personality research. https://CRAN.R-project.org/package=psych Stan Development Team. (2022a). Accessing the contents of a stanfit object. https://CRAN.R-project.org/package=rstan/vignettes/stanfit-objects.html Stan Development Team. (2022b). Stan reference manual, Version 2.29. https://mc-stan.org/docs/2_29/reference-manual/ Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; Bürkner, P.-C. (2021). Rank-normalization, folding, and localization: An improved $\\widehatR$ for assessing convergence of MCMC (with Discussion). Bayesian Analysis, 16(2), 667–718. https://doi.org/10.1214/20-BA1221 Wilke, C. O. (2020). Themes. https://wilkelab.org/cowplot/articles/themes.html Wilke, C. O. (2019). Fundamentals of data visualization. https://clauswilke.com/dataviz/ We will get a proper introduction to the binomial probability distribution in Section 11.1.2.↩︎ Kruschke briefly addressed this in his footnote #2 in Section 6.2 (p. 126). We will connect the Bernoulli and binomial approaches more fully in Chapter 21.↩︎ "],["jags-brms.html", "8 JAGS brms 8.1 JAGS brms and its relation to R 8.2 A complete example 8.3 Simplified scripts for frequently used analyses 8.4 Example: Difference of biases 8.5 Sampling from the prior distribution in JAGS brms 8.6 Probability distributions available in JAGS brms 8.7 Faster sampling with parallel processing in runjags brms::brm() 8.8 Tips for expanding JAGS brms models Session info", " 8 JAGS brms We, of course, will be using brms in place of JAGS. 8.1 JAGS brms and its relation to R In the opening paragraph in his GitHub repository for brms, Bürkner explained: The brms package provides an interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan, which is a C++ package for performing full Bayesian inference (see http://mc-stan.org/). The formula syntax is very similar to that of the package lme4 to provide a familiar and simple interface for performing regression analyses. A wide range of response distributions are supported, allowing users to fit – among others – linear, robust linear, count data, survival, response times, ordinal, zero-inflated, and even self-defined mixture models all in a multilevel context. Further modeling options include non-linear and smooth terms, auto-correlation structures, censored data, missing value imputation, and quite a few more. In addition, all parameters of the response distribution can be predicted in order to perform distributional regression. Multivariate models (i.e., models with multiple response variables) can be fit, as well. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. Model fit can easily be assessed and compared with posterior predictive checks, cross-validation, and Bayes factors. (emphasis in the original) Bürkner’s brms repository includes many helpful links, such as to where brms lives on CRAN, a list of blog posts highlighting brms, and a forum where users can ask questions about brms in specific or about Stan in general. You can install the current official version of brms in the same way you would any other R package (i.e., install.packages(\"brms\", dependencies = T)). If you want the current developmental version, you could download it from GitHub by executing the following. if (!requireNamespace(&quot;devtools&quot;)) { install.packages(&quot;devtools&quot;) } devtools::install_github(&quot;paul-buerkner/brms&quot;) 8.2 A complete example We express the likelihood for our coin toss example as \\[y_{i} \\sim \\operatorname{Bernoulli}(\\theta).\\] Our prior will be \\[\\theta \\sim \\operatorname{Beta}(\\alpha, \\beta).\\] Kruschke pictured the relationships among the data, the likelihood, and the prior in the model diagram in Figure 8.2 (p. 196). If you’re tricky, you can make those in R. I’m not going to walk my method out in great detail in this ebook. If you want a step-by-step tutorial, check my blog post, Make model diagrams, Kruschke style. Here’s the code for our version of Figure 8.2. library(tidyverse) library(patchwork) # plot of a beta density p1 &lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = &quot;grey67&quot;) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;beta&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(A)*&#39;, &#39;*italic(B)&quot;, size = 7, family = &quot;Times&quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) ## an annotated arrow # save our custom arrow settings my_arrow &lt;- arrow(angle = 20, length = unit(0.35, &quot;cm&quot;), type = &quot;closed&quot;) p2 &lt;- tibble(x = .5, y = 1, xend = .5, yend = 0) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow) + annotate(geom = &quot;text&quot;, x = .375, y = 1/3, label = &quot;&#39;~&#39;&quot;, size = 10, family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # bar plot of Bernoulli data p3 &lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = &quot;grey67&quot;, width = .4) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;Bernoulli&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .94, label = &quot;theta&quot;, size = 7, family = &quot;Times&quot;, parse = T) + xlim(-.75, 1.75) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # another annotated arrow p4 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p5 &lt;- tibble(x = 1, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = &quot;Times&quot;) + xlim(0, 2) + theme_void() layout &lt;- c( area(t = 1, b = 2, l = 1, r = 1), area(t = 3, b = 3, l = 1, r = 1), area(t = 4, b = 5, l = 1, r = 1), area(t = 6, b = 6, l = 1, r = 1), area(t = 7, b = 7, l = 1, r = 1) ) (p1 + p2 + p3 + p4 + p5) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Diagrams like Figure 8.2 should be scanned from the bottom up. This is because models of data always start with the data, then conceive of a likelihood function that describes the data values in terms of meaningful parameters, and finally determine a prior distribution over the parameters. (p. 196) 8.2.1 Load data. “Logically, models of data start with the data. We must know their basic scale and structure to conceive of a descriptive model” (p. 197). Here we load the data with the readr::read_csv() function, the tidyverse version of base R read.csv(). my_data &lt;- read_csv(&quot;data.R/z15N50.csv&quot;) Unlike what Kruschke wrote about JAGS, the brms package does not require us to convert the data into a list. It can handle data in lists or data frames, of which tibbles are a special case. Here are what the data look like. glimpse(my_data) ## Rows: 50 ## Columns: 1 ## $ y &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0… We might visualize them in a bar plot. library(cowplot) my_data %&gt;% mutate(y = y %&gt;% as.character()) %&gt;% ggplot(aes(x = y)) + geom_bar() + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + theme_minimal_hgrid() For the past couple chapters, we’ve been using the cowplot::theme_cowplot() theme for our plots. The cowplot package, however, includes a few more. The theme_minimal_grid() is similar, but subtracts the axis lines and adds in minimalistic grid lines. In his (2020) Themes vignette, however, Wilke showed vertical grid lines are often unsightly in bar plots. To avoid offending Wilke with our grid lines, we used the theme_minimal_hgrid() theme, which only added horizontal grid lines. Anyway, if you wanted to compute “Ntotal”, the number of rows in our tibble, one way is with count(). my_data %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 50 However, we’re not going to do anything with an “Ntotal” value. For brms, the data are fine in their current data frame form. No need for a dataList. 8.2.2 Specify model. Let’s open brms. library(brms) The brms package does not have code blocks following the JAGS format or the sequence in Kruschke’s diagrams. Rather, its syntax is modeled in part after the popular frequentist mixed-effects package, lme4. To learn more about how brms compares to lme4, see Bürkner’s (2017) overview, brms: An R package for Bayesian multilevel models using Stan. The primary function in brms is brm(). Into this one function we will specify the data, the model, the likelihood function, the prior(s), and any technical settings such as the number of MCMC chains, iterations, and so forth. You can order the arguments in any way you like. My typical practice is to start with data, family (i.e., the likelihood function), the model formula, and my priors. If there are any technical specifications such as the number of MCMC iterations I’d like to change from their default values, I usually do that last. Here’s how to fit the model. fit8.1 &lt;- brm(data = my_data, family = bernoulli(link = identity), formula = y ~ 1, prior(beta(2, 2), class = Intercept, lb = 0, ub = 1), iter = 500 + 3334, warmup = 500, chains = 3, seed = 8, file = &quot;fits/fit08.01&quot;) Also note our use of the file argument. This automatically saved the fit object as an external file. You don’t have to do that and you can avoid doing so by omitting the file argument. For a more detailed explanation of the brms::brm() function, spend some time with the brm section of the brms reference manual (Bürkner, 2022e). 8.2.3 Initialize chains. In Stan, and in brms by extension, the initial values have default settings. In the Initialization section of the Program Execution chapter in the Stan reference manual, Version 2.29 (Stan Development Team, 2022b) we read: If there are no user-supplied initial values, the default initialization strategy is to initialize the unconstrained parameters directly with values drawn uniformly from the interval \\((−2, 2)\\). The bounds of this initialization can be changed but it is always symmetric around \\(0\\). The value of \\(0\\) is special in that it represents the median of the initialization. An unconstrained value of \\(0\\) corresponds to different parameter values depending on the constraints declared on the parameters. In general, I do not recommend setting custom initial values in brms or Stan. Under the hood, Stan will transform the parameters to the unconstrained space in models where they are bounded. In our Bernoulli model, \\(\\theta\\) is bounded at 0 and 1. A little further down in the same section, we read: For parameters bounded above and below, the initial value of \\(0\\) on the unconstrained scale corresponds to a value at the midpoint of the constraint interval. For probability parameters, bounded below by \\(0\\) and above by \\(1\\), the transform is the inverse logit, so that an initial unconstrained value of \\(0\\) corresponds to a constrained value of \\(0.5\\), \\(-2\\) corresponds to \\(0.12\\) and \\(2\\) to \\(0.88\\). Bounds other than \\(0\\) and \\(1\\) are just scaled and translated. If you want to play around with this, have at it. In my experience, it sometimes helps to set these manually to zero, which you can do that by specifying inits = 0 within brm(). But if you really want to experiment, you might check out my blog post, Don’t forget your inits. 8.2.4 Generate chains. By default, brms will use 4 chains of 2,000 iterations each. The type of MCMC brms uses is Hamiltonian Monte Carlo (HMC). You can learn more about HMC at the Stan website, https://mc-stan.org, which includes resources such as the Stan user’s guide (Stan Development Team, 2022c), the Stan reference manual (Stan Development Team, 2022b), and a list of tutorials. McElreath has a nice intro lecture on MCMC in general and HMC in particular. Michael Bentacourt has some good lectures on Stan and HMC, such as here and here. And, of course, we will cover HMC with Kruschke in Chapter 14. Within each HMC chain, the first \\(n\\) iterations are warmups. Within the Stan-HMC paradigm, warmups are somewhat analogous to but not synonymous with burn-in iterations as done by the Gibbs sampling in JAGS. But HMC warmups are like Gibbs burn-ins in that both are discarded and not used to describe the posterior. As such, the brms default settings yield 1,000 post-warmup iterations for each of the 4 HMC chains. However, we specified iter = 500 + 3334, warmup = 500, chains = 3. Thus instead of defaults, we have 3 HMC chains. Each chain has 500 + 3,334 = 3,834 total iterations, of which 500 were discarded warmup iterations. To learn more about the warmup stage in Stan, check out the HMC Algorithm Parameters section of the MCMC Sampling chapter of the Stan reference manual. 8.2.5 Examine chains. The brms::plot() function returns a density and trace plot for each model parameter. plot(fit8.1) Note how the brms::plot() function simply took our model fit object as input. Other post-processing functions will require us to pass the posterior draws in the form of a data frame. To get ready for them, here we’ll save them as a data frame with the as_draws_df() function. draws &lt;- as_draws_df(fit8.1) If you want to display each chain as its own density, you can use the handy mcmc_dens_overlay() function from the bayesplot package. library(bayesplot) Now we’re ready to use our draws object within the mcmc_dens_overlay() function to return the overlaid densities. draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_dens_overlay(pars = vars(b_Intercept)) + theme_minimal_hgrid() The bayesplot::mcmc_acf() function will give us the autocorrelation plots. draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_acf(pars = vars(b_Intercept), lags = 35) + theme_minimal_hgrid() With brms functions, we get a sole \\(\\widehat R\\) value for each parameter rather than a running vector. rhat(fit8.1)[&quot;b_Intercept&quot;] ## b_Intercept ## 1.00177 We’ll have to employ brms::as.mcmc() and coda::gelman.plot() to make our running \\(\\widehat R\\) plot. fit8.1_c &lt;- as.mcmc(fit8.1) coda::gelman.plot(fit8.1_c[, &quot;b_Intercept&quot;, ]) For whatever reason, many of the package developers within the Stan/brms ecosystem don’t seem interested in shrink factor plots, like this. 8.2.5.1 The plotPost function How to plot your brms posterior distributions. We’ll get into plotting in just a moment. But before we do, here’s a summary of the model. print(fit8.1) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: my_data (Number of observations: 50) ## Draws: 3 chains, each with iter = 3834; warmup = 500; thin = 1; ## total post-warmup draws = 10002 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.32 0.06 0.20 0.44 1.00 3617 5069 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). To summarize a posterior in terms of central tendency, brms defaults to the mean value (i.e., the value in the ‘Estimate’ column of the print() output). In many of the other convenience functions, you can also request the median instead. For example, we can set robust = TRUE to get the ‘Estimate’ in terms of the median. posterior_summary(fit8.1, robust = T) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.3136384 0.06298898 0.19949123 0.4444603 ## lprior 0.2558942 0.10535036 -0.04273333 0.3927531 ## lp__ -32.0713842 0.30693948 -34.40289913 -31.8453143 Across functions, the intervals default to 95%. With print() and summary() you can adjust the level with a prob argument. For example, here we’ll use 50% intervals. print(fit8.1, prob = .5) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: my_data (Number of observations: 50) ## Draws: 3 chains, each with iter = 3834; warmup = 500; thin = 1; ## total post-warmup draws = 10002 ## ## Population-Level Effects: ## Estimate Est.Error l-50% CI u-50% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.32 0.06 0.27 0.36 1.00 3617 5069 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). But in many other brms convenience functions, you can use the probs argument to request specific percentile summaries. posterior_summary(fit8.1, probs = c(.025, .25, .75, .975)) ## Estimate Est.Error Q2.5 Q25 Q75 Q97.5 ## b_Intercept 0.3160699 0.06281598 0.19949123 0.2721476 0.3571788 0.4444603 ## lprior 0.2354402 0.11358623 -0.04273333 0.1726917 0.3203520 0.3927531 ## lp__ -32.3504057 0.71534517 -34.40289913 -32.5122278 -31.8979067 -31.8453143 Regardless of what prob or probs levels you use, brms functions always return percentile-based estimates. All this central tendency and interval talk will be important in a moment… When plotting the posterior distribution of a parameter estimated with brms, you typically do so working with the results of an object returned by one of the as_draws_ functions. Recall we already saved those results as draws. Here’s a look at draws. head(draws) ## # A draws_df: 6 iterations, 1 chains, and 3 variables ## b_Intercept lprior lp__ ## 1 0.26 0.133 -32 ## 2 0.19 -0.080 -34 ## 3 0.21 0.011 -33 ## 4 0.27 0.165 -32 ## 5 0.28 0.193 -32 ## 6 0.31 0.244 -32 ## # ... hidden reserved variables {&#39;.chain&#39;, &#39;.iteration&#39;, &#39;.draw&#39;} With draws in hand, we can use ggplot2 to do the typical distributional plots, such as with geom_histogram(). draws %&gt;% ggplot(aes(x = b_Intercept)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Theta via ggplot2::geom_histogram()&quot;, x = expression(theta)) + theme_minimal_hgrid() + theme(plot.title.position = &quot;plot&quot;) The bayesplot::mcmc_areas() function offers a nice way to depict the posterior densities, along with their percentile-based 50% and 95% ranges. mcmc_areas( draws, pars = vars(b_Intercept), prob = 0.5, prob_outer = 0.95, point_est = &quot;mean&quot; ) + scale_y_discrete(expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Theta via bayesplot::mcmc_areas()&quot;, x = expression(theta)) + theme_minimal_hgrid() + theme(plot.title.position = &quot;plot&quot;) brms doesn’t have a convenient way to compute the posterior mode or HDIs. Base R is no help, either. But Matthew Kay’s tidybayes package makes it easy to compute posterior modes and HDIs with handy functions like stat_halfeye() and stat_histinterval(). library(tidybayes) draws %&gt;% ggplot(aes(x = b_Intercept, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(theta*&quot; via tidybayes::stat_halfeye()&quot;), x = expression(theta)) + theme_minimal_hgrid() draws %&gt;% ggplot(aes(x = b_Intercept, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(theta*&quot; via tidybayes::stat_histinterval()&quot;), x = expression(theta)) + theme_minimal_hgrid() The tidybayes::stat_halfeye() function returns a density with a measure of the posterior’s central tendency in a dot and one or multiple interval bands as horizontal lines at the base of the density. The stat_histinterval() function returns much the same, but replaces the density with a histogram. For both functions, the point_interval = mode_hdi argument allowed us to request the mode to be our measure of central tendency and the highest posterior density intervals to be our type intervals. With .width = c(.95, .5), we requested our HDIs be at both the 95% and 50% levels. If we wanted to be more congruent with Kruschke’s plotting sensibilities, we could further modify tidybayes::stat_histinterval(). # this is unnecessary, but makes for nicer x-axis breaks my_breaks &lt;- mode_hdi(draws$b_Intercept)[, 1:3] %&gt;% pivot_longer(everything(), values_to = &quot;breaks&quot;) %&gt;% mutate(labels = breaks %&gt;% round(digits = 3)) # here&#39;s the main plot code draws %&gt;% ggplot(aes(x = b_Intercept, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = &quot;steelblue&quot;, slab_color = &quot;white&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_x_continuous(breaks = my_breaks$breaks, labels = my_breaks$labels) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;tidybayes::stat_histinterval() all gussied up&quot;, x = expression(theta)) + theme_minimal_hgrid() + theme(title = element_text(size = 10.5)) With the point_interval argument within stat_histinterval() and related functions, we can request different combinations of measures of central tendency (i.e., mean, median, mode) and interval types (i.e., percentile-based and HDIs). Although all of these are legitimate ways to summarize a posterior, they can yield somewhat different results. For example, here we’ll contrast our mode + HDI summary with a median + percentile-based interval summary using tidybayes::stat_pointinterval(). draws %&gt;% ggplot(aes(x = b_Intercept)) + stat_pointinterval(aes(y = 1), point_interval = median_qi, .width = c(.95, .5)) + stat_pointinterval(aes(y = 2), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = 1:2, labels = c(&quot;median_qi&quot;, &quot;mode_hdi&quot;)) + coord_cartesian(ylim = c(0, 3)) + labs(title = &quot;Theta via tidybayes::stat_pointinterval()&quot;, x = expression(theta)) + theme_minimal_vgrid() + theme(axis.line.y.left = element_blank(), axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), plot.title.position = &quot;plot&quot;, title = element_text(size = 10.5)) Similar, yet distinct. To get a sense of the full variety of ways tidybayes allows users to summarize and plot the results of a Bayesian model, check out Kay’s (2022) vignette, Slab + interval stats and geoms. Also, did you notice how we switched to theme_minimal_vgrid()? That’s how we added the vertical grid lines in the absence of horizontal grid lines. 8.3 Simplified scripts for frequently used analyses A lot has happened in R for Bayesian analysis since Kruschke wrote his (2015) text. In addition to our use of the tidyverse, the brms, bayesplot, and tidybayes packages offer an array of useful convenience functions. We can and occasionally will write our own. But really, the rich R ecosystem already has us pretty much covered. 8.4 Example: Difference of biases Here are our new data. my_data &lt;- read_csv(&quot;data.R/z6N8z2N7.csv&quot;) glimpse(my_data) ## Rows: 15 ## Columns: 2 ## $ y &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0 ## $ s &lt;chr&gt; &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reg… They look like this. library(ggthemes) my_data %&gt;% mutate(y = y %&gt;% as.character()) %&gt;% ggplot(aes(x = y, fill = s)) + geom_bar(show.legend = F) + scale_fill_colorblind() + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + theme_minimal_hgrid() + facet_wrap(~ s) Note our use of the scale_fill_colorblind() function from the ggthemes package (Arnold, 2021). When you want to use color to emphasize different factor levels, such as s, it’s a good idea to make sure those colors can be distinguished by folks who are colorblind. The scale_fill_colorblind() function provides a discrete palette of eight colorblind safe colors. If you’d prefer to use a different palette, but wand to make sure its accessible to folks with colorblindness, check out the colorblindr package (McWhite &amp; Wilke, 2021). Here’s our ggplot2 version of the model diagram in Figure 8.5. # plot of a beta density p1 &lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = &quot;steelblue&quot;) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;beta&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(A)*&#39;, &#39;*italic(B)&quot;, size = 7, family = &quot;Times&quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # an annotated arrow p2 &lt;- tibble(x = c(.35, .65), y = 1/3, label = c(&quot;&#39;~&#39;&quot;, &quot;italic(s)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # bar plot of Bernoulli data p3 &lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = &quot;steelblue&quot;, width = .4) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;Bernoulli&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .92, label = &quot;theta[italic(s)]&quot;, size = 7, family = &quot;Times&quot;, parse = T) + xlim(-.75, 1.75) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # another annotated arrow p4 &lt;- tibble(x = c(.35, .65), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)*&#39;|&#39;*italic(s)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p5 &lt;- tibble(x = 1, y = .5, label = &quot;italic(y)[italic(i)*&#39;|&#39;*italic(s)]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = &quot;Times&quot;) + xlim(0, 2) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 1, r = 1), area(t = 3, b = 3, l = 1, r = 1), area(t = 4, b = 5, l = 1, r = 1), area(t = 6, b = 6, l = 1, r = 1), area(t = 7, b = 7, l = 1, r = 1) ) # combine and plot! (p1 + p2 + p3 + p4 + p5) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Here is how we might fit the model with brms::brm(). fit8.2 &lt;- brm(data = my_data, family = bernoulli(identity), y ~ 0 + s, prior(beta(2, 2), class = b, lb = 0, ub = 1), iter = 2000, warmup = 500, cores = 4, chains = 4, seed = 8, file = &quot;fits/fit08.02&quot;) More typically, we’d parameterize the model as y ~ 1 + s. This form would yield an intercept and a slope. Behind the scenes, brms would treat the nominal s variable as an 0-1 coded dummy variable. One of the nominal levels would become the reverence category, depicted by the Intercept, and the difference between that and the other category would be the s slope. However, with our y ~ 0 + s syntax, we’ve suppressed the typical model intercept. The consequence is that each level of the nominal variable s gets its own intercept or [i] index, if you will. This is analogous to Kruschke’s y[i] ∼ dbern(theta[s[i]]) code. All that aside, here are the chains. plot(fit8.2, widths = c(2, 3)) The model summary() is as follows: summary(fit8.2) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 0 + s ## Data: my_data (Number of observations: 15) ## Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup draws = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sReginald 0.67 0.13 0.38 0.89 1.00 5152 3519 ## sTony 0.36 0.14 0.12 0.66 1.00 4629 4001 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The brms::pairs() function gets us the bulk of Figure 8.6. pairs(fit8.2, off_diag_args = list(size = 1/3, alpha = 1/3)) But to get at that difference-score distribution, we’ll have extract the posterior draws with as_draws_df(), make difference score with mutate(), and manually plot with ggplot2. draws &lt;- as_draws_df(fit8.2) draws &lt;- draws %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% mutate(`theta_Reginald - theta_Tony` = theta_Reginald - theta_Tony) glimpse(draws) ## Rows: 6,000 ## Columns: 8 ## $ theta_Reginald &lt;dbl&gt; 0.7757244, 0.6615629, 0.6392801, 0.5430331, 0.5246347, 0.525… ## $ theta_Tony &lt;dbl&gt; 0.3163358, 0.4048103, 0.3239676, 0.2919574, 0.2351882, 0.209… ## $ lprior &lt;dbl&gt; 0.30344208, 0.66373987, 0.59782687, 0.61339128, 0.47929929, … ## $ lp__ &lt;dbl&gt; -11.69349, -11.30482, -11.32344, -11.77508, -12.21795, -12.4… ## $ .chain &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ .iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… ## $ .draw &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… ## $ `theta_Reginald - theta_Tony` &lt;dbl&gt; 0.45938855, 0.25675262, 0.31531244, 0.25107566, 0.28944655, … long_draws &lt;- draws %&gt;% select(starts_with(&quot;theta&quot;)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;theta_Reginald&quot;, &quot;theta_Tony&quot;, &quot;theta_Reginald - theta_Tony&quot;))) long_draws %&gt;% ggplot(aes(x = value, y = 0, fill = name)) + stat_histinterval(point_interval = mode_hdi, .width = .95, slab_color = &quot;white&quot;, outline_bars = T, normalize = &quot;panels&quot;) + scale_fill_manual(values = colorblind_pal()(8)[2:4], breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + theme_minimal_hgrid() + facet_wrap(~ name, scales = &quot;free&quot;) Note how this time we used the colorblind_pal() function within scale_fill_manual() to manually select three of the of the scale_fill_colorblind() colors. Anyway, here’s a way to get the numeric summaries out of post. long_draws %&gt;% group_by(name) %&gt;% mode_hdi() ## # A tibble: 3 × 7 ## name value .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 theta_Reginald 0.709 0.405 0.908 0.95 mode hdi ## 2 theta_Tony 0.328 0.102 0.631 0.95 mode hdi ## 3 theta_Reginald - theta_Tony 0.334 -0.0720 0.670 0.95 mode hdi In this context, the mode_hdi() summary yields: name (i.e., the name we used to denote the parameters) value (i.e., the value of the measure of central tendency) .lower (i.e., the lower level of the 95% HDI) .upper (i.e., the upper level…) .width (i.e., what interval we used) .point (i.e., the type of measure of central tendency) .interval (i.e., the type of interval) 8.5 Sampling from the prior distribution in JAGS brms There are a few ways to sample from the prior distribution with brms. Here we’ll practice by setting sample_prior = \"only\". As a consequence, brm() will ignore the likelihood and return draws based solely on the model priors. fit8.3 &lt;- brm(data = my_data, family = bernoulli(identity), y ~ 0 + s, prior = c(prior(beta(2, 2), class = b, coef = sReginald), prior(beta(2, 2), class = b, coef = sTony), # this just sets the lower and upper bounds prior(beta(2, 2), class = b, lb = 0, ub = 1)), iter = 2000, warmup = 500, cores = 4, chains = 4, sample_prior = &quot;only&quot;, seed = 8, file = &quot;fits/fit08.03&quot;) Because we set sample_prior = \"only\", the as_draws_df() function will now return draws from the priors. draws &lt;- as_draws_df(fit8.3) %&gt;% select(starts_with(&quot;b_&quot;)) head(draws) ## # A tibble: 6 × 2 ## b_sReginald b_sTony ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.693 0.942 ## 2 0.156 0.448 ## 3 0.242 0.176 ## 4 0.452 0.445 ## 5 0.275 0.818 ## 6 0.987 0.397 With our prior draws in hand, we’re almost ready to make the prior histograms of Figure 8.7. But first we’ll want to determine the \\(z/N\\) values in order to mark them off in the plots. [You’ll note Kruschke did so with gray plus marks in his.] my_data %&gt;% group_by(s) %&gt;% summarise(z = sum(y), N = n()) %&gt;% mutate(`z/N` = z / N) ## # A tibble: 2 × 4 ## s z N `z/N` ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Reginald 6 8 0.75 ## 2 Tony 2 7 0.286 levels &lt;- c(&quot;theta_Reginald&quot;, &quot;theta_Tony&quot;, &quot;theta_Reginald - theta_Tony&quot;) d_line &lt;- tibble(value = c(.75, .286, .75 - .286), name = factor(c(&quot;theta_Reginald&quot;, &quot;theta_Tony&quot;, &quot;theta_Reginald - theta_Tony&quot;), levels = levels)) Behold the histograms of Figure 8.7. draws %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% mutate(`theta_Reginald - theta_Tony` = theta_Reginald - theta_Tony) %&gt;% pivot_longer(contains(&quot;theta&quot;)) %&gt;% mutate(name = factor(name, levels = levels)) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = colorblind_pal()(8)[5], normalize = &quot;panels&quot;) + geom_vline(data = d_line, aes(xintercept = value), linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(&quot;The dashed vertical lines mark off &quot;*italic(z[s])/italic(N[s]))) + theme_cowplot() + facet_wrap(~ name, scales = &quot;free&quot;) Here’s how you might make the scatter plot. draws %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% ggplot(aes(x = theta_Reginald, y = theta_Tony)) + geom_point(alpha = 1/4, color = colorblind_pal()(8)[6]) + coord_equal() + theme_minimal_grid() Or you could always use a two-dimensional density plot with stat_density_2d(). draws %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% ggplot(aes(x = theta_Reginald, y = theta_Tony)) + stat_density_2d(aes(fill = stat(density)), geom = &quot;raster&quot;, contour = F) + scale_fill_viridis_c(option = &quot;B&quot;, breaks = NULL) + scale_x_continuous(expression(theta[1]), expand = c(0, 0), limits = c(0, 1), breaks = 0:4 / 4, labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + scale_y_continuous(expression(theta[2]), expand = c(0, 0), limits = c(0, 1), breaks = 0:4 / 4, labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + coord_equal() + theme_minimal_grid() The viridis color palettes in functions like scale_fill_viridis_c() and scale_fill_viridis_d() are designed to be colorblind safe, too (Garnier, 2021). 8.6 Probability distributions available in JAGS brms [brms] has a large collection of frequently used probability distributions that are built-in. These distributions include the beta, gamma, normal, Bernoulli, and binomial along with many others. A complete list of distributions, and their [brms] names, can be found in [Bürkner’s (2022c) vignette Parameterization of response distributions in brms]. (Kruschke, 2015, pp. 213–214, emphasis added) 8.6.1 Defining new likelihood functions. In addition to all the likelihood functions listed in above mentioned vignette, you can also make your own likelihood functions. Bürkner explained the method in his (2022) vignette, Define custom response distributions with brms. 8.7 Faster sampling with parallel processing in runjags brms::brm() We don’t need to open another package to sample in parallel in brms. In fact, we’ve already been doing that. Take another look at the code use used for the last model, fit8.2. fit8.2 &lt;- brm(data = my_data, family = bernoulli(identity), y ~ 0 + s, prior(beta(2, 2), class = b, lb = 0, ub = 1), iter = 2000, warmup = 500, cores = 4, chains = 4, seed = 8, file = &quot;fits/fit08.02&quot;) See the cores = 4, chains = 4 arguments? With that bit of code, we told brms::brm() we wanted 4 chains, which we ran in parallel across 4 cores. 8.8 Tips for expanding JAGS brms models I’m in complete agreement with Kruschke, here: Often, the process of programming a model is done is stages, starting with a simple model and then incrementally incorporating complexifications. At each step, the model is checked for accuracy and efficiency. This procedure of incremental building is useful for creating a desired complex model from scratch, for expanding a previously created model for a new application, and for expanding a model that has been found to be inadequate in a posterior predictive check. (p. 218) Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggthemes_4.2.4 tidybayes_3.0.2 bayesplot_1.9.0 brms_2.18.0 Rcpp_1.0.9 cowplot_1.1.1 ## [7] patchwork_1.1.2 forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 ## [13] tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 igraph_1.3.4 ## [5] svUnit_1.0.6 splines_4.2.0 crosstalk_1.2.0 TH.data_1.1-1 ## [9] rstantools_2.2.0 inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 googlesheets4_1.0.1 ## [17] tzdb_0.3.0 modelr_0.1.8 RcppParallel_5.1.5 matrixStats_0.62.0 ## [21] vroom_1.5.7 xts_0.12.1 sandwich_3.0-2 prettyunits_1.1.1 ## [25] colorspace_2.0-3 rvest_1.0.2 ggdist_3.2.0 haven_2.5.1 ## [29] xfun_0.35 callr_3.7.3 crayon_1.5.2 jsonlite_1.8.3 ## [33] lme4_1.1-31 survival_3.4-0 zoo_1.8-10 glue_1.6.2 ## [37] gtable_0.3.1 gargle_1.2.0 emmeans_1.8.0 distributional_0.3.1 ## [41] pkgbuild_1.3.1 rstan_2.21.7 abind_1.4-5 scales_1.2.1 ## [45] mvtnorm_1.1-3 DBI_1.1.3 miniUI_0.1.1.1 viridisLite_0.4.1 ## [49] xtable_1.8-4 HDInterval_0.2.2 bit_4.0.4 stats4_4.2.0 ## [53] StanHeaders_2.21.0-7 DT_0.24 htmlwidgets_1.5.4 httr_1.4.4 ## [57] threejs_0.3.3 arrayhelpers_1.1-0 posterior_1.3.1 ellipsis_0.3.2 ## [61] pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 sass_0.4.2 ## [65] dbplyr_2.2.1 utf8_1.2.2 tidyselect_1.1.2 labeling_0.4.2 ## [69] rlang_1.0.6 reshape2_1.4.4 later_1.3.0 munsell_0.5.0 ## [73] cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 cli_3.5.0 ## [77] generics_0.1.3 broom_1.0.1 ggridges_0.5.3 evaluate_0.18 ## [81] fastmap_1.1.0 processx_3.8.0 knitr_1.40 bit64_4.0.5 ## [85] fs_1.5.2 nlme_3.1-159 projpred_2.2.1 mime_0.12 ## [89] xml2_1.3.3 compiler_4.2.0 shinythemes_1.2.0 rstudioapi_0.13 ## [93] gamm4_0.2-6 reprex_2.0.2 bslib_0.4.0 stringi_1.7.8 ## [97] highr_0.9 ps_1.7.2 Brobdingnag_1.2-8 lattice_0.20-45 ## [101] Matrix_1.4-1 nloptr_2.0.3 markdown_1.1 shinyjs_2.1.0 ## [105] tensorA_0.36.2 vctrs_0.5.1 pillar_1.8.1 lifecycle_1.0.3 ## [109] jquerylib_0.1.4 bridgesampling_1.1-2 estimability_1.4.1 httpuv_1.6.5 ## [113] R6_2.5.1 bookdown_0.28 promises_1.2.0.1 gridExtra_2.3 ## [117] codetools_0.2-18 boot_1.3-28 colourpicker_1.1.1 MASS_7.3-58.1 ## [121] gtools_3.9.3 assertthat_0.2.1 withr_2.5.0 shinystan_2.6.0 ## [125] multcomp_1.4-20 mgcv_1.8-40 parallel_4.2.0 hms_1.1.1 ## [129] grid_4.2.0 minqa_1.2.5 coda_0.19-4 rmarkdown_2.16 ## [133] googledrive_2.0.0 shiny_1.7.2 lubridate_1.8.0 base64enc_0.1-3 ## [137] dygraphs_1.1.1.6 References Arnold, J. B. (2021). ggthemes: Extra themes, scales and geoms for ’ggplot2’. https://CRAN.R-project.org/package=ggthemes Bürkner, P.-C. (2022). Define custom response distributions with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html Bürkner, P.-C. (2022c). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html Bürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01 Bürkner, P.-C. (2022e). brms: Bayesian regression models using ’Stan’. https://CRAN.R-project.org/package=brms Garnier, S. (2021). viridis: Default color maps from ’matplotlib’ [Manual]. https://CRAN.R-project.org/package=viridis Kay, M. (2022). Slab + interval stats and geoms. https://mjskay.github.io/ggdist/articles/slabinterval.html Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ McWhite, C. D., &amp; Wilke, C. O. (2021). colorblindr: Simulate colorblindness in R figures [Manual]. https://github.com/clauswilke/colorblindr Stan Development Team. (2022b). Stan reference manual, Version 2.29. https://mc-stan.org/docs/2_29/reference-manual/ Stan Development Team. (2022c). Stan user’s guide, Version 2.29. https://mc-stan.org/docs/2_29/stan-users-guide/index.html Wilke, C. O. (2020). Themes. https://wilkelab.org/cowplot/articles/themes.html "],["hierarchical-models.html", "9 Hierarchical Models 9.1 A single coin from a single mint 9.2 Multiple coins from a single mint 9.3 Shrinkage in hierarchical models 9.4 Speeding up JAGS brms 9.5 Extending the hierarchy: Subjects within categories Session info", " 9 Hierarchical Models As Kruschke put it, “There are many realistic situations that involve meaningful hierarchical structure. Bayesian modeling software makes it straightforward to specify and analyze complex hierarchical models” (2015, p. 221). IMO, brms makes it even easier than JAGS. Further down, we read: The parameters at different levels in a hierarchical model are all merely parameters that coexist in a joint parameter space. We simply apply Bayes’ rule to the joint parameter space, as we did for example when estimating two coin biases back in Figure 7.5, p. 167. To say it a little more formally with our parameters \\(\\theta\\) and \\(\\omega\\), Bayes’ rule applies to the joint parameter space: \\(p(\\theta, \\omega | D) \\propto p(D | \\theta, \\omega) p(\\theta, \\omega)\\). What is special to hierarchical models is that the terms on the right-hand side can be factored into a chain of dependencies, like this: \\[\\begin{align*} p(\\theta, \\omega | D) &amp; \\propto p(D | \\theta, \\omega) \\; p(\\theta, \\omega) \\\\ &amp; = p(D | \\theta) \\; p(\\theta | \\omega) \\; p(\\omega) \\end{align*}\\] The refactoring in the second line means that the data depend only on the value of \\(\\theta\\), in the sense that when the value \\(\\theta\\) is set then the data are independent of all other parameter values. Moreover, the value of \\(\\theta\\) depends on the value of \\(\\omega\\) and the value of \\(\\theta\\) is conditionally independent of all other parameters. Any model that can be factored into a chain of dependencies like [this] is a hierarchical model. (pp. 222–223) 9.1 A single coin from a single mint Recall from the last chapter that our likelihood is the Bernoulli distribution, \\[y_i \\sim \\operatorname{Bernoulli}(\\theta).\\] We’ll use the beta density for our prior distribution for \\(\\theta\\), \\[\\theta \\sim \\operatorname{Beta}(\\alpha, \\beta).\\] And we can re-express \\(\\alpha\\) and \\(\\beta\\) in terms of the mode \\(\\omega\\) and concentration \\(\\kappa\\), such that \\[\\alpha = \\omega(\\kappa - 2) + 1 \\;\\;\\; \\textrm{and} \\;\\;\\; \\beta = (1 - \\omega)(\\kappa - 2) + 1.\\] As a consequence, we can re-express \\(\\theta\\) as \\[\\theta \\sim \\operatorname{Beta}(\\omega(\\kappa - 2) + 1, (1 - \\omega)(\\kappa - 2) + 1).\\] On page 224, Kruschke wrote: “The value of \\(\\kappa\\) governs how near \\(\\theta\\) is to \\(\\omega\\), with larger values of \\(\\kappa\\) generating values of \\(\\theta\\) more concentrated near \\(\\omega\\).” To give a sense of that, we’ll simulate 20 beta distributions, all with \\(\\omega = .25\\) but with \\(\\theta\\) increasing from 10 to 200, by 10. We’ll then plot them with a little help from the ggridges package (Wilke, 2021). library(tidyverse) library(cowplot) library(ggridges) beta_by_k &lt;- function(k) { w &lt;- .25 tibble(x = seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(theta = dbeta(x = x, shape1 = w * (k - 2) + 1, shape2 = (1 - w) * (k - 2) + 1)) } tibble(k = seq(from = 10, to = 200, by = 10)) %&gt;% mutate(theta = map(k, beta_by_k)) %&gt;% unnest(theta) %&gt;% ggplot(aes(x = x, y = k, height = theta, group = k, fill = k)) + geom_vline(xintercept = .25, color = &quot;grey85&quot;, size = 1/2) + geom_ridgeline(size = 1/5, color = &quot;grey92&quot;, scale = 2) + scale_fill_viridis_c(expression(kappa), option = &quot;A&quot;) + scale_y_continuous(expression(kappa), breaks = seq(from = 10, to = 200, by = 10)) + xlab(expression(theta)) + theme_minimal_hgrid() Holding \\(\\omega\\) constant, the density gets more concentrated around \\(\\omega\\) as \\(\\kappa\\) increases. But back to the text: “Now we make the essential expansion of our scenario into the realm of hierarchical models. Instead of thinking of \\(\\omega\\) as fixed by prior knowledge, we think of it as another parameter to be estimated” (p. 224). In the hierarchical model diagram of Figure 9.1, Kruschke depicted how we might treat \\(\\omega\\) as a parameter controlled by the prior distribution, \\(\\operatorname{Beta}(A_\\omega, B_\\omega)\\). Here’s our version of the diagram. library(ggforce) library(patchwork) p1 &lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = &quot;grey67&quot;) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;beta&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(A[omega])*&#39;, &#39;*italic(B[omega])&quot;, size = 7, family = &quot;Times&quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. p2 &lt;- tibble(x = c(.5, .475, .26, .08, .06, .5, .55, .85, 1.15, 1.2), y = c(1, .7, .6, .5, .2, 1, .7, .6, .5, .2), line = rep(letters[2:1], each = 5)) %&gt;% ggplot(aes(x = x, y = y)) + geom_bspline(aes(color = line), size = 2/3, show.legend = F) + annotate(geom = &quot;text&quot;, x = 0, y = .125, label = &quot;omega(italic(K)-2)+1*&#39;, &#39;*(1-omega)(italic(K)-2)+1&quot;, size = 7, parse = T, family = &quot;Times&quot;, hjust = 0) + annotate(geom = &quot;text&quot;, x = 1/3, y = .7, label = &quot;&#39;~&#39;&quot;, size = 10, parse = T, family = &quot;Times&quot;) + scale_color_manual(values = c(&quot;grey75&quot;, &quot;black&quot;)) + scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) + ylim(0, 1) + theme_void() p3 &lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = &quot;grey67&quot;) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;beta&quot;, size = 7) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) ## an annotated arrow # save our custom arrow settings my_arrow &lt;- arrow(angle = 20, length = unit(0.35, &quot;cm&quot;), type = &quot;closed&quot;) p4 &lt;- tibble(x = .5, y = 1, xend = .5, yend = 0) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow) + annotate(geom = &quot;text&quot;, x = .375, y = 1/3, label = &quot;&#39;~&#39;&quot;, size = 10, family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # bar plot of Bernoulli data p5 &lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = &quot;grey67&quot;, width = .4) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;Bernoulli&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .94, label = &quot;theta&quot;, size = 7, family = &quot;Times&quot;, parse = T) + xlim(-.75, 1.75) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # another annotated arrow p6 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p7 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 1, r = 1), area(t = 4, b = 5, l = 1, r = 1), area(t = 3, b = 4, l = 1, r = 2), area(t = 6, b = 6, l = 1, r = 1), area(t = 7, b = 8, l = 1, r = 1), area(t = 9, b = 9, l = 1, r = 1), area(t = 10, b = 10, l = 1, r = 1) ) # combine and plot! (p1 + p3 + p2 + p4 + p5 + p6 + p7) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Note that whereas this model includes a hierarchical prior for \\(\\omega\\), the hyperparameter \\(K\\) is fixed across cases. 9.1.1 Posterior via grid approximation. When the parameters extend over a finite domain, and there are not too many of them, then we can approximate the posterior via grid approximation. In our present situation, we have the parameters \\(\\theta\\) and \\(\\omega\\) that both have finite domains, namely the interval \\([0, 1]\\). Therefore, a grid approximation is tractable and the distributions can be readily graphed. (p. 226) Given \\(\\alpha\\) and \\(\\beta\\), we can compute the corresponding mode \\(\\omega\\). To foreshadow, consider \\(\\text{beta}(2, 2)\\). alpha &lt;- 2 beta &lt;- 2 (alpha - 1) / (alpha + beta - 2) ## [1] 0.5 That is, the mode of \\(\\operatorname{Beta}(2, 2)\\) is \\(.5\\). We won’t be able to make the wireframe plots on the left of Figure 9.2, but we can make the others. We’ll make the initial data following Kruschke’s (p. 226) formulas. \\[p(\\theta, \\omega) = p(\\theta | \\omega) \\; p(\\omega) = \\operatorname{Beta} \\big (\\theta | \\omega (100 - 2) + 1, (1 - \\omega) (100 - 2) + 1 \\big ) \\; \\operatorname{Beta}(\\omega | 2, 2)\\] First, we’ll make a custom function, make_prior() based on the formulas. make_prior &lt;- function(theta, omega, alpha, beta, kappa) { # p(theta | omega) t &lt;- dbeta(x = theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) # p(omega) o &lt;- dbeta(x = omega, shape1 = alpha, shape2 = beta) # p(theta, omega) = p(theta | omega) * p(omega) return(t * o) } Next we’ll define the parameter space as a tightly-spaced sequence of values ranging from 0 to 1. parameter_space &lt;- seq(from = 0, to = 1, by = .01) Now we’ll use parameter_space to define the ranges for the two variables, theta and omega, which we’ll save in a tibble. We’ll then sequentially feed those theta and omega values into our make_prior() while manually specifying the desired values for alpha, beta, and kappa. d &lt;- # here we define the grid for our grid approximation crossing(theta = parameter_space, omega = parameter_space) %&gt;% # compute the joint prior mutate(prior = make_prior(theta, omega, alpha = 2, beta = 2, kappa = 100)) %&gt;% # convert the prior from the density metric to the probability metric mutate(prior = prior / sum(prior)) head(d) ## # A tibble: 6 × 3 ## theta omega prior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 ## 2 0 0.01 0 ## 3 0 0.02 0 ## 4 0 0.03 0 ## 5 0 0.04 0 ## 6 0 0.05 0 Now we’re ready to plot the top middle panel of Figure 9.2. d %&gt;% ggplot(aes(x = theta, y = omega, fill = prior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) + coord_equal() + theme_minimal_grid() + theme(legend.position = &quot;none&quot;) You could also make this with geom_tile(), but geom_raster() with interpolate = TRUE smooths the color transitions. Since we are going to be making a lot of plots like this in this chapter, we should consider streamlining our plotting code. In Chapter 19 of Wichkam’s (2016) ggplot2: Elegant graphics for data analysis, we learn how to make a custom geom. Here we’ll use those skills to wrap the bulk of the plot code from above into a single geom we’ll call geom_2dd(), for 2D-density plots. geom_2dd &lt;- function(...) { list( geom_raster(interpolate = T), scale_fill_viridis_c(option = &quot;A&quot;), scale_x_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5), scale_y_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5), coord_equal(), theme_minimal_grid(...), theme(legend.position = &quot;none&quot;) ) } Try it out. d %&gt;% ggplot(aes(x = theta, y = omega, fill = prior)) + geom_2dd() + labs(x = expression(theta), y = expression(omega)) If we collapse “the joint prior across \\(\\theta\\)” (i.e., group_by(omega) and then sum(prior)), we plot the marginal distribution for \\(p(\\omega)\\) as seen in the top right panel. library(viridis) a_purple &lt;- viridis_pal(option = &quot;A&quot;)(9)[4] d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, y = prior)) + geom_area(fill = a_purple) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5) + scale_y_continuous(expand = c(0, 0), limits = c(0, 0.035)) + labs(x = expression(omega), y = expression(Marginal~p(omega))) + coord_flip() + theme_cowplot() + panel_border() + theme(axis.line = element_blank()) Note how we loaded the viridis package (Garnier, 2021). That gave us access to the viridis_pal() function, which will allow us to discretize the viridis palettes and save the color names as objects. In our case, we discretized the \"A\" palette into nine colors and saved the fourth as a_purple. Here’s the color name. a_purple ## [1] &quot;#822681FF&quot; We’ll use that color in many of the plots to follow. It’ll be something of a signature color for this chapter. Anyway, since we are going to be making a lot of plots like this in this chapter, we’ll make another custom geom called geom_marginal(). geom_marginal &lt;- function(ul, ...) { list( geom_area(fill = viridis_pal(option = &quot;A&quot;)(9)[4]), scale_x_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5), scale_y_continuous(expand = c(0, 0), limits = c(0, ul)), theme_cowplot(...), panel_border(), theme(axis.line = element_blank()) ) } Try it out. d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, y = prior)) + geom_marginal(ul = 0.035) + labs(x = expression(omega), y = expression(Marginal~p(omega))) + coord_flip() We’ll follow a similar procedure to get the marginal probability distribution for theta. d %&gt;% group_by(theta) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta, y = prior)) + geom_marginal(ul = 0.035) + labs(x = expression(theta), y = expression(Marginal~p(theta))) We’ll use the filter() function to take the two slices from the posterior grid. Since we’re taking slices, we’re no longer working with the joint probability distribution. As such, our two marginal prior distributions for theta no longer sum to 1, which means they’re no longer in a probability metric. No worries. After we group by omega, we can simply divide prior by the sum() of prior which renormalizes the two slices “so that they are individually proper probability densities that sum to \\(1.0\\) over \\(\\theta\\)” (p. 226). d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(prior = prior / sum(prior)) %&gt;% mutate(label = factor(str_c(&quot;omega==&quot;, omega), levels = c(&quot;omega==0.75&quot;, &quot;omega==0.25&quot;))) %&gt;% ggplot(aes(x = theta, y = prior)) + geom_marginal(ul = 0.095) + labs(x = expression(theta), y = expression(p(theta*&quot;|&quot;*omega))) + facet_wrap(~ label, ncol = 1, labeller = label_parsed) As Kruschke pointed out at the top of page 228, these are indeed beta densities. Here’s proof. # we&#39;ll want this for the annotation text &lt;- tibble(theta = c(.75, .25), y = 10, label = c(&quot;Beta(74.5, 25.5)&quot;, &quot;Beta(25.5, 74.5)&quot;), omega = letters[1:2]) # here&#39;s the primary data for the plot tibble(theta = rep(parameter_space, times = 2), alpha = rep(c(74.5, 25.5), each = 101), beta = rep(c(25.5, 74.5), each = 101), omega = rep(letters[1:2], each = 101)) %&gt;% # the plot ggplot(aes(x = theta, fill = omega)) + geom_area(aes(y = dbeta(x = theta, shape1 = alpha, shape2 = beta))) + geom_text(data = text, aes(y = y, label = label, color = omega)) + scale_fill_viridis_d(option = &quot;A&quot;, begin = 2/9, end = 6/9) + scale_color_viridis_d(option = &quot;A&quot;, begin = 2/9, end = 6/9) + scale_x_continuous(expression(theta), expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5) + scale_y_continuous(&quot;density&quot;, expand = c(0, 0), limits = c(0, 11)) + theme_cowplot() + panel_border() + theme(axis.line = element_blank(), legend.position = &quot;none&quot;) But back on track, we need the Bernoulli likelihood function for the lower three rows of Figure 9.2. bernoulli_likelihood &lt;- function(theta, data) { n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } Time to feed theta and our data into the bernoulli_likelihood() function, which will allow us to make the 2-dimensional density plot in the middle of Figure 9.2. # define the data n &lt;- 12 z &lt;- 9 trial_data &lt;- rep(0:1, times = c(n - z, z)) # compute the likelihood d &lt;- d %&gt;% mutate(likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) # plot d %&gt;% ggplot(aes(x = theta, y = omega, fill = likelihood)) + geom_2dd() + labs(x = expression(theta), y = expression(omega)) Note how this plot demonstrates how the likelihood is solely dependent on \\(\\theta\\); it’s orthogonal to \\(\\omega\\). This is the visual consequence of Kruschke’s Formula 9.6, \\[\\begin{align*} p (\\theta, \\omega | y) &amp; = \\frac{p (y | \\theta, \\omega) \\; p (\\theta, \\omega)}{p (y)} \\\\ &amp; = \\frac{p (y | \\theta) \\; p (\\theta | \\omega) \\; p (\\omega)}{p (y)}. \\end{align*}\\] That is, in the second line of the equation, the probability of \\(y\\) was only conditional on \\(\\theta\\). But the reason we call this a hierarchical model is because the probability of \\(\\theta\\) itself is conditioned on \\(\\omega\\). The prior itself had a prior. From Formula 9.1, the posterior \\(p(\\theta, \\omega | D)\\) is proportional to \\(p(D | \\theta) \\; p(\\theta | \\omega) \\; p(\\omega)\\). Divide that by the normalizing constant and we’ll have it in a proper probability metric. Recall that we’ve already saved the results of \\(p(\\theta | \\omega) \\; p(\\omega)\\) in the prior column. So we just need to multiply prior by likelihood and divide by their sum. Our first depiction will be the middle panel of the second row from the bottom. d &lt;- d %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) d %&gt;% ggplot(aes(x = theta, y = omega, fill = posterior)) + geom_2dd() + labs(x = expression(theta), y = expression(omega)) Although the likelihood was orthogonal to \\(\\omega\\), conditioning the prior for \\(\\theta\\) on \\(\\omega\\) resulted in a posterior that was conditioned on both \\(\\theta\\) and \\(\\omega\\). Making the marginal plots for posterior is much like when making them for prior, above. # for omega d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, y = posterior)) + geom_marginal(ul = 0.035) + labs(x = expression(omega), y = expression(Marginal~p(omega*&quot;|&quot;*D))) + coord_flip() # for theta d %&gt;% group_by(theta) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_marginal(ul = 0.035) + labs(x = expression(theta), y = expression(Marginal~p(theta*&quot;|&quot;*D))) + coord_cartesian() Note that after we slice with filter(), the next two wrangling lines renormalize those posterior slices into probability metrics. That is, when we take a slice through the joint posterior at a particular value of \\(\\omega\\), and renormalize by dividing the sum of discrete probability masses in that slice, we get the conditional distribution \\(p(\\theta | \\omega, D)\\). d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(posterior = posterior / sum(posterior)) %&gt;% mutate(label = factor(str_c(&quot;omega==&quot;, omega), levels = c(&quot;omega==0.75&quot;, &quot;omega==0.25&quot;))) %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_marginal(ul = 0.1) + labs(x = expression(theta), y = expression(p(theta*&quot;|&quot;*omega))) + facet_wrap(~ label, ncol = 1, labeller = label_parsed, scales = &quot;free&quot;) In the next example depicted in Figure 9.3, we consider what happens when we combine the same data of 9 heads out of 12 trials to the same Bernoulli likelihood \\(p(y | \\theta)\\), but his time with a much lower \\(K\\) values expressing greater uncertainty in the \\(\\operatorname{Beta} \\big (\\theta | \\omega (6 - 2) + 1, (1 - \\omega) (6 - 2) + 1 \\big )\\) portion of the joint prior and with a more certain hyperprior for \\(\\omega\\), \\(\\operatorname{Beta}(\\omega | 20, 20)\\). To repeat the process for Figure 9.3, we’ll first compute the new joint prior. d &lt;- crossing(theta = parameter_space, omega = parameter_space) %&gt;% mutate(prior = make_prior(theta, omega, alpha = 20, beta = 20, kappa = 6)) %&gt;% mutate(prior = prior / sum(prior)) Here’s the initial data and the 2-dimensional density plot for the prior, the middle plot in the top row of Figure 9.3. d %&gt;% ggplot(aes(x = theta, y = omega, fill = prior)) + geom_2dd() + labs(x = expression(theta), y = expression(omega)) That higher certainty in \\(\\omega\\) resulted in a two-dimensional density plot where the values on the \\(y\\)-axis were concentrated near .5. This will have down-the-road consequences for the posterior. But before we get there, we’ll average over omega and theta to plot their marginal prior distributions. # for omega d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, y = prior)) + geom_marginal(ul = 0.052) + labs(x = expression(omega), y = expression(Marginal~p(omega))) + coord_flip() # for theta d %&gt;% group_by(theta) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta, y = prior)) + geom_marginal(ul = 0.039) + labs(x = expression(theta), y = expression(Marginal~p(theta))) Here are the two short plots in the right panel of the second row from the top of Figure 9.3. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(prior = prior / sum(prior)) %&gt;% mutate(label = factor(str_c(&quot;omega == &quot;, omega), levels = c(&quot;omega == 0.75&quot;, &quot;omega == 0.25&quot;))) %&gt;% ggplot(aes(x = theta, y = prior)) + geom_marginal(ul = 0.039) + labs(x = expression(theta), y = expression(p(theta*&quot;|&quot;*omega))) + facet_wrap(~ label, ncol = 1, labeller = label_parsed) Now we’re ready for the likelihood. # compute d &lt;- d %&gt;% mutate(likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) # plot d %&gt;% ggplot(aes(x = theta, y = omega, fill = likelihood)) + geom_2dd() + labs(x = expression(theta), y = expression(omega)) Now on to the posterior. Our first depiction will be the middle panel of the second row from the bottom of Figure 9.3. This will be \\(p(\\theta, \\omega | y)\\). # compute the posterior d &lt;- d %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) # plot d %&gt;% ggplot(aes(x = theta, y = omega, fill = posterior)) + geom_2dd() + labs(x = expression(theta), y = expression(omega)) Here are the marginal plots for the two dimensions in our posterior. # for omega d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, y = posterior)) + geom_marginal(ul = 0.052) + labs(x = expression(omega), y = expression(Marginal~p(omega*&quot;|&quot;*D))) + coord_flip() # for theta d %&gt;% group_by(theta) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_marginal(ul = 0.039) + labs(x = expression(theta), y = expression(Marginal~p(theta*&quot;|&quot;*D))) And we’ll finish off with the plots of Figure 9.3’s lower right panel. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(posterior = posterior / sum(posterior)) %&gt;% mutate(label = factor(str_c(&quot;omega==&quot;, omega), levels = c(&quot;omega==0.75&quot;, &quot;omega==0.25&quot;))) %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_marginal(ul = 0.039) + labs(x = expression(theta), y = expression(p(theta*&quot;|&quot;*omega))) + facet_wrap(~ label, ncol = 1, labeller = label_parsed, scales = &quot;free&quot;) In summary, Bayesian inference in a hierarchical model is merely Bayesian inference on a joint parameter space, but we look at the joint distribution (e.g., \\(p(\\theta, \\omega)\\)) in terms of its marginal on a subset of parameters (e.g., \\(p(\\omega)\\)) and its conditional distribution for other parameters (e.g., \\(p(\\theta | \\omega)\\)). We do this primarily because it is meaningful in the context of particular models. (p. 230) 9.2 Multiple coins from a single mint What if we collect data from more than one coin created by the mint? If each coin has its own distinct bias \\(\\theta_s\\), then we are estimating a distinct parameter value for each coin, and using all the data to estimate \\(\\omega\\). (p. 230) Kruschke broke down a model of this form with his diagram in Figure 9.4. Here’s our version of that figure. p1 &lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = a_purple) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;beta&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(A[omega])*&#39;, &#39;*italic(B[omega])&quot;, size = 7, family = &quot;Times&quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) p2 &lt;- tibble(x = c(.5, .475, .26, .08, .06, .5, .55, .85, 1.15, 1.2), y = c(1, .7, .6, .5, .2, 1, .7, .6, .5, .2), line = rep(letters[2:1], each = 5)) %&gt;% ggplot(aes(x = x, y = y)) + geom_bspline(aes(color = line), size = 2/3, show.legend = F) + annotate(geom = &quot;text&quot;, x = 0, y = .125, label = &quot;omega(italic(K)-2)+1*&#39;, &#39;*(1-omega)(italic(K)-2)+1&quot;, size = 7, parse = T, family = &quot;Times&quot;, hjust = 0) + annotate(geom = &quot;text&quot;, x = 1/3, y = .7, label = &quot;&#39;~&#39;&quot;, size = 10, parse = T, family = &quot;Times&quot;) + scale_color_manual(values = c(&quot;grey75&quot;, &quot;black&quot;)) + scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) + ylim(0, 1) + theme_void() p3 &lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = a_purple) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;beta&quot;, size = 7) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # an annotated arrow p4 &lt;- tibble(x = c(.35, .65), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(s)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # bar plot of Bernoulli data p5 &lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = a_purple, width = .4) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;Bernoulli&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .92, label = &quot;theta[italic(s)]&quot;, size = 7, family = &quot;Times&quot;, parse = T) + xlim(-.75, 1.75) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # another annotated arrow p6 &lt;- tibble(x = c(.35, .65), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)*&#39;|&#39;*italic(s)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p7 &lt;- tibble(x = 1, y = .5, label = &quot;italic(y)[italic(i)*&#39;|&#39;*italic(s)]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = &quot;Times&quot;) + xlim(0, 2) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 1, r = 1), area(t = 4, b = 5, l = 1, r = 1), area(t = 3, b = 4, l = 1, r = 2), area(t = 6, b = 6, l = 1, r = 1), area(t = 7, b = 8, l = 1, r = 1), area(t = 9, b = 9, l = 1, r = 1), area(t = 10, b = 10, l = 1, r = 1) ) # plot! (p1 + p3 + p2 + p4 + p5 + p6 + p7) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) The diagram accounts for multiple coins with the \\(s\\) index. 9.2.1 Posterior via grid approximation. Now we have two coins, the full prior distribution is a joint distribution over three parameters: \\(\\omega\\), \\(\\theta_1\\), and \\(\\theta_2\\). In a grid approximation, the prior is specified as a three-dimensional (3D) array that holds the prior probability at various grid points in the 3D space. (p. 233) The biases for both coins, \\(\\theta_1\\), and \\(\\theta_2\\), have the same prior \\(\\operatorname{Beta} \\big(\\theta_j| \\omega (5 - 2) + 1, (1 - \\omega)(5 - 2) + 1 \\big)\\), which, if it’s not apparent, is marked by the rather uncertain \\(K = 5\\). As in our first example depicted in Figure 9.2, we have a gentle hyperprior \\(\\operatorname{Beta}(\\omega | 2, 2)\\), which centers the posterior mode for \\(\\omega\\) at .5. To express this in plots, we’re going to have to update our make_prior() function. It was originally designed to handle two dimensions, \\(\\theta\\) and \\(\\omega\\). But now we have to update it to handle our three dimensions. make_prior &lt;- function(theta1, theta2, omega, alpha, beta, kappa) { # p(theta_1 | omega) t1 &lt;- dbeta(x = theta1, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) # p(theta_2 | omega) t2 &lt;- dbeta(x = theta2, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) # p(omega) o &lt;- dbeta(x = omega, shape1 = alpha, shape2 = beta) # p(theta1, theta2, omega) = p(theta1 | omega) * p(theta2 | omega) * p(omega) return(t1 * t2 * o) } Let’s make our new data object, d. d &lt;- crossing(theta_1 = parameter_space, theta_2 = parameter_space, omega = parameter_space) %&gt;% mutate(prior = make_prior(theta_1, theta_2, omega, alpha = 2, beta = 2, kappa = 5)) %&gt;% # here we normalize mutate(prior = prior / sum(prior)) glimpse(d) ## Rows: 1,030,301 ## Columns: 4 ## $ theta_1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ theta_2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ omega &lt;dbl&gt; 0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.… ## $ prior &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… Unlike what Kruschke said in the text (p. 233), we’re not using a 3D data array. Rather, we’re just using a tibble with which prior has been expanded across all possible dimensions of the three indexing variables: theta_1, theta_2, and omega. As you can see from the ‘Rows’ count, above, this makes for a very long tibble. “Because the parameter space is 3D, a distribution on it cannot easily be displayed on a 2D page. Instead, Figure 9.5 shows various marginal distributions” (p. 234). The consequence of that is when we marginalize, we’ll have to group by the two variables we’d like to retain for the plot. For example, the plots in the left and middle columns of the top row are the same save for their indices. So let’s just do the plot for theta_1. In order to marginalize over theta_2, we’ll need to group_by(theta_1, omega) and then summarise(prior = sum(prior)). d %&gt;% group_by(theta_1, omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = prior)) + geom_2dd() + labs(x = expression(theta[1]), y = expression(omega)) But we just have to average over omega and theta_1 to plot their marginal prior distributions. # for omega d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, y = prior)) + geom_marginal(ul = 0.041) + labs(x = expression(omega), y = expression(p(omega))) + coord_flip() # for theta d %&gt;% group_by(theta_1) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, y = prior)) + geom_marginal(ul = 0.041) + labs(x = expression(theta[1]), y = expression(p(theta[1]))) Before we make the plots in the middle row of Figure 9.5, we need to add the likelihoods. Recall that we’re presuming the coin flips contained in \\(D_1\\) and \\(D_2\\) are independent. Kruschke explained in Section 7.4.1, that independence of the data across the two coins means that the data from coin 1 depend only on the bias in coin 1, and the data from coin 2 depend only on the bias in coin 2, which can be expressed formally as \\(p(y_1 | \\theta_1, \\theta_2) = p(y_1 | \\theta_1)\\) and \\(p(y_2 | \\theta_1, \\theta_2) = p(y_2 | \\theta_2)\\). (p. 164) The likelihood function for our two series of coin flips is then \\[p(D | \\theta_1, \\theta_2) = \\left ( \\theta_1^{z_1} (1 - \\theta_1) ^ {N_1 - z_1} \\right ) \\left ( \\theta_2^{z_2} (1 - \\theta_2) ^ {N_2 - z_2} \\right ).\\] The upshot is we can compute the likelihoods for \\(D_1\\) and \\(D_2\\) separately and just multiply them together. # D1: 3 heads, 12 tails n &lt;- 15 z &lt;- 3 trial_data_1 &lt;- rep(0:1, times = c(n - z, z)) # D2: 4 heads, 1 tail n &lt;- 5 z &lt;- 4 trial_data_2 &lt;- rep(0:1, times = c(n - z, z)) d &lt;- d %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_1, data = trial_data_1), likelihood_2 = bernoulli_likelihood(theta = theta_2, data = trial_data_2)) %&gt;% mutate(likelihood = likelihood_1 * likelihood_2) head(d) ## # A tibble: 6 × 7 ## theta_1 theta_2 omega prior likelihood_1 likelihood_2 likelihood ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 0 0 ## 2 0 0 0.01 0 0 0 0 ## 3 0 0 0.02 0 0 0 0 ## 4 0 0 0.03 0 0 0 0 ## 5 0 0 0.04 0 0 0 0 ## 6 0 0 0.05 0 0 0 0 Now after a little group_by() followed by summarise() we can plot the two marginal likelihoods, the two plots in the middle row of Figure 9.5. # likelihood_1 d %&gt;% group_by(theta_1, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = likelihood)) + geom_2dd() + labs(x = expression(theta[1]), y = expression(omega)) # likelihood_2 d %&gt;% group_by(theta_2, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = likelihood)) + geom_2dd() + labs(x = expression(theta[2]), y = expression(omega)) The likelihoods look good. Next we compute the posterior in the same way we’ve done before: multiply the prior and the likelihood and then divide by their sum in order to convert the results to a probability metric. # compute d &lt;- d %&gt;% mutate(posterior = (prior * likelihood) / sum(prior * likelihood)) # posterior_1 d %&gt;% group_by(theta_1, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = posterior)) + geom_2dd() + labs(x = expression(theta[1]), y = expression(omega)) # posterior_2 d %&gt;% group_by(theta_2, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = posterior)) + geom_2dd() + labs(x = expression(theta[2]), y = expression(omega)) Here’s the right plot on the second row from the bottom, the posterior distribution for \\(\\omega\\). # for omega d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, y = posterior)) + geom_marginal(ul = 0.041) + labs(x = expression(omega), y = expression(p(omega*&quot;|&quot;*D))) + coord_flip() Now here are the marginal posterior plots on the bottom row of Figure 9.5. # for theta_1 d %&gt;% group_by(theta_1) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, y = posterior)) + geom_marginal(ul = 0.041) + labs(x = expression(theta[1]), y = expression(p(theta[1]*&quot;|&quot;*D))) # for theta_2 d %&gt;% group_by(theta_2) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, y = posterior)) + geom_marginal(ul = 0.041) + labs(x = expression(theta[2]), y = expression(p(theta[2]*&quot;|&quot;*D))) We’ll do this dog and pony one more time for Figure 9.6. Keeping the data and likelihood constant, we now set \\(K = 75\\) and but retain both \\(A_\\omega = 2\\) and \\(B_\\omega = 2\\). First, we make our new data object, d. d &lt;- crossing(theta_1 = parameter_space, theta_2 = parameter_space, omega = parameter_space) %&gt;% mutate(prior = make_prior(theta_1, theta_2, omega, alpha = 2, beta = 2, kappa = 75)) %&gt;% mutate(prior = prior / sum(prior)) Again, note how the only thing we changed from the last time was increasing kappa to 75. Also like last time, the plots in the left and middle columns of the top row are the same save for their indices. But unlike last time, we’ll make both in preparation for a grand plotting finale. You’ll see. One more step: for the 2D density plots in this section, we’ll omit the coord_equal() line from our custom geom_2dd() geom. This will help us with the formatting of our final plot. geom_2dd &lt;- function(...) { list( geom_raster(interpolate = T), scale_fill_viridis_c(option = &quot;A&quot;), scale_x_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5), scale_y_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5), # coord_equal(), theme_minimal_grid(...), theme(legend.position = &quot;none&quot;) ) } Okay, here are our two 2D prior density plots. p11 &lt;- d %&gt;% group_by(theta_1, omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = prior)) + geom_2dd(font_size = 10) + annotate(geom = &quot;text&quot;, x = .05, y = .925, label = &quot;p(list(theta[1], omega))&quot;, parse = T, color = &quot;white&quot;, hjust = 0) + labs(x = expression(theta[1]), y = expression(omega)) p12 &lt;- d %&gt;% group_by(theta_2, omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = prior)) + geom_2dd(font_size = 10) + annotate(geom = &quot;text&quot;, x = .05, y = .925, label = &quot;p(list(theta[2], omega))&quot;, parse = T, color = &quot;white&quot;, hjust = 0) + labs(x = expression(theta[2]), y = expression(omega)) p11 p12 Now we’ll average over omega and theta to plot their marginal prior distributions. # for omega p13 &lt;- d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, y = prior)) + geom_marginal(ul = 0.041, font_size = 10) + labs(x = expression(omega), y = expression(p(omega))) + coord_flip() # for theta_1 p21 &lt;- d %&gt;% group_by(theta_1) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, y = prior)) + geom_marginal(ul = 0.041, font_size = 10) + labs(x = expression(theta[1]), y = expression(p(theta[1]))) # for theta_2 p22 &lt;- d %&gt;% group_by(theta_2) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_2, y = prior)) + geom_marginal(ul = 0.041, font_size = 10) + labs(x = expression(theta[2]), y = expression(p(theta[2]))) p13 p21 p22 Let’s get those likelihoods in there and plot. # D1: 3 heads, 12 tails n &lt;- 15 z &lt;- 3 trial_data_1 &lt;- rep(0:1, times = c(n - z, z)) # D2: 4 heads, 1 tail n &lt;- 5 z &lt;- 4 trial_data_2 &lt;- rep(0:1, times = c(n - z, z)) # compute the likelihoods d &lt;- d %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_1, data = trial_data_1), likelihood_2 = bernoulli_likelihood(theta = theta_2, data = trial_data_2)) %&gt;% mutate(likelihood = likelihood_1 * likelihood_2) # plot likelihood_1 p31 &lt;- d %&gt;% group_by(theta_1, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = likelihood)) + geom_2dd(font_size = 10) + labs(x = expression(theta[1]), y = expression(omega)) # plot likelihood_2 p32 &lt;- d %&gt;% group_by(theta_2, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = likelihood)) + geom_2dd(font_size = 10) + labs(x = expression(theta[2]), y = expression(omega)) p31 p32 Compute the posterior and make the left and middle plots of the second row to the bottom of Figure 9.6. d &lt;- d %&gt;% mutate(posterior = (prior * likelihood) / sum(prior * likelihood)) # posterior_1 p41 &lt;- d %&gt;% group_by(theta_1, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = posterior)) + geom_2dd(font_size = 10) + annotate(geom = &quot;text&quot;, x = .05, y = .925, label = expression(p(list(theta[1], omega)*&quot;|&quot;*D)), parse = T, color = &quot;white&quot;, hjust = 0) + labs(x = expression(theta[1]), y = expression(omega)) # posterior_2 p42 &lt;- d %&gt;% group_by(theta_2, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = posterior)) + geom_2dd(font_size = 10) + annotate(geom = &quot;text&quot;, x = .05, y = .925, label = expression(p(list(theta[2], omega)*&quot;|&quot;*D)), parse = T, color = &quot;white&quot;, hjust = 0) + labs(x = expression(theta[2]), y = expression(omega)) p41 p42 Here’s the right plot on the same row, the posterior distribution for \\(\\omega\\). # for omega p43 &lt;- d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, y = posterior)) + geom_marginal(ul = 0.041, font_size = 10) + labs(x = expression(omega), y = expression(p(omega*&quot;|&quot;*D))) + coord_flip() p43 Finally, here are the marginal posterior plots on the bottom row of Figure 9.6. # for theta_1 p51 &lt;- d %&gt;% group_by(theta_1) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, y = posterior)) + geom_marginal(ul = 0.041, font_size = 10) + labs(x = expression(theta[1]), y = expression(p(theta[1]*&quot;|&quot;*D))) # for theta_2 p52 &lt;- d %&gt;% group_by(theta_2) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, y = posterior)) + geom_marginal(ul = 0.041, font_size = 10) + labs(x = expression(theta[2]), y = expression(p(theta[2]*&quot;|&quot;*D))) p51 p52 Did you notice how we saved each of plot from this last batch as objects? For the grand finale of this subsection, we’ll be stitching all those subplots together using syntax from the patchwork package. But before we do, we need to define three more subplots: the subplots with the annotation. text &lt;- tibble(x = 1, y = 10:8, label = c(&quot;Prior&quot;, &quot;list(A[omega]==2, B[omega]==2)&quot;, &quot;K==75&quot;), size = c(2, 1, 1)) p23 &lt;- text %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(aes(size = size), parse = T, hjust = 0, show.legend = F) + scale_size_continuous(range = c(3.5, 5.5)) + coord_cartesian(xlim = c(1, 2), ylim = c(4, 11)) + theme_cowplot(font_size = 10) + theme(axis.line = element_blank(), axis.text = element_text(color = &quot;white&quot;), axis.ticks = element_blank(), text = element_text(color = &quot;white&quot;)) text &lt;- tibble(x = 1, y = 10:8, label = c(&quot;Likelihood&quot;, &quot;D1: 3 heads, 12 tails&quot;, &quot;D2: 4 heads, 1 tail&quot;), size = c(2, 1, 1)) p33 &lt;- text %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(aes(size = size), hjust = 0, show.legend = F) + scale_size_continuous(range = c(3.5, 5.5)) + coord_cartesian(xlim = c(1, 2), ylim = c(4, 11)) + theme_cowplot(font_size = 10) + theme(axis.line = element_blank(), axis.text = element_text(color = &quot;white&quot;), axis.ticks = element_blank(), text = element_text(color = &quot;white&quot;)) p53 &lt;- ggplot() + annotate(geom = &quot;text&quot;, x = 1, y = 10, label = &quot;Posterior&quot;, size = 6, hjust = 0) + coord_cartesian(xlim = c(1, 2), ylim = c(3, 11)) + theme_cowplot(font_size = 10) + theme(axis.line = element_blank(), axis.text = element_text(color = &quot;white&quot;), axis.ticks = element_blank(), text = element_text(color = &quot;white&quot;)) Okay, let’s make the full version of Figure 9.6. (p11 / p21 / p31 / p41 / p51) | (p12 / p22 / p32 / p42 / p52) | (p13 / p23 / p33 / p43 / p53) Oh mamma! The grid approximation displayed in Figures 9.5 and 9.6 used combs of only \\([101]\\) points on each parameter (\\(\\omega\\), \\(\\theta_1\\), and \\(\\theta_2\\)). This means that the 3D grid had \\([101^3 = 1{,}030{,}301]\\) points, which is a size that can be handled easily on an ordinary desktop computer of the early \\(21\\)st century. It is interesting to remind ourselves that the grid approximation displayed in Figures 9.5 and 9.6 would have been on the edge of computability \\(50\\) years ago, and would have been impossible \\(100\\) years ago. The number of points in a grid approximation can get hefty in a hurry. If we were to expand the example by including a third coin, with its parameter \\(\\theta_3\\), then the grid would have \\([101^4 = 104{,}060{,}401]\\) points, which already strains small computers. Include a fourth coin, and the grid contains over \\([10\\) billion\\(]\\) points. Grid approximation is not a viable approach to even modestly large problems, which we encounter next. (p. 235) In case you didn’t catch it, we used different numbers of points to evaluate each parameter. Whereas Kruschke indicated in the text he only used 50, we used 101. That value of 101 came from how we defined our parameter_space with the code seq(from = 0, to = 1, by = .01). The reason we used a more densely-packed parameter space was to get smoother-looking 2D density plots. 9.2.2 A realistic model with MCMC. In this section, Kruschke freed up the previously fixed value of \\(K\\), now letting \\(\\kappa\\) vary hierarchically with a gamma prior. He depicted the model in Figure 9.7. Here’s our version of the figure. # a beta density p1 &lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = a_purple) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;beta&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(A[omega])*&#39;, &#39;*italic(B[omega])&quot;, size = 7, family = &quot;Times&quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # a gamma density p2 &lt;- tibble(x = seq(from = 0, to = 5, by = .01), d = (dgamma(x, 1.75, .85) / max(dgamma(x, 1.75, .85)))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = a_purple) + annotate(geom = &quot;text&quot;, x = 2.5, y = .2, label = &quot;gamma&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = 2.5, y = .6, label = &quot;list(italic(S)[kappa], italic(R)[kappa])&quot;, size = 7, family = &quot;Times&quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) p3 &lt;- tibble(x = c(.5, .475, .26, .08, .06, .5, .55, .85, 1.15, 1.175, 1.5, 1.4, 1, .25, .2, 1.5, 1.49, 1.445, 1.4, 1.39), y = c(1, .7, .6, .5, .2, 1, .7, .6, .5, .2, 1, .7, .6, .5, .2, 1, .75, .6, .45, .2), line = rep(letters[2:1], each = 5) %&gt;% rep(., times = 2), plot = rep(1:2, each = 10)) %&gt;% ggplot(aes(x = x, y = y, group = interaction(plot, line))) + geom_bspline(aes(color = line), size = 2/3, show.legend = F) + annotate(geom = &quot;text&quot;, x = 0, y = .1, label = &quot;omega(kappa-2)+1*&#39;, &#39;*(1-omega)(kappa-2)+1&quot;, size = 7, parse = T, family = &quot;Times&quot;, hjust = 0) + annotate(geom = &quot;text&quot;, x = c(1/3, 1.15), y = .7, label = &quot;&#39;~&#39;&quot;, size = 10, parse = T, family = &quot;Times&quot;) + scale_color_manual(values = c(&quot;grey75&quot;, &quot;black&quot;)) + scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) + ylim(0, 1) + theme_void() # another beta density p4 &lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = a_purple) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;beta&quot;, size = 7) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # an annotated arrow p5 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(s)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = 0.5, xend = 0.5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # bar plot of Bernoulli data p6 &lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = a_purple, width = .4) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;Bernoulli&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .94, label = &quot;theta&quot;, size = 7, family = &quot;Times&quot;, parse = T) + xlim(-.75, 1.75) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # another annotated arrow p7 &lt;- tibble(x = c(.35, .65), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)*&#39;|&#39;*italic(s)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p8 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])[&#39;|&#39;][italic(s)]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 1, r = 1), area(t = 1, b = 2, l = 2, r = 2), area(t = 4, b = 5, l = 1, r = 1), area(t = 3, b = 4, l = 1, r = 2), area(t = 6, b = 6, l = 1, r = 1), area(t = 7, b = 8, l = 1, r = 1), area(t = 9, b = 9, l = 1, r = 1), area(t = 10, b = 10, l = 1, r = 1) ) # plot! (p1 + p2 + p4 + p3 + p5 + p6 + p7 + p8) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) “Because the value of \\(\\kappa − 2\\) must be non-negative, the prior distribution on \\(\\kappa − 2\\) must not allow negative values” (p. 237). Gamma is one of the distributions with that property. The gamma distribution is defined by two parameters, its shape and rate. To get a sense of how those play out, here’ a look at the gamma densities of Figure 9.8. # how many points do you want in your sequence of x values? length &lt;- 150 # wrangle tibble(shape = c(.01, 1.56, 1, 6.25), rate = c(.01, .0312, .02, .125)) %&gt;% expand(nesting(shape, rate), x = seq(from = 0, to = 200, length.out = length)) %&gt;% mutate(mean = shape * 1 / rate, sd = sqrt(shape * (1 / rate)^2)) %&gt;% mutate(label = str_c(&quot;shape = &quot;, shape, &quot;, rate = &quot;, rate, &quot;\\nmean = &quot;, mean, &quot;, sd = &quot;, round(sd, 4))) %&gt;% # plot ggplot(aes(x = x, y = dgamma(x = x, shape = shape, rate = rate))) + geom_area(aes(fill = label)) + scale_fill_viridis_d(option = &quot;A&quot;, end = .9, breaks = NULL) + scale_x_continuous(expression(kappa), expand = expansion(mult = c(0, 0.05))) + scale_y_continuous(expression(p(kappa*&quot;|&quot;*s*&quot;,&quot;*r)), breaks = c(0, .01, .02), expand = expansion(mult = c(0, 0.05))) + coord_cartesian(xlim = c(0, 150)) + theme_cowplot(line_size = 0) + panel_border() + facet_wrap(~ label) ## Warning: Removed 1 rows containing non-finite values (`stat_align()`). You can find the formulas for the mean and \\(\\textit{SD}\\) for a given gamma distribution here. We used those formulas in the second mutate() statement for the data-prep stage of that last figure. Using \\(s\\) for shape and \\(r\\) for rate, Kruschke’s Equations 9.7 and 9.8 are as follows: \\[ s = \\frac{\\mu^2}{\\sigma^2} \\;\\;\\; \\text{and} \\;\\;\\; r = \\frac{\\mu}{\\sigma^2} \\;\\;\\; \\text{for mean} \\;\\;\\; \\mu &gt; 0 \\\\ s = 1 + \\omega r \\;\\;\\; \\text{where} \\;\\;\\; r = \\frac{\\omega + \\sqrt{\\omega^2 + 4\\sigma^2}}{2\\sigma^2} \\;\\;\\; \\text{for mode} \\;\\;\\; \\omega &gt; 0. \\] With those in hand, we can follow Kruschke’s DBDA2E-utilities.R file to make a couple convenience functions. gamma_s_and_r_from_mean_sd &lt;- function(mean, sd) { if (mean &lt;= 0) stop(&quot;mean must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) shape &lt;- mean^2 / sd^2 rate &lt;- mean / sd^2 return(list(shape = shape, rate = rate)) } gamma_s_and_r_from_mode_sd &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } They’re easy to put to use: gamma_s_and_r_from_mean_sd(mean = 10, sd = 100) ## $shape ## [1] 0.01 ## ## $rate ## [1] 0.001 gamma_s_and_r_from_mode_sd(mode = 10, sd = 100) ## $shape ## [1] 1.105125 ## ## $rate ## [1] 0.01051249 Here’s a more detailed look at the structure of their output. gamma_param &lt;- gamma_s_and_r_from_mode_sd(mode = 10, sd = 100) str(gamma_param) ## List of 2 ## $ shape: num 1.11 ## $ rate : num 0.0105 9.2.3 Doing it with JAGS brms. Unlike JAGS, the brms formula will not correspond as closely to the schematic in Figure 9.7. You’ll see in just a bit. 9.2.4 Example: Therapeutic touch. Load the data from the TherapeuticTouchData.csv file (see Rosa et al., 1998). my_data &lt;- read_csv(&quot;data.R/TherapeuticTouchData.csv&quot;) glimpse(my_data) ## Rows: 280 ## Columns: 2 ## $ y &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,… ## $ s &lt;chr&gt; &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;… Here are what the data look like. my_data %&gt;% mutate(y = y %&gt;% as.character()) %&gt;% ggplot(aes(y = y)) + geom_bar(aes(fill = stat(count))) + scale_fill_viridis_c(option = &quot;A&quot;, end = .7, breaks = NULL) + scale_x_continuous(breaks = 0:4 * 2, expand = c(0, NA), limits = c(0, 9)) + theme_minimal_vgrid() + panel_border() + facet_wrap(~ s, ncol = 7) ## Warning: `stat(count)` was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(count)` instead. And here’s our Figure 9.9. my_data %&gt;% group_by(s) %&gt;% summarize(mean = mean(y)) %&gt;% ggplot(aes(x = mean)) + geom_histogram(color = &quot;white&quot;, fill = a_purple, size = .2, binwidth = .1) + scale_x_continuous(&quot;Proportion Correct&quot;, limits = c(0, 1)) + scale_y_continuous(&quot;# Practitioners&quot;, expand = c(0, NA)) + theme_minimal_hgrid() Let’s open brms. library(brms) In applied statistics, the typical way to model a Bernoulli variable is with logistic regression. Instead of going through the pain of setting up a model in brms that mirrors the one in the text, I’m going to set up a hierarchical logistic regression model, instead. Note the family = bernoulli(link = logit) argument. In work-a-day regression with vanilla Gaussian variables, the prediction space is unbounded. But when we want to model the probability of a success for a Bernoulli variable (i.e., \\(\\theta\\)), we need to constrain the model to only produce predictions between 0 and 1. With logistic regression, we use a link function to do just that. The consequence is that instead of modeling the probability, \\(\\theta\\), we’re modeling the logit probability. In case you’re curious, the logit of \\(\\theta\\) follows the formula \\[\\operatorname{logit}(\\theta) = \\log (\\theta/[1 - \\theta] ).\\] But anyway, we’ll be doing logistic regression using the logit link. Kruschke covered this in detail in Chapter 21. The next new part of our syntax is (1 | s). As in the popular frequentist lme4 package (Bates et al., 2015, 2021), you specify random effects or group-level parameters with the (|) syntax in brms. On the left side of the |, you tell brms what parameters you’d like to make random (i.e., vary by group). On the right side of the |, you tell brms what variable you want to group the parameters by. In our case, we want the intercepts to vary over the grouping variable s. fit9.1 &lt;- brm(data = my_data, family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9, file = &quot;fits/fit09.01&quot;) As it turns out, the \\(N(0, 1.5)\\) prior is flat in the probability space for the intercept in a logistic regression model. We’ll explore that a little further down. The \\(N(0, 1)\\) prior for the random effect is actually a half Normal. That’s because brms defaults to bound \\(\\textit{SD}\\) parameters to zero and above. The half Normal prior for a hierarchical \\(\\textit{SD}\\) parameter in a logistic regression model is weakly regularizing and is conservative in the sense that it presumes some pooling is preferable to no pooling. If you wanted to take a lighter approach, you might use something like a cauchy(0, 5), instead. See the prior wiki by the Stan team for more ideas on priors. Here are the trace plots and posterior densities of the main parameters. plot(fit9.1, widths = c(2, 3)) The trace plots indicate no problems with convergence. We’ll need to extract the posterior draws with as_draws_df() and open the bayesplot package before we can examine the autocorrelations. draws &lt;- as_draws_df(fit9.1) library(bayesplot) One of the nice things about bayesplot is it returns ggplot2 objects. As such, we can amend their theme settings to be consistent with our other ggplot2 plots. Here we’ll amend bayesplot::mcmc_acf() to the theme_cowplot() theme. draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_acf(pars = vars(b_Intercept, sd_s__Intercept), lags = 10) + theme_cowplot() It appears fit9.1 had very low autocorrelations. Here we’ll examine the \\(N_{eff}/N\\) ratio. neff_ratio(fit9.1) %&gt;% mcmc_neff() + theme_cowplot(font_size = 12) The \\(N_{eff}/N\\) ratio values for our model parameters were excellent. Here’s a numeric summary of the model. print(fit9.1) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + (1 | s) ## Data: my_data (Number of observations: 280) ## Draws: 4 chains, each with iter = 20000; warmup = 1000; thin = 10; ## total post-warmup draws = 7600 ## ## Group-Level Effects: ## ~s (Number of levels: 28) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.28 0.18 0.01 0.68 1.00 7170 7051 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.25 0.14 -0.52 0.02 1.00 7205 6715 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll need brms::inv_logit_scaled() to convert the model parameters to predict \\(\\theta\\) rather than \\(\\operatorname{logit}(\\theta)\\). After the conversions, we’ll be ready to make the histograms in the lower portion of Figure 9.10. # load library(tidybayes) # wrangle draws_small &lt;- draws %&gt;% # convert the linear model parameters to the probability space with `inv_logit_scaled()` mutate(`theta[1]` = (b_Intercept + `r_s[S01,Intercept]`) %&gt;% inv_logit_scaled(), `theta[14]` = (b_Intercept + `r_s[S14,Intercept]`) %&gt;% inv_logit_scaled(), `theta[28]` = (b_Intercept + `r_s[S28,Intercept]`) %&gt;% inv_logit_scaled()) %&gt;% # make the difference distributions mutate(`theta[1] - theta[14]` = `theta[1]` - `theta[14]`, `theta[1] - theta[28]` = `theta[1]` - `theta[28]`, `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) %&gt;% select(starts_with(&quot;theta&quot;)) draws_small %&gt;% pivot_longer(everything()) %&gt;% # this line is unnecessary, but will help order the plots mutate(name = factor(name, levels = c(&quot;theta[1]&quot;, &quot;theta[14]&quot;, &quot;theta[28]&quot;, &quot;theta[1] - theta[14]&quot;, &quot;theta[1] - theta[28]&quot;, &quot;theta[14] - theta[28]&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = a_purple, breaks = 40, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme_minimal_hgrid() + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) If you wanted the specific values of the posterior modes and 95% HDIs, you could execute this. draws_small %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 6 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 theta[1] 0.421 0.205 0.517 0.95 mode hdi ## 2 theta[1] - theta[14] -0.002 -0.277 0.122 0.95 mode hdi ## 3 theta[1] - theta[28] -0.01 -0.423 0.065 0.95 mode hdi ## 4 theta[14] 0.427 0.284 0.577 0.95 mode hdi ## 5 theta[14] - theta[28] -0.004 -0.323 0.101 0.95 mode hdi ## 6 theta[28] 0.452 0.36 0.693 0.95 mode hdi And here are the Figure 9.10 scatter plots. p1 &lt;- draws_small %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[14]`)) + geom_abline(linetype = 2) + geom_point(color = a_purple, size = 1/8, alpha = 1/8) p2 &lt;- draws_small %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[28]`)) + geom_abline(linetype = 2) + geom_point(color = a_purple, size = 1/8, alpha = 1/8) p3 &lt;- draws_small %&gt;% ggplot(aes(x = `theta[14]`, y = `theta[28]`)) + geom_abline(linetype = 2) + geom_point(color = a_purple, size = 1/8, alpha = 1/8) (p1 + p2 + p3) &amp; coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) &amp; theme_minimal_grid() This is posterior distribution for the population estimate for \\(\\theta\\), which roughly corresponds to the upper right histogram of \\(\\omega\\) in Figure 9.10. # this part makes it easier to set the break points in `scale_x_continuous()` labels &lt;- draws %&gt;% transmute(theta = b_Intercept %&gt;% inv_logit_scaled()) %&gt;% mode_hdi() %&gt;% pivot_longer(theta:.upper) %&gt;% mutate(label = value %&gt;% round(3) %&gt;% as.character) draws %&gt;% mutate(theta = b_Intercept %&gt;% inv_logit_scaled()) %&gt;% ggplot(aes(x = theta, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = a_purple, breaks = 40) + scale_x_continuous(expression(theta), breaks = labels$value, labels = labels$label) + scale_y_continuous(NULL, breaks = NULL) + theme_minimal_hgrid() I’m not aware there’s a straight conversion to get \\(\\sigma\\) in a probability metric. As far as I can tell, you have to first use coef() to “extract [the] model coefficients, which are the sum of population-level effects and corresponding group-level effects” (Bürkner, 2022d, p. 58). With the model coefficient draws in hand, you can index them by posterior iteration, group them by that index, compute the iteration-level \\(\\textit{SD}\\)’s, and then plot the distribution of the \\(\\textit{SD}\\)’s. # the tibble of the primary data sigmas &lt;- coef(fit9.1, summary = F)$s %&gt;% as_tibble() %&gt;% mutate(iter = 1:n()) %&gt;% group_by(iter) %&gt;% pivot_longer(-iter) %&gt;% mutate(theta = inv_logit_scaled(value)) %&gt;% summarise(sd = sd(theta)) # this, again, is just to customize `scale_x_continuous()` labels &lt;- sigmas %&gt;% mode_hdi(sd) %&gt;% pivot_longer(sd:.upper) %&gt;% mutate(label = value %&gt;% round(3) %&gt;% as.character) # the plot sigmas %&gt;% ggplot(aes(x = sd, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = a_purple, breaks = 40) + scale_x_continuous(expression(paste(sigma, &quot; of &quot;, theta, &quot; in a probability metric&quot;)), breaks = labels$value, labels = labels$label) + scale_y_continuous(NULL, breaks = NULL) + theme_minimal_hgrid() And now you have a sense of how to do all those by hand, bayesplot::mcmc_pairs() offers a fairly quick way to get a good portion of Figure 9.10. color_scheme_set(&quot;purple&quot;) bayesplot_theme_set(theme_default() + theme_minimal_grid()) coef(fit9.1, summary = F)$s %&gt;% inv_logit_scaled() %&gt;% data.frame() %&gt;% rename(`theta[1]` = S01.Intercept, `theta[14]` = S14.Intercept, `theta[28]` = S28.Intercept) %&gt;% select(`theta[1]`, `theta[14]`, `theta[28]`) %&gt;% mcmc_pairs(off_diag_args = list(size = 1/8, alpha = 1/8)) Did you see how we slipped in the color_scheme_set() and bayesplot_theme_set() lines at the top? Usually, the plots made with bayesplot are easy to modify with ggplot2 syntax. Plots made with mcmc_pairs() function are one notable exception. On the back end, these made by combining multiple ggplot into a grid, a down-the-line result of which is they are difficult to modify. Happily, one can make some modifications beforehand by altering the global settings with the color_scheme_set() and bayesplot_theme_set() functions. You can learn more in the discussion on issue #128 on the bayesplot GitHub repo. Kruschke used a \\(\\operatorname{Beta}(1, 1)\\) prior for \\(\\omega\\). If you randomly draw from that prior and plot a histogram, you’ll see it was flat. set.seed(1) tibble(prior = rbeta(n = 1e5, 1, 1)) %&gt;% ggplot(aes(x = prior)) + geom_histogram(fill = a_purple, color = &quot;white&quot;, size = .2, binwidth = .05, boundary = 0) + scale_x_continuous(expression(omega), labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 1)) + theme_minimal_hgrid() You’ll note that plot corresponds to the upper right panel of Figure 9.11. Recall that we used a logistic regression model with a normal(0, 1.5) prior on the intercept. If you sample from normal(0, 1.5) and then convert the draws using brms::inv_logit_scaled(), you’ll discover that our normal(0, 1.5) prior was virtually flat on the probability scale. Here we’ll show the consequence of a variety of zero-mean Gaussian priors for the intercept of a logistic regression model. # define a function r_norm &lt;- function(i, n = 1e4) { set.seed(1) rnorm(n = n, mean = 0, sd = i) %&gt;% inv_logit_scaled() } # simulate and wrangle tibble(sd = seq(from = .25, to = 3, by = .25)) %&gt;% group_by(sd) %&gt;% mutate(prior = map(sd, r_norm)) %&gt;% unnest(prior) %&gt;% ungroup() %&gt;% mutate(sd = str_c(&quot;sd = &quot;, sd)) %&gt;% # plot! ggplot(aes(x = prior)) + geom_histogram(fill = a_purple, color = &quot;white&quot;, size = .2, binwidth = .05, boundary = 0) + scale_x_continuous(labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 1)) + theme_minimal_hgrid() + panel_border() + facet_wrap(~ sd) It appears that as \\(\\sigma\\) goes lower than 1.25, the prior becomes increasingly regularizing, pulling the estimate for \\(\\theta\\) to a neutral .5. However, as the prior’s \\(\\sigma\\) gets larger than 1.25, more and more of the probability mass ends up at extreme values. Next, Kruschke examined the prior distribution. There are a few ways to do this. Like in the last chapter, the one we’ll explore involves adding the sample_prior = \"only\" argument to the brm() function. When you do so, the results of the model are just the prior. That is, brm() leaves out the likelihood. This returns a bunch of draws from the prior predictive distribution. fit9.1_prior &lt;- brm(data = my_data, family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9, sample_prior = &quot;only&quot;, file = &quot;fits/fit09.01_prior&quot;) If we feed fit9.1_prior into the as_draws_df() function, we’ll get back a data frame of draws from the prior, but with the same parameter names we’d get from the posterior. prior_draws &lt;- as_draws_df(fit9.1_prior) head(prior_draws) ## # A draws_df: 6 iterations, 1 chains, and 32 variables ## b_Intercept sd_s__Intercept r_s[S01,Intercept] r_s[S02,Intercept] ## 1 -0.74 0.021 0.013 -0.0086 ## 2 -1.28 0.040 0.063 -0.0199 ## 3 -1.10 0.193 -0.296 0.2139 ## 4 -2.21 0.400 -0.244 -0.2008 ## 5 -1.82 0.170 -0.125 -0.0132 ## 6 0.59 1.246 2.077 1.4740 ## r_s[S03,Intercept] r_s[S04,Intercept] r_s[S05,Intercept] r_s[S06,Intercept] ## 1 -0.0023 -0.025 -0.0038 -0.0250 ## 2 -0.0229 0.025 0.0123 -0.0364 ## 3 -0.1691 -0.231 -0.1658 0.0543 ## 4 -0.8816 -0.802 -0.3677 -0.4067 ## 5 -0.1599 0.091 -0.1417 -0.0039 ## 6 -3.1299 -1.019 -0.9315 0.8389 ## # ... with 24 more variables ## # ... hidden reserved variables {&#39;.chain&#39;, &#39;.iteration&#39;, &#39;.draw&#39;} And here we’ll take a subset of the columns in prior_draws, transform the results to the probability metric, and save. prior_draws &lt;- prior_draws %&gt;% transmute(`theta[1]` = b_Intercept + `r_s[S01,Intercept]`, `theta[14]` = b_Intercept + `r_s[S14,Intercept]`, `theta[28]` = b_Intercept + `r_s[S28,Intercept]`) %&gt;% mutate_all(.funs = inv_logit_scaled) head(prior_draws) ## # A tibble: 6 × 3 ## `theta[1]` `theta[14]` `theta[28]` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.325 0.317 0.327 ## 2 0.228 0.222 0.223 ## 3 0.198 0.296 0.299 ## 4 0.0788 0.110 0.0687 ## 5 0.126 0.158 0.162 ## 6 0.935 0.803 0.874 Now we can use our prior_draws object to make the diagonal of the lower grid of Figure 9.11. prior_draws %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_histogram(fill = a_purple, color = &quot;white&quot;, size = .2, binwidth = .05, boundary = 0) + scale_x_continuous(labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 1)) + theme_minimal_hgrid() + panel_border() + facet_wrap(~ name) With a little subtraction, we can reproduce the plots in the upper triangle. prior_draws %&gt;% mutate(`theta[1] - theta[14]` = `theta[1]` - `theta[14]`, `theta[1] - theta[28]` = `theta[1]` - `theta[28]`, `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) %&gt;% pivot_longer(contains(&quot;-&quot;)) %&gt;% ggplot(aes(x = value)) + geom_histogram(fill = a_purple, color = &quot;white&quot;, size = .2, binwidth = .05, boundary = 0) + scale_y_continuous(NULL, breaks = NULL) + theme_minimal_hgrid() + panel_border() + facet_wrap(~ name) Those plots clarify our hierarchical logistic regression model was a little more regularizing than Kruschke’s. The consequence of our priors was more aggressive regularization, greater shrinkage toward zero. The prose in the next section of the text clarifies this isn’t necessarily a bad thing. Finally, here are the plots for the lower triangle in Figure 9.11. p1 &lt;- prior_draws %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[14]`)) + geom_point(color = a_purple, size = 1/8, alpha = 1/8) p2 &lt;- prior_draws %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[28]`)) + geom_point(color = a_purple, size = 1/8, alpha = 1/8) p3 &lt;- prior_draws %&gt;% ggplot(aes(x = `theta[14]`, y = `theta[28]`)) + geom_point(color = a_purple, size = 1/8, alpha = 1/8) (p1 + p2 + p3) &amp; geom_abline(linetype = 2) &amp; coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) &amp; theme_minimal_grid() In case you were curious, here are the Pearson’s correlation coefficients among the priors. cor(prior_draws) %&gt;% round(digits = 2) ## theta[1] theta[14] theta[28] ## theta[1] 1.00 0.73 0.72 ## theta[14] 0.73 1.00 0.73 ## theta[28] 0.72 0.73 1.00 9.3 Shrinkage in hierarchical models “In typical hierarchical models, the estimates of low-level parameters are pulled closer together than they would be if there were not a higher-level distribution. This pulling together is called shrinkage of the estimates” (p. 245, emphasis in the original) Further, shrinkage is a rational implication of hierarchical model structure, and is (usually) desired by the analyst because the shrunken parameter estimates are less affected by random sampling noise than estimates derived without hierarchical structure. Intuitively, shrinkage occurs because the estimate of each low-level parameter is influenced from two sources: (1) the subset of data that are directly dependent on the low-level parameter, and (2) the higher-level parameters on which the low-level parameter depends. The higher- level parameters are affected by all the data, and therefore the estimate of a low-level parameter is affected indirectly by all the data, via their influence on the higher-level parameters. (p. 247) Recall Formula 9.4 from page 223, \\[\\theta \\sim \\operatorname{Beta} \\big(\\omega(\\kappa - 2) + 1 \\big ), (1 - \\omega)(\\kappa - 2) + 1).\\] With that formula, we can express dbeta()’s shape1 and shape2 in terms of \\(\\omega\\) and \\(\\kappa\\) and make the shapes in Figure 9.12. omega &lt;- 0.5 kappa1 &lt;- 2.1 kappa2 &lt;- 15.8 tibble(x = seq(from = 0, to = 1, by = .001)) %&gt;% mutate(`kappa==2.1` = dbeta(x = x, shape1 = omega * (kappa1 - 2) + 1, shape2 = (1 - omega) * (kappa1 - 2) + 1), `kappa==15.8` = dbeta(x = x, shape1 = omega * (kappa2 - 2) + 1, shape2 = (1 - omega) * (kappa2 - 2) + 1)) %&gt;% pivot_longer(-x) %&gt;% mutate(name = factor(name, levels = c(&quot;kappa==2.1&quot;, &quot;kappa==15.8&quot;))) %&gt;% ggplot(aes(x = x, y = value)) + geom_area(fill = a_purple) + scale_y_continuous(expression(dbeta(theta*&quot;|&quot;*omega*&quot;, &quot;*kappa)), breaks = NULL) + xlab(expression(Data~Proportion~or~theta~value)) + theme_minimal_hgrid() + panel_border() + facet_wrap(~ name, labeller = label_parsed) This isn’t in the text, but it might help if we gave a sense of multilevel shrinkage by plotting the phenomena using the results from our model fit9.1. my_data %&gt;% group_by(s) %&gt;% summarise(p = mean(y)) %&gt;% mutate(theta = coef(fit9.1)$s[, 1, &quot;Intercept&quot;] %&gt;% inv_logit_scaled()) %&gt;% pivot_longer(-s) %&gt;% # add a little jitter to reduce the overplotting mutate(value = value + runif(n = n(), min = -0.02, max = 0.02), name = if_else(name == &quot;p&quot;, &quot;italic(z/N)&quot;, &quot;theta&quot;)) %&gt;% ggplot(aes(x = value, y = name, group = s)) + geom_point(color = alpha(a_purple, 1/2)) + geom_line(size = 1/3, alpha = 1/3) + scale_x_continuous(breaks = 0:5 / 5, expand = c(0.01, 0.01), limits = 0:1) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + labs(title = &quot;Multilevel shrinkage in fit9.1&quot;, x = &quot;data proportion or theta value&quot;) + theme_minimal_hgrid() + panel_border() The dots in the \\(z/N\\) row are the sample statistics. The dots in the \\(\\theta\\) row are the posterior means for each of the levels of s, the grouping variable in the my_data data. You’ll note that we jittered the values for both within the second mutate() line to help reduce the overplotting. If you don’t understand what that means, run the code without that line or set the values within runif() closer to zero. You’ll see. Anyway, for more on multilevel shrinkage and for plots of this kind, check out Efron and Morris’s classic (1977) paper, Stein’s paradox in statistics, and my blog post walking out one of their examples in brms. 9.4 Speeding up JAGS brms Here we’ll compare the time it takes to fit fit1 as either bernoulli(link = logit) or binomial(link = logit). # bernoulli start_time_bernoulli &lt;- proc.time() brm(data = my_data, family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9) stop_time_bernoulli &lt;- proc.time() # binomial start_time_binomial &lt;- proc.time() brm(data = my_data, family = binomial(link = logit), y | trials(1) ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9) stop_time_binomial &lt;- proc.time() See how we’re using proc.time() to record when we began and finished evaluating our brm() code? The last time we covered that was way back in Section 3.7.5. In that section, we also learned how subtracting the former from the latter yields the total elapsed time. stop_time_bernoulli - start_time_bernoulli ## user system elapsed ## 23.514 1.970 43.999 stop_time_binomial - start_time_binomial ## user system elapsed ## 20.497 1.463 43.058 These times are based on my current laptop (a 2019 MacBook Pro). Your mileage may vary. If you wanted to be rigorous about this, you could do this multiple times in a mini simulation. As to the issue of parallel processing, we’ve been doing this all along. Note our chains = 4, cores = 4 arguments. Since Kruschke wrote his text, we have other options for speeding up your brms models related to within-chain parallelization and the backend = \"cmdstanr\" option. For all the details, see Weber &amp; Bürkner’s (2022) vignette, Running brms models with within-chain parallelization. 9.5 Extending the hierarchy: Subjects within categories Many data structures invite hierarchical descriptions that may have multiple levels. Software such as JAGS [brms] makes it easy to implement hierarchical models, and Bayesian inference makes it easy to interpret the parameter estimates, even for complex nonlinear hierarchical models. Here, we take a look at one type of extended hierarchical model. (p. 251) As we will address below, our version of Figure 9.13 will look rather different from Kruschke’s. It’s something of a combination of the sensibilities from Figures 20.2 and 21.10. Even still, the diagram is of three-level model that shares many similarities to Kruschke’s and, as we will see, yields very similar results. # half-normal density p1 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = a_purple) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[italic(s)*&#39;|&#39;*italic(c)]&quot;, size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # second half-normal density p2 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = a_purple) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[italic(c)]&quot;, size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # annotated arrow p3 &lt;- tibble(x = .85, y = 1, xend = .5, yend = .25) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow) + annotate(geom = &quot;text&quot;, x = .54, y = .6, label = &quot;&#39;~&#39;&quot;, size = 10, family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # normal density p4 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = a_purple) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # second normal density p5 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = a_purple,) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;0&quot;, &quot;sigma[italic(s)*&#39;|&#39;*italic(c)]&quot;), size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # third normal density p6 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = a_purple) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;0&quot;, &quot;sigma[italic(c)]&quot;), size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # three annotated arrows p7 &lt;- tibble(x = c(.09, .48, .9), y = c(1, 1, 1), xend = c(.2, .425, .775), yend = c(.2, .2, .2)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow) + annotate(geom = &quot;text&quot;, x = c(.10, .42, .49, .81, .87), y = .6, label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(s)*&#39;|&#39;*italic(c)&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(c)&quot;), size = c(10, 10, 7, 10, 7), family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # likelihood formula p8 &lt;- tibble(x = .5, y = .5, label = &quot;logistic(beta[0]+sum()[italic(s)*&#39;|&#39;*italic(c)]*beta[&#39;[&#39;*italic(s)*&#39;|&#39;*italic(c)*&#39;]&#39;]*italic(x)[&#39;[&#39;*italic(s)*&#39;|&#39;*italic(c)*&#39;]&#39;](italic(i))+sum()[italic(c)]*beta[&#39;[&#39;*italic(c)*&#39;]&#39;]*italic(x)[&#39;[&#39;*italic(c)*&#39;]&#39;](italic(i)))&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # a second annotated arrow p9 &lt;- tibble(x = c(.375, .5), y = c(.75, .3), label = c(&quot;&#39;=&#39;&quot;, &quot;mu[italic(i)*&#39;|&#39;*italic(sc)]&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = .4, arrow = my_arrow) + xlim(0, 1) + theme_void() # binomial density p10 &lt;- tibble(x = 0:7) %&gt;% ggplot(aes(x = x, y = (dbinom(x, size = 7, prob = .625)) / max(dbinom(x, size = 7, prob = .625)))) + geom_col(fill = a_purple, width = .4) + annotate(geom = &quot;text&quot;, x = 3.5, y = .2, label = &quot;binomial&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = 7, y = .85, label = &quot;italic(N)[italic(i)*&#39;|&#39;*italic(s)]&quot;, size = 7, family = &quot;Times&quot;, parse = TRUE) + coord_cartesian(xlim = c(-1, 8), ylim = c(0, 1.2)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # another annotated arrow p11 &lt;- tibble(x = c(.375, .7), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)*&#39;|&#39;*italic(sc)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p12 &lt;- tibble(x = 1, y = .5, label = &quot;italic(y)[italic(i)*&#39;|&#39;*italic(sc)]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = &quot;Times&quot;) + xlim(0, 2) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 7, r = 9), area(t = 1, b = 2, l = 11, r = 13), area(t = 4, b = 5, l = 1, r = 3), area(t = 4, b = 5, l = 5, r = 7), area(t = 4, b = 5, l = 9, r = 11), area(t = 3, b = 4, l = 6, r = 8), area(t = 3, b = 4, l = 10, r = 12), area(t = 7, b = 8, l = 1, r = 11), area(t = 6, b = 7, l = 1, r = 11), area(t = 11, b = 12, l = 5, r = 7), area(t = 9, b = 11, l = 5, r = 7), area(t = 13, b = 14, l = 5, r = 7), area(t = 15, b = 15, l = 5, r = 7) ) # combine and plot! (p1 + p2 + p4 + p5 + p6 + p3 + p3 + p8 + p7 + p10 + p9 + p11 + p12) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Though we will be fitting a hierarchical model with subjects \\(s\\) within categories \\(c\\), the higher-level parameters will not be \\(\\omega\\) and \\(\\kappa\\). As we’ll go over, below, we will use the binomial distribution within a more conventional hierarchical logistic regression paradigm. In this paradigm, we have an overall intercept, often called \\(\\alpha\\) or \\(\\beta_0\\), which will be our analogue to Kruschke’s overall \\(\\omega\\). For the two grouping categories, \\(s\\) and \\(c\\), we will have \\(\\sigma\\) estimates, which express the variability within those grouping. You’ll see when we get there. 9.5.1 Example: Baseball batting abilities by position. Here are the batting average data. my_data &lt;- read_csv(&quot;data.R/BattingAverage.csv&quot;) glimpse(my_data) ## Rows: 948 ## Columns: 6 ## $ Player &lt;chr&gt; &quot;Fernando Abad&quot;, &quot;Bobby Abreu&quot;, &quot;Tony Abreu&quot;, &quot;Dustin Ack… ## $ PriPos &lt;chr&gt; &quot;Pitcher&quot;, &quot;Left Field&quot;, &quot;2nd Base&quot;, &quot;2nd Base&quot;, &quot;1st Bas… ## $ Hits &lt;dbl&gt; 1, 53, 18, 137, 21, 0, 0, 2, 150, 167, 0, 128, 66, 3, 1, … ## $ AtBats &lt;dbl&gt; 7, 219, 70, 607, 86, 1, 1, 20, 549, 576, 1, 525, 275, 12,… ## $ PlayerNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ PriPosNumber &lt;dbl&gt; 1, 7, 4, 4, 3, 1, 1, 3, 3, 4, 1, 5, 4, 2, 7, 4, 6, 8, 9, … In his footnote #6, Kruschke indicated he retrieved the data from http://www.baseball-reference.com/leagues/MLB/2012-standard-batting.shtml on December of 2012. To give a sense of the data, here are the number of occasions by primary position, PriPos, with their median at bat, AtBats, values. my_data %&gt;% group_by(PriPos) %&gt;% summarise(n = n(), median = median(AtBats)) %&gt;% arrange(desc(n)) ## # A tibble: 9 × 3 ## PriPos n median ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Pitcher 324 4 ## 2 Catcher 103 170 ## 3 Left Field 103 164 ## 4 1st Base 81 265 ## 5 3rd Base 75 267 ## 6 2nd Base 72 228. ## 7 Center Field 67 259 ## 8 Shortstop 63 205 ## 9 Right Field 60 340. As these data are aggregated, we’ll fit with an aggregated binomial model. This is still logistic regression. The Bernoulli distribution is a special case of the binomial distribution when the number of trials in each data point is 1 (see Bürkner, 2022c for details). Since our data are aggregated, the information encoded in Hits is a combination of multiple trials, which requires us to jump up to the more general binomial likelihood. Note the Hits | trials(AtBats) syntax. With that bit, we instructed brms that our criterion, Hits, is an aggregate of multiple trials and the number of trials is encoded in AtBats. Also note the (1 | PriPos) + (1 | PriPos:Player) syntax. In this model, we have two grouping factors, PriPos and Player. Thus we have two (|) arguments. But since players are themselves nested within positions, we have encoded that nesting with the (1 | PriPos:Player) syntax. For more on this style of syntax, see Kristoffer Magnusson’s handy blog post, Using R and lme/lmer to fit different two- and three-level longitudinal models. Since brms syntax is based on that from the earlier lme4 package, the basic syntax rules apply. Bürkner (2022d), of course, also covered these topics in the brmsformula subsection of the brms reference manual. fit9.2 &lt;- brm(data = my_data, family = binomial(link = logit), Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = .99), seed = 9, file = &quot;fits/fit09.02&quot;) The chains look good. plot(fit9.2, widths = c(2, 3)) Note how our color_scheme_set(\"purple\") line from back up in the mcmc_pairs() has effected the color scheme of brms::plot(). We might examine the autocorrelations within the chains. draws &lt;- as_draws_df(fit9.2) draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_acf(pars = vars(b_Intercept:`sd_PriPos:Player__Intercept`), lags = 8) + theme_minimal_hgrid() Here’s a histogram of the \\(N_{eff}/N\\) ratios. fit9.2 %&gt;% neff_ratio() %&gt;% mcmc_neff_hist(binwidth = .1) + yaxis_text() + theme_minimal_hgrid() Happily, most have a very favorable ratio. Here’s a numeric summary of the primary model parameters. print(fit9.2) ## Family: binomial ## Links: mu = logit ## Formula: Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player) ## Data: my_data (Number of observations: 948) ## Draws: 3 chains, each with iter = 3500; warmup = 500; thin = 1; ## total post-warmup draws = 9000 ## ## Group-Level Effects: ## ~PriPos (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.33 0.10 0.19 0.58 1.00 2846 4327 ## ## ~PriPos:Player (Number of levels: 948) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.14 0.01 0.12 0.15 1.00 3433 5479 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.17 0.11 -1.40 -0.94 1.00 1429 2851 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As far as I’m aware, brms offers three major ways to get the group-level parameters for a hierarchical model: using one of the as_draws functions, coef(), or fitted(). We’ll cover each, beginning with as_draws. In order to look at the autocorrelation plots, above, we already saved the results from as_draws_df(fit9.2) as draws. Let’s look at its structure with head(). head(draws) ## # A draws_df: 6 iterations, 1 chains, and 962 variables ## b_Intercept sd_PriPos__Intercept sd_PriPos:Player__Intercept ## 1 -1.1 0.22 0.13 ## 2 -1.1 0.25 0.12 ## 3 -1.2 0.20 0.15 ## 4 -1.1 0.29 0.12 ## 5 -1.1 0.22 0.13 ## 6 -1.1 0.26 0.14 ## r_PriPos[1st.Base,Intercept] r_PriPos[2nd.Base,Intercept] ## 1 0.074 0.068 ## 2 0.037 0.063 ## 3 0.156 0.101 ## 4 0.043 -0.013 ## 5 -0.013 0.053 ## 6 0.052 -0.016 ## r_PriPos[3rd.Base,Intercept] r_PriPos[Catcher,Intercept] ## 1 0.0371 -0.021 ## 2 0.0907 -0.018 ## 3 0.1773 0.081 ## 4 0.0021 -0.062 ## 5 -0.0028 -0.082 ## 6 0.1030 -0.015 ## r_PriPos[Center.Field,Intercept] ## 1 0.0682 ## 2 0.0098 ## 3 0.1379 ## 4 0.0159 ## 5 0.0725 ## 6 0.0879 ## # ... with 954 more variables ## # ... hidden reserved variables {&#39;.chain&#39;, &#39;.iteration&#39;, &#39;.draw&#39;} In the text, Kruschke described the model as having 968 parameters. Our draws data frame has one vector for each, with a couple others tacked onto the end. In the hierarchical logistic regression model, the group-specific parameters for the levels of PriPos are additive combinations of the global intercept vector, b_Intercept and each position-specific vector, r_PriPos[i.Base,Intercept], where i is a fill-in for the position of interest. And recall that since the linear model is of the logit of the criterion, we’ll need to use inv_logit_scaled() to convert that to the probability space. draws_small &lt;- draws %&gt;% transmute(`1st Base` = (b_Intercept + `r_PriPos[1st.Base,Intercept]`), Catcher = (b_Intercept + `r_PriPos[Catcher,Intercept]`), Pitcher = (b_Intercept + `r_PriPos[Pitcher,Intercept]`)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% # here we compute our difference distributions mutate(`Pitcher - Catcher` = Pitcher - Catcher, `Catcher - 1st Base` = Catcher - `1st Base`) head(draws_small) ## # A tibble: 6 × 5 ## `1st Base` Catcher Pitcher `Pitcher - Catcher` `Catcher - 1st Base` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.257 0.240 0.130 -0.109 -0.0177 ## 2 0.252 0.242 0.130 -0.112 -0.0102 ## 3 0.257 0.243 0.134 -0.108 -0.0142 ## 4 0.261 0.241 0.131 -0.110 -0.0197 ## 5 0.249 0.237 0.132 -0.105 -0.0127 ## 6 0.255 0.243 0.123 -0.120 -0.0125 If you take a glance at Figures 9.14 through 9.16 in the text, we’ll be making a lot of histograms of the same basic structure. To streamline our code a bit, we can make a custom histogram plotting function. make_histogram &lt;- function(data, mapping, title, xlim, ...) { ggplot(data, mapping) + geom_histogram(fill = viridis::viridis_pal(option = &quot;A&quot;)(9)[4], color = &quot;white&quot;, size = .2, bins = 30) + stat_pointinterval(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = title, x = expression(theta)) + coord_cartesian(xlim = xlim) + theme_minimal_hgrid() + panel_border() } We’ll do the same thing for the correlation plots. make_point &lt;- function(data, mapping, limits, ...) { ggplot(data, mapping) + geom_abline(linetype = 3, color = &quot;grey50&quot;) + geom_point(color = viridis::viridis_pal(option = &quot;A&quot;)(9)[4], size = 1/10, alpha = 1/20) + coord_cartesian(xlim = limits, ylim = limits) + theme_minimal_grid(line_size = 0) + panel_border() } To learn more about wrapping custom plots into custom functions, check out Chapter 19 of Wickham’s (2016) ggplot2: Elegant graphics for data analysis. Now we have our make_histogram() and make_point() functions, we’ll use grid.arrange() to paste together the left half of Figure 9.14. p1 &lt;- make_histogram(data = draws_small, aes(x = Pitcher), title = &quot;Pitcher&quot;, xlim = c(.1, .25)) p2 &lt;- make_histogram(data = draws_small, aes(x = `Pitcher - Catcher`), title = &quot;Pitcher - Catcher&quot;, xlim = c(-.15, 0)) p3 &lt;- make_point(data = draws_small, aes(x = Pitcher, y = Catcher), limits = c(.12, .25)) p4 &lt;- make_histogram(data = draws_small, aes(x = Catcher), title = &quot;Catcher&quot;, xlim = c(.1, .25)) p1 + p2 + p3 + p4 We could follow the same procedure to make the right portion of Figure 9.14. But instead, let’s switch gears and explore the second way brms affords us for plotting group-level parameters. This time, we’ll use coef(). Up in Section 9.2.4, we learned that we can use coef() to “extract [the] model coefficients, which are the sum of population-level effects and corresponding group-level effects” (Bürkner, 2022d, p. 58). The grouping level we’re interested in is PriPos, so we’ll use that to index the information returned by coef(). Since coef() returns a matrix, we’ll use as_tibble() to convert it to a tibble. coef_primary_position &lt;- coef(fit9.2, summary = F)$PriPos %&gt;% as_tibble() str(coef_primary_position) ## tibble [9,000 × 9] (S3: tbl_df/tbl/data.frame) ## $ 1st Base.Intercept : num [1:9000] -1.06 -1.09 -1.06 -1.04 -1.1 ... ## $ 2nd Base.Intercept : num [1:9000] -1.07 -1.06 -1.12 -1.1 -1.04 ... ## $ 3rd Base.Intercept : num [1:9000] -1.1 -1.03 -1.04 -1.08 -1.09 ... ## $ Catcher.Intercept : num [1:9000] -1.15 -1.14 -1.14 -1.15 -1.17 ... ## $ Center Field.Intercept: num [1:9000] -1.06 -1.11 -1.08 -1.07 -1.02 ... ## $ Left Field.Intercept : num [1:9000] -1.09 -1.12 -1.11 -1.05 -1.1 ... ## $ Pitcher.Intercept : num [1:9000] -1.9 -1.9 -1.86 -1.89 -1.89 ... ## $ Right Field.Intercept : num [1:9000] -1.038 -1.068 -1.053 -1.083 -0.986 ... ## $ Shortstop.Intercept : num [1:9000] -1.11 -1.12 -1.08 -1.1 -1.07 ... Keep in mind that coef() returns the values in the logit scale when used for logistic regression models. So we’ll have to use brms::inv_logit_scaled() to convert the estimates to the probability metric. After we’re done converting the estimates, we’ll then make the difference distributions. coef_small &lt;- coef_primary_position %&gt;% select(`1st Base.Intercept`, Catcher.Intercept, Pitcher.Intercept) %&gt;% transmute(`1st Base` = `1st Base.Intercept`, Catcher = Catcher.Intercept, Pitcher = Pitcher.Intercept) %&gt;% mutate_all(inv_logit_scaled) %&gt;% # here we make the difference distributions mutate(`Pitcher - Catcher` = Pitcher - Catcher, `Catcher - 1st Base` = Catcher - `1st Base`) head(coef_small) ## # A tibble: 6 × 5 ## `1st Base` Catcher Pitcher `Pitcher - Catcher` `Catcher - 1st Base` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.257 0.240 0.130 -0.109 -0.0177 ## 2 0.252 0.242 0.130 -0.112 -0.0102 ## 3 0.257 0.243 0.134 -0.108 -0.0142 ## 4 0.261 0.241 0.131 -0.110 -0.0197 ## 5 0.249 0.237 0.132 -0.105 -0.0127 ## 6 0.255 0.243 0.123 -0.120 -0.0125 Now we’re ready for the right half of Figure 9.14. p1 &lt;- make_histogram(data = coef_small, aes(x = Catcher), title = &quot;Catcher&quot;, xlim = c(.22, .27)) p2 &lt;- make_histogram(data = coef_small, aes(x = `Catcher - 1st Base`), title = &quot;Catcher - 1st Base&quot;, xlim = c(-.04, .01)) p3 &lt;- make_point(data = coef_small, aes(x = Catcher, y = `1st Base`), limits = c(.22, .27)) p4 &lt;- make_histogram(data = coef_small, aes(x = `1st Base`), title = &quot;1st Base&quot;, xlim = c(.22, .27)) p1 + p2 + p3 + p4 And if you wanted the posterior modes and HDIs, you’d use mode_hdi() after a little wrangling. coef_small %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 5 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1st Base 0.252 0.245 0.262 0.95 mode hdi ## 2 Catcher 0.241 0.233 0.249 0.95 mode hdi ## 3 Catcher - 1st Base -0.013 -0.024 -0.001 0.95 mode hdi ## 4 Pitcher 0.13 0.12 0.14 0.95 mode hdi ## 5 Pitcher - Catcher -0.11 -0.124 -0.098 0.95 mode hdi While we’re at it, we should capitalize on the opportunity to show how these results are the same as those derived from our as_draws_df() approach, above. draws_small %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 5 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1st Base 0.252 0.245 0.262 0.95 mode hdi ## 2 Catcher 0.241 0.233 0.249 0.95 mode hdi ## 3 Catcher - 1st Base -0.013 -0.024 -0.001 0.95 mode hdi ## 4 Pitcher 0.13 0.12 0.14 0.95 mode hdi ## 5 Pitcher - Catcher -0.11 -0.124 -0.098 0.95 mode hdi Success! For Figures 9.15 and 9.16, Kruschke drilled down further into the posterior. To drill along with him, we’ll take the opportunity to showcase fitted(), the third way brms affords us for plotting group-level parameters. # this will make life easier. just go with it name_list &lt;- c(&quot;Kyle Blanks&quot;, &quot;Bruce Chen&quot;, &quot;ShinSoo Choo&quot;, &quot;Ichiro Suzuki&quot;, &quot;Mike Leake&quot;, &quot;Wandy Rodriguez&quot;, &quot;Andrew McCutchen&quot;, &quot;Brett Jackson&quot;) # we&#39;ll define the data we&#39;d like to feed into `fitted()`, here nd &lt;- my_data %&gt;% filter(Player %in% name_list) %&gt;% # these last two lines aren&#39;t typically necessary, but they allow us to # arrange the rows in the same order we find the names in Figures 9.15 and 9.16 mutate(Player = factor(Player, levels = name_list)) %&gt;% arrange(Player) fitted_players &lt;- fitted(fit9.2, newdata = nd, scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% # rename the values as returned by `as_tibble()` set_names(name_list) %&gt;% # convert the values from the logit scale to the probability scale mutate_all(inv_logit_scaled) %&gt;% # in this last section, we make our difference distributions mutate(`Kyle Blanks - Bruce Chen` = `Kyle Blanks` - `Bruce Chen`, `ShinSoo Choo - Ichiro Suzuki` = `ShinSoo Choo` - `Ichiro Suzuki`, `Mike Leake - Wandy Rodriguez` = `Mike Leake` - `Wandy Rodriguez`, `Andrew McCutchen - Brett Jackson` = `Andrew McCutchen` - `Brett Jackson`) glimpse(fitted_players) ## Rows: 9,000 ## Columns: 12 ## $ `Kyle Blanks` &lt;dbl&gt; 0.2376884, 0.2748736, 0.2251325, 0.… ## $ `Bruce Chen` &lt;dbl&gt; 0.1347857, 0.1261347, 0.1409021, 0.… ## $ `ShinSoo Choo` &lt;dbl&gt; 0.3150916, 0.2539179, 0.2996450, 0.… ## $ `Ichiro Suzuki` &lt;dbl&gt; 0.2739827, 0.2817321, 0.2619542, 0.… ## $ `Mike Leake` &lt;dbl&gt; 0.1419472, 0.1854007, 0.1101245, 0.… ## $ `Wandy Rodriguez` &lt;dbl&gt; 0.1433120, 0.1238650, 0.1303600, 0.… ## $ `Andrew McCutchen` &lt;dbl&gt; 0.3249982, 0.3018530, 0.3014977, 0.… ## $ `Brett Jackson` &lt;dbl&gt; 0.2238559, 0.2199570, 0.2386586, 0.… ## $ `Kyle Blanks - Bruce Chen` &lt;dbl&gt; 0.10290271, 0.14873895, 0.08423044,… ## $ `ShinSoo Choo - Ichiro Suzuki` &lt;dbl&gt; 0.0411088952, -0.0278142211, 0.0376… ## $ `Mike Leake - Wandy Rodriguez` &lt;dbl&gt; -0.001364791, 0.061535640, -0.02023… ## $ `Andrew McCutchen - Brett Jackson` &lt;dbl&gt; 0.10114233, 0.08189596, 0.06283916,… Note our use of the scale = \"linear\" argument in the fitted() function. By default, fitted() returns predictions on the scale of the criterion. But we don’t want a list of successes and failures; we want player-level parameters. When you specify scale = \"linear\", you request fitted() return the values in the parameter scale. Here’s the left portion of Figure 9.15. p1 &lt;- make_histogram(data = fitted_players, aes(x = `Kyle Blanks`), title = &quot;Kyle Blanks (1st Base)&quot;, xlim = c(.05, .35)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `Kyle Blanks - Bruce Chen`), title = &quot;Kyle Blanks (1st Base) -\\nBruce Chen (Pitcher)&quot;, xlim = c(-.1, .25)) p3 &lt;- make_point(data = fitted_players, aes(x = `Kyle Blanks`, y = `Bruce Chen`), limits = c(.09, .35)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Bruce Chen`), title = &quot;Bruce Chen (Pitcher)&quot;, xlim = c(.05, .35)) p1 + p2 + p3 + p4 Figure 9.15, right: p1 &lt;- make_histogram(data = fitted_players, aes(x = `ShinSoo Choo`), title = &quot;ShinSoo Choo (Right Field)&quot;, xlim = c(.22, .34)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `ShinSoo Choo - Ichiro Suzuki`), title = &quot;ShinSoo Choo (Right Field) -\\nIchiro Suzuki (Right Field)&quot;, xlim = c(-.07, .07)) p3 &lt;- make_point(data = fitted_players, aes(x = `ShinSoo Choo`, y = `Ichiro Suzuki`), limits = c(.23, .32)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Ichiro Suzuki`), title = &quot;Ichiro Suzuki (Right Field)&quot;, xlim = c(.22, .34)) (p1 + p2 + p3 + p4) &amp; theme(title = element_text(size = 11)) Figure 9.16, left: p1 &lt;- make_histogram(data = fitted_players, aes(x = `Mike Leake`), title = &quot;Mike Leake (Pitcher)&quot;, xlim = c(.05, .35)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `Mike Leake - Wandy Rodriguez`), title = &quot;Mike Leake (Pitcher) -\\nWandy Rodriguez (Pitcher)&quot;, xlim = c(-.05, .25)) p3 &lt;- make_point(data = fitted_players, aes(x = `Mike Leake`, y = `Wandy Rodriguez`), limits = c(.07, .25)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Wandy Rodriguez`), title = &quot;Wandy Rodriguez (Pitcher)&quot;, xlim = c(.05, .35)) (p1 + p2 + p3 + p4) &amp; theme(title = element_text(size = 11)) Figure 9.16, right: p1 &lt;- make_histogram(data = fitted_players, aes(x = `Andrew McCutchen`), title = &quot;Andrew McCutchen (Center Field)&quot;, xlim = c(.15, .35)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `Andrew McCutchen - Brett Jackson`), title = &quot;Andrew McCutchen (Center Field) -\\nBrett Jackson (Center Field)&quot;, xlim = c(0, .20)) p3 &lt;- make_point(data = fitted_players, aes(x = `Andrew McCutchen`, y = `Brett Jackson`), limits = c(.15, .35)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Brett Jackson`), title = &quot;Brett Jackson (Center Field)&quot;, xlim = c(.15, .35)) (p1 + p2 + p3 + p4) &amp; theme(title = element_text(size = 11)) If you wanted the posterior modes and HDIs for any of the players and their contrasts, you’d use mode_hdi() after a little wrangling. fitted_players %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 12 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Andrew McCutchen 0.307 0.276 0.338 0.95 mode hdi ## 2 Andrew McCutchen - Brett Jackson 0.073 0.021 0.123 0.95 mode hdi ## 3 Brett Jackson 0.234 0.196 0.278 0.95 mode hdi ## 4 Bruce Chen 0.125 0.102 0.166 0.95 mode hdi ## 5 Ichiro Suzuki 0.274 0.246 0.304 0.95 mode hdi ## 6 Kyle Blanks 0.252 0.206 0.305 0.95 mode hdi ## 7 Kyle Blanks - Bruce Chen 0.116 0.063 0.178 0.95 mode hdi ## 8 Mike Leake 0.146 0.117 0.184 0.95 mode hdi ## 9 Mike Leake - Wandy Rodriguez 0.024 -0.016 0.07 0.95 mode hdi ## 10 ShinSoo Choo 0.275 0.245 0.305 0.95 mode hdi ## 11 ShinSoo Choo - Ichiro Suzuki -0.002 -0.042 0.042 0.95 mode hdi ## 12 Wandy Rodriguez 0.122 0.096 0.154 0.95 mode hdi To make our version of Figure 9.7, we’ll have to switch gears from player-specific effects to those specific to positions averaged over individual players. The fitted() approach will probably make this the easiest. To do this, we’ll need to specify values for AtBats, which will need to be a positive integer. However, since we’re asking for fitted values of the linear predictors by setting scale = \"linear\", any value meeting the positive-integer criterion will return the same results. We’ll keep things simple and set AtBats = 1. Another consideration is if we’d like to use fitted() to average across one of the hierarchical grouping parameters (i.e., (1 | PriPos:Player)), we’ll need to employ the re_formula argument. With the line re_formula = ~ (1 | PriPos), we’ll instruct fitted() to return the PriPos-specific effects after averaging across levels of Player. The rest is quire similar to our method from above. nd &lt;- my_data %&gt;% distinct(PriPos, AtBats = 1) fitted_positions &lt;- fitted(fit9.2, newdata = nd, re_formula = ~ (1 | PriPos), scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% set_names(distinct(my_data, PriPos) %&gt;% pull()) %&gt;% mutate_all(inv_logit_scaled) glimpse(fitted_positions) ## Rows: 9,000 ## Columns: 9 ## $ Pitcher &lt;dbl&gt; 0.1304204, 0.1296809, 0.1344783, 0.1312847, 0.1317621, … ## $ `Left Field` &lt;dbl&gt; 0.2522510, 0.2461144, 0.2469496, 0.2587589, 0.2490095, … ## $ `2nd Base` &lt;dbl&gt; 0.2562900, 0.2572787, 0.2468012, 0.2501712, 0.2618258, … ## $ `1st Base` &lt;dbl&gt; 0.2574109, 0.2523072, 0.2571286, 0.2607937, 0.2493394, … ## $ `3rd Base` &lt;dbl&gt; 0.2505139, 0.2626069, 0.2611564, 0.2530208, 0.2512057, … ## $ Catcher &lt;dbl&gt; 0.2397125, 0.2420747, 0.2429536, 0.2410910, 0.2366844, … ## $ Shortstop &lt;dbl&gt; 0.2487001, 0.2451489, 0.2534934, 0.2503236, 0.2559054, … ## $ `Center Field` &lt;dbl&gt; 0.2563912, 0.2472339, 0.2536382, 0.2556327, 0.2656253, … ## $ `Right Field` &lt;dbl&gt; 0.2614929, 0.2557405, 0.2585578, 0.2529148, 0.2716858, … Now we make and save the nine position-specific subplots for Figure 9.17. p1 &lt;- fitted_positions %&gt;% pivot_longer(everything(), values_to = &quot;theta&quot;) %&gt;% # though technically not needed, this line reorders the panels to match the text mutate(name = factor(name, levels = c(&quot;1st Base&quot;, &quot;Catcher&quot;, &quot;Pitcher&quot;, &quot;2nd Base&quot;, &quot;Center Field&quot;, &quot;Right Field&quot;, &quot;3rd Base&quot;, &quot;Left Field&quot;, &quot;Shortstop&quot;))) %&gt;% ggplot(aes(x = theta)) + geom_histogram(fill = a_purple, color = &quot;white&quot;, size = .1, binwidth = .0025) + stat_pointinterval(aes(y = 0), point_interval = mode_hdi, .width = .95, size = 1) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + coord_cartesian(xlim = c(.1, .28)) + theme_minimal_hgrid() + panel_border() + facet_wrap(~ name, nrow = 3, scales = &quot;free_y&quot;) In this code block, we’ll make the subplot for the overall batting average. Given the size of the model, it’s perhaps easiest to pull that information from the model with the fixef() function. p2 &lt;- fixef(fit9.2, summary = F) %&gt;% as_tibble() %&gt;% transmute(theta = inv_logit_scaled(Intercept), name = &quot;Overall&quot;) %&gt;% ggplot(aes(x = theta)) + geom_histogram(fill = a_purple, color = &quot;white&quot;, size = .2, binwidth = .005) + stat_pointinterval(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + coord_cartesian(xlim = c(.1, .28)) + theme_minimal_hgrid() + panel_border() + facet_wrap(~ name) Now combine the plots with a little patchwork magic. p3 &lt;- plot_spacer() p1 + (p2 / p3 / p3) + plot_layout(widths = c(3, 1)) Do note that, unlike Kruschke’s Figure 9.17, our subplots are all based on \\(\\theta\\) rather than \\(\\omega\\). This is because of our use of a hierarchical aggregated binomial, rather than the approach Kruschke took. Even so, look how similar the results are. Finally, we have only looked at a tiny fraction of the relations among the \\(968\\) parameters. We could investigate many more comparisons among parameters if we were specifically interested. In traditional statistical testing based on \\(p\\)-values (which will be discussed in Chapter 11), we would pay a penalty for even intending to make more comparisons. This is because a \\(p\\) value depends on the space of counter-factual possibilities created from the testing intentions. In a Bayesian analysis, however, decisions are based on the posterior distribution, which is based only on the data (and the prior), not on the testing intention. More discussion of multiple comparisons can be found in Section 11.4. (pp. 259–260) Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_3.0.2 bayesplot_1.9.0 brms_2.18.0 Rcpp_1.0.9 ## [5] viridis_0.6.2 viridisLite_0.4.1 patchwork_1.1.2 ggforce_0.4.1 ## [9] ggridges_0.5.3 cowplot_1.1.1 forcats_0.5.1 stringr_1.4.1 ## [13] dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 ## [17] tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 ## [4] igraph_1.3.4 svUnit_1.0.6 splines_4.2.0 ## [7] crosstalk_1.2.0 TH.data_1.1-1 rstantools_2.2.0 ## [10] inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 ## [16] googlesheets4_1.0.1 tzdb_0.3.0 modelr_0.1.8 ## [19] RcppParallel_5.1.5 matrixStats_0.62.0 vroom_1.5.7 ## [22] sandwich_3.0-2 xts_0.12.1 prettyunits_1.1.1 ## [25] colorspace_2.0-3 rvest_1.0.2 ggdist_3.2.0 ## [28] haven_2.5.1 xfun_0.35 callr_3.7.3 ## [31] crayon_1.5.2 jsonlite_1.8.3 lme4_1.1-31 ## [34] survival_3.4-0 zoo_1.8-10 glue_1.6.2 ## [37] polyclip_1.10-0 gtable_0.3.1 gargle_1.2.0 ## [40] emmeans_1.8.0 distributional_0.3.1 pkgbuild_1.3.1 ## [43] rstan_2.21.7 abind_1.4-5 scales_1.2.1 ## [46] mvtnorm_1.1-3 DBI_1.1.3 miniUI_0.1.1.1 ## [49] xtable_1.8-4 HDInterval_0.2.2 bit_4.0.4 ## [52] stats4_4.2.0 StanHeaders_2.21.0-7 DT_0.24 ## [55] htmlwidgets_1.5.4 httr_1.4.4 threejs_0.3.3 ## [58] arrayhelpers_1.1-0 posterior_1.3.1 ellipsis_0.3.2 ## [61] pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 ## [64] sass_0.4.2 dbplyr_2.2.1 utf8_1.2.2 ## [67] tidyselect_1.1.2 labeling_0.4.2 rlang_1.0.6 ## [70] reshape2_1.4.4 later_1.3.0 munsell_0.5.0 ## [73] cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [76] cli_3.5.0 generics_0.1.3 broom_1.0.1 ## [79] evaluate_0.18 fastmap_1.1.0 processx_3.8.0 ## [82] knitr_1.40 bit64_4.0.5 fs_1.5.2 ## [85] nlme_3.1-159 projpred_2.2.1 mime_0.12 ## [88] xml2_1.3.3 compiler_4.2.0 shinythemes_1.2.0 ## [91] rstudioapi_0.13 gamm4_0.2-6 reprex_2.0.2 ## [94] tweenr_2.0.0 bslib_0.4.0 stringi_1.7.8 ## [97] highr_0.9 ps_1.7.2 Brobdingnag_1.2-8 ## [100] lattice_0.20-45 Matrix_1.4-1 nloptr_2.0.3 ## [103] markdown_1.1 shinyjs_2.1.0 tensorA_0.36.2 ## [106] vctrs_0.5.1 pillar_1.8.1 lifecycle_1.0.3 ## [109] jquerylib_0.1.4 bridgesampling_1.1-2 estimability_1.4.1 ## [112] httpuv_1.6.5 R6_2.5.1 bookdown_0.28 ## [115] promises_1.2.0.1 gridExtra_2.3 codetools_0.2-18 ## [118] boot_1.3-28 colourpicker_1.1.1 MASS_7.3-58.1 ## [121] gtools_3.9.3 assertthat_0.2.1 withr_2.5.0 ## [124] shinystan_2.6.0 multcomp_1.4-20 mgcv_1.8-40 ## [127] parallel_4.2.0 hms_1.1.1 grid_4.2.0 ## [130] minqa_1.2.5 coda_0.19-4 rmarkdown_2.16 ## [133] googledrive_2.0.0 shiny_1.7.2 lubridate_1.8.0 ## [136] base64enc_0.1-3 dygraphs_1.1.1.6 References Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01 Bates, D., Maechler, M., Bolker, B., &amp; Steven Walker. (2021). lme4: Linear mixed-effects models using Eigen’ and S4. https://CRAN.R-project.org/package=lme4 Bürkner, P.-C. (2022c). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html Bürkner, P.-C. (2022d). brms reference manual, Version 2.17.0. https://CRAN.R-project.org/package=brms/brms.pdf Efron, B., &amp; Morris, C. (1977). Stein’s paradox in statistics. Scientific American, 236(5), 119–127. https://doi.org/10.1038/scientificamerican0577-119 Garnier, S. (2021). viridis: Default color maps from ’matplotlib’ [Manual]. https://CRAN.R-project.org/package=viridis Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Rosa, L., Rosa, E., Sarner, L., &amp; Barrett, S. (1998). A close look at therapeutic touch. JAMA, 279(13), 1005–1010. https://doi.org/10.1001/jama.279.13.1005 Weber, S., &amp; Bürkner, P.-C. (2022). Running brms models with within-chain parallelization. https://CRAN.R-project.org/package=brms/vignettes/brms_threading.html Wickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2-book.org/ Wilke, C. O. (2021). ggridges: Ridgeline Plots in ’ggplot2’. https://CRAN.R-project.org/package=ggridges "],["model-comparison-and-hierarchical-modeling.html", "10 Model Comparison and Hierarchical Modeling 10.1 General formula and the Bayes factor 10.2 Example: Two factories of coins 10.3 Solution by MCMC 10.4 Prediction: Model averaging 10.5 Model complexity naturally accounted for 10.6 Extreme sensitivity to prior distribution 10.7 Bonus: There’s danger ahead Session info Footnote", " 10 Model Comparison and Hierarchical Modeling There are situations in which different models compete to describe the same set of data… …Bayesian inference is reallocation of credibility over possibilities. In model comparison, the focal possibilities are the models, and Bayesian model comparison reallocates credibility across the models, given the data. In this chapter, we explore examples and methods of Bayesian inference about the relative credibilities of models. (Kruschke, 2015, pp. 265–266) In the text, the emphasis is on the Bayes Factor paradigm. While we will discuss that, we will also present the alternatives available with information criteria, model averaging, and model stacking. 10.1 General formula and the Bayes factor So far we have spoken of the data, denoted by \\(D\\) or \\(y\\); the model parameters, generically denoted \\(\\theta\\); the likelihood function, denoted \\(p(D | \\theta)\\); and the prior distribution, denoted \\(p(\\theta)\\). Now we add \\(m\\), which is a model index where \\(m = 1\\) stands for the first model, \\(m = 2\\) stands for the second model, and so on. So when we have more than one model in play, we might refer to the likelihood as \\(p_m(y | \\theta_m, m)\\) and the prior as \\(p_m(\\theta_m | m)\\). It’s also the case, then, that each model can be given a prior probability \\(p(m)\\). “The Bayes factor (BF) is the ratio of the probabilities of the data in models 1 and 2” (p. 268). This can be expressed simply as \\[\\text{BF} = \\frac{p(D | m = 1)}{p(D | m = 2)}.\\] Kruschke further explained that one convention for converting the magnitude of the BF to a discrete decision about the models is that there is “substantial” evidence for model \\(m = 1\\) when the BF exceeds \\(3.0\\) and, equivalently, “substantial” evidence for model \\(m = 2\\) when the BF is less than \\(1/3\\) (Jeffreys, 1961; Kass &amp; Raftery, 1995; Wetzels et al., 2011). (p. 268) However, as with \\(p\\)-values, effect sizes, and so on, BF values exist within continua and might should be evaluated in terms of degree more so than as ordered kinds. 10.2 Example: Two factories of coins Kruschke considered the coin bias of two factories, each described by the beta distribution. We can organize how to derive the \\(\\alpha\\) and \\(\\beta\\) parameters from \\(\\omega\\) and \\(\\kappa\\) with a tibble. library(tidyverse) d &lt;- tibble(factory = 1:2, omega = c(.25, .75), kappa = 12) %&gt;% mutate(alpha = omega * (kappa - 2) + 1, beta = (1 - omega) * (kappa - 2) + 1) d %&gt;% knitr::kable() factory omega kappa alpha beta 1 0.25 12 3.5 8.5 2 0.75 12 8.5 3.5 Thus given \\(\\omega_1 = .25\\), \\(\\omega_2 = .75\\) and \\(\\kappa = 12\\), we can describe the bias of the two coin factories as \\(\\operatorname{Beta}(\\theta_{[m = 1]} | 3.5, 8.5)\\) and \\(\\operatorname{Beta}(\\theta_{[m = 2]} | 8.5, 3.5)\\). With a little wrangling, we can use our d tibble to make the densities of Figure 10.2. But before we do, we should discuss plotting. In the past few chapters, we have explored different plotting conventions using themes from Wilke’s cowplot package, such as theme_cowplot() and theme_minimal_grid(). We also modified some of our plots using principles from Wilke’s (2019) text, Fundamentals of data visualization, and his (2020) Themes vignette. To further build on those principles, each chapter from here onward will have its own color scheme. The scheme in this chapter is based on Katsushika Hokusai’s (1820–1831) woodblock print, The great wave off Kanagawa. We can get a prearranged color palette based on The great wave off Kanagawa from Tyler Littlefield’s lisa package (Littlefield, 2020). library(lisa) lisa_palette(&quot;KatsushikaHokusai&quot;) ## * Work: The Great Wave off Kanagawa ## * Author: KatsushikaHokusai ## * Colors: #1F284C #2D4472 #6E6352 #D9CCAC #ECE2C6 plot(lisa_palette(&quot;KatsushikaHokusai&quot;)) The \"KatsushikaHokusai\" palette comes out of the box with five colors. However, we can use the lisa_palette() function to expand the palette by setting type = \"continuous\" and then increasing the n argument to a value larger than five. Here’s what happens when you set n = 9 and n = 1000. plot(lisa_palette(&quot;KatsushikaHokusai&quot;, n = 9, type = &quot;continuous&quot;)) plot(lisa_palette(&quot;KatsushikaHokusai&quot;, n = 1000, type = &quot;continuous&quot;)) Next, we will use the five base colors from \"KatsushikaHokusai\" to adjust the global theme default for all ggplots in this chapter. We can accomplish this with the ggplot2::theme_set() function. First, we start with the default theme_grey() as our base and then modify several of the settings with arguments within the theme() function. theme_set( theme_grey() + theme(text = element_text(color = lisa_palette(&quot;KatsushikaHokusai&quot;)[1]), axis.text = element_text(color = lisa_palette(&quot;KatsushikaHokusai&quot;)[1]), axis.ticks = element_line(color = lisa_palette(&quot;KatsushikaHokusai&quot;)[1]), legend.background = element_blank(), legend.box.background = element_blank(), legend.key = element_rect(fill = lisa_palette(&quot;KatsushikaHokusai&quot;)[5]), panel.background = element_rect(fill = lisa_palette(&quot;KatsushikaHokusai&quot;)[5], color = lisa_palette(&quot;KatsushikaHokusai&quot;)[1]), panel.grid = element_blank(), plot.background = element_rect(fill = lisa_palette(&quot;KatsushikaHokusai&quot;)[5], color = lisa_palette(&quot;KatsushikaHokusai&quot;)[5]), strip.background = element_rect(fill = lisa_palette(&quot;KatsushikaHokusai&quot;)[4]), strip.text = element_text(color = lisa_palette(&quot;KatsushikaHokusai&quot;)[1])) ) You can undo this by executing theme_set(theme_grey()). Next we’ll save the color names from a 9-color version of \"KatsushikaHokusai\" as a conveniently-named object, kh. We’ll use kh to adjust the fill and color settings within our plots on the fly. kh &lt;- lisa_palette(&quot;KatsushikaHokusai&quot;, 9, &quot;continuous&quot;) kh ## * Work: The Great Wave off Kanagawa ## * Author: KatsushikaHokusai ## * Colors: #1F284C #26365F #2D4472 #4D5362 #6E6352 ... and 4 more Okay, it’s time to get a sense of what we’ve done by making our version of Figure 10.2. length &lt;- 101 d %&gt;% expand(nesting(factory, alpha, beta), theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(label = str_c(&quot;factory &quot;, factory)) %&gt;% ggplot(aes(x = theta, y = dbeta(x = theta, shape1 = alpha, shape2 = beta))) + geom_area(fill = kh[6]) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + xlab(expression(theta)) + facet_wrap(~ label) We might recreate the top panel with geom_col(). tibble(Model = c(&quot;1&quot;, &quot;2&quot;), y = 1) %&gt;% ggplot(aes(x = Model, y = y)) + geom_col(width = .75, fill = kh[5]) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) Consider the Bernoulli bar plots in the bottom panels of Figure 10.2. The heights of the bars are arbitrary and just intended to give a sense of the Bernoulli distribution. If we wanted the heights to correspond to the Beta distributions above them, we might do so like this. crossing(factory = str_c(&quot;factory &quot;, 1:2), flip = factor(c(&quot;tails&quot;, &quot;heads&quot;), levels = c(&quot;tails&quot;, &quot;heads&quot;))) %&gt;% mutate(prob = c(.75, .25, .25, .75)) %&gt;% ggplot(aes(x = flip, y = prob)) + geom_col(width = .75, fill = kh[4]) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + xlab(NULL) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) + facet_wrap(~ factory) But now suppose we flip the coin nine times and get six heads. Given those data, what are the posterior probabilities of the coin coming from the head-biased or tail-biased factories? We will pursue the answer three ways: via formal analysis, grid approximation, and MCMC. (p. 270) Before we move on to a formal analysis, here’s a more faithful version of Kruschke’s Figure 10.2 based on the method from my blog post, Make model diagrams, Kruschke style. library(patchwork) library(ggforce) p1 &lt;- tibble(x = 1:2, d = c(.75, .75)) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = alpha(kh[5], .9), width = .45) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;categorical&quot;, size = 5, color = kh[1]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .85, label = &quot;italic(P[m])&quot;, size = 5, color = kh[1], family = &quot;Times&quot;, parse = TRUE) + coord_cartesian(xlim = c(-.5, 3.5), ylim = 0:1) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = kh[1])) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## an annotated arrow # save our custom arrow settings my_arrow &lt;- arrow(angle = 20, length = unit(0.35, &quot;cm&quot;), type = &quot;closed&quot;) p2 &lt;- tibble(x = .5, y = 1, xend = .5, yend = 0) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = kh[1]) + annotate(geom = &quot;text&quot;, x = .375, y = 1/3, label = &quot;&#39;~&#39;&quot;, size = 10, color = kh[1], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() p3 &lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 5, 10) / max(dbeta(x, 5, 10)))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = alpha(kh[4], .85)) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;beta&quot;, size = 5, color = kh[1]) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;list(italic(A)[1], italic(B)[1])&quot;, size = 5, color = kh[1], family = &quot;Times&quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = kh[1])) p4 &lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 10, 5) / max(dbeta(x, 10, 5)))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = kh[6]) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;beta&quot;, size = 5, color = kh[1]) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;list(italic(A)[2], italic(B)[2])&quot;, size = 5, color = kh[1], family = &quot;Times&quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = kh[1])) # bar plot of Bernoulli data p5 &lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = 1/3)) / max(dbinom(x, size = 1, prob = 1/3))) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = alpha(kh[4], .85), width = .4) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;Bernoulli&quot;, size = 7, color = kh[1]) + annotate(geom = &quot;text&quot;, x = .5, y = .94, label = &quot;theta&quot;, size = 7, color = kh[1], family = &quot;Times&quot;, parse = T) + xlim(-.75, 1.75) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = kh[1])) # another bar plot of Bernoulli data p6 &lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = 2/3)) / max(dbinom(x, size = 1, prob = 2/3))) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = kh[6], width = .4) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;Bernoulli&quot;, size = 7, color = kh[1]) + annotate(geom = &quot;text&quot;, x = .5, y = .94, label = &quot;theta&quot;, size = 7, color = kh[1], family = &quot;Times&quot;, parse = T) + xlim(-.75, 1.75) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = kh[1])) # another annotated arrow p7 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow, color = kh[1]) + xlim(0, 1) + theme_void() # some text p8 &lt;- tibble(x = 1, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = kh[1], parse = T, family = &quot;Times&quot;) + xlim(0, 2) + theme_void() # dashed borders p9 &lt;- tibble(x = c(0, 0.999, 0.999, 0, 1.001, 2, 2, 1.001), y = c(0, 0, 1, 1, 0, 0, 1, 1), z = rep(letters[1:2], each = 4)) %&gt;% ggplot(aes(x = x, y = y, group = z)) + geom_shape(fill = &quot;transparent&quot;, color = kh[1], linetype = 2, radius = unit(1, &#39;cm&#39;)) + scale_x_continuous(NULL, breaks = NULL, expand=c(0,0)) + scale_y_continuous(NULL, breaks = NULL, expand=c(0,0)) + theme_void() # define the layout layout &lt;- c( # cat area(t = 1, b = 5, l = 5, r = 9), area(t = 6, b = 8, l = 5, r = 9), # beta area(t = 9, b = 13, l = 2, r = 6), area(t = 9, b = 13, l = 8, r = 12), # arrow area(t = 14, b = 16, l = 2, r = 6), area(t = 14, b = 16, l = 8, r = 12), # bern area(t = 17, b = 21, l = 2, r = 6), area(t = 17, b = 21, l = 8, r = 12), area(t = 23, b = 25, l = 5, r = 9), area(t = 26, b = 27, l = 5, r = 9), area(t = 8, b = 23, l = 1, r = 13) ) # combine and plot! (p1 + p2 + p3 + p4 + p2 + p2 + p5 + p6 + p7 + p8 + p9) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Note how we used the geom_shape() function from the ggforce package (Thomas Lin Pedersen, 2021) to make the two dashed borders with the rounded edges. You can learn more from Pedersen’s (n.d.) vignette, Draw polygons with expansion/contraction and/or rounded corners — geom_shape. 10.2.1 Solution by formal analysis. Here we rehearse if we have a \\(\\operatorname{Beta}(\\theta, a, b)\\) prior for \\(\\theta\\) of the Bernoulli likelihood function, then the analytic solution for the posterior is \\(\\operatorname{Beta}(\\theta | z + a, N – z + b)\\). Within this paradigm, if you would like to compute \\(p(D | m)\\), don’t use the following function. If suffers from underflow with large values. p_d &lt;- function(z, n, a, b) { beta(z + a, n - z + b) / beta(a, b) } This version is more robust. p_d &lt;- function(z, n, a, b) { exp(lbeta(z + a, n - z + b) - lbeta(a, b)) } You’d use it like this to compute \\(p(D|m_1)\\). p_d(z = 6, n = 9, a = 3.5, b = 8.5) ## [1] 0.0004993439 So to compute our BF, \\(\\frac{p(D|m_1)}{p(D|m_2)}\\), you might use the p_d() function like this. p_d_1 &lt;- p_d(z = 6, n = 9, a = 3.5, b = 8.5) p_d_2 &lt;- p_d(z = 6, n = 9, a = 8.5, b = 3.5) p_d_1 / p_d_2 ## [1] 0.2135266 And if we computed the BF the other way, it’d look like this. p_d_2 / p_d_1 ## [1] 4.683258 Since the BF itself is only \\(\\text{BF} = \\frac{p(D | m = 1)}{p(D | m = 2)}\\), we’d need to bring in the priors for the models themselves to get the posterior probabilities, which follows the form \\[\\frac{p(m = 1 | D)}{p(m = 2 | D)} = \\left (\\frac{p(D | m = 1)}{p(D | m = 2)} \\right ) \\left ( \\frac{p(m = 1)}{p(m = 2)} \\right).\\] If for both our models \\(p(m) = .5\\), then the BF is the same it was, before. (p_d_1 * .5) / (p_d_2 * .5) ## [1] 0.2135266 As Kruschke pointed out, because we’re working in the probability metric, the sum of \\(p(m = 1 | D )\\) and \\(p(m = 2 | D )\\) must be 1. By simple algebra then, \\[p(m = 2 | D ) = 1 - p(m = 1 | D ).\\] Therefore, it’s also the case that \\[\\frac{p(m = 1 | D)}{1 - p(m = 1 | D)} = 0.2135266.\\] Thus, 0.2135266 is in an odds metric. If you want to convert odds to a probability, you follow the formula \\[\\text{odds} = \\frac{\\text{probability}}{1 - \\text{probability}}.\\] And with more algebraic manipulation, you can solve for the probability. \\[\\begin{align*} \\text{odds} &amp; = \\frac{\\text{probability}}{1 - \\text{probability}} \\\\ \\text{odds} - \\text{odds} \\cdot \\text{probability} &amp; = \\text{probability} \\\\ \\text{odds} &amp; = \\text{probability} + \\text{odds} \\cdot \\text{probability} \\\\ \\text{odds} &amp; = \\text{probability} (1 + \\text{odds}) \\\\ \\frac{\\text{odds}}{1 + \\text{odds}} &amp; = \\text{probability} \\end{align*}\\] Thus, the posterior probability for \\(m = 1\\) is \\[p(m = 1 | D) = \\frac{0.2135266}{1 + 0.2135266}.\\] We can express that in code like so. odds &lt;- (p_d_1 * .5) / (p_d_2 * .5) odds / (1 + odds) ## [1] 0.1759554 Relative to \\(m = 2\\), our posterior probability for \\(m = 1\\) is about .18. Therefore the posterior probability of \\(m = 2\\) is 1 minus that. 1 - (odds / (1 + odds)) ## [1] 0.8240446 Given the data, the two models and the prior assumption they were equally credible, we conclude \\(m = 2\\) is .82 probable. 10.2.2 Solution by grid approximation. As in earlier chapters, we won’t be able to make the wireframe plots on the left of Figure 10.3. But we can do some of the others. Here’s the upper right panel. p13 &lt;- tibble(omega = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(m_p = ifelse(omega %in% c(.25, .75), 15, 0)) %&gt;% ggplot(aes(xmin = 0, xmax = m_p, y = omega)) + geom_ribbon(fill = kh[4], color = kh[4]) + scale_x_continuous(expand = expansion(mult = c(0.002, 0.05)), limits = c(0, 25)) + scale_y_continuous(expand = expansion(mult = c(0, 0))) + labs(x = expression(Marginal~p(omega)), y = expression(omega)) p13 + labs(subtitle = &quot;Remember, the scale on the x is arbitrary.&quot;) Building on that, here’s the upper middle panel of the “two [prior] dorsal fins” (p. 271). d &lt;- crossing(omega = seq(from = 0, to = 1, length.out = length), theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(prior = ifelse(omega == .25, dbeta(theta, 3.5, 8.5), ifelse(omega == .75, dbeta(theta, 8.5, 3.5), 0))) p12 &lt;- d %&gt;% ggplot(aes(x = theta, y = omega, fill = prior)) + geom_raster(interpolate = T) + scale_fill_gradient(low = kh[1], high = kh[9], breaks = NULL) + scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = c(0, 0)) + scale_y_continuous(expression(omega), breaks = 0:5 / 5, expand = c(0, 0)) p12 This time we’ll separate \\(p_{m = 1}(\\theta)\\) and \\(p_{m = 2}(\\theta)\\) into the two short plots on the right of the next row down. p23 &lt;- d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% mutate(omega = factor(str_c(&quot;omega == &quot;, omega), levels = str_c(&quot;omega == &quot;, c(.75, .25)))) %&gt;% ggplot(aes(x = theta, y = prior)) + geom_area(fill = kh[4]) + scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = expansion(mult = c(0, 0))) + scale_y_continuous(expression(Marginal~p(theta*&quot;|&quot;*omega)), expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + facet_wrap(~ omega, ncol = 1, scales = &quot;free&quot;, labeller = label_parsed) p23 We can continue to build on those sensibilities for the middle panel of the same row. Here we’re literally adding \\(p_{m = 1}(\\theta)\\) to \\(p_{m = 2}(\\theta)\\) and taking their average. p22 &lt;- tibble(theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(d_75 = dbeta(x = theta, shape1 = 8.5, shape2 = 3.5), d_25 = dbeta(x = theta, shape1 = 3.5, shape2 = 8.5)) %&gt;% mutate(mean_prior = (d_75 + d_25) / 2) %&gt;% ggplot(aes(x = theta, y = mean_prior)) + geom_area(fill = kh[4]) + scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = expansion(mult = c(0, 0))) + scale_y_continuous(expression(Marginal~p(theta)), expand = expansion(mult = c(0, 0.05)), limits = c(0, 3)) p22 We need the Bernoulli likelihood function for the next step. bernoulli_likelihood &lt;- function(theta, data) { n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } Time to feed our data and the parameter space into bernoulli_likelihood(), which will allow us to make the 2-dimensional density plot at the heart of Figure 10.3. n &lt;- 9 z &lt;- 6 trial_data &lt;- rep(0:1, times = c(n - z, z)) d &lt;- d %&gt;% mutate(likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) p32 &lt;- d %&gt;% ggplot(aes(x = theta, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_gradient(low = kh[1], high = kh[9], breaks = NULL) + scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = c(0, 0)) + scale_y_continuous(expression(omega), breaks = 0:5 / 5, expand = c(0, 0)) p32 Now we just need the marginal likelihood, \\(p(D)\\), to compute the posterior. Our first depiction will be the middle panel of the second row from the bottom–the panel with the uneven dolphin fins. d &lt;- d %&gt;% mutate(marginal_likelihood = sum(prior * likelihood)) %&gt;% mutate(posterior = (prior * likelihood) / marginal_likelihood) p42 &lt;- d %&gt;% ggplot(aes(x = theta, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_gradient(low = kh[1], high = kh[9], breaks = NULL) + scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = c(0, 0)) + scale_y_continuous(expression(omega), breaks = 0:5 / 5, expand = c(0, 0)) p42 Here, then, is a way to get the panel in on the right of the second row from the bottom. p43 &lt;- d %&gt;% mutate(marginal = (posterior / max(posterior)) * 25) %&gt;% ggplot(aes(xmin = 0, xmax = marginal, y = omega)) + geom_ribbon(fill = kh[6], color = kh[6]) + scale_x_continuous(expression(omega), expand = expansion(mult = c(0.002, 0.05)), limits = c(0, 25)) + scale_y_continuous(expression(Marginal~p(omega*&quot;|&quot;*D)), expand = expansion(mult = c(0, 0))) p43 To make the middle bottom panel of Figure 10.3, we have to average the posterior values of \\(\\theta\\) over the grid of \\(\\omega\\) values. That is, we have to marginalize. p52 &lt;- d %&gt;% group_by(theta) %&gt;% summarise(marginal_theta = mean(posterior)) %&gt;% ggplot(aes(x = theta, y = marginal_theta)) + geom_area(fill = kh[6]) + scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = expansion(mult = c(0, 0))) + scale_y_continuous(expression(Marginal~p(theta*&quot;|&quot;*D)), expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) p52 For the lower right panel of Figure 10.3, we’ll filter() to our two focal values of \\(\\omega\\) and then facet by them. p53 &lt;- d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% mutate(omega = factor(str_c(&quot;omega == &quot;, omega), levels = str_c(&quot;omega == &quot;, c(.75, .25)))) %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_area(fill = kh[6]) + scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = expansion(mult = c(0, 0))) + scale_y_continuous(expression(Marginal~p(theta*&quot;|&quot;*omega*&quot;, &quot;*D)), expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + facet_wrap(~ omega, ncol = 1, scales = &quot;free&quot;, labeller = label_parsed) p53 Do note the different scales on the \\(y\\). Here’s what they’d look like on the same scale. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% mutate(omega = str_c(&quot;omega == &quot;, omega)) %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_area(fill = kh[6]) + scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = expansion(mult = c(0, 0))) + scale_y_continuous(expression(Marginal~p(theta*&quot;|&quot;*omega)), expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + facet_wrap(~ omega, ncol = 1, labeller = label_parsed) Hopefully that helps build the intuition of what Kruschke meant when he wrote “visual inspection suggests that the ratio of the heights is about 5 to 1, which matches the Bayes factor of 4.68 that we computed exactly in the previous section” (p. 273, emphasis in the original). Before we move on to the BF, let’s save a few more ggplots and combine them with the previous bunch to make the full version of Figure 10.3. p21 &lt;- tibble(x = 1, y = 8:7, label = c(&quot;Prior&quot;, &quot;K==12&quot;), size = c(2, 1)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(aes(size = size), color = kh[1], parse = T, hjust = 0, show.legend = F) + scale_size_continuous(range = c(3.5, 5.5)) + coord_cartesian(xlim = c(1, 2), ylim = c(4, 11)) + theme(axis.text = element_text(color = kh[9]), axis.ticks = element_blank(), panel.background = element_rect(color = kh[9]), text = element_text(color = kh[9])) p33 &lt;- tibble(x = 1, y = 8:7, label = c(&quot;Likelihood&quot;, &quot;D = 6 heads, 3 tails&quot;), size = c(2, 1)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(aes(size = size), hjust = 0, show.legend = F, color = kh[1]) + scale_size_continuous(range = c(3.5, 5.5)) + coord_cartesian(xlim = c(1, 2), ylim = c(4, 11)) + theme(axis.text = element_text(color = kh[9]), axis.ticks = element_blank(), panel.background = element_rect(color = kh[9]), text = element_text(color = kh[9])) p51 &lt;- ggplot() + annotate(geom = &quot;text&quot;, x = 1, y = 8, label = &quot;Posterior&quot;, size = 6, hjust = 0, color = kh[1]) + coord_cartesian(xlim = c(1, 2), ylim = c(3, 11)) + theme(axis.text = element_text(color = kh[9]), axis.ticks = element_blank(), panel.background = element_rect(color = kh[9]), text = element_text(color = kh[9])) p11 &lt;- plot_spacer() # combine and plot! (p11 / p21 / p11 / p11 / p51) | (p12 / p22 / p32 / p42 / p52) | (p13 / p23 / p33 / p43 / p53) Oh mamma! Using the grid, you might compute that BF like this. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% summarise(sum_posterior = sum(posterior)) %&gt;% mutate(model = c(&quot;model_1&quot;, &quot;model_2&quot;)) %&gt;% pivot_wider(-omega, names_from = model, values_from = sum_posterior) %&gt;% mutate(BF = model_2 / model_1) ## # A tibble: 1 × 3 ## model_1 model_2 BF ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.176 0.824 4.68 Please note4 how, in the previous section, Kruschke computed the BF as \\[\\frac{p(m = 1 | D)}{p(m = 2 | D)} = .213,\\] which we achieved with this code: p_d_1 / p_d_2 ## [1] 0.2135266 Here, we’re flipping the ratio to \\[\\frac{p(m = 2 | D)}{p(m = 1 | D)},\\] which is why we now have a BF near 5. p_d_2 / p_d_1 ## [1] 4.683258 But anyway, both the posterior distributions in the figure and the BF indicate \\(\\omega = .75\\) is a better representation of the data than \\(\\omega = .25\\). 10.3 Solution by MCMC Kruschke started with: “For large, complex models, we cannot derive \\(p(D | m)\\) analytically or with grid approximation, and therefore we will approximate the posterior probabilities using MCMC methods” (p. 274). He’s not kidding. Welcome to modern Bayes. 10.3.1 Nonhierarchical MCMC computation of each model’s marginal likelihood. Before you get excited, Kruschke warned: “For complex models, this method might not be tractable. [But] for the simple application here, however, the method works well, as demonstrated in the next section” (p. 277). 10.3.1.1 Implementation with JAGS brms. Load brms. library(brms) Let’s save the trial_data as a tibble. trial_data &lt;- tibble(y = trial_data) Time to learn a new brms skill. When you want to enter variables into the parameters defining priors in brms::brm(), you need to specify them using the stanvar() function. Since we want to do this for two variables, we’ll use stanvar() twice and save the results as an object, conveniently named stanvars. omega &lt;- .75 kappa &lt;- 12 stanvars &lt;- stanvar( omega * (kappa - 2) + 1, name = &quot;my_alpha&quot;) + stanvar((1 - omega) * (kappa - 2) + 1, name = &quot;my_beta&quot;) Now we have our stanvars object, we are ready to fit the first model (i.e., the model for which \\(\\omega = .75\\)). fit10.1 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, # stanvars lets us do this prior(beta(my_alpha, my_beta), class = Intercept, lb = 0, ub = 1), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, stanvars = stanvars, file = &quot;fits/fit10.01&quot;) Note how we fed our stanvars object into the stanvars function. Anyway, let’s inspect the chains. plot(fit10.1, widths = c(2, 3)) They look great. Now we glance at the model summary. print(fit10.1) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: trial_data (Number of observations: 9) ## Draws: 4 chains, each with iter = 11000; warmup = 1000; thin = 1; ## total post-warmup draws = 40000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.69 0.10 0.48 0.86 1.00 14225 17083 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Next we’ll follow Kruschke and extract the posterior draws, saving them as theta. theta &lt;- as_draws_df(fit10.1) head(theta) ## # A draws_df: 6 iterations, 1 chains, and 3 variables ## b_Intercept lprior lp__ ## 1 0.68 1.025 -6.2 ## 2 0.75 1.129 -6.4 ## 3 0.52 0.016 -7.5 ## 4 0.56 0.337 -7.0 ## 5 0.75 1.128 -6.5 ## 6 0.73 1.113 -6.3 ## # ... hidden reserved variables {&#39;.chain&#39;, &#39;.iteration&#39;, &#39;.draw&#39;} The fixef() function will return the posterior summaries for the model intercept (i.e., \\(\\theta\\)). We can then index and save the desired summaries. fixef(fit10.1) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.6889111 0.09827117 0.4827282 0.8631757 (mean_theta &lt;- fixef(fit10.1)[1]) ## [1] 0.6889111 (sd_theta &lt;- fixef(fit10.1)[2]) ## [1] 0.09827117 Now we’ll convert them to the \\(\\alpha\\) and \\(\\beta\\) parameters, a_post and b_post, respectively. a_post &lt;- mean_theta * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1) b_post &lt;- (1 - mean_theta) * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1) Recall we’ve already defined several values. n &lt;- 9 z &lt;- 6 omega &lt;- .75 kappa &lt;- 12 The reason we’re saving all these values is we’re aiming to compute \\(p(D)\\), the probability of the data (i.e., the marginal likelihood), given the model. But our intermediary step will be computing its reciprocal, \\(\\frac{1}{p(D)}\\). Here we’ll express Kruschke’s oneOverPD as a function, one_over_pd(). one_over_pd &lt;- function(theta) { mean(dbeta(theta, a_post, b_post ) / (theta^z * (1 - theta)^(n - z) * dbeta(theta, omega * (kappa - 2) + 1, (1 - omega) * (kappa - 2) + 1 ))) } We’re ready to use one_over_pd() to help compute \\(p(D)\\). theta %&gt;% summarise(pd = 1 / one_over_pd(theta = b_Intercept)) ## # A tibble: 1 × 1 ## pd ## &lt;dbl&gt; ## 1 0.00234 That matches up nicely with Kruschke’s value at the top of page 278! Let’s rinse, wash, and repeat for \\(\\omega = .25\\). First, we’ll need to redefine omega and our stanvars. omega &lt;- .25 stanvars &lt;- stanvar( omega * (kappa - 2) + 1, name = &quot;my_alpha&quot;) + stanvar((1 - omega) * (kappa - 2) + 1, name = &quot;my_beta&quot;) Fit the model. fit10.2 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, prior(beta(my_alpha, my_beta), class = Intercept, lb = 0, ub = 1), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, stanvars = stanvars, file = &quot;fits/fit10.02&quot;) We’ll do the rest in bulk. theta &lt;- as_draws_df(fit10.2) mean_theta &lt;- fixef(fit10.2)[1] sd_theta &lt;- fixef(fit10.2)[2] a_post &lt;- mean_theta * (mean_theta * (1 - mean_theta) / sd_theta^2 - 1) b_post &lt;- (1 - mean_theta) * (mean_theta * (1 - mean_theta) / sd_theta^2 - 1) theta %&gt;% summarise(pd = 1 / one_over_pd(theta = b_Intercept)) ## # A tibble: 1 × 1 ## pd ## &lt;dbl&gt; ## 1 0.000499 Boom! 10.3.2 Hierarchical MCMC computation of relative model probability is not available in brms: We’ll cover information criteria instead. I’m not aware of a way to specify a model “in which the top-level parameter is the index across models” in brms (p. 278). If you know of a way, share your code. However, we do have options. We can compare and weight models using information criteria, about which you can learn more here or here. The LOO and WAIC are two primary information criteria available for brms. You can compute them for a given model with the loo() and waic() functions, respectively. Here’s a quick example of how to use the waic() function. waic(fit10.1) ## ## Computed from 40000 by 9 log-likelihood matrix ## ## Estimate SE ## elpd_waic -6.2 1.3 ## p_waic 0.5 0.1 ## waic 12.4 2.7 We’ll explain that output in a bit. Before we do, you should know the current recommended workflow for information criteria with brms models is to use the add_criterion() function, which will allow us to compute information-criterion-related output and save it to our brms fit objects. Here’s how to do that with both our fits. fit10.1 &lt;- add_criterion(fit10.1, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit10.2 &lt;- add_criterion(fit10.2, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) You can extract the same WAIC output for fit10.1 we saw above by executing fit10.1$criteria$waic. Here we look at the LOO summary for fit10.2, instead. fit10.2$criteria$loo ## ## Computed from 40000 by 9 log-likelihood matrix ## ## Estimate SE ## elpd_loo -7.1 0.3 ## p_loo 0.5 0.0 ## looic 14.1 0.6 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. You get a wealth of output, more of which can be seen by executing str(fit10.1$criteria$loo). First, notice the message “All Pareto k estimates are good (k &lt; 0.5).” Pareto \\(k\\) values can be used for diagnostics (Vehtari &amp; Gabry, 2022a, Plotting Pareto \\(k\\) diagnostics). Each case in the data gets its own \\(k\\) value and we like it when those \\(k\\)’s are low. The makers of the loo package (Vehtari et al., 2017; Vehtari et al., 2022) get worried when \\(k\\) values exceed 0.7 and, as a result, we will get warning messages when they do. Happily, we have no such warning messages in this example. In the main section, we get estimates for the expected log predictive density (elpd_loo), the estimated effective number of parameters (p_loo), and the Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO; looic). Each estimate comes with a standard error (i.e., SE). Like other information criteria, the LOO values aren’t of interest in and of themselves. However, the estimate of one model’s LOO relative to that of another can be of great interest. We generally prefer models with lower information criteria. With the loo_compare() function, we can compute a formal difference score between two models. loo_compare(fit10.1, fit10.2, criterion = &quot;loo&quot;) ## elpd_diff se_diff ## fit10.1 0.0 0.0 ## fit10.2 -0.8 1.6 The loo_compare() output rank orders the models such that the best fitting model appears on top. All models receive a difference score relative to the best model. Here the best fitting model is fit10.1 and since the LOO for fit10.1 minus itself is zero, the values in the top row are all zero. Each difference score also comes with a standard error. In this case, even though fit10.1 has the lower estimates, the standard error is twice the magnitude of the difference score. So the LOO difference score puts the two models on similar footing. You can do a similar analysis with the WAIC estimates. In addition to difference-score comparisons, you can also use the LOO or WAIC for AIC-type model weighting. In brms, you do this with the model_weights() function. (mw &lt;- model_weights(fit10.1, fit10.2)) ## fit10.1 fit10.2 ## 0.8399257 0.1600743 I don’t know that I’d call these weights probabilities, but they do sum to one. In this case, the analysis suggests we put about five times more weight to fit10.1 relative to fit10.2. mw[1] / mw[2] ## fit10.1 ## 5.247101 With brms::model_weights(), we have a variety of weighting schemes available to us. Since we didn’t specify any in the weights argument, we used the default \"stacking\"method, which is described in Yao et al. (2018). Vehtari has written about the paper on Gelman’s blog, too. But anyway, the point is that different weighting schemes might not produce the same results. For example, here’s the result from weighting using the WAIC. model_weights(fit10.1, fit10.2, weights = &quot;waic&quot;) ## fit10.1 fit10.2 ## 0.698708 0.301292 The results are similar, for sure. But they’re not the same. The stacking method via the brms default weights = \"stacking\" is the current preferred method by the folks on the Stan team (e.g., the authors of the above linked paper). For more on stacking and other weighting schemes, see Vehtari and Gabry’s (2022b) vignette, Bayesian Stacking and Pseudo-BMA weights using the loo package, or Vehtari’s modelselection_tutorial GitHub repository. But don’t worry. We will have more opportunities to practice with information criteria, model weights, and such later in this ebook. 10.3.2.1 Using [No need to use] pseudo-priors to reduce autocorrelation. Since we didn’t use Kruschke’s method from the last subsection, we don’t have the same worry about autocorrelation. For example, here are the autocorrelation plots for fit10.1. library(bayesplot) color_scheme_set( scheme = c(lisa_palette(&quot;KatsushikaHokusai&quot;, n = 9, type = &quot;continuous&quot;)[6:1]) ) theta %&gt;% mutate(chain = .chain) %&gt;% mcmc_acf(pars = &quot;b_Intercept&quot;, lags = 35) Our autocorrelations were a little high for HMC, but nowhere near pathological. The results for fit10.2 were similar. Before we move on, note our use of bayesplot::color_scheme_set(), which allowed us to customize the color scheme bayesplot used within the plot. Based on that code, here is our new color scheme for all plots made by bayesplot. color_scheme_view() color_scheme_get() ## custom ## 1 #A3977F ## 2 #6E6352 ## 3 #4D5362 ## 4 #2D4472 ## 5 #26365F ## 6 #1F284C In case you were curious, here is the default. color_scheme_view(scheme = &quot;blue&quot;) Anyway, as you might imagine from the moderate autocorrelations, the \\(N_{eff}/N\\) ratio for b_Intercept wasn’t great. neff_ratio(fit10.1)[1] %&gt;% mcmc_neff() + yaxis_text(hjust = 0) But we specified a lot of post-warmup draws, so we’re still in good shape. Plus, the \\(\\widehat R\\) was fine. rhat(fit10.1)[1] ## b_Intercept ## 1.00027 10.3.3 Models with different “noise” distributions in JAGS brms. Probability distribution[s are] sometimes [called “noise”] distribution[s] because [they describe] the random variability of the data values around the underlying trend. In more general applications, different models can have different noise distributions. For example, one model might describe the data as log-normal distributed, while another model might describe the data as gamma distributed. (p. 288) If there are more than one plausible noise distributions for our data, we might want to compare the models. Kruschke then gave us a general trick in the form of this JAGS code: data { C &lt;- 10000 # JAGS does not warn if too small! for (i in 1:N) { ones[i] &lt;- 1 } } model { for (i in 1:N) { spy1[i] &lt;- pdf1(y[i], parameters1) / C # where pdf1 is a formula spy2[i] &lt;- pdf2(y[i], parameters2) / C # where pdf2 is a formula spy[i] &lt;- equals(m,1) * spy1[i] + equals(m, 2) * spy2[i] ones[i] ~ dbern(spy[i]) } parameters1 ~ dprior1... parameters2 ~ dprior2... m ~ dcat(mPriorProb[]) mPriorProb[1] &lt;- .5 mPriorProb[2] &lt;- .5 } I’m not aware that we can do this within the Stan/brms framework. If I’m in error and you know how, please share your code. However, we do have options. In anticipation of Chapter 16, let’s consider Gaussian-like data with thick tails. We might generate some like this. # how many draws would you like? n &lt;- 1e3 set.seed(10) (d &lt;- tibble(y = rt(n, df = 7))) ## # A tibble: 1,000 × 1 ## y ## &lt;dbl&gt; ## 1 0.0214 ## 2 -0.987 ## 3 0.646 ## 4 -0.237 ## 5 0.977 ## 6 -0.200 ## 7 0.781 ## 8 -1.09 ## 9 1.83 ## 10 -0.682 ## # … with 990 more rows The resulting data look like this. d %&gt;% ggplot(aes(x = y)) + geom_histogram(color = kh[9], fill = kh[3], size = .1, bins = 30) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + theme(panel.grid = element_blank()) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. As you’d expect with a small-\\(\\nu\\) Student’s \\(t\\), some of our values are far from the central clump. If you don’t recall, Student’s \\(t\\)-distribution has three parameters: \\(\\nu\\), \\(\\mu\\), and \\(\\sigma\\). The Gaussian is a special case of Student’s \\(t\\) where \\(\\nu = \\infty\\). As \\(\\nu\\) gets small, the distribution allocates more mass in the tails. From a Gaussian perspective, the small-\\(\\nu\\) Student’s \\(t\\) expects more outliers–though it’s a little odd calling them outliers from a small-\\(\\nu\\) Student’s \\(t\\) perspective. Let’s see how well the Gaussian versus the Student’s \\(t\\) likelihoods handle the data. Here we’ll use fairly liberal priors. fit10.3 &lt;- brm(data = d, family = gaussian, y ~ 1, prior = c(prior(normal(0, 5), class = Intercept), # by default, this has a lower bound of 0 prior(normal(0, 5), class = sigma)), chains = 4, cores = 4, seed = 10, file = &quot;fits/fit10.03&quot;) fit10.4 &lt;- brm(data = d, family = student, y ~ 1, prior = c(prior(normal(0, 5), class = Intercept), prior(normal(0, 5), class = sigma), # this is the brms default prior for nu prior(gamma(2, 0.1), class = nu)), chains = 4, cores = 4, seed = 10, file = &quot;fits/fit10.04&quot;) In case you were curious, here’s what that default gamma(2, 0.1) prior on nu looks like. tibble(x = seq(from = 1, to = 100, by = 1)) %&gt;% ggplot(aes(x = x, y = dgamma(x, 2, 0.1))) + geom_area(fill = kh[5]) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + scale_x_continuous(expression(italic(p)(nu)), expand = expansion(mult = c(0, 0.01)), limits = c(0, 100)) That prior puts most of the probability mass below 50, but the right tail gently fades off into the triple digits, allowing for the possibility of larger estimates. We can use the posterior_summary() function to get a compact look at the parameter summaries. posterior_summary(fit10.3)[1:2, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -0.03 0.04 -0.11 0.05 ## sigma 1.25 0.03 1.20 1.31 posterior_summary(fit10.4)[1:3, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -0.01 0.04 -0.08 0.06 ## sigma 0.98 0.04 0.91 1.05 ## nu 5.74 1.00 4.18 8.09 Now we can compare the two approaches using information criteria. For kicks, we’ll use the WAIC. fit10.3 &lt;- add_criterion(fit10.3, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit10.4 &lt;- add_criterion(fit10.4, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(fit10.3, fit10.4, criterion = &quot;waic&quot;) ## elpd_diff se_diff ## fit10.4 0.0 0.0 ## fit10.3 -60.7 40.3 Based on the WAIC difference, we have some support for preferring the Student’s \\(t\\), but do notice how wide that SE was. We can also compare the models using model weights. Here we’ll use the default weighting scheme. model_weights(fit10.3, fit10.4) ## fit10.3 fit10.4 ## 0.02873619 0.97126381 Virtually all of the stacking weight was placed on the Student’s-\\(t\\) model, fit10.4. Remember what that \\(p(\\nu)\\) looked like? Here’s our posterior distribution for \\(\\nu\\). as_draws_df(fit10.4) %&gt;% ggplot(aes(x = nu)) + geom_histogram(color = kh[9], fill = kh[3], size = .1, bins = 30) + scale_x_continuous(expression(italic(p)(nu*&quot;|&quot;*italic(D))), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + coord_cartesian(xlim = c(0, 21)) + labs(subtitle = expression(&quot;Recall that for the Gaussian, &quot;*nu==infinity.)) Even though our prior for \\(\\nu\\) was relatively weak, the posterior ended up concentrated on values in the middle-single-digit range. Recall the data-generating value was 7. We can also compare the models using posterior-predictive checks. There are a variety of ways we might do this, but the most convenient way is with brms::pp_check(), which is itself a wrapper for the family of ppc functions from the bayesplot package. pp_check(fit10.3, ndraws = 50) + coord_cartesian(xlim = c(-10, 10)) + ggtitle(&quot;Gaussian model&quot;) pp_check(fit10.4, ndraws = 50) + coord_cartesian(xlim = c(-10, 10)) + ggtitle(&quot;Student-t model&quot;) The default pp_check() setting allows us to compare the density of the data \\(y\\) (i.e., the dark blue) with 10 densities simulated from the posterior \\(y_\\text{rep}\\) (i.e., the light blue). By ndraws = 50, we adjusted that default to 50 simulated densities. We prefer models that produce \\(y_\\text{rep}\\) distributions resembling \\(y\\). Though the results from both models were similar, the simulated distributions from fit10.4 mimicked the original data a little more convincingly. To learn more about this approach to posterior predictive checks, check out Gabry’s (2022) vignette, Graphical posterior predictive checks using the bayesplot package. 10.4 Prediction: Model averaging In many applications of model comparison, the analyst wants to identify the best model and then base predictions of future data on that single best model, denoted with index \\(b\\). In this case, predictions of future \\(\\hat y\\) are based exclusively on the likelihood function \\(p_b(\\hat y | \\theta_b, m = b)\\) and the posterior distribution \\(p_b(\\theta_b | D, m = b)\\) of the winning model: \\[p(\\hat y | D, m = b) = \\int \\text d \\theta_b \\; p_b (\\hat y | \\theta_b, m = b) p_b(\\theta_b | D, m = b)\\] But the full model of the data is actually the complete hierarchical structure that spans all the models being compared, as indicated in Figure 10.1 (p. 267). Therefore, if the hierarchical structure really expresses our prior beliefs, then the most complete prediction of future data takes into account all the models, weighted by their posterior credibilities. In other words, we take a weighted average across the models, with the weights being the posterior probabilities of the models. Instead of conditionalizing on the winning model, we have \\[\\begin{align*} p (\\hat y | D) &amp; = \\sum_m p (\\hat y | D, m) p (m | D) \\\\ &amp; = \\sum_m \\int \\text d \\theta_m \\; p_m (\\hat y | \\theta_m, m) p_m(\\theta_m | D, m) p (m | D) \\end{align*}\\] This is called model averaging. (p. 289) Okay, while the concept of model averaging is of great interest, we aren’t going to be able to follow this approach to it within the Stan/brms paradigm. This, recall, is because our paradigm doesn’t allow for a hierarchical organization of models in the same way JAGS does. However, we can still play the model averaging game with extensions of our model weighting paradigm, above. Before we get into the details, recall that there were two models of mints that created the coin, with one mint being tail-biased with mode \\(\\omega = 0.25\\) and one mint being head-biased with mode \\(\\omega = 0.75\\) The two subpanels in the lower-right [of Figure 10.3] illustrate the posterior distributions on \\(\\omega\\) within each model, \\(p(\\theta | D, \\omega = 0.25)\\) and \\(p(\\theta | D, \\omega = 0.75)\\) The winning model was \\(\\omega = 0.75\\), and therefore the predicted value of future data, based on the winning model alone, would use \\(p(\\theta | D, \\omega = 0.75)\\). (p. 289) Here’s the histogram for \\(p(\\theta | D, \\omega = 0.75)\\), which we generate from our fit10.1. library(tidybayes) as_draws_df(fit10.1) %&gt;% ggplot(aes(x = b_Intercept, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = c(.95, .5), fill = kh[6], slab_color = kh[5], color = kh[2], breaks = 40, slab_size = .25, outline_bars = T) + scale_x_continuous(expression(italic(p)(theta*&quot;|&quot;*italic(D)*&quot;, &quot;*omega==.75)), expand = expansion(mult = c(0, 0)), breaks = 0:5 / 5, limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0.02, 0.05))) + labs(subtitle = &quot;The posterior for the probability, given fit10.1&quot;) But the overall model included \\(\\omega = 0.75\\), and if we use the overall model, then the predicted value of future data should be based on the complete posterior summed across values of \\(\\omega\\). The complete posterior distribution [is] \\(p(\\theta | D)\\) (p. 289). The cool thing about the model weighting stuff we learned about earlier is that you can use those model weights to average across models. Again, we’re not weighting the models by posterior probabilities the way Kruschke discussed in text, but the spirit is similar. We can use the brms::pp_average() function to make posterior predictive prediction with mixtures of the models, weighted by our chosen weighting scheme. Here, we’ll go with the default stacking weights. nd &lt;- tibble(y = 1) pp_a &lt;- pp_average(fit10.1, fit10.2, newdata = nd, # this line is not necessary, # but you should see how to choose weighing methods weights = &quot;stacking&quot;, method = &quot;fitted&quot;, summary = F) %&gt;% as_tibble() %&gt;% set_names(&quot;theta&quot;) # what does this produce? head(pp_a) ## # A tibble: 6 × 1 ## theta ## &lt;dbl&gt; ## 1 0.751 ## 2 0.520 ## 3 0.558 ## 4 0.755 ## 5 0.725 ## 6 0.818 We can plot our model-averaged \\(\\theta\\) with a little help from good old tidybayes::stat_histinterval(). pp_a %&gt;% ggplot(aes(x = theta, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = c(.95, .5), fill = kh[6], slab_color = kh[5], color = kh[2], breaks = 40, slab_size = .25, outline_bars = T) + scale_x_continuous(expression(italic(p)(theta*&quot;|&quot;*italic(D))), expand = expansion(mult = c(0, 0)), breaks = 0:5 / 5, limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0.02, 0.05))) + labs(subtitle = &quot;The posterior for the probability, given the\\nweighted combination of fit10.1 and fit10.2&quot;) As Kruschke concluded, “you can see the contribution of \\(p(\\theta | D, \\omega = 0.25)\\) as the extended leftward tail” (p. 289). Interestingly enough, that looks a lot like the marginal density of \\(p (\\theta | D)\\) across values of \\(\\omega\\) we made with grid approximation in Figure 10.3, doesn’t it? 10.5 Model complexity naturally accounted for A complex model (usually) has an inherent advantage over a simpler model because the complex model can find some combination of its parameter values that match the data better than the simpler model. There are so many more parameter options in the complex model that one of those options is likely to fit the data better than any of the fewer options in the simpler model. The problem is that data are contaminated by random noise, and we do not want to always choose the more complex model merely because it can better fit noise. Without some way of accounting for model complexity, the presence of noise in data will tend to favor the complex model. Bayesian model comparison compensates for model complexity by the fact that each model must have a prior distribution over its parameters, and more complex models must dilute their prior distributions over larger parameter spaces than simpler models. Thus, even if a complex model has some particular combination of parameter values that fit the data well, the prior probability of that particular combination must be small because the prior is spread thinly over the broad parameter space. (pp. 289–290) Now our two models are: the “must-be-fair” model \\(p(\\theta | D, \\kappa = 1{,}000)\\), and the “anything’s-possible” model \\(p(\\theta | D, \\kappa = 2)\\). They look like this. # how granular to you want the theta sequence? n &lt;- 1e3 # simulate the data tibble(omega = .5, kappa = c(1000, 2), model = c(&quot;The must-be-fair model&quot;, &quot;The anything&#39;s-possible model&quot;)) %&gt;% expand(nesting(omega, kappa, model), theta = seq(from = 0, to = 1, length.out = n)) %&gt;% mutate(density = dbeta(theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% # plot ggplot(aes(x = theta, y = density)) + geom_area(fill = kh[5]) + scale_x_continuous(expression(theta), expand = expansion(mult = c(0, 0)), breaks = 0:5 / 5, labels = c(&quot;0&quot;, &quot;.2&quot;, &quot;.4&quot;, &quot;.6&quot;, &quot;.8&quot;, &quot;1&quot;), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(subtitle = &quot;Note that in this case, their y-axes are on the same scale.&quot;) + facet_wrap(~ model) Here’s how you might compute the \\(\\alpha\\) and \\(\\beta\\) values for the corresponding beta distributions. tibble(omega = .5, kappa = c(1000, 2), model = c(&quot;The must-be-fair model&quot;, &quot;The anything&#39;s-possible model&quot;)) %&gt;% mutate(alpha = omega * (kappa - 2) + 1, beta = (1 - omega) * (kappa - 2) + 1) ## # A tibble: 2 × 5 ## omega kappa model alpha beta ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5 1000 The must-be-fair model 500 500 ## 2 0.5 2 The anything&#39;s-possible model 1 1 With those in hand, we can use our p_d() function to compute the Bayes factor based on flipping a coin \\(N = 20\\) times and observing \\(z = 15\\) heads. # the data summaries z &lt;- 15 n &lt;- 20 p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1) ## [1] 0.3229023 Let’s try again, this time supposing we observe \\(z = 15\\) heads out of \\(N = 20\\) coin flips. z &lt;- 11 p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1) ## [1] 3.337148 The anything’s-possible model loses because it pays the price of having a small prior probability on the values of \\(\\theta\\) near the data proportion, while the must-be-fair model has large prior probability on \\(\\theta\\) values sufficiently near the data proportion to be credible. Thus, in Bayesian model comparison, a simpler model can win if the data are consistent with it, even if the complex model fits just as well. The complex model pays the price of having small prior probability on parameter values that describe simple data. (p. 291) 10.5.1 Caveats regarding nested model comparison. A frequently encountered special case of comparing models of different complexity occurs when one model is “nested” within the other. Consider a model that implements all the meaningful parameters we can contemplate for the particular application. We call that the full model. We might consider various restrictions of those parameters, such as setting some of them to zero, or forcing some to be equal to each other. A model with such a restriction is said to be nested within the full model. (p. 291) Kruschke didn’t walk out the examples in this section. But for the sake of practice, let’s work through the first one. “Recall the hierarchical model of baseball batting abilities” from Chapter 9 (p. 291). Let’s reload those data. my_data &lt;- read_csv(&quot;data.R/BattingAverage.csv&quot;) glimpse(my_data) ## Rows: 948 ## Columns: 6 ## $ Player &lt;chr&gt; &quot;Fernando Abad&quot;, &quot;Bobby Abreu&quot;, &quot;Tony Abreu&quot;, &quot;Dustin Ack… ## $ PriPos &lt;chr&gt; &quot;Pitcher&quot;, &quot;Left Field&quot;, &quot;2nd Base&quot;, &quot;2nd Base&quot;, &quot;1st Bas… ## $ Hits &lt;dbl&gt; 1, 53, 18, 137, 21, 0, 0, 2, 150, 167, 0, 128, 66, 3, 1, … ## $ AtBats &lt;dbl&gt; 7, 219, 70, 607, 86, 1, 1, 20, 549, 576, 1, 525, 275, 12,… ## $ PlayerNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ PriPosNumber &lt;dbl&gt; 1, 7, 4, 4, 3, 1, 1, 3, 3, 4, 1, 5, 4, 2, 7, 4, 6, 8, 9, … “The full model has a distinct modal batting ability, \\(\\omega_c\\) , for each of the nine fielding positions. The full model also has distinct concentration parameters for each of the nine positions” (p. 291). Let’s fit that model again. fit9.2 &lt;- brm(data = my_data, family = binomial(link = logit), Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = .99), seed = 9, file = &quot;fits/fit09.02&quot;) Next we’ll consider a restricted version of fit9.2 “in which all infielders (first base, second base, etc.) are grouped together versus all outfielders (right field, center field, and left field). In this restricted model, we are forcing the modal batting abilities of all the outfielders to be the same, that is, \\(\\omega_\\text{left field} = \\omega_\\text{center field} = \\omega_\\text{right field}\\)” (p. 291). To fit that model, we’ll need to make a new variable PriPos_small which is identical to its parent variable PriPos except that it collapses those three positions into our new category Outfield. my_data &lt;- my_data %&gt;% mutate(PriPos_small = if_else(PriPos %in% c(&quot;Center Field&quot;, &quot;Left Field&quot;, &quot;Right Field&quot;), &quot;Outfield&quot;, PriPos)) Now use update() to fit the restricted model. fit10.5 &lt;- update(fit9.2, newdata = my_data, formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos_small) + (1 | PriPos_small:Player), iter = 3500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = .99), seed = 10, file = &quot;fits/fit10.05&quot;) ## The desired updates require recompiling the model Unlike with what Kruschke alluded to in the prose, here we’ll compare the two models with the WAIC. fit9.2 &lt;- add_criterion(fit9.2, criterion = &quot;waic&quot;) fit10.5 &lt;- add_criterion(fit10.5, criterion = &quot;waic&quot;) loo_compare(fit9.2, fit10.5, criterion = &quot;waic&quot;) ## elpd_diff se_diff ## fit9.2 0.0 0.0 ## fit10.5 -0.1 1.2 Based on the WAIC difference score, they’re near equivalent. Now let’s see how their WAIC weights shake out. model_weights(fit9.2, fit10.5, weights = &quot;waic&quot;) %&gt;% round(2) ## fit9.2 fit10.5 ## 0.53 0.47 In this case, just a little more of the weight went to the full model, fit9.2. The overall pattern between the WAIC difference and the WAIC weights was uncertainty. Make sure to use good substantive reasoning when comparing models. 10.6 Extreme sensitivity to prior distribution In many realistic applications of Bayesian model comparison, the theoretical emphasis is on the difference between the models’ likelihood functions. For example, one theory predicts planetary motions based on elliptical orbits around the sun, and another theory predicts planetary motions based on circular cycles and epicycles around the earth. The two models involve very different parameters. In these sorts of models, the form of the prior distribution on the parameters is not a focus, and is often an afterthought. But, when doing Bayesian model comparison, the form of the prior is crucial because the Bayes factor integrates the likelihood function weighted by the prior distribution. (p. 292) However, “the sensitivity of Bayes factors to prior distributions is well known in the literature (e.g., Kass &amp; Raftery, 1995; Liu &amp; Aitkin, 2008; Vanpaemel, 2010),” and furthermore, when comparing Bayesian models using the methods Kruschke outlined in this chapter of the text, “different forms of vague priors can yield very different Bayes factors” (p. 293). In the two BFs to follow, we compare the must-be-fair model and the anything’s-possible models from Section 10.5 to new data: \\(z = 65, N = 100\\). z &lt;- 65 n &lt;- 100 p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1) ## [1] 0.125287 The resulting 0.13 favored the anything’s-possible model. Another way to express the anything’s-possible model is with the Haldane prior, which sets the two parameters within the beta distribution to be a) equivalent and b) quite small (i.e., 0.01 in this case). p_d(z, n, a = 500, b = 500) / p_d(z, n, a = .01, b = .01) ## [1] 5.728066 Now we flipped to favoring the must-be-fair model. You might be asking, Wait, kind of distribution did that Haldane prior produce? Here we compare it to the \\(\\operatorname{Beta}(1, 1)\\). # save this text for later text &lt;- c(&quot;Uninformative prior, Beta(1, 1)&quot;, &quot;Haldane prior, Beta(0.01, 0.01)&quot;) # how granular to you want the theta sequence? length &lt;- 1e3 # simulate the data tibble(alpha = c(1, .01), beta = c(1, .01), model = factor(text, levels = text)) %&gt;% expand(nesting(alpha, beta, model), theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(density = dbeta(theta, shape1 = alpha, shape2 = beta)) %&gt;% # plot ggplot(aes(x = theta, y = density)) + geom_area(fill = kh[3]) + scale_x_continuous(expression(theta), expand = expansion(mult = c(0, 0)), breaks = 0:5 / 5, labels = c(&quot;0&quot;, &quot;.2&quot;, &quot;.4&quot;, &quot;.6&quot;, &quot;.8&quot;, &quot;1&quot;), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;We have two anything’s-possible models!&quot;, subtitle = &quot;These y-axes are on the same scale.&quot;) + facet_wrap(~ model) ## Warning: Removed 2 rows containing non-finite values (`stat_align()`). Before we can complete the analyses of this subsection, we’ll need to define our version of Kruschke’s HDIofICDF function(), hdi_of_icdf(). Like we’ve done in previous chapters, here we mildly reformat the function. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } And here we’ll make a custom variant to be more useful within the context of the map2() function. hdi_of_qbeta &lt;- function(shape1, shape2) { hdi_of_icdf(name = qbeta, shape1 = shape1, shape2 = shape2) %&gt;% data.frame() %&gt;% mutate(level = c(&quot;ll&quot;, &quot;ul&quot;)) %&gt;% spread(key = level, value = &quot;.&quot;) } Recall that when we combine a \\(\\operatorname{Beta}(\\theta | \\alpha, \\beta)\\) prior with the results of a Bernoulli likelihood, we get a posterior defined by \\(\\operatorname{Beta}(\\theta | z + \\alpha, N - z + \\beta)\\). d &lt;- tibble(model = c(&quot;Uniform&quot;, &quot;Haldane&quot;), prior_a = c(1, .01), prior_b = c(1, .01)) %&gt;% mutate(posterior_a = z + prior_a, posterior_b = n - z + prior_b) d ## # A tibble: 2 × 5 ## model prior_a prior_b posterior_a posterior_b ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Uniform 1 1 66 36 ## 2 Haldane 0.01 0.01 65.0 35.0 Now we’ll use our custom hdi_of_qbeta() to compute the HDIs. ( d &lt;- d %&gt;% mutate(levels = map2(posterior_a, posterior_b, hdi_of_qbeta)) %&gt;% unnest(levels) ) ## # A tibble: 2 × 7 ## model prior_a prior_b posterior_a posterior_b ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Uniform 1 1 66 36 0.554 0.738 ## 2 Haldane 0.01 0.01 65.0 35.0 0.556 0.742 Let’s compare those HDIs in a plot. d %&gt;% ggplot(aes(x = ll, xend = ul, y = model, yend = model)) + geom_segment(size = 1, color = kh[2]) + scale_x_continuous(expression(theta), expand = expansion(mult = c(0, 0)), breaks = 0:5 / 5, labels = c(&quot;0&quot;, &quot;.2&quot;, &quot;.4&quot;, &quot;.6&quot;, &quot;.8&quot;, &quot;1&quot;), limits = c(0, 1)) + labs(subtitle = &quot;Those two sets of HDIs are quite similar.\\nIt almost seems silly their respective BFs\\nare so different.&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) “The HDIs are virtually identical. In particular, for either prior, the posterior distribution rules out \\(\\theta = 0.5\\), which is to say that the must-be-fair hypothesis is not among the credible values” (p. 294). 10.6.1 Priors of different models should be equally informed. “We have established that seemingly innocuous changes in the vagueness of a vague prior can dramatically change a model’s marginal likelihood, and hence its Bayes factor in comparison with other models. What can be done to ameliorate the problem” (p. 294)? Kruschke posed one method might be taking a small representative portion of the data in hand and use them to make an empirically-based prior for the remaining set of data. From our previous example, “suppose that the \\(10\\%\\) subset has \\(6\\) heads in \\(10\\) flips, so the remaining \\(90\\%\\) of the data has \\(z = 65 − 6\\) and \\(N = 100 − 10\\)” (p. 294). Here are the new Bayes factors based on that method. z &lt;- 65 - 6 n &lt;- 100 - 10 # Peaked vs Uniform p_d(z, n, a = 500 + 6, b = 500 + 10 - 6) / p_d(z, n, a = 1 + 6, b = 1 + 10 - 6) ## [1] 0.05570509 # Peaked vs Haldane p_d(z, n, a = 500 + 6, b = 500 + 10 - 6) / p_d(z, n, a = .01 + 6, b = .01 + 10 - 6) ## [1] 0.05748123 Now the two Bayes Factors are nearly the same. It’s not in the text, but let’s compare these three models using brms, information criteria, model weights, model averaging, and posterior predictive checks. First, we’ll save the \\(z\\) and \\(N\\) information as a tibble with a series of 0’s and 1’s. z &lt;- 65 n &lt;- 100 trial_data &lt;- tibble(y = rep(0:1, times = c(n - z, z))) glimpse(trial_data) ## Rows: 100 ## Columns: 1 ## $ y &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… Next, fit the three models with brms::brm(). fit10.6 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, prior(beta(500, 500), class = Intercept, lb = 0, ub = 1), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, file = &quot;fits/fit10.06&quot;) fit10.7 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, # Uniform prior(beta(1, 1), class = Intercept, lb = 0, ub = 1), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, file = &quot;fits/fit10.07&quot;) fit10.8 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, # Haldane prior(beta(0.01, 0.01), class = Intercept, lb = 0, ub = 1), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, file = &quot;fits/fit10.08&quot;) Compare the models by the LOO. fit10.6 &lt;- add_criterion(fit10.6, criterion = &quot;loo&quot;) fit10.7 &lt;- add_criterion(fit10.7, criterion = &quot;loo&quot;) fit10.8 &lt;- add_criterion(fit10.8, criterion = &quot;loo&quot;) loo_compare(fit10.6, fit10.7, fit10.8) ## elpd_diff se_diff ## fit10.7 0.0 0.0 ## fit10.8 0.0 0.1 ## fit10.6 -2.9 2.7 Based on the LOO comparisons, none of the three models was a clear favorite. Although both versions of the anything’s-possible model (i.e., fit10.7 and fit10.8) had lower numeric estimates than the must-be-fair model (i.e., fit10.6), the standard errors on the difference scores were the same magnitude as the difference estimates themselves. As for comparing the two variants of the anything’s-possible model directly, their LOO estimates were almost indistinguishable. Now let’s see what happens when we compute their model weights. Here we’ll contrast the LOO weights with the stacking weights. mw &lt;- model_weights(fit10.6, fit10.7, fit10.8, weights = &quot;stacking&quot;) mw %&gt;% round(digits = 2) ## fit10.6 fit10.7 fit10.8 ## 0.11 0.88 0.01 model_weights(fit10.6, fit10.7, fit10.8, weights = &quot;loo&quot;) %&gt;% round(digits = 2) ## fit10.6 fit10.7 fit10.8 ## 0.03 0.49 0.48 The evidence varied a bit by the specific weighting scheme. Across both, the model with the uniform prior (fit10.7) did arguably the best, but the model with the Haldane prior (fit10.8) was clearly in the running. Overall, the evidence for one versus another was weak. Like we did earlier with fit10.1 and fit10.2, we can use the pp_average() function to compute the stacking weighted posterior for \\(\\theta\\). pp_average(fit10.6, fit10.7, fit10.8, newdata = nd, weights = mw, method = &quot;fitted&quot;, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = V1, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = c(.95, .5), fill = kh[6], slab_color = kh[5], color = kh[2], breaks = 40, slab_size = .25, outline_bars = T) + scale_x_continuous(expression(italic(p)(theta*&quot;|&quot;*italic(D))), expand = c(0, 0), breaks = 0:5 / 5, labels = c(&quot;0&quot;, &quot;.2&quot;, &quot;.4&quot;, &quot;.6&quot;, &quot;.8&quot;, &quot;1&quot;), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0.01, 0.05))) + labs(subtitle = &quot;The posterior for the probability, given the weighted\\ncombination of fit10.6, fit10.7, and fit10.8&quot;) Did you notice the weights = mw argument, there? From the pp_average.brmsfit section of the brms reference manual (Bürkner, 2022d, p. 177), we read “weights may also be be a numeric vector of pre-specified weights.” Since we saved the results of model_weights() as an object mw, we were able to capitalize on that feature. If you leave out that argument, you’ll have to wait a bit for brms to compute those weights again from scratch. Just for the sake of practice, we can also compare the models with separate posterior predictive checks using pp_check(). p1 &lt;- pp_check(fit10.6, type = &quot;bars&quot;, ndraws = 1e3) + ggtitle(&quot;fit10.6&quot;, subtitle = expression(&quot;Beta&quot;*(500*&quot;, &quot;*500))) p2 &lt;- pp_check(fit10.7, type = &quot;bars&quot;, ndraws = 1e3) + ggtitle(&quot;fit10.7&quot;, subtitle = expression(&quot;Beta&quot;*(1*&quot;, &quot;*1))) p3 &lt;- pp_check(fit10.8, type = &quot;bars&quot;, ndraws = 1e3) + ggtitle(&quot;fit10.8&quot;, subtitle = expression(&quot;Beta&quot;*(0.01*&quot;, &quot;*0.01))) ((p1 + p2 + p3) &amp; scale_x_continuous(breaks = 0:1) &amp; scale_y_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, 80))) + plot_layout(guides = &#39;collect&#39;) Instead of the default 10, this time we used 1,000 posterior simulations from each fit, which we summarized with dot and error bars. This method did a great job showing how little fit10.6 learned from the data. Another nice thing about this method is it reveals how similar the results are between fit10.7 and fit10.8, the two alternate versions of the anything’s-possible model. Also, did you notice how we used limits = c(0, 80) when combining the plots with patchwork? Holding the scale of the \\(y\\)-axis constant makes it easier to compare results across plots. 10.7 Bonus: There’s danger ahead If you’re new to model comparison with Bayes factors, information criteria, model stacking and so on, you should know these methods are still subject to spirited debate amongst scholars. For a recent example, see Gronau and Wagenmakers’ (2019) Limitations of Bayesian leave-one-out cross-validation for model selection, which criticized the LOO. Their paper was commented on by D. J. Navarro (2019), Chandramouli &amp; Shiffrin (2019), and Vehtari et al. (2019). You can find Gronau and Wagenmakers’ (2019) rejoinder here. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_3.0.2 bayesplot_1.9.0 brms_2.18.0 Rcpp_1.0.9 ## [5] ggforce_0.4.1 patchwork_1.1.2 lisa_0.1.2 forcats_0.5.1 ## [9] stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 ## [13] tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 ## [4] igraph_1.3.4 svUnit_1.0.6 splines_4.2.0 ## [7] crosstalk_1.2.0 TH.data_1.1-1 rstantools_2.2.0 ## [10] inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 ## [16] googlesheets4_1.0.1 tzdb_0.3.0 modelr_0.1.8 ## [19] RcppParallel_5.1.5 matrixStats_0.62.0 vroom_1.5.7 ## [22] xts_0.12.1 sandwich_3.0-2 prettyunits_1.1.1 ## [25] colorspace_2.0-3 rvest_1.0.2 ggdist_3.2.0 ## [28] haven_2.5.1 xfun_0.35 callr_3.7.3 ## [31] crayon_1.5.2 jsonlite_1.8.3 lme4_1.1-31 ## [34] survival_3.4-0 zoo_1.8-10 glue_1.6.2 ## [37] polyclip_1.10-0 gtable_0.3.1 gargle_1.2.0 ## [40] emmeans_1.8.0 distributional_0.3.1 pkgbuild_1.3.1 ## [43] rstan_2.21.7 abind_1.4-5 scales_1.2.1 ## [46] mvtnorm_1.1-3 DBI_1.1.3 miniUI_0.1.1.1 ## [49] xtable_1.8-4 HDInterval_0.2.2 bit_4.0.4 ## [52] stats4_4.2.0 StanHeaders_2.21.0-7 DT_0.24 ## [55] htmlwidgets_1.5.4 httr_1.4.4 threejs_0.3.3 ## [58] arrayhelpers_1.1-0 posterior_1.3.1 ellipsis_0.3.2 ## [61] pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 ## [64] sass_0.4.2 dbplyr_2.2.1 utf8_1.2.2 ## [67] tidyselect_1.1.2 labeling_0.4.2 rlang_1.0.6 ## [70] reshape2_1.4.4 later_1.3.0 munsell_0.5.0 ## [73] cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [76] cli_3.5.0 generics_0.1.3 broom_1.0.1 ## [79] ggridges_0.5.3 evaluate_0.18 fastmap_1.1.0 ## [82] bit64_4.0.5 processx_3.8.0 knitr_1.40 ## [85] fs_1.5.2 nlme_3.1-159 mime_0.12 ## [88] projpred_2.2.1 xml2_1.3.3 compiler_4.2.0 ## [91] shinythemes_1.2.0 rstudioapi_0.13 gamm4_0.2-6 ## [94] reprex_2.0.2 tweenr_2.0.0 bslib_0.4.0 ## [97] stringi_1.7.8 highr_0.9 ps_1.7.2 ## [100] Brobdingnag_1.2-8 lattice_0.20-45 Matrix_1.4-1 ## [103] nloptr_2.0.3 markdown_1.1 shinyjs_2.1.0 ## [106] tensorA_0.36.2 vctrs_0.5.1 pillar_1.8.1 ## [109] lifecycle_1.0.3 jquerylib_0.1.4 bridgesampling_1.1-2 ## [112] estimability_1.4.1 httpuv_1.6.5 R6_2.5.1 ## [115] bookdown_0.28 promises_1.2.0.1 gridExtra_2.3 ## [118] codetools_0.2-18 boot_1.3-28 colourpicker_1.1.1 ## [121] MASS_7.3-58.1 gtools_3.9.3 assertthat_0.2.1 ## [124] withr_2.5.0 shinystan_2.6.0 multcomp_1.4-20 ## [127] mgcv_1.8-40 parallel_4.2.0 hms_1.1.1 ## [130] grid_4.2.0 minqa_1.2.5 coda_0.19-4 ## [133] rmarkdown_2.16 googledrive_2.0.0 shiny_1.7.2 ## [136] lubridate_1.8.0 base64enc_0.1-3 dygraphs_1.1.1.6 Footnote References Bürkner, P.-C. (2022d). brms reference manual, Version 2.17.0. https://CRAN.R-project.org/package=brms/brms.pdf Chandramouli, S. H., &amp; Shiffrin, R. M. (2019). Commentary on Gronau and Wagenmakers. Computational Brain &amp; Behavior, 2(1), 12–21. https://doi.org/10.1007/s42113-018-0017-1 Gabry, J. (2022). Graphical posterior predictive checks using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/graphical-ppcs.html Gronau, Q. F., &amp; Wagenmakers, E.-J. (2019). Limitations of Bayesian leave-one-out cross-validation for model selection. Computational Brain &amp; Behavior, 2(1), 1–11. https://doi.org/10.1007/s42113-018-0011-7 Gronau, Q. F., &amp; Wagenmakers, E.-J. (2019). Rejoinder: More limitations of Bayesian leave-one-out cross-validation. Computational Brain &amp; Behavior, 2(1), 35–47. https://doi.org/10.1007/s42113-018-0022-4 Hokusai, K. (1820–1831). The great wave off Kanagawa. Jeffreys, H. (1961). Theory of probability. Oxford University Press. https://global.oup.com/academic/product/theory-of-probability-9780198503682?cc=us&amp;lang=en&amp; Kass, R. E., &amp; Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90(430), 773–795. https://www.stat.washington.edu/raftery/Research/PDF/kass1995.pdf Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Littlefield, T. (2020). lisa: Color palettes from color lisa [Manual]. https://CRAN.R-project.org/package=lisa Liu, C. C., &amp; Aitkin, M. (2008). Bayes factors: Prior sensitivity and model generalizability. Journal of Mathematical Psychology, 52(6), 362–375. https://doi.org/10.1016/j.jmp.2008.03.002 Navarro, D. J. (2019). Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection. Computational Brain &amp; Behavior, 2(1), 28–34. https://doi.org/10.1007/s42113-018-0019-z Pedersen, Thomas Lin. (n.d.). Draw polygons with expansion/contraction and/or rounded corners geom_shape. Retrieved September 11, 2020, from https://ggforce.data-imaginist.com/reference/geom_shape.html Pedersen, Thomas Lin. (2021). ggforce: Accelerating ’ggplot2’ [Manual]. https://CRAN.R-project.org/package=ggforce Vanpaemel, W. (2010). Prior sensitivity in theory testing: An apologia for the Bayes factor. Journal of Mathematical Psychology, 54(6), 491–498. https://doi.org/10.1016/j.jmp.2010.07.003 Vehtari, A., &amp; Gabry, J. (2022a). Using the loo Package (Version \\(&gt;\\)= 2.0.0). https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html Vehtari, A., &amp; Gabry, J. (2022b, March 23). Bayesian stacking and pseudo-BMA weights using the loo package. https://CRAN.R-project.org/package=loo/vignettes/loo2-weights.html Vehtari, A., Gabry, J., Magnusson, M., Yao, Y., &amp; Gelman, A. (2022). loo: Efficient leave-one-out cross-validation and WAIC for bayesian models. https://CRAN.R-project.org/package=loo/ Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4 Vehtari, A., Simpson, D. P., Yao, Y., &amp; Gelman, A. (2019). Limitations of “Limitations of Bayesian leave-one-out cross-validation for model selection.” Computational Brain &amp; Behavior, 2(1), 22–27. https://doi.org/10.1007/s42113-018-0020-6 Wetzels, R., Matzke, D., Lee, M. D., Rouder, J. N., Iverson, G. J., &amp; Wagenmakers, E.-J. (2011). Statistical evidence in experimental psychology: An empirical comparison using 855 t tests. Perspectives on Psychological Science, 6(3), 291–298. https://doi.org/10.1177/1745691611406923 Wilke, C. O. (2020). Themes. https://wilkelab.org/cowplot/articles/themes.html Wilke, C. O. (2019). Fundamentals of data visualization. https://clauswilke.com/dataviz/ Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A. (2018). Using stacking to average Bayesian predictive distributions (with discussion). Bayesian Analysis, 13(3), 917–1007. https://doi.org/10.1214/17-BA1091 I point this out because in previous versions of this book, I accidentally flipped this order. Thankfully, Omid Ghasemi caught the mistake and kindly pointed it out in GitHub issue #34.↩︎ "],["null-hypothesis-significance-testing.html", "11 Null Hypothesis Significance Testing 11.1 Paved with good intentions 11.2 Prior knowledge 11.3 Confidence interval and highest density interval 11.4 Multiple comparisons 11.5 What a sampling distribution is good for Session info Footnote", " 11 Null Hypothesis Significance Testing It’s worth repeating a couple paragraphs from page 298 (Kruschke, 2015, emphasis in the original): The logic of conventional NHST goes like this. Suppose the coin is fair (i.e., \\(\\theta = 0.50\\)). Then, when we flip the coin, we expect that about half the flips should come up heads. If the actual number of heads is far greater or fewer than half the flips, then we should reject the hypothesis that the coin is fair. To make this reasoning precise, we need to figure out the exact probabilities of all possible outcomes, which in turn can be used to figure out the probability of getting an outcome as extreme as (or more extreme than) the actually observed outcome. This probability, of getting an outcome from the null hypothesis that is as extreme as (or more extreme than) the actual outcome, is called a “\\(p\\) value.” If the \\(p\\) value is very small, say less than \\(5\\%\\), then we decide to reject the null hypothesis. Notice that this reasoning depends on defining a space of all possible outcomes from the null hypothesis, because we have to compute the probabilities of each outcome relative to the space of all possible outcomes. The space of all possible outcomes is based on how we intend to collect data. For example, was the intention to flip the coin exactly \\(N\\) times? In that case, the space of possible outcomes contains all sequences of exactly \\(N\\) flips. Was the intention to flip until the \\(z\\)th head appeared? In that case, the space of possible outcomes contains all sequences for which the \\(z\\)th head appears on the last flip. Was the intention to flip for a fixed duration? In that case, the space of possible outcomes contains all combinations of \\(N\\) and \\(z\\) that could be obtained in that fixed duration. Thus, a more explicit definition of a \\(p\\) value is the probability of getting a sample outcome from the hypothesized population that is as extreme as or more extreme than the actual outcome when using the intended sampling and testing procedures. 11.1 Paved with good intentions Kruschke started off this section with a random sequence of 7 heads (H) and 17 tails (T). This is a little silly, but I wanted to challenge myself to randomly generate a series of 24 H and T characters for which there were 7 Hs. The base R sample() function gets us part of the way there. sample(c(&quot;H&quot;, &quot;T&quot;), size = 24, replace = T) ## [1] &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; ## [20] &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; I wanted the solution to be reproducible, which required I find the appropriate seed for set.seed(). To do that, I made a custom h_counter() function into which I could input an arbitrary seed value and retrieve the count of H. I then fed a sequence of integers into h_counter() and filtered the output to find which seed produces the desirable outcome. library(tidyverse) h_counter &lt;- function(seed) { set.seed(seed) coins &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), size = 24, replace = T) length(which(coins == &quot;H&quot;)) } coins &lt;- tibble(seed = 1:200) %&gt;% mutate(n_heads = map_dbl(seed, h_counter)) coins %&gt;% filter(n_heads == 7) ## # A tibble: 2 × 2 ## seed n_heads ## &lt;int&gt; &lt;dbl&gt; ## 1 115 7 ## 2 143 7 Looks like set.seed(115) will work. set.seed(115) sample(c(&quot;H&quot;, &quot;T&quot;), size = 24, replace = T) ## [1] &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; ## [20] &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; The sequence isn’t in the exact order as the data from page 300, but they do have the crucial ratio of heads to tails. We’ll plot the sequence in a moment. But before we do, let’s talk theme and color scheme. In the last chapter, we experimented with making global alterations to the default ggplot2 theme using the theme_set() function and based our new color settings on an iconic woodblock print of the sea. We’ll extend that nautical theme in this chapter by basing our color scheme on RIFT SCULL by contemporary artist, James Jean (2009). We can get a prearranged color palette based on RIFT SCULL from the lisa package. library(lisa) plot(lisa_palette(&quot;JamesJean&quot;)) Use lisa_palette(\"JamesJean\") to customize our global settings. # change the default settings theme_set( theme_grey() + theme(text = element_text(color = &quot;lemonchiffon&quot;), axis.text = element_text(color = &quot;lemonchiffon&quot;), axis.ticks = element_line(color = &quot;lemonchiffon&quot;), legend.background = element_blank(), legend.box.background = element_blank(), legend.key = element_rect(fill = lisa_palette(&quot;JamesJean&quot;)[4]), panel.background = element_rect(fill = lisa_palette(&quot;JamesJean&quot;)[4], color = &quot;lemonchiffon&quot;), panel.grid = element_blank(), plot.background = element_rect(fill = lisa_palette(&quot;JamesJean&quot;)[4], color = lisa_palette(&quot;JamesJean&quot;)[4])) ) # save the five colors with a simple name jj &lt;- lisa_palette(&quot;JamesJean&quot;) jj ## * Work: RIFT SCULL ## * Author: JamesJean ## * Colors: #51394E #F6DE7D #C8AF8A #658385 #B04838 You can undo the above with ggplot2::theme_set(ggplot2::theme_grey()). Here’s the bar plot of our 24-flip sequence. set.seed(115) tibble(flips = sample(c(&quot;H&quot;, &quot;T&quot;), size = 24, replace = T)) %&gt;% ggplot(aes(x = flips)) + geom_hline(yintercept = c(7, 17), color = jj[2]) + geom_bar(fill = jj[1]) + scale_y_continuous(breaks = c(0, 7, 17), expand = expansion(mult = c(0, 0.05))) It seems that there were fewer heads than what we would expect from the hypothesis of fairness. We would like to derive the probability of getting a proportion of heads that is \\(7/24\\) or smaller if the null hypothesis is true. (p. 300) 11.1.1 Definition of \\(p\\) value. In summary, the likelihood function defines the probability for a single measurement, and the intended sampling process defines the cloud of possible sample outcomes. The null hypothesis is the likelihood function with its specific value for parameter \\(\\theta\\), and the cloud of possible samples is defined by the stopping and testing intentions, denoted \\(I\\). Each imaginary sample generated from the null hypothesis is summarized by a descriptive statistic, denoted \\(D_{\\theta, I}\\). In the case of a sample of coin flips, the descriptive summary statistic is \\(z / N\\) , the proportion of heads in the sample. Now, imagine generating infinitely many samples from the null hypothesis using stopping and testing intention \\(I\\) ; this creates a cloud of possible summary values \\(D_{\\theta, I}\\), each of which has a particular probability. The probability distribution over the cloud of possibilities is the sampling distribution: \\(p (D_{\\theta, I} | \\theta, I )\\). To compute the \\(p\\) value, we want to know how much of that cloud is as extreme as, or more extreme than, the actually observed outcome. To define “extremeness” we must determine the typical value of \\(D_{\\theta, I}\\), which is usually defined as the expected value, \\(E [D_{\\theta, I}]\\) (recall Equations 4.5 and 4.6). This typical value is the center of the cloud of possibilities. An outcome is more “extreme” when it is farther away from the central tendency. The \\(p\\) value of the actual outcome is the probability of getting a hypothetical outcome that is as or more extreme. Formally, we can express this as \\[p \\text{ value} = p (D_{\\theta, I} \\succcurlyeq D_\\text{actual} | \\theta, I)\\] where “\\(\\succcurlyeq\\)” in this context means “as extreme as or more extreme than, relative to the expected value from the hypothesis.” Most introductory applied statistics textbooks suppress the sampling intention \\(I\\) from the definition, but precedents for making the sampling intention explicit can be found in Wagenmakers (2007, Online Supplement A) and additional references cited therein. (p. 301) Here’s how one might make a version of the grids Kruschke displayed in Figure 11.2. library(patchwork) level &lt;- c(0:8, &quot;...&quot;) d &lt;- crossing(N = factor(level, levels = level), z = factor(level, levels = level)) %&gt;% mutate( label = case_when( N == &quot;...&quot; ~ &quot;...&quot;, z == &quot;...&quot; ~ &quot;...&quot;, as.double(z) &gt; as.double(N) ~ &quot;-&quot;, as.double(z) &lt;= as.double(N) ~ &quot;&quot; ), fill = case_when( N == &quot;5&quot; ~ 1, N %in% c(&quot;4&quot;, &quot;6&quot;) ~ 2, N %in% c(&quot;3&quot;, &quot;7&quot;) ~ 3, N %in% c(&quot;1&quot;, &quot;2&quot;, &quot;8&quot;, &quot;...&quot;) ~ 4 )) %&gt;% mutate(z = fct_rev(z)) %&gt;% filter(N != &quot;0&quot;) p1 &lt;- d %&gt;% ggplot(aes(N, z, label = label, fill = N == &quot;5&quot;)) + geom_tile(color = alpha(jj[2], .25)) + geom_text(color = jj[2]) + scale_fill_manual(values = jj[c(4, 1)], breaks = NULL) + scale_x_discrete(position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + theme(axis.ticks = element_blank()) p2 &lt;- d %&gt;% ggplot(aes(N, z, label = label, fill = z == &quot;4&quot;)) + geom_tile(color = alpha(jj[2], .25)) + geom_text(color = jj[2]) + scale_fill_manual(values = jj[c(4, 1)], breaks = NULL) + scale_x_discrete(position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + theme(axis.ticks = element_blank()) p3 &lt;- d %&gt;% ggplot(aes(N, z, label = label, fill = fill)) + geom_tile(color = alpha(jj[2], .25)) + geom_text(color = jj[2]) + scale_fill_gradient(low = jj[1], high = jj[4], breaks = NULL) + scale_x_discrete(position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + theme(axis.ticks = element_blank()) # combine and plot p1 | p2 | p3 11.1.2 With intention to fix \\(N\\). In this section, “the space of possible outcomes is restricted to combinations of \\(z\\) and \\(N\\) for which \\(N\\) is fixed at \\(N = 24\\)” (p. 302). What is the probability of getting a particular number of heads when \\(N\\) is fixed? The answer is provided by the binomial probability distribution, which states that the probability of getting \\(z\\) heads out of \\(N\\) flips is \\[ p(z | N, \\theta) = \\begin{pmatrix} N \\\\ z \\end{pmatrix} \\theta^z (1 - \\theta)^{N - z}\\] where the notation \\(\\tbinom{N}{z}\\) [is a shorthand notation defined in more detail in the text. It has to do with factorials and, getting more to the point,] the number of ways of allocating \\(z\\) heads among \\(N\\) flips, without duplicate counting of equivalent allocations, is \\(N !/[(N − z)!z!]\\). This factor is also called the number of ways of choosing \\(z\\) items from \\(N\\) possibilities, or “\\(N\\) choose \\(z\\)” for short, and is denoted \\(\\tbinom{N}{z}\\). Thus, the overall probability of getting \\(z\\) heads in \\(N\\) flips is the probability of any particular sequence of \\(z\\) heads in \\(N\\) flips times the number of ways of choosing \\(z\\) slots from among the \\(N\\) possible flips. (p. 303, emphasis in the original) To do factorials in R, use the factorial() function. E.g., we can use the formula \\(N! / [(N − z)!z!]\\) like so: n &lt;- 24 z &lt;- 7 factorial(n) / (factorial(n - z) * factorial(z)) ## [1] 346104 That value, recall, is a count, “the number of ways of allocating \\(z\\) heads among \\(N\\) flips, without duplicate counting of equivalent allocations” (p. 303). That formula’s a little cumbersome to work with. We can make our programming lives easier by wrapping it into a function. n_choose_z &lt;- function(n, z) { factorial(n) / (factorial(n - z) * factorial(z)) } Now we can employ our n_choose_z() function to help make the data we’ll use for Figure 11.3.b. Here are the data. flips &lt;- tibble(z = 0:24) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), # this just rescales `n_choose_z` `Sample Proportion z/N` = z / n) head(flips, n = 10) ## # A tibble: 10 × 4 ## z n_choose_z `p(z/N)` `Sample Proportion z/N` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0.0000000596 0 ## 2 1 24 0.00000143 0.0417 ## 3 2 276 0.0000165 0.0833 ## 4 3 2024 0.000121 0.125 ## 5 4 10626 0.000633 0.167 ## 6 5 42504 0.00253 0.208 ## 7 6 134596 0.00802 0.25 ## 8 7 346104 0.0206 0.292 ## 9 8 735471 0.0438 0.333 ## 10 9 1307504 0.0779 0.375 Instead of our custom n_choose_z() function, we could have also used the base R choose function. Here’s a quick comparison. tibble(z = 0:5) %&gt;% mutate(n_choose_z = n_choose_z(n, z), choose = choose(n, z)) ## # A tibble: 6 × 3 ## z n_choose_z choose ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 1 ## 2 1 24 24 ## 3 2 276 276 ## 4 3 2024 2024 ## 5 4 10626 10626 ## 6 5 42504 42504 Now here’s the histogram of that sampling distribution. flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(z/N)`, fill = z &lt;= 7, color = z &lt;= 7)) + geom_col(width = .025) + scale_fill_manual(values = jj[1:2]) + scale_color_manual(values = jj[1:2]) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Implied Sampling Distribution&quot;) We can get the one-sided \\(p\\)-value with a quick filter() and summarise(). flips %&gt;% filter(z &lt;= 7) %&gt;% summarise(p_value = sum(`p(z/N)`)) ## # A tibble: 1 × 1 ## p_value ## &lt;dbl&gt; ## 1 0.0320 Here’s Figure 11.3.a. tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = .5) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col(fill = jj[1]) + scale_y_continuous(expand = expansion(mult = c(0, 0)), limits = c(0, 1)) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.5&quot;)) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) As Kruschke wrote on page 304, “It is important to understand that the sampling distribution is a probability distribution over samples of data, and is not a probability distribution over parameter values.” Here is the probability “of getting exactly \\(z = 7\\) heads in \\(N = 24\\) flips” (p. 304, emphasis in the original): flips %&gt;% filter(z == 7) ## # A tibble: 1 × 4 ## z n_choose_z `p(z/N)` `Sample Proportion z/N` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 346104 0.0206 0.292 It was already sitting there in our p(z/N) column. Well okay, fine, you do have to multiply it by 100 to convert it to a percentage the way Kruschke presented it at the bottom of page 304. flips %&gt;% filter(z == 7) %&gt;% summarise(`probability in a percentage metric` = (100 * `p(z/N)`) %&gt;% round(digits = 3)) ## # A tibble: 1 × 1 ## `probability in a percentage metric` ## &lt;dbl&gt; ## 1 2.06 Kruschke then pointed out that this is a one-tailed \\(p\\)-value. We often reject or fail to reject the null hypothesis based on two-sided \\(p\\)-values. In practice, we can convert a one-sided \\(p\\)-value to a two-sided \\(p\\)-value by multiplying it by 2. Here’s what that looks like in this case. flips %&gt;% filter(z &lt;= 7) %&gt;% summarise(`one-sided p-value` = sum(`p(z/N)`), `two-sided p-value` = 2 * sum(`p(z/N)`)) ## # A tibble: 1 × 2 ## `one-sided p-value` `two-sided p-value` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0320 0.0639 Here’s the conclusion for our particular case. The actual observation was \\(z/N = 7/24\\). The one-tailed probability is \\(p = 0.032\\), which was computed from Equation 11.4, and is shown in Figure 11.3. Because the \\(p\\) value is not less than \\(2.5\\%\\), we do not reject the null hypothesis that \\(\\theta = 0.5\\). In NHST parlance, we would say that the result “has failed to reach significance.” This does not mean we accept the null hypothesis; we merely suspend judgment regarding rejection of this particular hypothesis. Notice that we have not determined any degree of belief in the hypothesis that \\(\\theta = 0.5\\). The hypothesis might be true or might be false; we suspend judgment. (p. 305, emphasis in the original) 11.1.3 With intention to fix \\(z\\). In this subsection, “\\(z\\) is fixed in advance and \\(N\\) is the random variable. We don’t talk about the probability of getting \\(z\\) heads out of \\(N\\) flips, we instead talk about the probability of taking \\(N\\) flips to get \\(z\\) heads” (p. 306). This time we’re interested in What is the probability of taking \\(N\\) flips to get \\(z\\) heads? To answer this question, consider this: We know that the \\(N\\)th flip is the \\(z\\)th head, because that is what caused flipping to stop. Therefore the previous \\(N - 1\\) flips had \\(z - 1\\) heads in some random sequence. The probability of getting \\(z - 1\\) heads in \\(N - 1\\) flips is \\(\\tbinom{N - 1}{z - 1} \\theta^{z-1} (1 - \\theta)^{N - z}\\). The probability that the last flip comes up heads is \\(\\theta\\). Therefore, the probability that it takes \\(N\\) flips to get \\(z\\) heads is \\[\\begin{align*} p(N | z, \\theta) &amp; = \\begin{pmatrix} N-1 \\\\ z-1 \\end{pmatrix} \\theta^{z-1} (1 - \\theta)^{N - z} \\cdot \\theta \\\\ &amp; = \\begin{pmatrix} N-1 \\\\ z-1 \\end{pmatrix} \\theta^z (1 - \\theta)^{N - z} \\\\ &amp; = \\frac{z}{N} \\begin{pmatrix} N \\\\ z \\end{pmatrix} \\theta^z (1 - \\theta)^{N - z} \\end{align*}\\] (This distribution is sometimes called the “negative binomial” 5 but that term sometimes refers to other formulations and can be confusing, so I will not use it here.) This is a sampling distribution, like the binomial distribution, because it specifies the relative probabilities of all the possible data outcomes for the hypothesized fixed value of \\(\\theta\\) and the intended stopping rule. (p. 306, emphasis added) With that formula in hand, here’s how to generate the data for Figure 11.4.b. theta &lt;- .5 # we have to stop somewhere. where should we stop? highest_n &lt;- 100 flips &lt;- tibble(n = 7:highest_n) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n, `p(N|z,theta)` = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) To keep things simple, we just went up to \\(N = 100\\). At the bottom of page 306, Kruschke described the probability “spikes” for various values of \\(N\\) when \\(z/N = 7/7\\), \\(z/N = 7/8\\), and \\(z/N = 7/9\\). We have those spike values in the Sample Proportion z/N column of our flips data. Here are those first three spikes. flips %&gt;% head(n = 3) ## # A tibble: 3 × 5 ## n n_choose_z `p(z/N)` `Sample Proportion z/N` `p(N|z,theta)` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 1 4.95e-12 1 0.00781 ## 2 8 8 3.96e-11 0.875 0.0273 ## 3 9 36 1.78e-10 0.778 0.0547 Those values correspond to the rightmost vertical lines in our Figure 11.4.b, below. flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`, fill = n &gt;= 24, color = n &gt;= 24)) + geom_col(width = .005) + scale_fill_manual(values = jj[1:2]) + scale_color_manual(values = jj[1:2]) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;) Our Figure 11.4.a is the same as Figure 11.3.a, above. I won’t repeat it, here. We got the formula for that last variable, p(N|z,theta), from Formula 11.6 on page 306. You’ll note how Kruschke continued to refer to it as \\(p(z|N)\\) in his Figure 11.4. It’s entirely opaque, to me, how \\(p(z|N) = p(N|z, \\theta)\\). I’m just going with it. Here’s the \\(p\\)-value, expressed two ways. flips %&gt;% filter(n &gt;= 24) %&gt;% summarise(`one-sided p-value` = sum(`p(N|z,theta)`) %&gt;% round(digits = 3), `two-sided p-value` = (2 * sum(`p(N|z,theta)`)) %&gt;% round(digits = 3)) ## # A tibble: 1 × 2 ## `one-sided p-value` `two-sided p-value` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.017 0.035 If you experiment a bit with the highest_n value from above, you’ll see that the exact value for the \\(p\\)-value is dependent on what \\(N\\) you go up to. Even though our data (\\(z = 2, N = 24\\)) is the same as in the last section, we came ended up with different \\(p\\)-values and different conclusions about the null hypothesis. Though we failed to reject the null in the last section, we rejected it here. As Kruschke warned us in the beginning of the chapter, NHST depends on sampling intentions. 11.1.4 With intention to fix duration. In this subsection, neither \\(N\\) nor \\(z\\) is fixed… The key to analyzing this scenario is specifying how various combinations of \\(z\\) and \\(N\\) can arise when sampling for a fixed duration. There is no single, uniquely “correct” specification, because there are many different real-world constraints on sampling through time. But one approach is to think of the sample size \\(N\\) as a random value. (p. 308). Here’s a glance at the Poisson distribution for which \\(\\lambda = 24\\). The mean is colored yellow. tibble(x = 1:50) %&gt;% mutate(y = dpois(x = x, lambda = 24)) %&gt;% ggplot(aes(x = x, y = y, fill = x == 24, color = x == 24)) + geom_col(width = .5) + scale_fill_manual(values = jj[1:2]) + scale_color_manual(values = jj[1:2]) + scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + theme(legend.position = &quot;none&quot;) In the note for Figure 11.5, Kruschke explained the “sample sizes are drawn randomly from a Poisson distribution with mean \\(\\lambda\\)”. Earlier in the prose he explained “\\(\\lambda\\) was set to \\(24\\) merely to match \\(N\\) and make the example most comparable to the preceding examples” (p. 309). To do such a simulation, one must choose how many draws to take from \\(\\operatorname{Poisson}(24)\\). Here’s an example where we take just one. set.seed(11) n_iter &lt;- 1 flips &lt;- tibble(iter = 1:n_iter, n = rpois(n_iter, lambda = 24)) %&gt;% mutate(z = map(n, ~seq(from = 0, to = ., by = 1))) %&gt;% unnest(z) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n) flips ## # A tibble: 22 × 6 ## iter n z n_choose_z `p(z/N)` `Sample Proportion z/N` ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 21 0 1 0.000000477 0 ## 2 1 21 1 21 0.0000100 0.0476 ## 3 1 21 2 210 0.000100 0.0952 ## 4 1 21 3 1330 0.000634 0.143 ## 5 1 21 4 5985 0.00285 0.190 ## 6 1 21 5 20349 0.00970 0.238 ## 7 1 21 6 54264 0.0259 0.286 ## 8 1 21 7 116280 0.0554 0.333 ## 9 1 21 8 203490 0.0970 0.381 ## 10 1 21 9 293930 0.140 0.429 ## # … with 12 more rows As indicated in our n column, by chance we drew a 21. We then computed the same values for all possible values of z, ranging from 0 to 21. But this doesn’t make for a very interesting plot, nor does it make for the same kind of plot Kruschke made in Figure 11.5.b. flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(z/N)`, fill = z &lt;= 7, color = z &lt;= 7)) + geom_col(width = .01) + scale_fill_manual(values = jj[1:2]) + scale_color_manual(values = jj[1:2]) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + theme(legend.position = &quot;none&quot;) Instead we have to take many draws to take from \\(\\operatorname{Poisson}(24)\\). Here’s what it looks like when we take 10,000. n_iter &lt;- 10000 set.seed(11) flips &lt;- tibble(iter = 1:n_iter, n = rpois(n_iter, lambda = 24)) %&gt;% mutate(z = map(n, ~seq(from = 0, to = ., by = 1))) %&gt;% unnest(z) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n) flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(z/N)`, fill = z &lt;= 7, color = z &lt;= 7)) + geom_col(width = .003, size = 1/15) + scale_fill_manual(values = jj[1:2]) + scale_color_manual(values = jj[1:2]) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + coord_cartesian(ylim = c(0, 0.03)) + theme(legend.position = &quot;none&quot;) I played around with the simulation a bit and this is about as good as I’ve gotten. If you have a solution that more faithfully reproduces what Kruschke did, please share your code in my GitHub issue #16. Here’s my attempt at the \\(p\\)-value. flips %&gt;% filter(z &lt;= 7) %&gt;% summarise(`one-sided p-value` = sum(`p(z/N)`)) ## # A tibble: 1 × 1 ## `one-sided p-value` ## &lt;dbl&gt; ## 1 0.000218 It’s unclear, to me, why it’s so much lower than the one Kruschke reported in the text. 11.1.5 With intention to make multiple tests. In the preceding sections we have seen that when a coin is flipped \\(N = 24\\) times and comes up \\(z = 7\\) heads, the \\(p\\) value can be \\(0.032\\) or \\(0.017\\) or \\(0.024\\) or other values. The change in \\(p\\) is caused by the dependence of the imaginary cloud of possibilities on the stopping intention. Typical NHST textbooks never mention the dependency of \\(p\\) values on the stopping intention, but they often do discuss the dependency of \\(p\\) values on the testing intention. In this section we will see how testing intentions affect the imaginary cloud of possibilities that determines the \\(p\\) value. (p. 310) Kruschke then went into an example of flipping two coins and how this would require we consider an overall false-positive rate based on proportions from either of the two coins. With respect to the probabilities of two independent events, I do recall that \\(p(A \\text{ or } B) = p(A) + p(B)\\). But sadly, I’m not sure how to incorporate this knowledge to reproduce the simulation for this section. If you know how to do the simulation properly, please share your code in my GitHub issue #17. In his footnote #4 on page 311, Kruschke reported there was a direct relation between the \\(p\\)-values in this section and in those from the simulation for Figure 11.3. They follow the equation \\[p_\\text{figure 11.6} = 1 - (1 - p_\\text{figure 11.3})^2.\\] Using code, we get this. 1 - (1 - 0.03195733)^2 ## [1] 0.06289339 11.1.6 Soul searching. Within the context of NHST, the solution is to establish the true intention of the researcher. This is the approach taken explicitly when applying corrections for multiple tests. The analyst determines what the truly intended tests are, and determines whether those testing intentions were honestly conceived a priori or post hoc (i.e., motivated only after seeing the data), and then computes the appropriate \\(p\\) value. The same approach should be taken for stopping rules: The data analyst should determine what the truly intended stopping rule was, and then compute the appropriate \\(p\\) value. Unfortunately, determining the true intentions can be difficult. Therefore, perhaps researchers who use \\(p\\) values to make decisions should be required to publicly pre-register their intended stopping rule and tests, before collecting the data. (p. 314, emphasis in the original) 11.1.7 Bayesian analysis. Happily for us, “the Bayesian interpretation of data does not depend on the covert sampling and testing intentions of the data collector” (p. 314). 11.2 Prior knowledge The main thing to note in this section is Kruschke changed the motivating example to one of flipping a flat-headed nail. Now we’re considering it “heads” when the nail lands on its head and “tails” when the nail lands such that its point is touching the ground. It’s still the case that \\(N = 24, z = 7\\). 11.2.1 NHST analysis. Nothing for us, here. 11.2.2 Bayesian analysis. If you recall from Chapter 6, we need a function to compute the Bernoulli likelihood. bernoulli_likelihood &lt;- function(theta, data) { n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } We now consider the analysis based on two priors. The first, \\(\\operatorname{Beta}(2, 20)\\), expresses the assumption the nail is biased towards “tails.” The second, \\(\\operatorname{Beta}(11, 11)\\), expresses the assumption the nail is fair. There are a handful of steps before we can use our bernoulli_likelihood() function to make the plot data. All these are repeats from Chapter 6. # we need these to compute the likelihood n &lt;- 24 z &lt;- 7 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) # (i.e., data) d_nail &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %&gt;% # (i.e., theta) mutate(Prior = dbeta(x = theta, shape1 = 2, shape2 = 20)) %&gt;% mutate(Likelihood = bernoulli_likelihood(theta = theta, # (i.e., p(D | theta)) data = trial_data)) %&gt;% mutate(evidence = sum(Likelihood * Prior)) %&gt;% # (i.e., p(D)) mutate(Posterior = Likelihood * Prior / evidence) # (i.e., p(theta | D)) glimpse(d_nail) ## Rows: 1,000 ## Columns: 5 ## $ theta &lt;dbl&gt; 0.000000000, 0.001001001, 0.002002002, 0.003003003, 0.00400… ## $ Prior &lt;dbl&gt; 0.0000000, 0.4124961, 0.8094267, 1.1912097, 1.5582533, 1.91… ## $ Likelihood &lt;dbl&gt; 0.000000e+00, 9.900280e-22, 1.245822e-19, 2.092598e-18, 1.5… ## $ evidence &lt;dbl&gt; 5.260882e-05, 5.260882e-05, 5.260882e-05, 5.260882e-05, 5.2… ## $ Posterior &lt;dbl&gt; 0.000000e+00, 7.762627e-18, 1.916792e-15, 4.738222e-14, 4.5… Here’s the left column of Figure 11.7. p1 &lt;- d_nail %&gt;% ggplot(aes(x = theta, y = Prior)) + geom_area(fill = jj[1]) + scale_x_continuous(expand = expansion(mult = 0)) + scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*2*&quot;, &quot;*20))) p2 &lt;- d_nail %&gt;% ggplot(aes(x = theta, y = Likelihood)) + geom_area(fill = jj[5]) + scale_x_continuous(expand = expansion(mult = 0)) + scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Likelihood (Bernoulli)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d_nail %&gt;% ggplot(aes(x = theta, y = Posterior)) + geom_area(fill = jj[2]) + scale_x_continuous(expand = expansion(mult = 0)) + scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*9*&quot;, &quot;*37))) (p1 / p2 / p3) &amp; theme(panel.grid = element_blank()) If we’d like the 95% HDIs, we’ll need to redefine the hdi_of_icdf() function. hdi_of_icdf &lt;- function(name = qbeta, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Note that this time we made the qbeta() function the default setting for the name argument. Here are the HDIs for the prior and posterior, above. hdi_of_icdf(shape1 = 2, shape2 = 20) ## [1] 0.002600585 0.208030932 hdi_of_icdf(shape1 = 2 + z, shape2 = 20 + (n - z)) ## [1] 0.08839668 0.31043265 To get the left column of Figure 11.7, we have to update the data with our new prior, \\(\\operatorname{Beta}(11, 11)\\). # here are the data based on our updated beta(11, 11) prior d_coin &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(Prior = dbeta(x = theta, shape1 = 11, shape2 = 11)) %&gt;% mutate(Likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% mutate(Posterior = Likelihood * Prior / sum(Likelihood * Prior)) # The updated plots: p1 &lt;- d_coin %&gt;% ggplot(aes(x = theta, y = Prior)) + geom_area(fill = jj[1]) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*11*&quot;, &quot;*11))) p2 &lt;- d_coin %&gt;% ggplot(aes(x = theta, y = Likelihood)) + geom_area(fill = jj[5]) + labs(title = &quot;Likelihood (Bernoulli)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d_coin %&gt;% ggplot(aes(x = theta, y = Posterior)) + geom_area(fill = jj[2]) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*18*&quot;, &quot;*28))) (p1 / p2 / p3) &amp; scale_x_continuous(expand = expansion(mult = 0)) &amp; scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) &amp; theme(panel.grid = element_blank()) Here are the corresponding HDIs for \\(\\operatorname{Beta}(11, 11)\\) and \\(\\operatorname{Beta}(11 + 7, 11 + 24 - 7)\\). hdi_of_icdf(shape1 = 11, shape2 = 11) ## [1] 0.2978068 0.7021932 hdi_of_icdf(shape1 = 11 + z, shape2 = 11 + (n - z)) ## [1] 0.2539378 0.5312685 11.2.2.1 Priors are overt and relevant. In this subsection’s opening paragraph, Kruschke opined: Prior beliefs are overt, explicitly debated, and founded on publicly accessible previous research. A Bayesian analyst might have personal priors that differ from what most people think, but if the analysis is supposed to convince an audience, then the analysis must use priors that the audience finds palatable. It is the job of the Bayesian analyst to make cogent arguments for the particular prior that is used. (p. 317) 11.3 Confidence interval and highest density interval This section defines CIs and provides examples. It shows that, while CIs ameliorate some of the problems of \\(p\\) values, ultimately CIs suffer the same problems as \\(p\\) values because CIs are defined in terms of \\(p\\) values. Bayesian posterior distributions, on the other hand, provide the needed information. (p. 318) 11.3.1 CI depends on intention. The primary goal of NHST is determining whether a particular “null” value of a parameter can be rejected. One can also ask what range of parameter values would not be rejected. This range of nonrejectable parameter values is called the CI. (There are different ways of defining an NHST CI; this one is conceptually the most general and coherent with NHST precepts.) The \\(95\\%\\) CI consists of all values of \\(\\theta\\) that would not be rejected by a (two-tailed) significance test that allows \\(5\\%\\) false alarms. (p. 318, emphasis in the original) Figure 11.8 depicts the sampling distributions for \\(\\theta = .126\\) (top row) and \\(\\theta = .511\\) (bottom row). Here’s the upper- and lower-left panels of Figure 11.8. p1 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .126, .126)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col(fill = jj[1]) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.126&quot;)) p2 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .511, .511)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col(fill = jj[1]) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.511&quot;)) (p1 / p2) &amp; scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) &amp; theme(title = element_text(size = 10), axis.ticks.x = element_blank(), panel.grid = element_blank()) Here are the corresponding upper- and lower-right panels. p1 &lt;- tibble(z = 0:24, y = dbinom(0:24, size = 24, prob = .126)) %&gt;% ggplot(aes(x = z/25, y = y, fill = z &gt;= 7)) + geom_col(width = .025) p2 &lt;- tibble(z = 0:24, y = dbinom(0:24, size = 24, prob = .511)) %&gt;% ggplot(aes(x = z / 24, y = y, fill = z &lt;= 7)) + geom_col(width = .025) (p1 / p2) &amp; labs(title = &quot;Implied Sampling Distribution&quot;, x = &quot;Sample Proportion z/N&quot;, y = &quot;p(z/N)&quot;) &amp; scale_fill_manual(values = jj[1:2]) &amp; scale_y_continuous(expand = expansion(mult = c(0, 0.05))) &amp; coord_cartesian(xlim = c(0, 1)) &amp; theme(legend.position = &quot;none&quot;) Figure 11.9 considers the sampling distributions for two hypothetical populations, \\(\\theta = .126\\) (top row) and \\(\\theta = .484\\) (bottom row). This time the assumption is \\(z\\) is fixed at 7. For this figure, we’ll continue to develop our patchwork skills. Before the big reveal, we’ll make the subplots in two phases. Here are the two on the left. p1 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .126, .126)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col(fill = jj[1]) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.126&quot;)) + scale_y_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, 1)) + theme(title = element_text(size = 10), axis.ticks.x = element_blank()) p2 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .484, .484)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col(fill = jj[1]) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.484&quot;)) + scale_y_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, 1)) + theme(title = element_text(size = 10), axis.ticks.x = element_blank()) Now make the two on the right. theta &lt;- .126 flips &lt;- tibble(n = 7:100) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n, `p(N|z,theta)` = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) p3 &lt;- flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`, fill = n &lt;= 24, color = n &lt;= 24)) + geom_col(width = .005) + scale_fill_manual(values = jj[1:2]) + scale_color_manual(values = jj[1:2]) + scale_x_continuous(breaks = 0:5 * 0.2, limits = c(0, 1)) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;) theta &lt;- .484 flips &lt;- tibble(n = 7:100) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n, `p(N|z,theta)` = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) p4 &lt;- flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`, fill = n &gt;= 24, color = n &gt;= 24)) + geom_col(width = .005) + scale_fill_manual(values = jj[1:2]) + scale_color_manual(values = jj[1:2]) + scale_x_continuous(breaks = 0:5 * 0.2, limits = c(0, 1)) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;) Here we arrange all four of the Figure 11.9 subplots. (p1 + p3 + p2 + p4) + plot_layout(widths = c(2, 4)) Figure 11.10 is a depiction of when an experimenter intended to stop when a fixed duration expired. This time the two rows are based on \\(\\theta = .135\\) (top row) and \\(\\theta = .497\\) (bottom row). We’ll follow the same general procedure from the last figure. Here are the subplots on the left. p1 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .135, .135)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col(fill = jj[1]) + scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.135&quot;)) + coord_cartesian(ylim = c(0, 1)) + theme(axis.ticks.x = element_blank()) p2 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .497, .497)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col(fill = jj[1]) + scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.497&quot;)) + coord_cartesian(ylim = c(0, 1)) + theme(axis.ticks.x = element_blank()) Like with Figure 11.5.b, my attempts for the right panels of Figure 11.10 just aren’t quite right. If you understand where I’m going wrong with the simulation, please share your code in my GitHub issue #18. z_maker &lt;- function(i) { set.seed(i) n &lt;- rpois(n = 1, lambda = 24) seq(from = 0, to = n, by = 1) } theta &lt;- .135 p3 &lt;- tibble(seed = 1:100) %&gt;% mutate(z = map(seed, z_maker)) %&gt;% unnest(z) %&gt;% group_by(seed) %&gt;% mutate(n = n()) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n, `p(N|z,theta)` = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`, fill = `Sample Proportion z/N` &gt;= 7 / 24)) + geom_col(width = .004) + scale_fill_manual(values = jj[1:2]) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;) theta &lt;- .497 p4 &lt;- tibble(seed = 1:100) %&gt;% mutate(z = map(seed, z_maker)) %&gt;% unnest(z) %&gt;% group_by(seed) %&gt;% mutate(n = n()) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n, `p(N|z,theta)` = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`, fill = `Sample Proportion z/N` &lt;= 7 / 24)) + geom_col(width = .004) + scale_fill_manual(values = jj[1:2]) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;) (p1 + p3 + p2 + p4) + plot_layout(widths = c(2, 4)) Let’s leave failure behind. Here’s the two left panels for Figure 11.11. p1 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .11, .11)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col(fill = jj[1]) + scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) + coord_cartesian(ylim = c(0, 1)) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta[1]==&quot;.11;&quot;*~theta[2]==&quot;.11&quot;)) p2 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .539, .539)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col(fill = jj[1]) + scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) + coord_cartesian(ylim = c(0, 1)) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta[1]==&quot;.539;&quot;*~theta[2]==&quot;.539&quot;)) (p1 / p2) &amp; coord_cartesian(ylim = c(0, 1)) &amp; theme(title = element_text(size = 10), axis.ticks.x = element_blank()) Much like with Figure 11.6, I don’t understand how to do the simulation properly for the right panels of Figure 11.11. If you’ve got it, please share your code in my GitHub issue #19. 11.3.1.1 CI is not a distribution. A CI is merely two end points. A common misconception of a confidence interval is that it indicates some sort of probability distribution over values of \\(\\theta\\). It is very tempting to think that values of \\(\\theta\\) in the middle of a CI should be more believable than values of \\(\\theta\\) at or beyond the limits of the CI. … Methods for imposing a distribution upon a CI seem to be motivated by a natural Bayesian intuition: Parameter values that are consistent with the data should be more credible than parameter values that are not consistent with the data (subject to prior credibility). If we were confined to frequentist methods, then the various proposals outlined above would be expressions of that intuition. But we are not confined to frequentist methods. Instead, we can express our natural Bayesian intuitions in fully Bayesian formalisms. (pp. 323–324) 11.3.2 Bayesian HDI. “The \\(95\\%\\) HDI consists of those values of \\(\\theta\\) that have at least some minimal level of posterior credibility, such that the total probability of all such \\(\\theta\\) values is \\(95\\%\\)” (p. 324). Once again, here’s how to analytically compute the 95% HDIs for our example of \\(z = 7, N = 24\\) and the prior of \\(\\operatorname{Beta}(11, 11)\\). hdi_of_icdf(shape1 = 11 + z, shape2 = 11 + (n - z)) ## [1] 0.2539378 0.5312685 11.4 Multiple comparisons It’s worth quoting Kruschke at length: When comparing multiple conditions, a key goal in NHST is to keep the overall false alarm rate down to a desired maximum such as \\(5\\%\\). Abiding by this constraint depends on the number of comparisons that are to be made, which in turn depends on the intentions of the experimenter. In a Bayesian analysis, however, there is just one posterior distribution over the parameters that describe the conditions. That posterior distribution is unaffected by the intentions of the experimenter, and the posterior distribution can be examined from multiple perspectives however is suggested by insight and curiosity. (p. 325) 11.4.1 NHST correction for experiment wise error. In NHST, we have to take into account all comparisons we intend for the whole experiment. Suppose we set a criterion for rejecting the null such that each decision has a “per-comparison” (PC) false alarm rate of \\(\\alpha_\\text{PC}\\), e.g., \\(5\\%\\). Our goal is to determine the overall false alarm rate when we conduct several comparisons. To get there, we do a little algebra. First, suppose the null hypothesis is true, which means that the groups are identical, and we get apparent differences in the samples by chance alone. This means that we get a false alarm on a proportion \\(\\alpha_\\text{PC}\\) of replications of a comparison test. Therefore, we do not get a false alarm on the complementary proportion \\(1 - \\alpha_\\text{PC}\\) of replications. If we run \\(c\\) independent comparison tests, then the probability of not getting a false alarm on any of the tests is \\((1 - \\alpha_\\text{PC})^c\\). Consequently, the probability of getting at least one false alarm is \\(1 - (1 - \\alpha_\\text{PC})^c\\). We call that probability of getting at least one false alarm, across all the comparisons in the experiment, the “experimentwise” false alarm rate, denoted \\(\\alpha_\\text{EW}\\). (pp. 325–326, emphasis in the original) Here’s what this looks like in when \\(\\alpha_\\text{PC} = .05\\) and \\(c = 36\\). alpha_pc &lt;- .05 c &lt;- 36 # the probability of not getting a false alarm on any of the tests (1 - alpha_pc)^c ## [1] 0.1577792 # the probability of getting at least one false alarm is 1 - (1 - alpha_pc)^c ## [1] 0.8422208 For kicks and giggles, it might be interesting to plot this. tibble(c = 1:100) %&gt;% mutate(p1 = (1 - alpha_pc)^c, p2 = 1 - (1 - alpha_pc)^c) %&gt;% pivot_longer(-c, values_to = &quot;probability&quot;) %&gt;% ggplot(aes(x = c, y = probability, color = name)) + geom_line(size = 1.2) + geom_text(data = tibble( c = c(85, 75, 70), probability = c(.08, .9, .82), label = c(&quot;no false alarms&quot;, &quot;at least one false alarm&quot;, &quot;(i.e., experimentwise false alarm rate)&quot;), name = c(&quot;p1&quot;, &quot;p2&quot;, &quot;p2&quot;) ), aes(label = label)) + scale_color_manual(values = jj[1:2]) + scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) + xlab(&quot;the number of independent tests, c&quot;) + theme(legend.position = &quot;none&quot;) One way to keep the experimentwise false alarm rate down to 5% is by reducing the permitted false alarm rate for the individual comparisons, i.e., setting a more stringent criterion for rejecting the null hypothesis in individual comparisons. One often-used re-setting is the Bonferroni correction, which sets \\(\\alpha_\\text{PC} = \\alpha_\\text{EW}^\\text{desired} / c\\). Here’s how to apply the Bonferroni correction to our example if the desired false-alarm rate is .05. alpha_pc &lt;- .05 c &lt;- 36 # the Bonferroni correction alpha_pc / c ## [1] 0.001388889 Again, it might be useful to plot the consequence of Bonferroni’s correction on \\(\\alpha\\) for different levels of \\(c\\). tibble(c = 1:100) %&gt;% mutate(a_ew = alpha_pc^c) %&gt;% ggplot(aes(x = c, y = a_ew)) + geom_line(color = jj[2], size = 1.2) + scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + scale_y_continuous(expand = expansion(mult = c(0.0015, 0)), limits = c(0, .05)) + xlab(&quot;the number of independent tests, c&quot;) + theme(panel.grid = element_blank()) A little shocking, isn’t it? If you put it on a log scale, you’ll see the relationship is linear. tibble(c = 1:100) %&gt;% mutate(a_ew = alpha_pc^c) %&gt;% ggplot(aes(x = c, y = a_ew)) + geom_line(color = jj[1], size = 1.2) + scale_y_log10() + scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + xlab(&quot;the number of independent tests, c&quot;) + theme(panel.grid = element_blank()) But just look at how low the values on the \\(y\\)-axis get. Frequentists have other correction procedures available to them. The Bayesian approach is different. 11.4.2 Just one Bayesian posterior no matter how you look at it. In a Bayesian analysis, the interpretation of the data is not influenced by the experimenter’s stopping and testing intentions (assuming that those intentions do not affect the data). A Bayesian analysis yields a posterior distribution over the parameters of the model. The posterior distribution is the complete implication of the data. The posterior distribution can be examined in as many different ways as the analyst deems interesting; various comparisons of groups are merely different perspectives on the posterior distribution. (p. 328) 11.4.3 How Bayesian analysis mitigates false alarms. From page 329: “How, then, does a Bayesian analysis address the problem of false alarms? By incorporating prior knowledge into the structure of the model.” One of the more powerful ways to do so is by using hierarchical models whenever possible (e.g., Gelman et al., 2012). 11.5 What a sampling distribution is good for “Sampling distributions tell us the probability of imaginary outcomes given a parameter value and an intention, \\(p(D_{\\theta, I}|\\theta, I)\\), instead of the probability of parameter values given the actual data, \\((\\theta|D_\\text{actual})\\).” 11.5.1 Planning an experiment. Gelman touched on these sensibilities in a recent blog post. 11.5.2 Exploring model predictions (posterior predictive check). There’s no shortage of PPC talk on Gelman’s blog (e.g., here or here or here). Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.1.2 lisa_0.1.2 forcats_0.5.1 stringr_1.4.1 ## [5] dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 ## [9] tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] lubridate_1.8.0 assertthat_0.2.1 digest_0.6.30 ## [4] utf8_1.2.2 R6_2.5.1 cellranger_1.1.0 ## [7] backports_1.4.1 reprex_2.0.2 evaluate_0.18 ## [10] httr_1.4.4 highr_0.9 pillar_1.8.1 ## [13] rlang_1.0.6 googlesheets4_1.0.1 readxl_1.4.1 ## [16] rstudioapi_0.13 jquerylib_0.1.4 rmarkdown_2.16 ## [19] labeling_0.4.2 googledrive_2.0.0 munsell_0.5.0 ## [22] broom_1.0.1 compiler_4.2.0 modelr_0.1.8 ## [25] xfun_0.35 pkgconfig_2.0.3 htmltools_0.5.3 ## [28] tidyselect_1.1.2 bookdown_0.28 fansi_1.0.3 ## [31] crayon_1.5.2 tzdb_0.3.0 dbplyr_2.2.1 ## [34] withr_2.5.0 grid_4.2.0 jsonlite_1.8.3 ## [37] gtable_0.3.1 lifecycle_1.0.3 DBI_1.1.3 ## [40] magrittr_2.0.3 scales_1.2.1 cli_3.5.0 ## [43] stringi_1.7.8 cachem_1.0.6 farver_2.1.1 ## [46] fs_1.5.2 xml2_1.3.3 bslib_0.4.0 ## [49] ellipsis_0.3.2 generics_0.1.3 vctrs_0.5.1 ## [52] tools_4.2.0 glue_1.6.2 hms_1.1.1 ## [55] fastmap_1.1.0 colorspace_2.0-3 gargle_1.2.0 ## [58] rvest_1.0.2 knitr_1.40 haven_2.5.1 ## [61] sass_0.4.2 Footnote References Agresti, A. (2015). Foundations of linear and generalized linear models. John Wiley &amp; Sons. https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034 Atkins, D. C., Baldwin, S. A., Zheng, C., Gallop, R. J., &amp; Neighbors, C. (2013). A tutorial on count regression and zero-altered count models for longitudinal substance use data. Psychology of Addictive Behaviors, 27(1), 166. https://doi.org/10.1037/a0029508 Gelman, A., Hill, J., &amp; Yajima, M. (2012). Why we (usually) don’t have to worry about multiple comparisons. Journal of Research on Educational Effectiveness, 5(2), 189–211. https://doi.org/10.1080/19345747.2011.618213 Jean, J. (2009). RIFT SCULL. Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Kurz, A. S. (2021). Statistical rethinking with brms, ggplot2, and the tidyverse: Second Edition (version 0.2.0). https://bookdown.org/content/4857/ Kurz, A. S. (2020). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/ Wagenmakers, E.-J. (2007). A practical solution to the pervasive problems of p values. Psychonomic Bulletin &amp; Review, 14(5), 779–804. https://doi.org/10.3758/BF03194105 When your criterion variable is a count, you typically model it with the Poisson likelihood. We’ll get some practice with the Poisson likelihood in Chapter 24. Though we won’t cover this in this text, it turns out the Poisson makes some strong assumptions. The negative binomial likelihood is sometimes used as a robust alternative to the Poisson when those assumptions are violated. Sadly, we won’t cover negative-binomial regression at all in this ebook. If you’d like to learn more, check out either edition of McElreath’s Statistical Rethinking (2020, 2015), my brms translations of those texts (Kurz, 2021, 2020), the authoritative text by Agresti (2015), or the great tutorial paper by Atkins et al. (2013).↩︎ "],["bayesian-approaches-to-testing-a-point-null-hypothesis.html", "12 Bayesian Approaches to Testing a Point (“Null”) Hypothesis 12.1 The estimation approach 12.2 The model-comparison approach 12.3 Relations of parameter estimation and model comparison 12.4 Estimation and model comparison? Session info", " 12 Bayesian Approaches to Testing a Point (“Null”) Hypothesis Suppose that you have collected some data, and now you want to answer the question, Is there a non-zero effect or not? Is the coin fair or not? Is there better-than-chance accuracy or not? Is there a difference between groups or not? In the previous chapter, [Kruschke] argued that answering this type of question via null hypothesis significance testing (NHST) has deep problems. This chapter describes Bayesian approaches to the question. (Kruschke, 2015, p 335) 12.1 The estimation approach Throughout this book, we have used Bayesian inference to derive a posterior distribution over a parameter of interest, such as the bias \\(\\theta\\) of a coin. We can then use the posterior distribution to discern the credible values of the parameter. If the null value is far from the credible values, then we reject the null value as not credible. But if all the credible values are virtually equivalent to the null value, then we can accept the null value. (p. 336) 12.1.1 Region of practical equivalence. Kruschke began: “A region of practical equivalence (ROPE) indicates a small range of parameter values that are considered to be practically equivalent to the null value for purposes of the particular application” (p. 336, emphasis in the original) Before we get to plotting, let’s talk about themes and color. For the plots in this chapter, we’ll take our color palette from the fishualize package (Schiettekatte et al., 2022), which provides a range of color palettes based on fish species. Our palette will be \"Ostorhinchus_angustatus\", which is based on Ostorhinchus angustatus. library(fishualize) scales::show_col(fish(n = 5, option = &quot;Ostorhinchus_angustatus&quot;)) We’ll base our overall global plot theme on cowplot::theme_cowplot(), and use theme() to make a few color changes based on \"Ostorhinchus_angustatus\". library(tidyverse) library(cowplot) oa &lt;- fish(n = 5, option = &quot;Ostorhinchus_angustatus&quot;) theme_set( theme_cowplot() + theme(panel.background = element_rect(fill = oa[1], color = oa[1]), strip.background = element_rect(fill = oa[3]), strip.text = element_text(color = oa[5])) ) oa ## [1] &quot;#FBFCEBFF&quot; &quot;#F0E990FF&quot; &quot;#DB9CABFF&quot; &quot;#6C402CFF&quot; &quot;#291E15FF&quot; You can undo the above with ggplot2::theme_set(ggplot2::theme_grey()). Here’s a plot of Kruschke’s initial coin flip ROPE. tibble(xmin = .45, xmax = .55) %&gt;% ggplot() + geom_rect(aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = oa[2]) + annotate(geom = &quot;text&quot;, x = .5, y = .5, label = &quot;ROPE&quot;, color = oa[5]) + scale_x_continuous(breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Kruschke&#39;s coin flip ROPE&quot;, x = expression(theta)) In the first example (p. 336), we have \\(z = 325\\) heads out of \\(N = 500\\) coin flips. To visualize the analysis, we’ll need the Bernoulli likelihood. bernoulli_likelihood &lt;- function(theta, data) { n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } Now we’ll follow the typical steps to combine the prior, which is flat in this case, and the likelihood to get the posterior. # the data summaries n &lt;- 500 z &lt;- 325 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) # (i.e., data) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %&gt;% # (i.e., theta) # recall Beta(1, 1) is flat mutate(prior = dbeta(theta, shape1 = 1, shape2 = 1), # (i.e., p(theta)) likelihood = bernoulli_likelihood(theta = theta, # (i.e., p(D | theta)) data = trial_data)) %&gt;% mutate(posterior = likelihood * prior / sum(prior * likelihood)) # (i.e., p(theta | D)) glimpse(d) ## Rows: 1,000 ## Columns: 4 ## $ theta &lt;dbl&gt; 0.000000000, 0.001001001, 0.002002002, 0.003003003, 0.004004004, 0.005005005, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ likelihood &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ posterior &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… Now we can plot the results. ggplot(data = d) + geom_rect(xmin = .45, xmax = .55, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = oa[2]) + geom_area(aes(x = theta, y = posterior), fill = oa[4]) + annotate(geom = &quot;text&quot;, x = .5, y = .01, label = &quot;ROPE&quot;, color = oa[5]) + scale_x_continuous(breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Nope, that density ain&#39;t in that ROPE.&quot;, x = expression(theta)) With the formula by \\(\\operatorname{Beta}(\\theta | z + \\alpha, N - z + \\beta)\\), we can analytically compute the Beta parameters for the posterior. (alpha &lt;- z + 1) ## [1] 326 (beta &lt;- n - z + 1) ## [1] 176 With the hdi_of_icdf() function, we’ll compute the HDIs. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Compute those HDIs and save them as h. ( h &lt;- hdi_of_icdf(name = qbeta, shape1 = alpha, shape2 = beta) ) ## [1] 0.6075644 0.6909070 Now let’s remake the plot from above, this time with the analytically-derived HDI values. tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %&gt;% ggplot() + geom_rect(xmin = .45, xmax = .55, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = oa[2]) + geom_area(aes(x = theta, y = dbeta(theta, shape1 = alpha, shape2 = beta)), fill = oa[4]) + geom_segment(x = h[1], xend = h[2], y = 0, yend = 0, size = 1, color = oa[3]) + annotate(geom = &quot;text&quot;, x = .5, y = 17.5, label = &quot;ROPE&quot;, color = oa[5]) + annotate(geom = &quot;text&quot;, x = .65, y = 4, label = &quot;95%\\nHDI&quot;, color = oa[1]) + scale_x_continuous(breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;That `hdi_of_icdf()` function really came through, for us.&quot;, x = expression(theta)) In his second example (p. 337), Kruschke considered \\(z = 490\\) heads out of \\(N = 1{,}000\\) flips. # we need these to compute the likelihood n &lt;- 1000 z &lt;- 490 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %&gt;% # (i.e., theta) mutate(prior = dbeta(theta, shape1 = 1, shape2 = 1), # (i.e., p(theta)) likelihood = bernoulli_likelihood(theta = theta, # (i.e., p(D | theta)) data = trial_data)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) %&gt;% # (i.e., p(theta | D)) ggplot() + geom_rect(xmin = .45, xmax = .55, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = oa[2]) + geom_area(aes(x = theta, y = posterior), fill = oa[4]) + scale_x_continuous(breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;This posterior sits right within the ROPE.&quot;, x = expression(theta)) Here are the new HDIs. hdi_of_icdf(name = qbeta, shape1 = z + 1, shape2 = n - z + 1) ## [1] 0.4590949 0.5209562 Further down the section, Kruschke offered some perspective on the ROPE approach. The ROPE limits, by definition, cannot be uniquely “correct,” but instead are established by practical aims, bearing in mind that wider ROPEs yield more decisions to accept the ROPEd value and fewer decision to reject the ROPEd value. In many situations, the exact limit of the ROPE can be left indeterminate or tacit, so that the audience of the analysis can use whatever ROPE is appropriate at the time, as competing theories and measuring devices evolve. When the HDI is far from the ROPEd value, the exact ROPE is inconsequential because the ROPEd value would be rejected for any reasonable ROPE. When the HDI is very narrow and overlaps the target value, the HDI might again fall within any reasonable ROPE, again rendering the exact ROPE inconsequential. When, however, the HDI is only moderately narrow and near the target value, the analysis can report how much of the posterior falls within a ROPE as a function of different ROPE widths… It is important to be clear that any discrete decision about rejecting or accepting a null value does not exhaustively capture our knowledge about the parameter value. Our knowledge about the parameter value is described by the full posterior distribution. When making a binary decision, we have merely compressed all that rich detail into a single bit of information. The broader goal of Bayesian analysis is conveying an informative summary of the posterior, and where the value of interest falls within that posterior. Reporting the limits of an HDI region is more informative than reporting the declaration of a reject/accept decision. By reporting the HDI and other summary information about the posterior, different readers can apply different ROPEs to decide for themselves whether a parameter is practically equivalent to a null value. The decision procedure is separate from the Bayesian inference. The Bayesian part of the analysis is deriving the posterior distribution. The decision procedure uses the posterior distribution, but does not itself use Bayes’ rule. (pp. 338–339, emphasis in the original) Full disclosure: I’m not a fan of the ROPE method. Though we’re following along with the text and covering it, here, I will deemphasize it in later sections. Kruschke then went on to compare the ROPE with frequentist equivalence tests. This is a part of the literature I have not waded into, yet. It appears psychologist Daniël Lakens and colleagues gave written a bit in the topic, recently. Interested readers might start with Lakens et al. (2020), Lakens et al. (2018), or Lakens &amp; Delacre (2018). 12.1.2 Some examples. Kruschke referenced an analysis from way back in Chapter 9. We’ll need to re-fit the model. First we import data. my_data &lt;- read_csv(&quot;data.R/BattingAverage.csv&quot;) glimpse(my_data) ## Rows: 948 ## Columns: 6 ## $ Player &lt;chr&gt; &quot;Fernando Abad&quot;, &quot;Bobby Abreu&quot;, &quot;Tony Abreu&quot;, &quot;Dustin Ackley&quot;, &quot;Matt Adams&quot;, … ## $ PriPos &lt;chr&gt; &quot;Pitcher&quot;, &quot;Left Field&quot;, &quot;2nd Base&quot;, &quot;2nd Base&quot;, &quot;1st Base&quot;, &quot;Pitcher&quot;, &quot;Pitc… ## $ Hits &lt;dbl&gt; 1, 53, 18, 137, 21, 0, 0, 2, 150, 167, 0, 128, 66, 3, 1, 81, 180, 36, 150, 0,… ## $ AtBats &lt;dbl&gt; 7, 219, 70, 607, 86, 1, 1, 20, 549, 576, 1, 525, 275, 12, 8, 384, 629, 158, 5… ## $ PlayerNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22… ## $ PriPosNumber &lt;dbl&gt; 1, 7, 4, 4, 3, 1, 1, 3, 3, 4, 1, 5, 4, 2, 7, 4, 6, 8, 9, 1, 2, 5, 1, 1, 7, 2,… Let’s load brms and, while we’re at it, tidybayes. library(brms) library(tidybayes) Fit the model and retain its original name, fit9.2. fit9.2 &lt;- brm(data = my_data, family = binomial(link = logit), Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = .99), seed = 9, file = &quot;fits/fit09.02&quot;) Let’s use the coef() function with summary = FALSE to pull the relevant posterior draws. c &lt;- coef(fit9.2, summary = F)$PriPos %&gt;% as_tibble() str(c) ## tibble [9,000 × 9] (S3: tbl_df/tbl/data.frame) ## $ 1st Base.Intercept : num [1:9000] -1.06 -1.09 -1.06 -1.04 -1.1 ... ## $ 2nd Base.Intercept : num [1:9000] -1.07 -1.06 -1.12 -1.1 -1.04 ... ## $ 3rd Base.Intercept : num [1:9000] -1.1 -1.03 -1.04 -1.08 -1.09 ... ## $ Catcher.Intercept : num [1:9000] -1.15 -1.14 -1.14 -1.15 -1.17 ... ## $ Center Field.Intercept: num [1:9000] -1.06 -1.11 -1.08 -1.07 -1.02 ... ## $ Left Field.Intercept : num [1:9000] -1.09 -1.12 -1.11 -1.05 -1.1 ... ## $ Pitcher.Intercept : num [1:9000] -1.9 -1.9 -1.86 -1.89 -1.89 ... ## $ Right Field.Intercept : num [1:9000] -1.038 -1.068 -1.053 -1.083 -0.986 ... ## $ Shortstop.Intercept : num [1:9000] -1.11 -1.12 -1.08 -1.1 -1.07 ... As we pointed out in Chapter 9, keep in mind that coef() returns the values in the logit scale when used for logistic regression models. So we’ll have to use brms::inv_logit_scaled() to convert the estimates to the probability metric. We can make the difference distributions after we’ve converted the estimates. c_small &lt;- c %&gt;% mutate_all(inv_logit_scaled) %&gt;% transmute(`Pitcher - Catcher` = Pitcher.Intercept - Catcher.Intercept, `Catcher - 1st Base` = Catcher.Intercept - `1st Base.Intercept`) head(c_small) ## # A tibble: 6 × 2 ## `Pitcher - Catcher` `Catcher - 1st Base` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.109 -0.0177 ## 2 -0.112 -0.0102 ## 3 -0.108 -0.0142 ## 4 -0.110 -0.0197 ## 5 -0.105 -0.0127 ## 6 -0.120 -0.0125 After a little wrangling, we’ll be ready to re-plot the relevant parts of Figure 9.14. c_small %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;Pitcher - Catcher&quot;, &quot;Catcher - 1st Base&quot;))) %&gt;% ggplot(aes(x = value)) + geom_rect(xmin = -0.05, xmax = 0.05, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = oa[2]) + stat_histinterval(aes(y = 0), point_interval = mode_hdi, .width = .95, fill = oa[4], colour = oa[3], breaks = 20, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;The ROPE ranges from −0.05 to +0.05&quot;, x = expression(theta)) + coord_cartesian(xlim = c(-.125, .125)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ name, scales = &quot;free&quot;) ## Warning: Unknown or uninitialised column: `linewidth`. ## Warning: Using the `size` aesthietic with geom_segment was deprecated in ggplot2 3.4.0. ## ℹ Please use the `linewidth` aesthetic instead. ## Warning: Unknown or uninitialised column: `linewidth`. In order to re-plot part of Figure 9.15, we’ll need to employ fitted() to snatch the player-specific posteriors. # this will make life easier. just go with it name_list &lt;- c(&quot;ShinSoo Choo&quot;, &quot;Ichiro Suzuki&quot;) # we&#39;ll define the data we&#39;d like to feed into `fitted()`, here nd &lt;- my_data %&gt;% filter(Player %in% c(name_list)) %&gt;% # these last two lines aren&#39;t typically necessary, but they allow us to # arrange the rows in the same order we find the names in Figures 9.15 and 9/16 mutate(Player = factor(Player, levels = c(name_list))) %&gt;% arrange(Player) f &lt;- fitted(fit9.2, newdata = nd, scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% mutate_all(inv_logit_scaled) %&gt;% set_names(name_list) %&gt;% # in this last section, we make our difference distributions mutate(`ShinSoo Choo - Ichiro Suzuki` = `ShinSoo Choo` - `Ichiro Suzuki`) glimpse(f) ## Rows: 9,000 ## Columns: 3 ## $ `ShinSoo Choo` &lt;dbl&gt; 0.3150916, 0.2539179, 0.2996450, 0.2481647, 0.3179521, 0.25… ## $ `Ichiro Suzuki` &lt;dbl&gt; 0.2739827, 0.2817321, 0.2619542, 0.2645591, 0.2732071, 0.26… ## $ `ShinSoo Choo - Ichiro Suzuki` &lt;dbl&gt; 0.0411088952, -0.0278142211, 0.0376908094, -0.0163944150, 0… Now we’re ready to go. f %&gt;% ggplot() + geom_rect(xmin = -0.05, xmax = 0.05, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = oa[2]) + stat_histinterval(aes(x = `ShinSoo Choo - Ichiro Suzuki`, y = 0), point_interval = mode_hdi, .width = .95, fill = oa[4], color = oa[3], breaks = 40) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;ShinSoo Choo - Ichiro Suzuki&quot;, x = expression(theta)) + coord_cartesian(xlim = c(-.125, .125)) 12.1.3 Differences of correlated parameters. Kruschke didn’t explicate where he got the data for Figure 12.1. If we’re willing to presume a multivariate normal distribution, we can get close using the MASS::mvrnorm() function. You can get the basic steps from Sven Hohenstein’s answer to this stats.stacheschange question. # first we&#39;ll make a correlation matrix # a correlation of .9 seems about right correlation_matrix &lt;- matrix(c(1, .9, .9, 1), nrow = 2, ncol = 2) # next we&#39;ll specify the means and standard deviations mu &lt;- c(.58, .42) sd &lt;- c(.1, .1) # now we&#39;ll use the correlation matrix and standard deviations to make a covariance matrix covariance_matrix &lt;- sd %*% t(sd) * correlation_matrix # after setting our seed, we&#39;re ready to simulate set.seed(12) d &lt;- MASS::mvrnorm(n = 1000, mu = mu, Sigma = covariance_matrix) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;theta[&quot;, 1:2, &quot;]&quot;)) Now it only takes some light wrangling to prepare the data to make the three histograms in the left panel of Figure 12.1. d %&gt;% mutate(`theta[1]-theta[2]` = `theta[1]` - `theta[2]`) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = oa[4], color = oa[3], breaks = 30, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + facet_wrap(~ name, scales = &quot;free_y&quot;, labeller = label_parsed) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Here’s the scatter plot, showing the correlation. I think we got pretty close! d %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[2]`)) + geom_abline(color = oa[2]) + geom_point(size = 1/2, color = oa[3], alpha = 1/4) + scale_x_continuous(expression(theta[1]), breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) + scale_y_continuous(expression(theta[2]), breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) + coord_equal() To make the plots in the right panel of Figure 12.1, we just need to convert the correlation from .9 to -.9. # this time we&#39;ll make the correlations -.9 correlation_matrix &lt;- matrix(c(1, -.9, -.9, 1), nrow = 2, ncol = 2) # we&#39;ll have to redo the covariance matrix covariance_matrix &lt;- sd %*% t(sd) * correlation_matrix # here&#39;s the updated data set.seed(1) d &lt;- MASS::mvrnorm(n = 1000, mu = mu, Sigma = covariance_matrix) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;theta[&quot;, 1:2, &quot;]&quot;)) Here are our right-panel Figure 12.1 histograms. d %&gt;% mutate(`theta[1]-theta[2]` = `theta[1]` - `theta[2]`) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = oa[4], color = oa[3], breaks = 20, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + facet_wrap(~ name, scales = &quot;free_y&quot;, labeller = label_parsed) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Behold the second scatter plot. d %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[2]`)) + geom_abline(color = oa[2]) + geom_point(size = 1/2, color = oa[3], alpha = 1/4) + scale_x_continuous(expression(theta[1]), breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) + scale_y_continuous(expression(theta[2]), breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) + coord_equal() In summary, the marginal distributions of two parameters do not indicate the relationship between the parameter values. The joint distribution of the two parameters might have positive or negative correlation (or even a non-linear dependency), and therefore the difference of the parameter values should be explicitly examined. (pp. 341–342) 12.1.4 Why HDI and not equal-tailed interval? Though Kruschke told us Figure 12.2 was of a gamma distribution, he didn’t tell us the parameters for that particular gamma. After playing around for a bit, it appeared dgamma(x, 2, 0.2) worked pretty well. tibble(x = seq(from = 0, to = 40, by = .1)) %&gt;% ggplot(aes(x = x, y = dgamma(x, shape = 2, rate = 0.2))) + geom_area(fill = oa[4]) + scale_x_continuous(expand = expansion(mult = c(0, 0.05))) + scale_y_continuous(&quot;density&quot;, expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Gamma(2, 0.2)&quot;) + coord_cartesian(xlim = c(0, 35)) If you want to get the quantile-based intervals (i.e., the ETIs), you can plug in the desired quantiles into the qgamma() function. (ex &lt;- qgamma(c(.025, .975), shape = 2, rate = 0.2)) ## [1] 1.211046 27.858217 To analytically derive the gamma HDIs, we just use the good old hdi_of_icdf() function. ( hx &lt;- hdi_of_icdf(name = qgamma, shape = 2, rate = 0.2) ) ## [1] 0.2118165 23.8258411 Next you need to determine how high up to go on the y-axis. For the quantile-based intervals, the ETIs, you can use dgamma(). The trick is pump the output of qgamma() right into dgamma(). ( ey &lt;- qgamma(c(.025, .975), shape = 2, rate = .2) %&gt;% dgamma(shape = 2, rate = 0.2) ) ## [1] 0.038021620 0.004239155 We follow the same basic principle to get the \\(y\\)-axis values for the HDIs. ( hy &lt;- hdi_of_icdf(name = qgamma, shape = 2, rate = 0.2) %&gt;% dgamma(shape = 2, rate = 0.2) ) ## [1] 0.008121227 0.008121233 Now we’ve computed all those values, we can collect them into a tibble with the necessary coordinates to make the ETI and HDI lines in our plot. ( lines &lt;- tibble(interval = rep(c(&quot;eti&quot;, &quot;hdi&quot;), each = 4), x = c(ex, hx) %&gt;% rep(., each = 2), y = c(ey[1], 0.0003, 0.0003, ey[2], 0, hy, 0)) ) ## # A tibble: 8 × 3 ## interval x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 eti 1.21 0.0380 ## 2 eti 1.21 0.0003 ## 3 eti 27.9 0.0003 ## 4 eti 27.9 0.00424 ## 5 hdi 0.212 0 ## 6 hdi 0.212 0.00812 ## 7 hdi 23.8 0.00812 ## 8 hdi 23.8 0 Technically, those second and third y-values should be zero. I’ve set them a touch higher so they don’t get obscured by the \\(x\\)-axis in the plot. Anyway, we’re finally ready to plot a more complete version of Figure 12.2. # for the annotation text &lt;- tibble(x = c(15, 12), y = c(.004, .012), label = c(&quot;95% ETI&quot;, &quot;95% HDI&quot;), interval = c(&quot;eti&quot;, &quot;hdi&quot;)) # plot! tibble(x = seq(from = 0, to = 40, by = .1)) %&gt;% ggplot(aes(x = x)) + geom_area(aes(y = dgamma(x, 2, 0.2)), fill = oa[4]) + geom_path(data = lines, aes(y = y, color = interval), size = 1) + geom_text(data = text, aes(y = y, color = interval, label = label)) + scale_color_manual(values = oa[c(5, 1)]) + scale_x_continuous(&quot;Parameter Value&quot;, expand = expansion(mult = c(0, 0.05))) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + coord_cartesian(xlim = c(0, 35)) + theme(legend.position = &quot;none&quot;) To repeat, ETIs are the only types of intervals available directly by the brms package. When using the default print() or summary() output for a brm() model, the 95% ETIs are displayed in the ‘l-95% CI’ and ‘u-95% CI’ columns. print(fit9.2) ## Family: binomial ## Links: mu = logit ## Formula: Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player) ## Data: my_data (Number of observations: 948) ## Draws: 3 chains, each with iter = 3500; warmup = 500; thin = 1; ## total post-warmup draws = 9000 ## ## Group-Level Effects: ## ~PriPos (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.33 0.10 0.19 0.58 1.00 2846 4327 ## ## ~PriPos:Player (Number of levels: 948) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.14 0.01 0.12 0.15 1.00 3433 5479 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.17 0.11 -1.40 -0.94 1.00 1429 2851 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). In the output of most other brms functions, the 95% ETIs appear in the Q2.5 and Q97.5 columns. Take fitted(), for example. fitted(fit9.2, newdata = nd, scale = &quot;linear&quot;) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] -0.9694413 0.07686162 -1.123835 -0.8203555 ## [2,] -0.9675914 0.07442985 -1.112812 -0.8207858 But as we just did, above, you can always use the convenience functions from the tidybayes package (e.g., mean_hdi()) to get HDIs from a brms fit. fitted(fit9.2, newdata = nd, scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mean_hdi(value) ## # A tibble: 2 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 V1 -0.969 -1.13 -0.823 0.95 mean hdi ## 2 V2 -0.968 -1.11 -0.823 0.95 mean hdi As you may have gathered, Kruschke clearly prefers using HDIs over ETIs. His preference isn’t without controversy. If you’d like to explore the topic further, here are links to discussion threads on HDIs in [the link to the corresponding thread in the Stan forum and the posterior GitHub repo. 12.2 The model-comparison approach Recall that the motivating issue for this chapter is the question, Is the null value of a parameter credible? The previous section answered the question in terms of parameter estimation. In that approach, we started with a possibly informed prior distribution and examined the posterior distribution. In this section we take a different approach. Some researchers prefer instead to pose the question in terms of model comparison. In this framing of the question, the focus is not on estimating the magnitude of the parameter. Instead, the focus is on deciding which of two hypothetical prior distributions is least incredible. One prior expresses the hypothesis that the parameter value is exactly the null value. The alternative prior expresses the hypothesis that the parameter could be any value, according to some form of broad distribution. (p. 344) 12.2.1 Is a coin fair or not? Some (e.g., Lee &amp; Webb, 2005; Zhu &amp; Lu, 2004) have argued the Haldane prior is superior to the uniform \\(\\operatorname{Beta}(1, 1)\\) when choosing an uninformative prior for \\(\\theta\\). The Haldane, recall, is \\(\\operatorname{Beta}(\\epsilon, \\epsilon)\\), where \\(\\epsilon\\) is some small value approaching zero (e.g., 0.01). We’ll use our typical steps with the grid approximation to compute the data for the left column of Figure 12.3 (i.e., the column based on the Haldane prior). # we need these to compute the likelihood n &lt;- 24 z &lt;- 7 epsilon &lt;- .01 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(prior = dbeta(x = theta, shape1 = epsilon, shape2 = epsilon), likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% # we have to slice off the first and last values because they go to infinity on the prior, # which creates problems when computing the denominator `sum(likelihood * prior)` (i.e., p(D)) slice(2:999) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) head(d) ## # A tibble: 6 × 4 ## theta prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00100 4.67 9.90e-22 1.61e-15 ## 2 0.00200 2.35 1.25e-19 1.02e-13 ## 3 0.00300 1.58 2.09e-18 1.15e-12 ## 4 0.00400 1.19 1.54e-17 6.39e-12 ## 5 0.00501 0.952 7.22e-17 2.40e-11 ## 6 0.00601 0.796 2.54e-16 7.07e-11 Here’s the left column of Figure 12.3. p1 &lt;- d %&gt;% ggplot(aes(x = theta, y = prior)) + geom_area(fill = oa[4]) + annotate(geom = &quot;text&quot;, x = .1, y = 4, label = expression(epsilon == 0.01), size = 3.5, color = oa[5]) + labs(title = &quot;Prior (beta)&quot;, y = expression(&quot;Beta&quot;*(theta*&quot;|&quot;*epsilon*&quot;, &quot;*epsilon))) + coord_cartesian(ylim = c(0, 5)) p2 &lt;- d %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_area(fill = oa[4]) + labs(title = &quot;Likelihood (Bernoulli)&quot;, y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_area(fill = oa[4]) + labs(title = &quot;Posterior (beta)&quot;, y = expression(&quot;Beta&quot;*(theta*&quot;|&quot;*7.01*&quot;, &quot;*17.01))) library(patchwork) (p1 / p2 / p3) &amp; scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) &amp; scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) &amp; theme(panel.grid = element_blank()) We can calculate the beta parameters for the posterior using the formula \\(\\operatorname{Beta}(\\theta | z + \\alpha, N - z + \\beta)\\). # alpha z + epsilon ## [1] 7.01 # beta n - z + epsilon ## [1] 17.01 We need updated data for the right column, based on the \\(\\operatorname{Beta}(2, 4)\\) prior. alpha &lt;- 2 beta &lt;- 4 d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(prior = dbeta(x = theta, shape1 = alpha, shape2 = beta), likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% # no need to `slice(2:999)` this time mutate(posterior = likelihood * prior / sum(likelihood * prior)) head(d) ## # A tibble: 6 × 4 ## theta prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 ## 2 0.00100 0.0200 9.90e-22 8.91e-20 ## 3 0.00200 0.0398 1.25e-19 2.24e-17 ## 4 0.00300 0.0595 2.09e-18 5.62e-16 ## 5 0.00400 0.0791 1.54e-17 5.50e-15 ## 6 0.00501 0.0986 7.22e-17 3.21e-14 Now here’s the right column of Figure 12.3. p1 &lt;- d %&gt;% ggplot(aes(x = theta, y = prior)) + geom_area(fill = oa[4]) + labs(title = &quot;Prior (beta)&quot;, y = expression(&quot;Beta&quot;*(theta*&quot;|&quot;*2*&quot;, &quot;*4))) + coord_cartesian(ylim = c(0, 5)) p2 &lt;- d %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_area(fill = oa[4]) + labs(title = &quot;Likelihood (Bernoulli)&quot;, y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_area(fill = oa[4]) + labs(title = &quot;Posterior (beta)&quot;, y = expression(&quot;Beta&quot;*(theta*&quot;|&quot;*9*&quot;, &quot;*21))) (p1 / p2 / p3) &amp; scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) &amp; scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) Here are those beta parameters for that posterior. # alpha z + alpha ## [1] 9 # beta n - z + beta ## [1] 21 Following the formula for the null hypothesis (Equation 2.1, p. 344), \\[p(z, N|M_\\text{null}) = \\theta_\\text{null}^z(1 - \\theta_\\text{null})^{(N - z)},\\] we can compute the probability of the data given the null hypothesis. theta &lt;- .5 (p_d_null &lt;- theta ^ z * (1 - theta) ^ (n - z)) ## [1] 5.960464e-08 The formula for the marginal likelihood for the alternative hypothesis \\(M_\\text{alt}\\) is \\[p(z, N| M_\\text{alt}) = \\frac{\\operatorname{Beta}(z + \\alpha_\\text{alt}, N - z + \\beta_\\text{alt})}{\\operatorname{Beta}(\\alpha_\\text{alt}, \\beta_\\text{alt})}.\\] We can make our own p_d() function to compute the probability of the data given alternative hypotheses. Here we’ll simplify the function a bit to extract z and n out of the environment. p_d &lt;- function(a, b) { beta(z + a, n - z + b) / beta(a, b) } With p_d_null and our p_d() function in hand, we can reproduce and extend the results in Kruschke’s Equation 12.4 (p. 345). options(scipen = 999) tibble(shape1 = c(2, 1, .1, .01, .001, .0001, .00001), shape2 = c(4, 1, .1, .01, .001, .0001, .00001)) %&gt;% mutate(p_d = p_d(a = shape1, b = shape2), p_d_null = p_d_null) %&gt;% mutate(bf = p_d / p_d_null) %&gt;% # this just reduces the amount of significant digits in the output mutate_all(round, digits = 6) ## # A tibble: 7 × 5 ## shape1 shape2 p_d p_d_null bf ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 4 0 0 3.72 ## 2 1 1 0 0 1.94 ## 3 0.1 0.1 0 0 0.421 ## 4 0.01 0.01 0 0 0.0481 ## 5 0.001 0.001 0 0 0.00488 ## 6 0.0001 0.0001 0 0 0.000489 ## 7 0.00001 0.00001 0 0 0.000049 options(scipen = 0) Did you notice our use of options(scipen)? With the first line, we turned off scientific notation in the print output. We turned scientific notation back on with the second line. But back to the text, for now, notice that when the alternative prior is uniform, with \\(a_\\text{alt} = b_\\text{alt} = 1.000\\), the Bayes’ factor shows a (small) preference for the alternative hypothesis, but when the alternative prior approximates the Haldane, the Bayes’ factor shows a strong preference for the null hypothesis. As the alternative prior gets closer to the Haldane limit, the Bayes’ factor changes by orders of magnitude. Thus, as we have seen before (e.g. Section 10.6, p. 292), the Bayes’ factor is very sensitive to the choice of prior distribution. (p. 345, emphasis added) On page 346, Kruschke showed some of the 95% HDIs for the marginal distributions of the various \\(M_\\text{alt}\\)s. We could compute those one at a time with hdi_of_icdf(). But why not work in bulk? Like we did in Chapter 10, let’s make a custom variant hdi_of_qbeta(), which will be more useful within the context of map2(). hdi_of_qbeta &lt;- function(shape1, shape2) { hdi_of_icdf(name = qbeta, shape1 = shape1, shape2 = shape2) %&gt;% data.frame() %&gt;% mutate(level = c(&quot;ll&quot;, &quot;ul&quot;)) %&gt;% spread(key = level, value = &quot;.&quot;) } Compute the HDIs. tibble(shape1 = z + c(2, 1, .1, .01, .001, .0001, .00001), shape2 = n - z + c(4, 1, .1, .01, .001, .0001, .00001)) %&gt;% mutate(h = map2(shape1, shape2, hdi_of_qbeta)) %&gt;% unnest(h) %&gt;% mutate_at(vars(ends_with(&quot;l&quot;)), .funs = ~round(., digits = 4)) ## # A tibble: 7 × 4 ## shape1 shape2 ll ul ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 21 0.145 0.462 ## 2 8 18 0.141 0.483 ## 3 7.1 17.1 0.124 0.472 ## 4 7.01 17.0 0.122 0.471 ## 5 7.00 17.0 0.122 0.471 ## 6 7.00 17.0 0.122 0.471 ## 7 7.00 17.0 0.122 0.471 As Kruschke mused, if we consider the posterior distribution instead of the Bayes’ factor, we see that the posterior distribution on \\(\\theta\\) within the alternative model is only slightly affected by the prior… In all cases, the 95% HDI excludes the null value, although a wide ROPE might overlap the HDI. Thus, the explicit estimation of the bias parameter robustly indicates that the null value should be rejected, but perhaps only marginally. This contrasts with the Bayes’ factor, model-comparison approach, which rejected the null or accepted the null depending on the alternative prior. Further, of the Bayes’ factors in Equation 12.4, which is most appropriate? If your analysis is driven by the urge for a default, uninformed alternative prior, then the prior that best approximates the Haldane is most appropriate. Following from that, we should strongly prefer the null hypothesis to the Haldane alternative. While this is mathematically correct, it is meaningless for an applied setting because the Haldane alternative represents nothing remotely resembling a credible alternative hypothesis. The Haldane prior sets prior probabilities of virtually zero at all values of \\(\\theta\\) except \\(\\theta = 0\\) and \\(\\theta = 1\\). There are very few applied settings where such a U-shaped prior represents a genuinely meaningful theory. (p. 346). 12.2.1.1 Bayes’ factor can accept null with poor precision. Here are the steps to make the left column of Figure 12.4 (i.e., the column based on very weak data and the Haldane prior). # we need these to compute the likelihood n &lt;- 2 z &lt;- 1 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(prior = dbeta(x = theta, shape1 = epsilon, shape2 = epsilon), likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% # like before, we have to slice off the first and last values because they go to infinity on the # prior, which creats problems when computing the denominator `sum(likelihood * prior)` (i.e., p(D)) slice(2:999) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) p1 &lt;- d %&gt;% ggplot(aes(x = theta, y = prior)) + geom_area(fill = oa[4]) + annotate(geom = &quot;text&quot;, x = .1, y = 4, label = expression(epsilon == 0.01), size = 3.5, color = oa[5]) + labs(title = &quot;Prior (beta)&quot;, y = expression(&quot;Beta&quot;*(theta*&quot;|&quot;*epsilon*&quot;, &quot;*epsilon))) + coord_cartesian(ylim = c(0, 5)) p2 &lt;- d %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_area(fill = oa[4]) + labs(title = &quot;Likelihood (Bernoulli)&quot;, y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_area(fill = oa[4]) + labs(title = &quot;Posterior (beta)&quot;, y = expression(&quot;Beta&quot;*(theta*&quot;|&quot;*7.01*&quot;, &quot;*17.01))) + coord_cartesian(ylim = c(0, 0.005)) (p1 / p2 / p3) &amp; scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) &amp; scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) That is one flat posterior! Here are the shape parameters and the HDIs. (alpha &lt;- z + epsilon) ## [1] 1.01 (beta &lt;- n - z + epsilon) ## [1] 1.01 hdi_of_icdf(name = qbeta, shape1 = alpha, shape2 = beta) %&gt;% round(digits = 3) ## [1] 0.026 0.974 How do we compute the BF? theta &lt;- .5 a &lt;- epsilon b &lt;- epsilon # pD_{null} pD_{alternative} (theta ^ z * (1 - theta) ^ (n - z)) / (beta(z + a, n - z + b) / beta(a, b)) ## [1] 51 Just like in the text, “the Bayes’ factor is \\(51.0\\) in favor of the null hypothesis” (p. 347)! Here are the steps to make the right column of Figure 12.4, which is based on stronger data and a flat \\(\\operatorname{Beta}(1, 1)\\) prior. # we need these to compute the likelihood n &lt;- 14 z &lt;- 7 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %&gt;% mutate(prior = dbeta(x = theta, shape1 = alpha, shape2 = beta), likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% # no need to `slice(2:999)` this time mutate(posterior = likelihood * prior / sum(likelihood * prior)) p1 &lt;- d %&gt;% ggplot(aes(x = theta, y = prior)) + geom_area(fill = oa[4]) + labs(title = &quot;Prior (beta)&quot;, y = expression(&quot;Beta&quot;*(theta*&quot;|&quot;*1*&quot;, &quot;*1))) + coord_cartesian(ylim = c(0, 3.5)) p2 &lt;- d %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_area(fill = oa[4]) + labs(title = &quot;Likelihood (Bernoulli)&quot;, y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_area(fill = oa[4]) + labs(title = &quot;Posterior (beta)&quot;, y = expression(&quot;Beta&quot;*(theta*&quot;|&quot;*8*&quot;, &quot;*8))) + coord_cartesian(ylim = c(0, 0.0035)) (p1 / p2 / p3) &amp; scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) &amp; scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) Here are the updated shape parameters and the HDIs. epsilon &lt;- 1 (alpha &lt;- z + epsilon) ## [1] 8 (beta &lt;- n - z + epsilon) ## [1] 8 hdi_of_icdf(name = qbeta, shape1 = alpha, shape2 = beta) %&gt;% round(digits = 3) ## [1] 0.266 0.734 Those HDIs are still pretty wide, but much less so than before. Let’s compute the BF. a &lt;- 1 b &lt;- 1 # pD_{null} pD_{alternative} (theta ^ z * (1 - theta) ^ (n - z)) / (beta(z + a, n - z + b) / beta(a, b)) ## [1] 3.14209 A BF of 3.14 in favor of the null is lackluster evidence. And happily so given the breadth of the HDIs. Kruschke discussed how we’d need \\(z = 1{,}200\\) and \\(N = 2{,}400\\) before the posterior HDIs would fit within a narrow ROPE like .48 and .52. Here’s what that would look like based on the priors from Figure 12.4. epsilon &lt;- 0.01 z &lt;- 1200 n &lt;- 2400 alpha &lt;- z + epsilon beta &lt;- n - z + epsilon d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) # the Haldane-based plot p1 &lt;- d %&gt;% ggplot(aes(x = theta, y = dbeta(theta, shape1 = alpha, shape2 = beta))) + geom_rect(xmin = .48, xmax = .52, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = oa[2]) + geom_area(fill = oa[4]) + annotate(geom = &quot;text&quot;, x = .05, y = 35, label = expression(epsilon == 0.01), size = 3.5) + ggtitle(&quot;This posterior used the Haldane prior.&quot;) # redefine the Beta parameters alpha &lt;- z + 1 beta &lt;- n - z + 1 # the Beta(1, 1)-based plot p2 &lt;- d %&gt;% ggplot(aes(x = theta, y = dbeta(theta, shape1 = alpha, shape2 = beta))) + geom_rect(xmin = .48, xmax = .52, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = oa[2]) + geom_area(fill = oa[4]) + ggtitle(&quot;This time we used the flat Beta(1, 1).&quot;) (p1 / p2) &amp; scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) &amp; scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) There is no way around this inconvenient statistical reality: high precision demands a large sample size (and a measurement device with minimal possible noise). But when we are trying to accept a specific value of \\(\\theta\\), is seems logically appropriate that we should have a reasonably precise estimate indicating that specific value. (p. 348) 12.2.2 Are different groups equal or not? Researchers often want to ask the question, Are the groups different or not? As a concrete example, suppose we conduct an experiment about the effect of background music on the ability to remember. As a simple test of memory, each person tries to memorize the same list of \\(20\\) words (such as “chair,” “shark,” “radio,” etc.). They see each word for a specific time, and then, after a brief retention interval, recall as many words as they can. (p. 348) If you look in Kruschke’s OneOddGroupModelComp2E.R file, you can get his simulation code. Here we use a dramatically simplified version. This attempt does not exactly reproduce what his script did, but it gets it in spirit. # For each subject, specify the condition they were in, # the number of trials they experienced, and the number correct. n_g &lt;- 20 # number of subjects per group n_t &lt;- 20 # number of trials per subject set.seed(12) my_data &lt;- tibble(condition = factor(c(&quot;Das Kruschke&quot;, &quot;Mozart&quot;, &quot;Bach&quot;, &quot;Beethoven&quot;), levels = c(&quot;Das Kruschke&quot;, &quot;Mozart&quot;, &quot;Bach&quot;, &quot;Beethoven&quot;)), group_means = c(.40, .50, .51, .52)) %&gt;% expand(nesting(condition, group_means), row = 1:20) %&gt;% mutate(id = 1:80, n_g = n_g, n_t = n_t) %&gt;% mutate(n_recalled = rbinom(n_g, n_t, group_means)) head(my_data) ## # A tibble: 6 × 7 ## condition group_means row id n_g n_t n_recalled ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Das Kruschke 0.4 1 1 20 20 5 ## 2 Das Kruschke 0.4 2 2 20 20 10 ## 3 Das Kruschke 0.4 3 3 20 20 11 ## 4 Das Kruschke 0.4 4 4 20 20 7 ## 5 Das Kruschke 0.4 5 5 20 20 6 ## 6 Das Kruschke 0.4 6 6 20 20 4 Here are the means for n_recalled, by condition. my_data %&gt;% group_by(condition) %&gt;% summarise(mean_n_recalled = mean(n_recalled)) ## # A tibble: 4 × 2 ## condition mean_n_recalled ## &lt;fct&gt; &lt;dbl&gt; ## 1 Das Kruschke 7.05 ## 2 Mozart 10.2 ## 3 Bach 10.1 ## 4 Beethoven 10.0 12.2.2.1 Model specification in JAGS brms. Recall that although brms does accommodate models based on the Bernoulli likelihood, it doesn’t do so when the data are aggregated. With our aggregate Bernoulli data, we’ll have to use the conventional binomial likelihood, instead. We’ll compute two models. Our full model will be \\[\\begin{align*} \\text{n_recalled}_{ij} &amp; \\sim \\operatorname{Binomial}(n = 20, \\theta_{j}), \\text{where} \\\\ \\operatorname{logit}(\\theta_j) &amp; = \\beta_{0_j}. \\end{align*}\\] In our equation, \\(\\beta_{0_j}\\) is the group-specific intercept within the logistic regression model. We’ll use the \\(N(0, 1.5)\\) prior for the intercept. Though it appears strongly regularizing in the log-odds space, it’s quite flat on the \\(\\theta\\) space. If we wanted to be more conservative in the \\(\\theta\\) space, we might use something more like \\(N(0, 1)\\). fit12.1 &lt;- brm(data = my_data, family = binomial, n_recalled | trials(20) ~ 0 + condition, prior(normal(0, 1.5), class = b), iter = 3000, warmup = 1000, cores = 4, chains = 4, seed = 12, file = &quot;fits/fit12.01&quot;) Here’s the summary for the full model. print(fit12.1) ## Family: binomial ## Links: mu = logit ## Formula: n_recalled | trials(20) ~ 0 + condition ## Data: my_data (Number of observations: 80) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## conditionDasKruschke -0.61 0.10 -0.81 -0.41 1.00 9677 6284 ## conditionMozart 0.05 0.10 -0.14 0.25 1.00 8523 5380 ## conditionBach 0.02 0.10 -0.18 0.21 1.00 9961 6336 ## conditionBeethoven 0.01 0.10 -0.19 0.21 1.00 10292 6409 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Do keep in mind that our results will differ from Kruschke’s because of two factors. First, we simulated slightly different data. In the limit, I suspect our data simulation approaches would converge. But we’re far from the limit. Second, we used a different likelihood to model the data, which resulted in slightly different priors. Yet even with those substantial limitations, our results are pretty close. To make the top portion of Figure 12.5, we’ll need to extract the condition-specific parameters. For that, we’ll employ fixef() and then wrangle a bit. post &lt;- fixef(fit12.1, summary = F) %&gt;% as_tibble() %&gt;% transmute(theta_1 = conditionDasKruschke, theta_2 = conditionMozart, theta_3 = conditionBach, theta_4 = conditionBeethoven) %&gt;% mutate_all(inv_logit_scaled) %&gt;% transmute(`theta[1]-theta[2]` = theta_1 - theta_2, `theta[1]-theta[3]` = theta_1 - theta_3, `theta[1]-theta[4]` = theta_1 - theta_4, `theta[2]-theta[3]` = theta_2 - theta_3, `theta[2]-theta[4]` = theta_2 - theta_4, `theta[3]-theta[4]` = theta_3 - theta_4) glimpse(post) ## Rows: 8,000 ## Columns: 6 ## $ `theta[1]-theta[2]` &lt;dbl&gt; -0.15148211, -0.17075463, -0.17448224, -0.10356167, -0.20511807, -0.11… ## $ `theta[1]-theta[3]` &lt;dbl&gt; -0.17019749, -0.14368880, -0.15623204, -0.08408056, -0.19805737, -0.11… ## $ `theta[1]-theta[4]` &lt;dbl&gt; -0.19184146, -0.08753143, -0.09952291, -0.12529708, -0.17794878, -0.12… ## $ `theta[2]-theta[3]` &lt;dbl&gt; -0.0187153820, 0.0270658273, 0.0182502052, 0.0194811021, 0.0070607053,… ## $ `theta[2]-theta[4]` &lt;dbl&gt; -0.0403593504, 0.0832231996, 0.0749593320, -0.0217354152, 0.0271692912… ## $ `theta[3]-theta[4]` &lt;dbl&gt; -0.021643968, 0.056157372, 0.056709127, -0.041216517, 0.020108586, -0.… Now we have the wrangled data, we’re ready to convert them to the long format and plot the top of Figure 12.5. post %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + geom_vline(xintercept = 0, color = oa[2]) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = oa[4], color = oa[3], breaks = 30, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-.25, .25)) + facet_wrap(~ name, labeller = label_parsed) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Also, do note we’re working with the \\(\\theta\\) parameters in our aggregated binomial models, rather than \\(\\omega\\)’s. Here’s how you’d get the posterior mean and HDI summaries. post %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 6 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 theta[1]-theta[2] -0.156 -0.225 -0.089 0.95 mode hdi ## 2 theta[1]-theta[3] -0.149 -0.222 -0.09 0.95 mode hdi ## 3 theta[1]-theta[4] -0.148 -0.217 -0.083 0.95 mode hdi ## 4 theta[2]-theta[3] 0.008 -0.06 0.074 0.95 mode hdi ## 5 theta[2]-theta[4] 0.01 -0.059 0.082 0.95 mode hdi ## 6 theta[3]-theta[4] -0.007 -0.068 0.07 0.95 mode hdi If we wanted to know what proportion of the difference distributions were greater than zero, we could do something like this. post %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(p = mean(value &gt; 0)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 6 × 2 ## name p ## &lt;chr&gt; &lt;dbl&gt; ## 1 theta[1]-theta[2] 0 ## 2 theta[1]-theta[3] 0 ## 3 theta[1]-theta[4] 0 ## 4 theta[2]-theta[3] 0.599 ## 5 theta[2]-theta[4] 0.613 ## 6 theta[3]-theta[4] 0.526 I got this idea from the great Tristan Mahr, who pointed out that conditional tests like value &gt; 0 compute a vector of TRUE and FALSE values. By nesting that within mean(), you end up with the proportion of those values that are TRUE. With our Stan/brms method, we don’t have an analogue to the lower portion of Figure 12.5 because we are not fitting the full and restricted models within a single run. Thus, there’s no plot to show the chains traversing from \\(M_\\text{full}\\) to \\(M_\\text{restricted}\\). Rather, our fit12.1 was just of \\(M_\\text{full}\\). Now we’ll fit \\(M_\\text{restricted}\\), which we’ll save as fit12.2. fit12.2 &lt;- brm(data = my_data, family = binomial, n_recalled | trials(20) ~ 1, prior(normal(0, 1.5), class = Intercept), iter = 3000, warmup = 1000, cores = 4, chains = 4, seed = 12, file = &quot;fits/fit12.02&quot;) Here we’ll compare the two models with the LOO. fit12.1 &lt;- add_criterion(fit12.1, criterion = &quot;loo&quot;) fit12.2 &lt;- add_criterion(fit12.2, criterion = &quot;loo&quot;) loo_compare(fit12.1, fit12.2) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.1 0.0 0.0 -172.4 4.4 3.3 0.4 344.9 8.8 ## fit12.2 -12.0 5.5 -184.4 6.9 1.2 0.2 368.8 13.8 The LOO comparison suggests fit12.1, the full model with the condition-specific intercepts, is an improvement over the restricted one-intercept-only model. We can also compare the models with their weights via the model_weights() function. Here we’ll use weights = \"loo\" criterion. model_weights(fit12.1, fit12.2, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit12.1 fit12.2 ## 1 0 Recall that within a given comparison, the weights sum to 1, with better fitting models tending closer to 1 than the other(s). In this case, almost all the weight went to the \\(M_\\text{full}\\), fit12.1. 12.2.2.2 Bonus: Hypothesis testing in brms. Disclaimer: I am not a fan of hypothesis testing within the Bayesian framework. Outside of pedagogical material like this, I do not use these methods. However, it’d seem negligent not to at least mention the convenience function designed for that purpose in brms: the hypothesis() function. From the hypothesis.brmsfit section in the brms reference manual (Bürkner, 2022d, p. 109) we read: Among others, hypothesis computes an evidence ratio (Evid.Ratio) for each hypothesis. For a one-sided hypothesis, this is just the posterior probability (Post.Prob) under the hypothesis against its alternative. That is, when the hypothesis is of the form a &gt; b, the evidence ratio is the ratio of the posterior probability of a &gt; b and the posterior probability of a &lt; b. In this example, values greater than one indicate that the evidence in favor of a &gt; b is larger than evidence in favor of a &lt; b. For an two-sided (point) hypothesis, the evidence ratio is a Bayes factor between the hypothesis and its alternative computed via the Savage-Dickey density ratio method. That is the posterior density at the point of interest divided by the prior density at that point. Values greater than one indicate that evidence in favor of the point hypothesis has increased after seeing the data. In order to calculate this Bayes factor, all parameters related to the hypothesis must have proper priors and argument sample_prior of function brm must be set to \"yes\". Otherwise Evid.Ratio (and Post.Prob) will be NA. Please note that, for technical reasons, we cannot sample from priors of certain parameters classes. Most notably, these include overall intercept parameters (prior class \"Intercept\") as well as group-level coefficients. When interpreting Bayes factors, make sure that your priors are reasonable and carefully chosen, as the result will depend heavily on the priors. In particular, avoid using default priors. Following the a &lt; b format, let’s say we wanted to test the hypothesis \\(\\theta_\\text{Das Kruschke} &lt; \\theta_\\text{Bach}\\), based on fit12.1. If we convert the relevant parameters from the log-odds metric to the probability scale with inv_logit_scaled(), we can specify that hypothesis as a string and place it into the hypothesis() function. hypothesis(fit12.1, &quot;inv_logit_scaled(conditionDasKruschke) &lt; inv_logit_scaled(conditionBach)&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star ## 1 (inv_logit_scaled... &lt; 0 -0.15 0.03 -0.21 -0.1 Inf 1 * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. In the Estimate through CI.Upper columns, we got the typical brms summary statistics for model parameters. Those CIs, recall, are ETIs rather than HDIs. To interpret the rest, we read further from the brms reference manual that The Evid.Ratio may sometimes be 0 or Inf implying very small or large evidence, respectively, in favor of the tested hypothesis. For one-sided hypotheses pairs, this basically means that all posterior samples are on the same side of the value dividing the two hypotheses. In that sense, instead of 0 or Inf, you may rather read it as Evid.Ratio smaller 1 / S or greater S, respectively, where S denotes the number of posterior samples used in the computations. The argument alpha specifies the size of the credible interval (i.e., Bayesian confidence interval). For instance, if we tested a two-sided hypothesis and set alpha = 0.05 (\\(5\\%\\)) an, the credible interval will contain 1 -alpha = 0.95 (\\(95\\%\\)) of the posterior values. Hence, alpha * 100\\(\\%\\) of the posterior values will lie foutside of the credible interval. Although this allows testing of hypotheses in a similar manner as in the frequentist null-hypothesis testing framework, we strongly argue against using arbitrary cutoffs (e.g., p &lt; .05) to determine the ‘existence’ of an effect. In this case, the entire posterior distribution (i.e., all the iterations of the chains) was below zero and we ended up with an Evid.Ratio = Inf. Our Bayes factor blew up. If we’d like to test a point null hypothesis, we might reformat the equation to \\(\\theta_\\text{Das Kruschke} = \\theta_\\text{Bach}\\). hypothesis(fit12.1, &quot;inv_logit_scaled(conditionDasKruschke) = inv_logit_scaled(conditionBach)&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star ## 1 (inv_logit_scaled... = 0 -0.15 0.03 -0.22 -0.09 NA NA * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. Here we no longer get summary information in the Evid.Ratio and Post.Prob columns. But we do get that posterior summary information and we also get that little * symbol in the Star column, which was based on the brms default alpha = 0.05. Let’s see what happens when we test a different kind of directional hypothesis, \\(\\theta_\\text{Motzart} - \\theta_\\text{Bach} &gt; 0\\). hypothesis(fit12.1, &quot;inv_logit_scaled(conditionMozart) - inv_logit_scaled(conditionBach) &gt; 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star ## 1 (inv_logit_scaled... &gt; 0 0.01 0.03 -0.05 0.06 1.49 0.6 ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. Here we get an underwhelming BF of 1.49. The posterior probability that hypothesis versus its logical alternative is .59. Notice we no longer have a * in the Star column. One last thing about the hypothesis() function: you can feed it into the plot() function to get a quick plot of the results. Here’s what that looks like from our last example. hypothesis(fit12.1, &quot;inv_logit_scaled(conditionMozart) - inv_logit_scaled(conditionBach) &gt; 0&quot;) %&gt;% plot() We won’t use the hypothesis() much in this ebook. But if you’re interested, there are other tricky ways to make good use of it. To learn more, check out Vourre’s handy blog post, How to calculate contrasts from a fitted brms model. 12.3 Relations of parameter estimation and model comparison Back to the text, Kruschke wrapped up this section by explaining the model comparison focuses on the null value and whether its local probability increases from prior to posterior. The parameter estimation considers the entire posterior distribution, including the uncertainty (i.e., HDI) of the parameter estimate relative to the ROPE. The derivation of the Bayes’ factor by considering the null value in parameter estimation is known as the Savage-Dickey method. A lucid explanation is provided by Wagenmakers, Lodewyckx, Kuriyal, and Grasman (2010), who also provide some historical references and applications to MCMC analysis of hierarchical models. (pp. 353–354) Hey, we just read about that Savage-Dickey method when learning about the brms::hypothesis() function! 12.4 Estimation and model comparison? I’ll leave this for you to decide. Here’s Kruschke: “As mentioned above, neither method for null value assessment (parameter estimation or model comparison) is uniquely ‘correct.’ The two approaches merely pose the question of the null value in different ways” (p. 354). If you’d like to read more on comparisons between the HDI, ROPE, and Bayes factor methods, check out the (2021) simulation study by Linde and colleagues or the follow-up (2021) preprint by Campbell and Gustafson. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.1.2 tidybayes_3.0.2 brms_2.18.0 Rcpp_1.0.9 cowplot_1.1.1 ## [6] forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 ## [11] tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 fishualize_0.2.3 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 igraph_1.3.4 ## [5] svUnit_1.0.6 splines_4.2.0 crosstalk_1.2.0 TH.data_1.1-1 ## [9] rstantools_2.2.0 inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 googlesheets4_1.0.1 ## [17] tzdb_0.3.0 modelr_0.1.8 RcppParallel_5.1.5 matrixStats_0.62.0 ## [21] vroom_1.5.7 xts_0.12.1 sandwich_3.0-2 prettyunits_1.1.1 ## [25] colorspace_2.0-3 rvest_1.0.2 ggdist_3.2.0 haven_2.5.1 ## [29] xfun_0.35 callr_3.7.3 crayon_1.5.2 jsonlite_1.8.3 ## [33] lme4_1.1-31 survival_3.4-0 zoo_1.8-10 glue_1.6.2 ## [37] gtable_0.3.1 gargle_1.2.0 emmeans_1.8.0 distributional_0.3.1 ## [41] pkgbuild_1.3.1 rstan_2.21.7 abind_1.4-5 scales_1.2.1 ## [45] mvtnorm_1.1-3 DBI_1.1.3 miniUI_0.1.1.1 xtable_1.8-4 ## [49] HDInterval_0.2.2 bit_4.0.4 stats4_4.2.0 StanHeaders_2.21.0-7 ## [53] DT_0.24 htmlwidgets_1.5.4 httr_1.4.4 threejs_0.3.3 ## [57] arrayhelpers_1.1-0 posterior_1.3.1 ellipsis_0.3.2 pkgconfig_2.0.3 ## [61] loo_2.5.1 farver_2.1.1 sass_0.4.2 dbplyr_2.2.1 ## [65] utf8_1.2.2 labeling_0.4.2 tidyselect_1.1.2 rlang_1.0.6 ## [69] reshape2_1.4.4 later_1.3.0 munsell_0.5.0 cellranger_1.1.0 ## [73] tools_4.2.0 cachem_1.0.6 cli_3.5.0 generics_0.1.3 ## [77] broom_1.0.1 ggridges_0.5.3 evaluate_0.18 fastmap_1.1.0 ## [81] processx_3.8.0 knitr_1.40 bit64_4.0.5 fs_1.5.2 ## [85] nlme_3.1-159 projpred_2.2.1 mime_0.12 xml2_1.3.3 ## [89] compiler_4.2.0 bayesplot_1.9.0 shinythemes_1.2.0 rstudioapi_0.13 ## [93] gamm4_0.2-6 curl_4.3.2 png_0.1-7 reprex_2.0.2 ## [97] bslib_0.4.0 stringi_1.7.8 highr_0.9 ps_1.7.2 ## [101] Brobdingnag_1.2-8 lattice_0.20-45 Matrix_1.4-1 nloptr_2.0.3 ## [105] markdown_1.1 shinyjs_2.1.0 tensorA_0.36.2 vctrs_0.5.1 ## [109] pillar_1.8.1 lifecycle_1.0.3 jquerylib_0.1.4 bridgesampling_1.1-2 ## [113] estimability_1.4.1 httpuv_1.6.5 R6_2.5.1 bookdown_0.28 ## [117] promises_1.2.0.1 gridExtra_2.3 codetools_0.2-18 boot_1.3-28 ## [121] MASS_7.3-58.1 colourpicker_1.1.1 gtools_3.9.3 assertthat_0.2.1 ## [125] withr_2.5.0 shinystan_2.6.0 multcomp_1.4-20 mgcv_1.8-40 ## [129] parallel_4.2.0 hms_1.1.1 grid_4.2.0 minqa_1.2.5 ## [133] coda_0.19-4 rmarkdown_2.16 googledrive_2.0.0 shiny_1.7.2 ## [137] lubridate_1.8.0 base64enc_0.1-3 dygraphs_1.1.1.6 References Bürkner, P.-C. (2022d). brms reference manual, Version 2.17.0. https://CRAN.R-project.org/package=brms/brms.pdf Campbell, H., &amp; Gustafson, P. (2021). Re: Linde et al.(2021) factor, HDI-ROPE and frequentist equivalence testing are actually all equivalent. https://arxiv.org/abs/2104.07834 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Lakens, D., &amp; Delacre, M. (2018). Equivalence testing and the second generation p-value. https://doi.org/10.31234/osf.io/7k6ay Lakens, D., McLatchie, N., Isager, P. M., Scheel, A. M., &amp; Dienes, Z. (2020). Improving inferences about null effects with Bayes factors and equivalence tests. The Journals of Gerontology: Series B, 75(1), 45–57. https://doi.org/10.1093/geronb/gby065 Lakens, D., Scheel, A. M., &amp; Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963 Lee, M. D., &amp; Webb, M. R. (2005). Modeling individual differences in cognition. Psychonomic Bulletin &amp; Review, 12(4), 605–621. https://doi.org/10.3758/BF03196751 Linde, M., Tendeiro, J. N., Selker, R., Wagenmakers, E.-J., &amp; van Ravenzwaaij, D. (2021). Decisions about equivalence: A comparison of TOST, HDI-ROPE, and the Bayes factor. Psychological Methods. https://doi.org/10.1037/met0000402 Schiettekatte, N. M. D., Brandl, S. J., &amp; Casey, J. M. (2022). fishualize: Color palettes based on fish species [Manual]. https://CRAN.R-project.org/package=fishualize Wagenmakers, E.-J., Lodewyckx, T., Kuriyal, H., &amp; Grasman, R. (2010). Bayesian hypothesis testing for psychologists: A tutorial on the Savage method. Cognitive Psychology, 60(3), 158–189. https://doi.org/10.1016/j.cogpsych.2009.12.001 Zhu, M., &amp; Lu, A. Y. (2004). The counter-intuitive non-informative prior for the Bernoulli family. Journal of Statistics Education, 12(2), 3. https://doi.org/10.1080/10691898.2004.11910734 "],["goals-power-and-sample-size.html", "13 Goals, Power, and Sample Size 13.1 The will to power 13.2 Computing power and sample size 13.3 Sequential testing and the goal of precision 13.4 Discussion Session info", " 13 Goals, Power, and Sample Size Researchers collect data in order to achieve a goal. Sometimes the goal is to show that a suspected underlying state of the world is credible; other times the goal is to achieve a minimal degree of precision on whatever trends are observed. Whatever the goal, it can only be probabilistically achieved, as opposed to definitely achieved, because data are replete with random noise that can obscure the underlying state of the world. Statistical power is the probability of achieving the goal of a planned empirical study, if a suspected underlying state of the world is true. (Kruschke, 2015, p. 359, emphasis in the original) 13.1 The will to power “In this section, [Kruschke laid out a] framework for research and data analysis [that might lead] to a more precise definition of power and how to compute it” (p. 360). 13.1.1 Goals and obstacles. The three research goals Kruschke dealt with in this chapter were: to reject a null value for a parameter, to confirm the legitimacy of a particular parameter value, and to estimate a parameter with reasonable precision. All these could, of course, be extended to contexts involving multiple parameters and all of these were dealt with using 95% HDIs. 13.1.2 Power. Because of random noise, the goal of a study can be achieved only probabilistically. The probability of achieving the goal, given the hypothetical state of the world and the sampling plan, is called the power of the planned research. In traditional null hypothesis significance testing (NHST), power has only one goal (rejecting the null hypothesis), and there is one conventional sampling plan (stop at predetermined sample size) and the hypothesis is only a single specific value of the parameter. In traditional statistics, that is the definition of power. That definition is generalized in this book to include other goals, other sampling plans, and hypotheses that involve an entire distribution on parameters. (p. 361, emphasis in the original) Three primary methods to increase power are: reducing measurement error, increasing the effect size, and increasing the sample size. Kruschke then laid out a five-step procedure to compute power within a Bayesian workflow. Use theory/prior information to specify hypothetical distributions for all parameter values in the model. Use those distributions to generate synthetic data according to the planned sampling method. Fit the proposed model–including the relevant priors–with the synthetic data. Use the posterior to determine whether we attained the research goal. Repeat the procedure many times (i.e., using different set.seed() values) to get a distribution of results. 13.1.3 Sample size. The best that a large sample can do is exactly reflect the data-generating distribution. If the data-generating distribution has considerable mass straddling the null value, then the best we can do is get estimates that include and straddle the null value. As a simple example, suppose that we think that a coin may be biased, and the data-generating hypothesis entertains four possible values of \\(\\theta\\), with \\(p (\\theta = 0.5) = 25 \\%\\), \\(p (\\theta = 0.6) = 25 \\%\\), \\(p (\\theta = 0.7) = 25 \\%\\), and \\(p (\\theta = 0.8) = 25 \\%\\). Because \\(25 \\%\\) of the simulated data come from a fair coin, the maximum probability of excluding \\(\\theta = 0.5\\), even with a huge sample, is \\(75 \\%\\). Therefore, when planning the sample size for an experiment, it is crucial to decide what a realistic goal is. If there are good reasons to posit a highly certain data-generating hypothesis, perhaps because of extensive previous results, then a viable goal may be to exclude a null value. On the other hand, if the data-generating hypothesis is somewhat vague, then a more reasonable goal is to attain a desired degree of precision in the posterior. (p. 364, emphasis in the original) 13.1.4 Other expressions of goals. I’m going to skip over these. In the remainder of the chapter, it will be assumed that the goal of the research is estimation of the parameter values, starting with a viable prior. The resulting posterior distribution is then used to assess whether the goal was achieved. (p. 366) 13.2 Computing power and sample size As our first worked-out example, consider the simplest case: Data from a single coin. Perhaps we are polling a population and we want to precisely estimate the preferences for candidates A or B. Perhaps we want to know if a drug has more than a 50% cure rate. (p. 366) 13.2.1 When the goal is to exclude a null value. Usually it is more intuitively accessible to get prior data, or to think of idealized prior data, than to directly specify a distribution over parameter values. For example, based on knowledge about the application domain, we might have \\(2000\\) actual or idealized flips of the coin for which the result showed \\(65\\%\\) heads. Therefore we’ll describe the data-generating hypothesis as a beta distribution with a mode of \\(0.65\\) and concentration based on \\(2000\\) flips after a uniform “proto-prior”: \\(\\operatorname{beta}(\\theta | 0.65 \\cdot (2000 - 2) + 1, (1 - 0.65) \\cdot (2000 - 2) + 1)\\). (p. 366) We’ll look at that in a plot in just a moment. In the last chapter, we settled on a color palette and augmented our global plotting theme with help from the fishualize package. In this chapter we’ll keep with our fish-centric palette approach, this time based on Chaetodon ephippium. library(tidyverse) library(fishualize) scales::show_col(fish(n = 9, option = &quot;Chaetodon_ephippium&quot;)) ce &lt;- fish(n = 9, option = &quot;Chaetodon_ephippium&quot;) theme_set( theme_grey() + theme(text = element_text(color = ce[1]), axis.text = element_text(color = ce[1]), axis.ticks = element_line(color = ce[1]), legend.background = element_blank(), legend.box.background = element_blank(), legend.key = element_rect(fill = ce[5]), panel.background = element_rect(fill = ce[5], color = ce[4]), panel.grid = element_blank(), strip.background = element_rect(fill = ce[1], color = ce[1]), strip.text = element_text(color = &quot;white&quot;)) ) Here’s what \\(\\operatorname{Beta}(\\theta | 0.65 \\cdot (2{,}000 - 2) + 1, (1 - 0.65) \\cdot (2{,}000 - 2) + 1)\\) looks like. kappa &lt;- 2000 omega &lt;- .65 tibble(theta = seq(from = 0, to = 1, by = .001)) %&gt;% mutate(prior = dbeta(theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% ggplot(aes(x = theta, y = prior)) + geom_area(fill = ce[3]) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(subtitle = expression(&quot;Behold our Beta&quot;*(1299.7*&#39;, &#39;*700.3)*&quot; prior. It&#39;s rather peaked&quot;), x = expression(theta)) If we wanted to take some random draws from that prior, say 5, we’d do something like this. n &lt;- 5 set.seed(13) rbeta(n, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) ## [1] 0.6430548 0.6532279 0.6250891 0.6475884 0.6351476 Now let’s just take one draw and call it bias. n &lt;- 1 set.seed(13) bias &lt;- rbeta(n, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) print(bias) ## [1] 0.6430548 Do note that whereas Kruschke based his discussion on a bias of 0.638, we’re moving forward with our randomly-drawn 0.643. Anyways, now we simulate flipping a coin with that bias \\(N\\) times. The simulated data have \\(z\\) heads and \\(N − z\\) tails. The proportion of heads, \\(z/N\\), will tend to be around \\([0.643]\\), but will be higher or lower because of randomness in the flips. (p. 367) # pick some large number n &lt;- 1e3 set.seed(13) tibble(flips = rbernoulli(n = n, p = bias)) %&gt;% summarise(n = n(), z = sum(flips)) %&gt;% mutate(`proportion of heads` = z / n) ## # A tibble: 1 × 3 ## n z `proportion of heads` ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1000 652 0.652 And indeed our samples did tend around \\(\\theta =.643\\). Had we increased our number of draws by an order of magnitude or two, our proportion of heads would have been even closer to the true data-generating value. Though he presented Table 13.1 in this section, Kruschke walked out how he came to those values in the following sections. We’ll get to them in just a bit. 13.2.2 Formal solution and implementation in R. I’ve been playing around with this a bit. If you look closely at the code block on page 369, you’ll see that Kruschke’s minNforHDIpower() function requires the HDIofICDF() function from his DBDA2E-utilities.R file, which we usually recast as hdi_of_icdf(). hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Just to warm up, consider a beta distribution for which \\(\\omega = .5\\) and \\(\\kappa = 2{,}000\\). Here are the 95% HDIs. omega &lt;- .5 kappa &lt;- 2000 hdi_of_icdf(name = qbeta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) ## [1] 0.4780947 0.5219053 Those look a whole lot like the ROPE values Kruschke specified in his example at the bottom of page 370. But we’re getting ahead of ourselves. Now that we have our hdi_of_icdf() function, we’re ready to define our version of minNforHDIpower(), which I’m calling min_n_for_hdi_power(). min_n_for_hdi_power &lt;- function(gen_prior_mode, gen_prior_n, hdi_max_width = NULL, null_value = NULL, rope = c(max(0, null_value - 0.02), min(1, null_value + 0.02)), desired_power = 0.8, aud_prior_mode = 0.5, aud_prior_n = 2, hdi_mass = 0.95, init_samp_size = 20, verbose = TRUE) { # Check for argument consistency: if (!xor(is.null(hdi_max_width), is.null(null_value))) { stop(&quot;One and only one of `hdi_max_width` and `null_value` must be specified.&quot;) } # Convert prior mode and N to a, b parameters of beta distribution: gen_prior_a &lt;- gen_prior_mode * (gen_prior_n - 2) + 1 gen_prior_b &lt;- (1.0 - gen_prior_mode) * (gen_prior_n - 2) + 1 aud_prior_a &lt;- aud_prior_mode * (aud_prior_n - 2) + 1 aud_prior_b &lt;- (1.0 - aud_prior_mode) * (aud_prior_n - 2) + 1 # Initialize loop for incrementing `sample_size`: sample_size &lt;- init_samp_size not_powerful_enough = TRUE # Increment `sample_size` until desired power is achieved: while(not_powerful_enough) { z_vec &lt;- 0:sample_size # vector of all possible z values for N flips. # Compute probability of each z value for data-generating prior: p_z_vec &lt;- exp(lchoose(sample_size, z_vec) + lbeta(z_vec + gen_prior_a, sample_size - z_vec + gen_prior_b) - lbeta(gen_prior_a, gen_prior_b)) # For each z value, compute posterior HDI: # `hdi_matrix` will hold HDI limits for each z: hdi_matrix &lt;- matrix(0, nrow = length(z_vec), ncol = 2) for (z_id_x in 1:length(z_vec)) { z &lt;- z_vec[z_id_x] hdi_matrix[z_id_x, ] &lt;- hdi_of_icdf(qbeta, shape1 = z + aud_prior_a, shape2 = sample_size - z + aud_prior_b, width = hdi_mass) } # Compute HDI widths: hdi_width &lt;- hdi_matrix[, 2] - hdi_matrix[, 1] # Sum the probabilities of outcomes with satisfactory HDI widths: if (!is.null(hdi_max_width)) { power_hdi &lt;- sum(p_z_vec[hdi_width &lt; hdi_max_width]) } # Sum the probabilities of outcomes with HDI excluding `rope`: if (!is.null(null_value)) { power_hdi &lt;- sum(p_z_vec[hdi_matrix[, 1] &gt; rope[2] | hdi_matrix[, 2] &lt; rope[1]]) } if (verbose) { cat(&quot; For sample size = &quot;, sample_size, &quot;, power = &quot;, power_hdi, &quot;\\n&quot;, sep = &quot;&quot;); flush.console() } if (power_hdi &gt; desired_power) { # If desired power is attained, not_powerful_enough = FALSE } else { sample_size &lt;- sample_size + 1 # set flag to stop, # otherwise # increment the sample size. } } # End while( not_powerful_enough ). # Return the sample size that achieved the desired power: return(sample_size) } Other than altering Kruschke’s formatting a little bit, the only meaningful change I made to the code was removing the line that checked for the HDIofICD() function and then source()ed it, if necessary. Following along with Kruschke on page 370, here’s an example for which \\(\\omega_\\text{data generating} = .75\\), \\(\\kappa = 2{,}000\\), the ROPE is \\([.48, .52]\\), and the desired power is the conventional .8. min_n_for_hdi_power(gen_prior_mode = .75, gen_prior_n = 2000, hdi_max_width = NULL, null_value = .5, rope = c(.48, .52), desired_power = .8, aud_prior_mode = .5, aud_prior_n = 2, hdi_mass = .95, init_samp_size = 20, verbose = TRUE) ## For sample size = 20, power = 0.6159196 ## For sample size = 21, power = 0.5655352 ## For sample size = 22, power = 0.6976802 ## For sample size = 23, power = 0.6521637 ## For sample size = 24, power = 0.606033 ## For sample size = 25, power = 0.7245362 ## For sample size = 26, power = 0.6832871 ## For sample size = 27, power = 0.7836981 ## For sample size = 28, power = 0.7479021 ## For sample size = 29, power = 0.7103786 ## For sample size = 30, power = 0.8009259 ## [1] 30 Just like in the text, the necessary \\(N = 30\\). Unlike in the text, I increased the value of init_samp_size from 5 to 20 to keep the output a reasonable length. To clarify what we just did, in that function call, the data-generating distribution has a mode of \\(0.75\\) and concentration of \\(2000\\), which means that the hypothesized world is pretty certain that coins have a bias of \\(0.75\\). The goal is to exclude a null value of \\(0.5\\) with a ROPE from \\(0.48\\) to \\(0.52\\). The desired power [is] \\(80\\%\\). The audience prior is uniform. When the function is executed, it displays the power for increasing values of sample size, until stopping at \\(N = 30\\). (p. 370) If it’s unclear why the “audience prior is uniform”, consider this. kappa &lt;- 2 omega &lt;- .5 tibble(theta = seq(from = 0, to = 1, by = .01)) %&gt;% mutate(prior = dbeta(theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% ggplot(aes(x = theta, y = prior)) + geom_area(fill = ce[3]) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + coord_cartesian(ylim = c(0, 1.25)) + labs(title = &quot;Behold the uniform audience prior.&quot;, x = expression(theta)) If you work out the algebra with omega and kappa, you’ll see this is a \\(\\operatorname{Beta}(1, 1)\\). Thus, aud_prior_n is \\(\\kappa\\) and aud_prior_mode is \\(\\omega\\). Here we’ll wrap our min_n_for_hdi_power() function into a simple sim_power() function for use with purrr::map2(). sim_power &lt;- function(mode, power) { min_n_for_hdi_power(gen_prior_mode = mode, gen_prior_n = 2000, hdi_max_width = NULL, null_value = .5, rope = c(.48, .52), desired_power = power, aud_prior_mode = .5, aud_prior_n = 2, hdi_mass = .95, init_samp_size = 1, verbose = TRUE) } Here we use the two functions to compute the values in Table 13.1 on page 367. sim &lt;- crossing(mode = seq(from = .6, to = .85, by = .05), power = c(.7, .8, .9)) %&gt;% mutate(results = map2_dbl(mode, power, sim_power)) The results look like this. print(sim) ## # A tibble: 18 × 3 ## mode power results ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.6 0.7 238 ## 2 0.6 0.8 309 ## 3 0.6 0.9 430 ## 4 0.65 0.7 83 ## 5 0.65 0.8 109 ## 6 0.65 0.9 150 ## 7 0.7 0.7 40 ## 8 0.7 0.8 52 ## 9 0.7 0.9 74 ## 10 0.75 0.7 25 ## 11 0.75 0.8 30 ## 12 0.75 0.9 43 ## 13 0.8 0.7 16 ## 14 0.8 0.8 19 ## 15 0.8 0.9 27 ## 16 0.85 0.7 7 ## 17 0.85 0.8 14 ## 18 0.85 0.9 16 It takes just a tiny bit of wrangling to reproduce Table 13.1. sim %&gt;% pivot_wider(names_from = mode, values_from = results) %&gt;% knitr::kable() power 0.6 0.65 0.7 0.75 0.8 0.85 0.7 238 83 40 25 16 7 0.8 309 109 52 30 19 14 0.9 430 150 74 43 27 16 13.2.3 When the goal is precision. Recall that if we have \\(\\operatorname{Beta}(a, b)\\) prior for \\(\\theta\\) of the Bernoulli likelihood function, then the analytic solution for the posterior is \\(\\operatorname{Beta}(\\theta | z + a, N – z + b)\\). In our first example, \\(z = 6\\) out of \\(N = 10\\) randomly selected voters preferred candidate A and we started with a flat \\(\\operatorname{Beta}(\\theta | 1, 1)\\) prior. We can check that our posterior is indeed \\(\\operatorname{Beta}(7, 5)\\) by working through the algebra. z &lt;- 6 n &lt;- 10 # posterior alpha z + 1 ## [1] 7 # posterior beta n - z + 1 ## [1] 5 Here’s how we compute the 95% HDIs. ( h &lt;- hdi_of_icdf(name = qbeta, shape1 = 7, shape2 = 5) ) ## [1] 0.3182322 0.8414276 The \\(\\operatorname{Beta}(7, 5)\\) distribution looks like this. tibble(theta = seq(from = 0, to = 1, by = .01)) %&gt;% mutate(density = dbeta(theta, shape1 = 7, shape2 = 5)) %&gt;% ggplot(aes(x = theta, y = density)) + geom_area(fill = ce[3]) + geom_segment(x = h[1], xend = h[2], y = 0.01, yend = 0.01, size = 1.2, color = ce[9]) + annotate(geom = &quot;text&quot;, x = .6, y = 1/3, label = &quot;95% HDI&quot;, color = &quot;white&quot;) + scale_x_continuous(NULL, breaks = c(0, h[1], z / n, h[2], 1) %&gt;% round(2)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + ggtitle(expression(&quot;Beta&quot;*(7*&quot;, &quot;*5))) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. “It turns out, in this case, that we can never have a sample size large enough to achieve the goal of \\(80\\%\\) of the HDIs falling above \\(\\theta = 0.5\\). To see why,” keep reading in the text (p. 371). Happily, there is a more useful goal, however. Instead of trying to reject a particular value of \\(\\theta\\), we set as our goal a desired degree of precision in the posterior estimate. For example, our goal might be that the \\(95\\%\\) HDI has width less than \\(0.2\\), at least \\(80\\%\\) of the time. (p. 371) If you look back up at our min_n_for_hdi_power() defining code, above, you’ll see that “One and only one of hdi_max_width and null_value must be specified.” So if we want to determine the necessary \\(N\\) for an 95% HDI width of less than .2, we need to set hdi_max_width = .2 and null_value = NULL. min_n_for_hdi_power(gen_prior_mode = .75, gen_prior_n = 10, hdi_max_width = .2, # look here null_value = NULL, rope = NULL, desired_power = .8, aud_prior_mode = .5, aud_prior_n = 2, hdi_mass = .95, init_samp_size = 75, verbose = TRUE) ## For sample size = 75, power = 0.5089359 ## For sample size = 76, power = 0.5337822 ## For sample size = 77, power = 0.5235513 ## For sample size = 78, power = 0.5474934 ## For sample size = 79, power = 0.5706373 ## For sample size = 80, power = 0.5929882 ## For sample size = 81, power = 0.6145578 ## For sample size = 82, power = 0.6353626 ## For sample size = 83, power = 0.6554231 ## For sample size = 84, power = 0.6747629 ## For sample size = 85, power = 0.6934076 ## For sample size = 86, power = 0.7113842 ## For sample size = 87, power = 0.7287209 ## For sample size = 88, power = 0.7716517 ## For sample size = 89, power = 0.787177 ## For sample size = 90, power = 0.8266938 ## [1] 90 Just like in the last section, here I set init_samp_size to a higher value than in the text in order to keep the output reasonably short. To reproduce the results in Table 13.2, we’ll need to adjust the min_n_for_hdi_power() parameters within our sim_power() function. sim_power &lt;- function(mode, power) { min_n_for_hdi_power(gen_prior_mode = mode, gen_prior_n = 10, hdi_max_width = .2, null_value = NULL, rope = NULL, desired_power = power, aud_prior_mode = .5, aud_prior_n = 2, hdi_mass = .95, init_samp_size = 50, verbose = TRUE) } sim &lt;- crossing(mode = seq(from = .6, to = .85, by = .05), power = c(.7, .8, .9)) %&gt;% mutate(results = map2_dbl(mode, power, sim_power)) Let’s make that table. sim %&gt;% pivot_wider(names_from = mode, values_from = results) %&gt;% knitr::kable() power 0.6 0.65 0.7 0.75 0.8 0.85 0.7 91 90 88 86 81 75 0.8 92 92 91 90 87 82 0.9 93 93 93 92 91 89 What did that audience prior look like? kappa &lt;- 2 omega &lt;- .5 tibble(theta = seq(from = 0, to = 1, by = .1)) %&gt;% mutate(density = dbeta(theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% ggplot(aes(x = theta, y = density)) + geom_area(fill = ce[3]) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Behold the uniform audience prior.&quot;, x = expression(theta)) Here are what the beta distributions based on the sim look like. sim %&gt;% rename(n = results) %&gt;% expand(nesting(mode, power, n), theta = seq(from = 0, to = 1, by = .01)) %&gt;% mutate(density = dbeta(theta, shape1 = mode * (n - 2) + 1, shape2 = (1 - mode) * (n - 2) + 1), mode = str_c(&quot;omega == &quot;, mode)) %&gt;% ggplot(aes(x = theta, y = density)) + geom_vline(xintercept = .5, color = ce[8]) + geom_area(fill = ce[3]) + scale_x_continuous(expression(theta), labels = c(&quot;0&quot;, &quot;&quot;, &quot;.5&quot;, &quot;&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;The power and mode values are in the rows and columns, respectively.&quot;) + facet_grid(power ~ mode, labeller = label_parsed) Toward the end of the section, Kruschke mentioned the required sample size shoots up if our desired HDI width is 0.1. Here’s the simulation. sim_power &lt;- function(mode, power) { min_n_for_hdi_power(gen_prior_mode = mode, gen_prior_n = 10, hdi_max_width = .1, null_value = NULL, rope = NULL, desired_power = power, aud_prior_mode = .5, aud_prior_n = 2, hdi_mass = .95, init_samp_size = 300, # save some time and up this parameter verbose = TRUE) } sim &lt;- crossing(mode = seq(from = .6, to = .85, by = .05), power = c(.7, .8, .9)) %&gt;% mutate(results = map2_dbl(mode, power, sim_power)) Display the results in a table like before. sim %&gt;% pivot_wider(names_from = mode, values_from = results) %&gt;% knitr::kable() power 0.6 0.65 0.7 0.75 0.8 0.85 0.7 373 370 364 352 332 303 0.8 378 376 373 367 354 334 0.9 380 380 379 378 373 363 13.2.4 Monte Carlo approximation of power. The previous sections illustrated the ideas of power and sample size for a simple case in which the power could be computed by mathematical derivation. [If your field is like mine, this will not be the norm for your research projects.] In this section, we approximate the power by Monte Carlo simulation. The R script for this simple case serves as a template for more realistic applications. The R script is named Jags-Ydich-Xnom1subj-MbernBeta-Power.R, which is the name for the JAGS program for dichotomous data from a single “subject” suffixed with the word “Power.” As you read through the script, presented below, remember that you can find information about any general R command by using the help function in R, as explained in Section 3.3.1 (p. 39). (p. 372) The code in Kruschke’s Jags-Ydich-Xnom1subj-MbernBeta-Power.R file also makes use of the content in his Jags-Ydich-Xnom1subj-MbernBeta.R file. As is often the case, the code in both is JAGS and base-R centric. We’ll be taking a different approach. I’ll walk you through. First, let’s fire up brms. library(brms) This won’t be of much concern for some of the complex models we’ll be fitting in later chapters. But for simple models like this, a lot of the time you spend waiting for brms::brm() to return your posterior and its summary has to do with compilation time. The issue of compilation goes into technical details I just don’t have the will to go through right now. But if we can avoid or minimize compilation time, it’ll be a boon for our power simulations. As it turns out, we can. The first time we fit our desired model, we have to compile. But once we have that initial fit object in hand, we can reuse it with the update() function, which will allow us to avoid further compilation. So that’s what we’re going to do, here. We’re going to fit the model once and save it. # how many rows of 0&#39;s and 1&#39;s should we have in the data? n &lt;- 74 # should the values in the data be of single draws (i.e., 1), or summaries? size &lt;- 1 # what is the population mode for theta we&#39;d like to base this all on? omega &lt;- .7 # fit that joint fit13.1 &lt;- brm(data = tibble(y = rbinom(n, size, omega)), family = bernoulli(link = identity), y ~ 1, prior(beta(1, 1), class = Intercept, lb = 0, ub = 1), warmup = 1000, iter = 3000, chains = 4, cores = 1, seed = 13, file = &quot;fits/fit13.01&quot;) You may (or not) recall that we covered how to time an operation in R back in Section 3.7.5. When you’re setting up a Monte Carlo power study, it can be important to use those time-tracking skills to get a sense of how long it takes to fit your models. While I was setting this model up, I experimented with keeping the default cores = 1 or setting my typical cores = 4. As it turns out, with a very simple model like this, cores = 1 was a little faster. If you’re fitting one model, that’s no big deal. But in a situation where you’re fitting 100 or 1,000, you’ll want to make sure you’re fitting them as efficiently as possible. But anyway, our practice will be to keep all the specifications in fit constant across the simulations. So choose them wisely. If you look deep into the bowels of the Jags-Ydich-Xnom1subj-MbernBeta.R file, you’ll see Kruschke used the flat \\(\\operatorname{Beta}(1, 1)\\) prior, which is where our prior(beta(1, 1), class = Intercept) code came from. This is the audience prior. We aren’t particularly concerned about the data we simulated with the data = tibble(y = rbinom(n, size, omega)) line. The main thing is that they follow the same basic structure our subsequent data will. To make sure we’re not setting ourselves up to fail, we might make sure the chains look okay. plot(fit13.1, widths = c(2, 3)) Looks like a dream. Let’s move forward and run the simulation proper. In his script file, Kruschke suggested we simulate with large \\(N\\)’s like 1,000 or so. Since this is just an example, I’m gonna cut that to 100. # how many simulations would you like? n_sim &lt;- 100 # specify omega and kappa of the hypothetical parameter distribution omega &lt;- .7 kappa &lt;- 2000 # make it reproducible set.seed(13) sim1 &lt;- # define some of the parameters tibble(n = n, size = size, theta = rbeta(n_sim, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% # simulate the data mutate(data = pmap(list(n, size, theta), rbinom)) %&gt;% # fit the models on the simulated data mutate(fit = map(data, ~update(fit13.1, newdata = list(y = .)))) What have we done? you might ask. head(sim1) ## # A tibble: 6 × 5 ## n size theta data fit ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 74 1 0.693 &lt;int [74]&gt; &lt;brmsfit&gt; ## 2 74 1 0.703 &lt;int [74]&gt; &lt;brmsfit&gt; ## 3 74 1 0.676 &lt;int [74]&gt; &lt;brmsfit&gt; ## 4 74 1 0.698 &lt;int [74]&gt; &lt;brmsfit&gt; ## 5 74 1 0.686 &lt;int [74]&gt; &lt;brmsfit&gt; ## 6 74 1 0.695 &lt;int [74]&gt; &lt;brmsfit&gt; The theta column contains the draws from the hypothesized parameter distribution, which we’ve indicated is hovering tightly around .7. The data column is nested inthe sense that within each row, we’ve saved an entire \\(N = 74\\) row tibble. Most importantly, the fit column contains the brms::brm() objects for each of our 100 simulations. See that last mutate() line, above? That’s where those came from. Within the purrr::map() function, we fed our simulated data sets, one row at a time, into the update() function via the newdata argument. Because we used update() based on our initial fit, we avoided subsequent compilation times and just sampled like a boss. Before we move on, I should give some credit. The foundations of this workflow come from Wickham’s talk, Managing many models with R. I got some additional help on twitter from Phil Straforelli. We still have some work to do. Next, we’ll want to make a custom function that will make it easy to compute the intercept HDIs for each of our fits. library(tidybayes) get_hdi &lt;- function(fit) { fit %&gt;% as_draws_df() %&gt;% # yields the highest-density *continuous* interval mode_hdci(b_Intercept) %&gt;% select(.lower:.upper) } # how does it work? get_hdi(fit13.1) ## # A tibble: 1 × 2 ## .lower .upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.651 0.842 Now we’ll apply that function to our fits tibble to pull those simulated HDIs. Then we’ll program in the markers for the ROPE and HDI width criteria, perform logical tests to see whether they were passed within each of the 100 simulations, and summarize the tests. sim1 %&gt;% # get those HDIs and `unnest()` mutate(hdi = map(fit, get_hdi)) %&gt;% unnest(hdi) %&gt;% # define our test criteria mutate(rope_ll = .48, rope_ul = .52, hdi_width = .2) %&gt;% mutate(pass_rope = .lower &gt; rope_ul | .upper &lt; rope_ll, pass_width = (.upper - .lower) &lt; hdi_width) %&gt;% # summarize those joints summarise(power_rope = mean(pass_rope), power_width = mean(pass_width)) ## # A tibble: 1 × 2 ## power_rope power_width ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.91 0.39 Those are our power estimates. To compute their HDIs, just increase them by a factor of 100 and plug them into the formulas within the shape arguments in hdi_of_icdf(). # HDIs for the ROPE power estimate hdi_of_icdf(name = qbeta, shape1 = 1 + 91, shape2 = 1 + n_sim - 91) %&gt;% round(digits = 2) ## [1] 0.84 0.96 # HDIs for the width power estimate hdi_of_icdf(name = qbeta, shape1 = 1 + 39, shape2 = 1 + n_sim - 39) %&gt;% round(digits = 2) ## [1] 0.30 0.49 Following the middle of page 375, we’ll want to do the whole thing again with \\(\\kappa = 10\\) and \\(N = 91\\). Before we run the next simulation, notice how our first approach had us saving the model fits within our sim1 object. When the models are simple and based on small data and when you’re only simulating 100 times, this isn’t a huge deal. But saving 1,000+ brms::brm() fit objects of hierarchical models will bog you down. So for our next simulation, we’ll only save the HDIs from our get_hdi() function. # how many rows of 0s and 1s should we have in the data? n &lt;- 91 # how many simulations would you like? n_sim &lt;- 100 # specify omega and kappa of the hypothetical parameter distribution omega &lt;- .7 kappa &lt;- 10 # make it reproducible set.seed(13) # simulate sim2 &lt;- tibble(n = n, size = size, theta = rbeta(n_sim, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% mutate(data = pmap(list(n, size, theta), rbinom)) %&gt;% mutate(hdi = map(data, ~update(fit13.1, newdata = list(y = .)) %&gt;% get_hdi())) Since we saved the HDI estimates in the hdi column, we can just unnest() them and summarize our power results. sim2 %&gt;% unnest(hdi) %&gt;% mutate(rope_ll = .48, rope_ul = .52, hdi_width = .2) %&gt;% mutate(pass_rope = .lower &gt; rope_ul | .upper &lt; rope_ll, pass_width = (.upper - .lower) &lt; hdi_width) %&gt;% summarise(power_rope = mean(pass_rope), power_width = mean(pass_width)) ## # A tibble: 1 × 2 ## power_rope power_width ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.71 0.92 Compute the corresponding HDIs. # HDIs for the ROPE power estimate hdi_of_icdf(name = qbeta, shape1 = 1 + 71, shape2 = 1 + n_sim - 71) %&gt;% round(digits = 2) ## [1] 0.62 0.79 # HDIs for the width power estimate hdi_of_icdf(name = qbeta, shape1 = 1 + 92, shape2 = 1 + n_sim - 92) %&gt;% round(digits = 2) ## [1] 0.86 0.96 In general, the [workflow] presented here can be used as a template for power calculations of complex models. Much of the [workflow] remains the same. The most challenging part for complex models is generating the simulated data… Generating simulated data is challenging from a programming perspective merely to get all the details right; patience and perseverance will pay off. (p. 375) 13.2.5 Power from idealized or actual data. In practice, it is often more intuitive to specify actual or idealized data that express the hypothesis, than it is to specify top-level parameter properties. The idea is that we start with the actual or idealized data and then use Bayes’ rule to generate the corresponding distribution on parameter values. (p. 376, emphasis in the original) Here are the idealized parameters Kruschke outlined on pages 377–378. # specify idealized hypothesis: ideal_group_mean &lt;- 0.65 ideal_group_sd &lt;- 0.07 ideal_n_subj &lt;- 100 # more subjects =&gt; higher confidence in hypothesis ideal_n_trl_per_subj &lt;- 100 # more trials =&gt; higher confidence in hypothesis These parameters are for binomial data. To parameterize \\(\\theta\\) in terms of a mean and standard deviation, we need to define the beta_ab_from_mean_sd() function. beta_ab_from_mean_sd &lt;- function(mean, sd) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) kappa &lt;- mean * (1 - mean) / sd^2 - 1 if (kappa &lt;= 0) stop(&quot;invalid combination of mean and sd&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } Now generate data consistent with these values using a tidyverse-style workflow. b &lt;- beta_ab_from_mean_sd(ideal_group_mean, ideal_group_sd) # make the results reproducible set.seed(13) d &lt;- # make a subject index and generate random theta values for idealized subjects tibble(s = 1:ideal_n_subj, theta = rbeta(ideal_n_subj, b$a, b$b)) %&gt;% # transform the theta values to exactly match idealized mean and SD mutate(theta_transformed = ((theta - mean(theta)) / sd(theta)) * ideal_group_sd + ideal_group_mean) %&gt;% # `theta_transformed` must be between 0 and 1 mutate(theta_transformed = ifelse(theta_transformed &gt;= 0.999, 0.999, ifelse(theta_transformed &lt;= 0.001, 0.001, theta_transformed))) %&gt;% # generate idealized data very close to thetas mutate(z = round(theta_transformed * ideal_n_trl_per_subj)) %&gt;% # create vector of 0&#39;s and 1&#39;s matching the z values generated above mutate(y = map(z, ~c(rep(1, .), rep(0, ideal_n_trl_per_subj - .)))) %&gt;% unnest(y) Our main variables are s and y. You can think of the rest as showing our work. Here’s a peek. str(d) ## tibble [10,000 × 5] (S3: tbl_df/tbl/data.frame) ## $ s : int [1:10000] 1 1 1 1 1 1 1 1 1 1 ... ## $ theta : num [1:10000] 0.604 0.604 0.604 0.604 0.604 ... ## $ theta_transformed: num [1:10000] 0.601 0.601 0.601 0.601 0.601 ... ## $ z : num [1:10000] 60 60 60 60 60 60 60 60 60 60 ... ## $ y : num [1:10000] 1 1 1 1 1 1 1 1 1 1 ... We are going to follow the same procedure we did when we originally fit the model to the therapeutic touch data in Chapter 9. Instead of reproducing the model Kruschke presented in his scripts, we are going to fit a hierarchical logistic regression model. fit13.2 &lt;- brm(data = d, family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 13, file = &quot;fits/fit13.02&quot;) Unlike in the text, we had no need for thinning our chains. Our effective sample size estimates were fine. print(fit13.2) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + (1 | s) ## Data: d (Number of observations: 10000) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~s (Number of levels: 100) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.23 0.03 0.17 0.29 1.00 1779 1920 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.63 0.03 0.57 0.69 1.00 3559 2871 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s a look at our two main parameters, our version of the top panels of Figure 13.3. as_draws_df(fit13.2) %&gt;% pivot_longer(b_Intercept:sd_s__Intercept) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = ce[3], color = ce[9], breaks = 40, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Remember, these are in the log-odds metric.&quot;, x = NULL) + facet_wrap(~ name, scales = &quot;free&quot;) Now we have a distribution of parameter values consistent with our idealized hypothesis, but we did not have to figure out the top-level constants in the model. We merely specified the idealized tendencies in the data and expressed our confidence by its amount… So we now have a large set of representative parameter values for conducting a power analysis. (pp. 378–379) With brms, you can sample from those model-implied parameter values with the fitted() function. By default, it will return values in the probability metric for our logistic model. Here we’ll specify a group-level (i.e., s) value that was not in the data. We’ll feed that new value into the newdata argument and set allow_new_levels = T. We’ll also set summary = F, which will return actual probability values rather than a summary. set.seed(13) f &lt;- fitted(fit13.2, newdata = tibble(s = 0), allow_new_levels = T, summary = F) %&gt;% data.frame() %&gt;% set_names(&quot;theta&quot;) str(f) ## &#39;data.frame&#39;: 4000 obs. of 1 variable: ## $ theta: num 0.678 0.6 0.704 0.665 0.587 ... Here’s what that looks like. f %&gt;% ggplot(aes(x = theta, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = ce[3], color = ce[9], breaks = 20) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Behold our \\&quot;distribution of parameter values consistent\\nwith our idealized hypothesis.\\&quot;&quot;, x = expression(theta)) + xlim(0, 1) We can make a custom function to sample from \\(\\theta\\). We might call it sample_theta(). sample_theta &lt;- function(seed, n_subj) { set.seed(seed) bind_cols(s = 1:n_subj, sample_n(f, size = n_subj, replace = T)) } # take it for a spin sample_theta(seed = 13, n_subj = 5) ## s theta ## 1 1 0.6877173 ## 2 2 0.5817205 ## 3 3 0.7514178 ## 4 4 0.4830872 ## 5 5 0.6417880 Now let’s say I wanted to use our little sample_theta() function to sample \\(\\theta\\) values for three people s and then use those \\(\\theta\\) values to sample three draws from the corresponding Bernoulli distribution. We might do that like this. sample_theta(seed = 13, n_subj = 3) %&gt;% mutate(y = map(theta, rbinom, n = 3, size = 1)) %&gt;% unnest(y) ## # A tibble: 9 × 3 ## s theta y ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 0.688 1 ## 2 1 0.688 0 ## 3 1 0.688 1 ## 4 2 0.582 1 ## 5 2 0.582 0 ## 6 2 0.582 0 ## 7 3 0.751 1 ## 8 3 0.751 1 ## 9 3 0.751 0 Notice how after we sampled from \\(\\theta\\), we still needed to take two more steps to simulate the desired data. So perhaps a better approach would be to wrap all those steps into one function and call it something like sample_data(). sample_data &lt;- function(seed, n_subj, n_trial) { set.seed(seed) bind_cols(s = 1:n_subj, sample_n(f, size = n_subj, replace = T)) %&gt;% mutate(y = map(theta, rbinom, n = n_trial, size = 1)) %&gt;% unnest(y) } # test it out sample_data(seed = 13, n_subj = 3, n_trial = 3) ## # A tibble: 9 × 3 ## s theta y ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 0.688 1 ## 2 1 0.688 0 ## 3 1 0.688 1 ## 4 2 0.582 1 ## 5 2 0.582 0 ## 6 2 0.582 0 ## 7 3 0.751 1 ## 8 3 0.751 1 ## 9 3 0.751 0 Here’s how we’d use our sample_data() function to make several data sets within the context of a nested tibble. tibble(seed = 1:4) %&gt;% mutate(data = map(seed, sample_data, n_subj = 14, n_trial = 47)) ## # A tibble: 4 × 2 ## seed data ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;tibble [658 × 3]&gt; ## 2 2 &lt;tibble [658 × 3]&gt; ## 3 3 &lt;tibble [658 × 3]&gt; ## 4 4 &lt;tibble [658 × 3]&gt; With this data type, Kruschke indicated he ran the power analysis twice, using different selections of subjects and trials. In both cases there [was] a total of \\(658\\) trials, but in the first case there [were] \\(14\\) subjects with \\(47\\) trials per subject, and in the second case there [were] seven subjects with \\(94\\) trials per subject. (p. 381) Before running the simulations in full, we fit the model once and save that fit to iteratively reuse with update(). # how many subjects should we have? n_subj &lt;- 14 # how many trials should we have? n &lt;- 47 # fit that joint fit13.3 &lt;- brm(data = sample_theta(seed = 13, n_subj = 14) %&gt;% mutate(y = map(theta, rbinom, n = n, size = 1)) %&gt;% unnest(y), family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 13, file = &quot;fits/fit13.03&quot;) Check real quick to make sure the fit turned out okay. print(fit13.3) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + (1 | s) ## Data: sample_theta(seed = 13, n_subj = 14) %&gt;% mutate(y (Number of observations: 658) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~s (Number of levels: 14) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.24 0.14 0.02 0.54 1.01 1215 1608 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.66 0.11 0.44 0.87 1.00 2622 2358 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Looks fine. Our new model simulation carries with it some new goals. In this example, [Kruschke] considered goals for achieving precision and exceeding a ROPE around the null value, at both the group level and individual level. For the group level, the goals are for the \\(95\\%\\) HDI on the group mode, \\(\\omega\\), to fall above the ROPE around the null value, and for the width of the HDI to be less than \\(0.2\\). For the individual level, the goals are for at least one of the \\(\\theta_s\\)s \\(95\\%\\) HDIs to exceed the ROPE with none that fall below the ROPE, and for all the \\(\\theta_s\\)s \\(95\\%\\) HDIs to have widths less than \\(0.2\\). (pp. 379–380) Now since we used an aggregated binomial model, we don’t have a population-level \\(\\omega\\) parameter. Rather, we have a population \\(\\theta\\). So like before, our first goal is for the population \\(\\theta\\) to fall above the range \\([.48, .52]\\). The second corresponding width goal is also like before; we want \\(\\theta\\) to have a width of less than 0.2. But since our aggregated binomial model parameterized \\(\\theta\\) in the log-odds metric, we’ll have to update our get_hdi() function, which we’ll strategically rename get_theta_hdi(). get_theta_hdi &lt;- function(fit) { fit %&gt;% as_draws_df() %&gt;% transmute(theta = inv_logit_scaled(b_Intercept)) %&gt;% # yields the highest-density *continuous* interval mode_hdci() %&gt;% select(.lower:.upper) } # how does it work? get_theta_hdi(fit13.3) ## # A tibble: 1 × 2 ## .lower .upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.612 0.707 As for the individual-level goals, the two Kruschke outlined in the text apply to our model in a straightforward way. But we will need one more custom function designed to pull the \\(\\theta_s\\)’s for the \\(\\theta_s\\)’s. Let’s call this one get_theta_s_hdi(). get_theta_s_hdi &lt;- function(fit) { n_col &lt;- coef(fit, summary = F)$s[, , &quot;Intercept&quot;] %&gt;% ncol() coef(fit, summary = F)$s[, , &quot;Intercept&quot;] %&gt;% data.frame() %&gt;% set_names(1:n_col) %&gt;% mutate_all(inv_logit_scaled) %&gt;% pivot_longer(everything(), names_to = &quot;s&quot;) %&gt;% mutate(s = as.numeric(s)) %&gt;% group_by(s) %&gt;% # yields the highest-density *continuous* interval mode_hdci(value) %&gt;% select(s, .lower:.upper) %&gt;% rename(.lower_s = .lower, .upper_s = .upper) } # how does it work? get_theta_s_hdi(fit13.3) ## # A tibble: 14 × 3 ## s .lower_s .upper_s ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.624 0.819 ## 2 2 0.535 0.726 ## 3 3 0.611 0.785 ## 4 4 0.512 0.708 ## 5 5 0.548 0.733 ## 6 6 0.601 0.780 ## 7 7 0.575 0.748 ## 8 8 0.564 0.744 ## 9 9 0.506 0.705 ## 10 10 0.573 0.752 ## 11 11 0.607 0.786 ## 12 12 0.518 0.711 ## 13 13 0.572 0.744 ## 14 14 0.582 0.753 With sim2, we avoided saving our model brms::brm() fit objects by using map(data, ~update(fit1, newdata = list(y = .)) %&gt;% get_hdi()). That is, within the purrr::map() function, we first used update() to update the fit to the new data and then pumped that directly into get_hdi(), which simply returned our intervals. Though slick, this approach won’t work here because we want to pump our updated model fit into two functions, both get_theta_hdi() and get_theta_s_hdi(). Our work-around will be to make a custom function that updates the fit, saves it as an object, inserts that fit object into both get_theta_hdi() and get_theta_s_hdi(), binds their results together, and the only returns the intervals. We’ll call this function fit_then_hdis(). fit_then_hdis &lt;- function(data, seed) { fit &lt;- update(fit13.3, newdata = data, seed = seed) cbind(get_theta_hdi(fit), get_theta_s_hdi(fit)) } Now we’re ready to simulate. # how many subjects should we have? n_subj &lt;- 14 # how many trials should we have? n_trial &lt;- 47 # how many simulations would you like? n_sim &lt;- 100 sim3 &lt;- tibble(seed = 1:n_sim) %&gt;% mutate(data = map(seed, sample_data, n_subj = n_subj, n_trial = n_trial)) %&gt;% mutate(hdi = map2(data, seed, fit_then_hdis)) If we hold these by the criteria of each \\(\\text{HDI}_{\\theta_s} &gt; \\text{ROPE}\\) and all to have widths less than 0.2, It looks like our initial data-generating fit13.3 is in the ballpark. Here are the results for the full power analysis, sim3. sim3 &lt;- sim3 %&gt;% unnest(hdi) %&gt;% # here we determine whether we passed at the group level mutate(pass_rope_theta = .lower &gt; .52 | .upper &lt; .48, pass_width_theta = (.upper - .lower) &lt; .2) %&gt;% # the s-level thetas require two steps. # first, we&#39;ll outline the three criteria mutate(exceed_rope_theta_s = .lower_s &gt; .52, below_rope_theta_s = .upper_s &lt; .48, narrow_width_theta_s = (.upper_s - .lower_s) &lt; .2) %&gt;% # second, we&#39;ll evaluate those criteria by group group_by(seed) %&gt;% mutate(pass_rope_theta_s = sum(exceed_rope_theta_s) &gt; 0 &amp; sum(below_rope_theta_s) == 0, pass_width_theta_s = mean(narrow_width_theta_s) == 1) %&gt;% ungroup() head(sim3) Summarize the results. sim3 %&gt;% summarise(power_rope_theta = mean(pass_rope_theta), power_width_theta = mean(pass_width_theta)) ## # A tibble: 1 × 2 ## power_rope_theta power_width_theta ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 sim3 %&gt;% summarise(power_rope_theta_s = mean(pass_rope_theta_s), power_width_theta_s = mean(pass_width_theta_s)) ## # A tibble: 1 × 2 ## power_rope_theta_s power_width_theta_s ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.35 The power estimates for power_rope_theta, power_width_theta, and power_rope_theta_s were all the same, 1. Only the estimate for power_width_theta_s was unique. Here are the two sets of HDIs for the power estimate values. hdi_of_icdf(name = qbeta, shape1 = 1 + 100, shape2 = 1 + n_sim - 100) %&gt;% round(digits = 2) ## [1] 0.97 1.00 hdi_of_icdf(name = qbeta, shape1 = 1 + 35, shape2 = 1 + n_sim - 35) %&gt;% round(digits = 2) ## [1] 0.26 0.45 Hopefully it isn’t a surprise our values differ from those in the text. We (a) used a different model and (b) used fewer simulation iterations. But I trust you get the overall idea. Like in the text, let’s do the simulation again. # how many subjects should we have? n_subj &lt;- 7 # how many trials should we have? n_trial &lt;- 94 # how many simulations would you like? n_sim &lt;- 100 sim4 &lt;- tibble(seed = 1:n_sim) %&gt;% mutate(data = map(seed, sample_data, n_subj = n_subj, n_trial = n_trial)) %&gt;% mutate(hdi = map2(data, seed, fit_then_hdis)) Wrangle before summarizing. sim4 &lt;- sim4 %&gt;% unnest(hdi) %&gt;% mutate(pass_rope_theta = .lower &gt; .52 | .upper &lt; .48, pass_width_theta = (.upper - .lower) &lt; .2) %&gt;% mutate(exceed_rope_theta_s = .lower_s &gt; .52, below_rope_theta_s = .upper_s &lt; .48, narrow_width_theta_s = (.upper_s - .lower_s) &lt; .2) %&gt;% group_by(seed) %&gt;% mutate(pass_rope_theta_s = sum(exceed_rope_theta_s) &gt; 0 &amp; sum(below_rope_theta_s) == 0, pass_width_theta_s = mean(narrow_width_theta_s) == 1) %&gt;% ungroup() Summarize the results. sim4 %&gt;% summarise(power_rope_theta = mean(pass_rope_theta), power_width_theta = mean(pass_width_theta)) ## # A tibble: 1 × 2 ## power_rope_theta power_width_theta ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.97 0.95 sim4 %&gt;% summarise(power_rope_theta_s = mean(pass_rope_theta_s), power_width_theta_s = mean(pass_width_theta_s)) ## # A tibble: 1 × 2 ## power_rope_theta_s power_width_theta_s ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.88 Now compute the HDIs for power_rope_theta and power_width_theta. hdi_of_icdf(name = qbeta, shape1 = 1 + 97, shape2 = 1 + n_sim - 97) %&gt;% round(digits = 2) ## [1] 0.92 0.99 hdi_of_icdf(name = qbeta, shape1 = 1 + 95, shape2 = 1 + n_sim - 95) %&gt;% round(digits = 2) ## [1] 0.90 0.98 Second, we now compute the HDIs for power_rope_theta_s and power_width_theta_s. hdi_of_icdf(name = qbeta, shape1 = 1 + 100, shape2 = 1 + n_sim - 100) %&gt;% round(digits = 2) ## [1] 0.97 1.00 hdi_of_icdf(name = qbeta, shape1 = 1 + 88, shape2 = 1 + n_sim - 88) %&gt;% round(digits = 2) ## [1] 0.81 0.93 The results from our simulations contrast with those in the text. Though the results are similar with respect to \\(\\theta_s\\), they are markedly different with regards to our \\(\\theta\\) versus the text’s \\(\\omega\\). But Kruschke’s point is still sound: This example illustrates a general trend in hierarchical estimates. If you want high precision at the individual level, you need lots of data within individuals. If you want high precision at the group level, you need lots of individuals (without necessarily lots of data per individual, but more is better). (p. 382) Here are the new idealized settings from the lower part of page 382. ideal_group_mean &lt;- 0.65 ideal_group_sd &lt;- 0.07 ideal_n_subj &lt;- 10 # instead of 100 ideal_n_trl_per_subj &lt;- 10 # instead of 100 b &lt;- beta_ab_from_mean_sd(ideal_group_mean, ideal_group_sd) set.seed(13) d &lt;- tibble(s = 1:ideal_n_subj, theta = rbeta(ideal_n_subj, b$a, b$b)) %&gt;% mutate(theta_transformed = ((theta - mean(theta)) / sd(theta)) * ideal_group_sd + ideal_group_mean) %&gt;% mutate(theta_transformed = ifelse(theta_transformed &gt;= 0.999, 0.999, ifelse(theta_transformed &lt;= 0.001, 0.001, theta_transformed))) %&gt;% mutate(z = round(theta_transformed * ideal_n_trl_per_subj)) %&gt;% mutate(y = map(z, ~c(rep(1, .), rep(0, ideal_n_trl_per_subj - .)))) %&gt;% unnest(y) head(d) ## # A tibble: 6 × 5 ## s theta theta_transformed z y ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.604 0.659 7 1 ## 2 1 0.604 0.659 7 1 ## 3 1 0.604 0.659 7 1 ## 4 1 0.604 0.659 7 1 ## 5 1 0.604 0.659 7 1 ## 6 1 0.604 0.659 7 1 Fit the \\(\\theta\\)-generating model. fit13.4 &lt;- update(fit13.2, newdata = d, cores = 4, seed = 13, file = &quot;fits/fit13.04&quot;) ## The desired updates require recompiling the model Check to make sure things look alright. print(fit13.4) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + (1 | s) ## Data: d (Number of observations: 100) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~s (Number of levels: 10) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.23 0.19 0.01 0.68 1.00 2519 1793 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.62 0.22 0.19 1.07 1.00 3798 2940 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s a look at our two main parameters, our version of the bottom panels of Figure 13.3. as_draws_df(fit13.4) %&gt;% pivot_longer(b_Intercept:sd_s__Intercept) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = ce[3], color = ce[9], breaks = 40, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Remember, these are in the log-odds metric.&quot;, x = NULL) + facet_wrap(~ name, scales = &quot;free&quot;) Now redefine our fitted() object, f, which gets pumped into the sample_data() function. set.seed(13) f &lt;- fitted(fit13.4, newdata = tibble(s = 0), allow_new_levels = T, summary = F) %&gt;% data.frame() %&gt;% set_names(&quot;theta&quot;) Here’s what our updated distribution of \\(\\theta\\) values looks like. f %&gt;% ggplot(aes(x = theta, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = c(.95, .5), fill = ce[3], color = ce[9], breaks = 40) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Behold our \\&quot;distribution of parameter values consistent\\nwith our idealized hypothesis.\\&quot;&quot;, x = expression(theta)) + coord_cartesian(xlim = c(0, 1)) Note the distribution is wider than the previous one. Anyway, now we’re good to go. Here’s our version of the first power analysis for these settings. # how many subjects should we have? n_subj &lt;- 14 # how many trials should we have? n_trial &lt;- 47 # how many simulations would you like? n_sim &lt;- 100 sim5 &lt;- tibble(seed = 1:n_sim) %&gt;% mutate(data = map(seed, sample_data, n_subj = n_subj, n_trial = n_trial)) %&gt;% mutate(hdi = map2(data, seed, fit_then_hdis)) Wrangle before summarizing. sim5 &lt;- sim5 %&gt;% unnest(hdi) %&gt;% mutate(pass_rope_theta = .lower &gt; .52 | .upper &lt; .48, pass_width_theta = (.upper - .lower) &lt; .2) %&gt;% mutate(exceed_rope_theta_s = .lower_s &gt; .52, below_rope_theta_s = .upper_s &lt; .48, narrow_width_theta_s = (.upper_s - .lower_s) &lt; .2) %&gt;% group_by(seed) %&gt;% mutate(pass_rope_theta_s = sum(exceed_rope_theta_s) &gt; 0 &amp; sum(below_rope_theta_s) == 0, pass_width_theta_s = mean(narrow_width_theta_s) == 1) %&gt;% ungroup() Summarize the results. sim5 %&gt;% summarise(power_rope_theta = mean(pass_rope_theta), power_width_theta = mean(pass_width_theta)) ## # A tibble: 1 × 2 ## power_rope_theta power_width_theta ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 sim5 %&gt;% summarise(power_rope_theta_s = mean(pass_rope_theta_s), power_width_theta_s = mean(pass_width_theta_s)) ## # A tibble: 1 × 2 ## power_rope_theta_s power_width_theta_s ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.99 0.21 First compute the HDIs for power_rope_theta and power_width_theta. hdi_of_icdf(name = qbeta, shape1 = 1 + 100, shape2 = 1 + n_sim - 100) %&gt;% round(digits = 2) ## [1] 0.97 1.00 hdi_of_icdf(name = qbeta, shape1 = 1 + 100, shape2 = 1 + n_sim - 100) %&gt;% round(digits = 2) ## [1] 0.97 1.00 Second, we compute the HDIs for power_rope_theta_s and power_width_theta_s. hdi_of_icdf(name = qbeta, shape1 = 1 + 99, shape2 = 1 + n_sim - 99) %&gt;% round(digits = 2) ## [1] 0.95 1.00 hdi_of_icdf(name = qbeta, shape1 = 1 + 21, shape2 = 1 + n_sim - 21) %&gt;% round(digits = 2) ## [1] 0.14 0.30 The classical definition of power in NHST assumes a specific value for the parameters without any uncertainty. The classical approach can compute power for different specific parameter values, but the approach does not weigh the different values by their credibility. One consequence is that for the classical approach, retrospective power is extremely uncertain, rendering it virtually useless, because the estimated powers at the two ends of the confidence interval are close to the baseline false alarm rate and \\(100\\%\\) (Gerard, Smith, &amp; Weerakkody, 2009; Nakagawa &amp; Foster, 2004; O’Keefe, 2007; Steidl, Hayes, &amp; Schauber, 1997; Sun, Pan, &amp; Wang, 2011; L. Thomas, 1997). (p. 383) 13.3 Sequential testing and the goal of precision In classical power analysis, it is assumed that the goal is to reject the null hypothesis. For many researchers, the sine qua non of research is to reject the null hypothesis. The practice of NHST is so deeply institutionalized in scientific journals that it is difficult to get research findings published without showing “significant” results, in the sense of \\(p &lt; 0.05\\). As a consequence, many researchers will monitor data as they are being collected and stop collecting data only when \\(p &lt; 0.05\\) (conditionalizing on the current sample size) or when their patience runs out. This practice seems intuitively not to be problematic because the data collected after testing previous data are not affected by the previously collected data. For example, if I flip a coin repeatedly, the probability of heads on the next flip is not affected by whether or not I happened to check whether \\(p &lt; 0.05\\) on the previous flip. Unfortunately, that intuition about independence across flips only tells part of story. What’s missing is the realization that the stopping procedure biases which data are sampled, because the procedure stops only when extreme values happen to be randomly sampled… The remainder of this section shows examples of sequential testing with different decision criteria. We consider decisions by \\(p\\) values, BFs, HDIs with ROPEs, and precision. We will see that decisions by \\(p\\) values not only lead to \\(100\\%\\) false alarms (with infinite patience), but also lead to biased estimates that are more extreme than the true value. The two Bayesian methods both can decide to accept the null hypothesis, and therefore do not lead to \\(100\\%\\) false alarms, but both do produce biased estimates because they stop when extreme values are sampled. Stopping when precision is achieved produces accurate estimates. (pp. 383–385, emphasis in the original) 13.3.1 Examples of sequential tests. To start our sequence of simulated coin flips, we’ll set the total number of trials we’d like, n_trial, specify our bias, and set out seed. Like Kruschke did in the text, we’ll do \\(\\theta = .5\\) first. n_trial &lt;- 700 bias &lt;- .5 set.seed(13) coin.5 &lt;- tibble(n = 1:n_trial, flip = rbinom(n = n_trial, size = 1, prob = bias)) %&gt;% mutate(z = cumsum(flip)) head(coin.5) ## # A tibble: 6 × 3 ## n flip z ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 ## 2 2 0 1 ## 3 3 0 1 ## 4 4 0 1 ## 5 5 1 2 ## 6 6 0 2 Here’s a little custom function that will fit frequentist logistic regression models for each combination of n and z and then extract the associated \\(p\\)-value. fit_glm &lt;- function(n, z) { d &lt;- tibble(y = rep(1:0, times = c(z, n - z))) glm(data = d, y ~ 1, family = binomial(link = &quot;logit&quot;)) %&gt;% broom::tidy() %&gt;% select(p.value) %&gt;% pull() } # here&#39;s how it works fit_glm(n = 5, z = 2) ## [1] 0.6569235 Use fit_glm() to compute the \\(p\\)-values. coin.5 &lt;- coin.5 %&gt;% mutate(p = map2_dbl(n, z, fit_glm)) head(coin.5) ## # A tibble: 6 × 4 ## n flip z p ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1 1.00 ## 2 2 0 1 1 ## 3 3 0 1 0.571 ## 4 4 0 1 0.341 ## 5 5 1 2 0.657 ## 6 6 0 2 0.423 For the Bayes factors, Kruschke indicated these were computed based on Equation 12.3 from page 344. That equation followed the form \\[ \\frac{p(z, N | M_\\text{alt})}{p(z, N | M_\\text{null})} = \\frac{B (z + a_\\text{alt}, N - z + b_\\text{alt}) / B (a_\\text{alt}, b_\\text{alt})}{\\theta_\\text{null}^z (1 - \\theta_\\text{null})^{(N - z)}}. \\] To ground ourselves a bit, here’s some of the content from the page that followed the equation: For a default alternative prior, the beta distribution is supposed to be uninformed, according to particular mathematical criteria. Intuition might suggest that a uniform distribution suits this requirement, that is, \\(\\operatorname{beta} (\\theta | 1, 1)\\). Instead, some argue that the most appropriate uninformed beta distribution is \\(\\operatorname{beta}(\\theta | \\epsilon, \\epsilon)\\), where \\(\\epsilon\\) is a small number approaching zero (p. 344) The \\(\\operatorname{Beta}(\\epsilon, \\epsilon)\\), recall, is the Haldane prior. Often times, \\(\\epsilon = 0.01\\). That will be our approach here, too. Let’s make another custom function. log_bf &lt;- function(n, z, theta) { # define epsilon for the Haldane prior e &lt;- 0.01 # compute p(d | H_0) p_d_null &lt;- theta ^ z * (1 - theta) ^ (n - z) # compute p(d | H_1) p_d_alt &lt;- beta(z + e, n - z + e) / beta(e, e) # compute BF bf &lt;- p_d_alt / p_d_null # take the log log(bf) } Here’s how it works. log_bf(n = 6, z = 2, theta = bias) ## [1] -4.152328 Now we’ll use it in bulk. coin.5 &lt;- coin.5 %&gt;% mutate(log_bf = map2_dbl(n, z, log_bf, theta = .5)) head(coin.5) ## # A tibble: 6 × 5 ## n flip z p log_bf ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1.00 0 ## 2 2 0 1 1 -3.93 ## 3 3 0 1 0.571 -3.93 ## 4 4 0 1 0.341 -3.65 ## 5 5 1 2 0.657 -4.33 ## 6 6 0 2 0.423 -4.15 To compute the HDIs for each iteration, we’ll want to use the hdi_of_qbeta() function from Chapters 10 and 12. hdi_of_qbeta &lt;- function(shape1, shape2) { hdi_of_icdf(name = qbeta, shape1 = shape1, shape2 = shape2) %&gt;% data.frame() %&gt;% mutate(level = c(&quot;ll&quot;, &quot;ul&quot;)) %&gt;% spread(key = level, value = &quot;.&quot;) } Here’s how it works. hdi_of_qbeta(3, 3) ## ll ul ## 1 0.1466328 0.8533672 Put it to use. coin.5 &lt;- coin.5 %&gt;% mutate(hdi = map2(n, z, ~hdi_of_qbeta(.y + 1, .x - .y + 1))) %&gt;% unnest(hdi) %&gt;% mutate(width = ul - ll) head(coin.5) ## # A tibble: 6 × 8 ## n flip z p log_bf ll ul width ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1.00 0 0.224 1.00 0.776 ## 2 2 0 1 1 -3.93 0.0943 0.906 0.811 ## 3 3 0 1 0.571 -3.93 0.0438 0.772 0.729 ## 4 4 0 1 0.341 -3.65 0.0260 0.670 0.644 ## 5 5 1 2 0.657 -4.33 0.105 0.761 0.656 ## 6 6 0 2 0.423 -4.15 0.0805 0.685 0.604 We’re finally ready to define the five subplots for our version of Figure 13.4. p1 &lt;- coin.5 %&gt;% ggplot(aes(x = n, y = z / n)) + geom_hline(yintercept = .5, color = ce[9]) + geom_line(color = ce[1]) + geom_point(size = 2/3, color = ce[1]) + scale_x_continuous(NULL, breaks = 0:7 * 100) + scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) p2 &lt;- coin.5 %&gt;% ggplot(aes(x = n, y = p)) + geom_hline(yintercept = .05, color = ce[9]) + geom_line(aes(color = p &lt; .05)) + geom_point(aes(color = p &lt; .05), size = 2/3) + scale_color_manual(values = ce[c(2, 7)], breaks = NULL) + scale_x_continuous(NULL, breaks = 0:7 * 100) + scale_y_continuous(expression(italic(p)*&quot;-value&quot;), expand = expansion(mult = 0), limits = c(0, 1)) p3 &lt;- coin.5 %&gt;% ggplot(aes(x = n, y = log_bf)) + geom_hline(yintercept = -1.1, color = ce[9]) + geom_line(color = ce[7]) + geom_point(aes(color = log_bf &lt; -1.1 | log_bf &gt; 1.1), alpha = 1/2, size = 2/3) + annotate(geom = &quot;text&quot;, x = 60, y = -1.5, label = &quot;accept the null&quot;, color = ce[1]) + scale_color_manual(values = ce[c(2, 7)], breaks = NULL) + scale_x_continuous(NULL, breaks = 0:7 * 100) + ylab(expression(log(BF))) p4 &lt;- coin.5 %&gt;% ggplot(aes(x = n)) + geom_hline(yintercept = c(.45, .55), color = ce[9]) + geom_linerange(aes(ymin = ll, ymax = ul, color = ll &gt; .45 &amp; ul &lt; .55), alpha = 1/2) + scale_color_manual(values = ce[c(2, 7)], breaks = NULL) + scale_x_continuous(NULL, breaks = 0:7 * 100) + scale_y_continuous(&quot;95% HDI&quot;, expand = expansion(mult = 0), limits = c(0, 1)) p5 &lt;- coin.5 %&gt;% ggplot(aes(x = n, y = width)) + geom_hline(yintercept = .08, color = ce[9]) + geom_line(aes(color = width &lt; .08)) + geom_point(aes(color = width &lt; .08), alpha = 1/2, size = 2/3) + scale_color_manual(values = ce[c(2, 7)], breaks = NULL) + scale_x_continuous(NULL, breaks = 0:7 * 100) + scale_y_continuous(&quot;HDI width&quot;, expand = expansion(mult = 0), limits = c(0, 1)) With syntax from the patchwork package, we’ll arrange them one atop another. library(patchwork) (p1 / p2 / p3 / p4 / p5) + plot_annotation(title = expression(theta==0.5)) Now let’s compute data of the same form, but based on \\(\\theta = .65\\). This time we’ll do all the data wrangling steps in one code block. n_trial &lt;- 700 bias &lt;- .65 set.seed(13) coin.65 &lt;- # n, flip, and z tibble(n = 1:n_trial, flip = rbinom(n = n_trial, size = 1, prob = bias)) %&gt;% mutate(z = cumsum(flip)) %&gt;% # p-values mutate(p = map2_dbl(n, z, fit_glm)) %&gt;% # log(BF) mutate(log_bf = map2_dbl(n, z, log_bf, theta = .5)) %&gt;% # HDIs mutate(hdi = map2(n, z, ~hdi_of_qbeta(.y + 1, .x - .y + 1))) %&gt;% unnest(hdi) %&gt;% # HDI width mutate(width = ul - ll) head(coin.65) ## # A tibble: 6 × 8 ## n flip z p log_bf ll ul width ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 1.00 2.22e-16 0.00000000317 0.776 0.776 ## 2 2 1 1 1 -3.93e+ 0 0.0943 0.906 0.811 ## 3 3 1 2 0.571 -3.93e+ 0 0.228 0.956 0.729 ## 4 4 1 3 0.341 -3.65e+ 0 0.330 0.974 0.644 ## 5 5 0 3 0.657 -4.33e+ 0 0.239 0.895 0.656 ## 6 6 1 4 0.423 -4.15e+ 0 0.315 0.919 0.604 Here is the code for our version of Figure 13.5. p1 &lt;- coin.65 %&gt;% ggplot(aes(x = n, y = z / n)) + geom_hline(yintercept = .65, color = ce[9]) + geom_line(color = ce[1]) + geom_point(size = 2/3) + scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) p2 &lt;- coin.65 %&gt;% ggplot(aes(x = n, y = p)) + geom_hline(yintercept = .05, color = ce[9]) + geom_line(aes(color = p &lt; .05)) + geom_point(aes(color = p &lt; .05), alpha = 1/2, size = 2/3) + scale_color_manual(values = ce[c(2, 7)], breaks = NULL) + scale_y_continuous(expression(italic(p)*&quot;-value&quot;), expand = expansion(mult = 0), limits = c(0, 1)) p3 &lt;- coin.65 %&gt;% ggplot(aes(x = n, y = log_bf)) + geom_hline(yintercept = c(-1.1, 1.1), color = ce[9]) + geom_line(color = ce[1]) + geom_point(aes(color = log_bf &lt; -1.1 | log_bf &gt; 1.1), alpha = 1/2, size = 2/3) + annotate(geom = &quot;text&quot;, x = 60, y = c(-8, 28), label = c(&quot;accept the null&quot;, &quot;reject the null&quot;), color = ce[1]) + scale_color_manual(values = ce[c(2, 7)], breaks = NULL) + scale_y_continuous(expression(log(BF)), limits = c(-10, 30)) p4 &lt;- coin.65 %&gt;% ggplot(aes(x = n)) + geom_hline(yintercept = c(.45, .55), color = ce[9]) + geom_linerange(aes(ymin = ll, ymax = ul, color = ll &gt; .55 | ul &lt; .45), alpha = 1/2) + scale_color_manual(values = ce[c(2, 7)], breaks = NULL) + scale_y_continuous(&quot;95% HDI&quot;, expand = expansion(mult = 0), limits = c(0, 1)) p5 &lt;- coin.65 %&gt;% ggplot(aes(x = n, y = width)) + geom_hline(yintercept = .08, color = ce[9]) + geom_line(aes(color = width &lt; .08)) + geom_point(aes(color = width &lt; .08), alpha = 1/2, size = 2/3) + scale_color_manual(values = ce[c(2, 7)], breaks = NULL) + scale_y_continuous(&quot;HDI width&quot;, expand = expansion(mult = 0), limits = c(0, 1)) (p1 / p2 / p3 / p4 / p5) &amp; scale_x_continuous(NULL, breaks = 0:7 * 100) &amp; plot_annotation(title = expression(theta==0.65)) 13.3.2 Average behavior of sequential tests. This section is still in the works. In short, I’m not quite sure how to pull off the simulations. If you’ve got the chops, please share your code in my GitHub issue #20. 13.4 Discussion 13.4.1 Power and multiple comparisons. In NHST, the overall \\(p\\) value for any particular test is increased when the test is considered in the space of all other intended tests… Bayesian power analysis is not affected by intending multiple tests. In Bayesian analysis, the decision is based on the posterior distribution, which is determined by the data in hand, whether actual or simulated, and not by what other tests are intended. In Bayesian analysis, the probability of achieving a goal, that is the power, is determined only by the data-generating process (which includes the stopping rule) and not by the cloud of counterfactual samples (which includes other tests). (p. 393) For more thoughts on the multiple comparisons issue, check out this post from Gelman’s blog. 13.4.2 Power: prospective, retrospective, and replication. There are different types of power analysis, depending on the source of the hypothetical distribution over parameter values and the prior used for analyzing the simulated data. The most typical and useful type is prospective power analysis. In prospective power analysis, research is being planned for which there has not yet been any data collected. The hypothetical distribution over parameter values comes from either theory or idealized data or actual data from related research. (p. 393, emphasis in the original) Prospective is probably the type that comes to mind with you think of “power analyses.” On the other hand, retrospective power analysis refers to a situation in which we have already collected data from a research project, and we want to determine the power of the research we conducted. In this case, we can use the posterior distribution, derived from the actual data, as the representative parameter values for generating new simulated data. (This is tantamount to a posterior predictive check.) In other words, at a step in the posterior MCMC chain, the parameter values are used to generate simulated data. The simulated data are then analyzed with the same Bayesian model as the actual data, and the posterior from the simulated data is examined for whether or not the goals are achieved. (p. 393, emphasis in the original) Though researchers sometimes use retrospective power analyses and are sometimes asked to perform them during the peer-review process, they are generally looked down upon within methodological circles. A recent controversy arose on the issue within the surgical literature. To dip your toes into the topic, check out this post by Reaction Watch or this post by Zad Chow or these two (here, here) posts from Gelman’s blog. Finally, suppose that we have already collected some data, and we want to know the probability that we would achieve our goal if we exactly replicated the experiment. In other words, if we were simply to collect a new batch of data, what is the probability that we would achieve our goal in the replicated study, also taking into account the results of the first set of data? This is the replication power. As with retrospective power analysis, we use the actual posterior derived from the first sample of data as the data generator. But for analysis of the simulated data, we again use the actual posterior from first sample of data, because that is the best-informed prior for the follow-up experiment. An easy way to execute this analysis by MCMC is as follows: Use the actual set of data with a skeptical-audience prior to generate representative parameter values and representative simulated data. Then, concatenate the original data with the novel simulated data and update the original skeptical-audience prior with the enlarged data set. This technique is tantamount to using the posterior of the original data set as the prior for the novel simulated data. (p. 394, emphasis in the original) 13.4.3 Power analysis requires verisimilitude of simulated data. Power analysis is only useful when the simulated data imitate actual data. We generate simulated data from a descriptive model that has uncertainty in its parameter values, but we assume that the model is a reasonably good description of the actual data. If the model is instead a poor description of the actual data, then the simulated data do not imitate actual data, and inferences from the simulated data are not very meaningful. It is advisable, therefore, to check that the simulated data accurately reflect the actual data. (p. 394) 13.4.4 The importance of planning. Conducting a power analysis in advance of collecting data is very important and valuable. Often in real research, a fascinating theory and clever experimental manipulation imply a subtle effect. It can come as a shock to the researcher when power analysis reveals that detecting the subtle effect would take many hundreds of subjects! But the shock of power analysis is far less than the pain of actually running dozens of subjects and finding highly uncertain estimates of the sought-after effect. (p. 395) For more handy uses of power analyses, keep reading in the text. For more practice with simulation approaches to Bayesian power analyses with brms, check out my blog series on the topic. You might start with the first post, Bayesian power analysis: Part I. Prepare to reject \\(H_0\\) with simulation. For a critique of the precision approach to Bayesian power analysis, check out the blog post by Richard Morey, Power and precision. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.1.2 tidybayes_3.0.2 brms_2.18.0 Rcpp_1.0.9 ## [5] fishualize_0.2.3 forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 ## [9] purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 tibble_3.1.8 ## [13] ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 ## [4] igraph_1.3.4 svUnit_1.0.6 splines_4.2.0 ## [7] crosstalk_1.2.0 TH.data_1.1-1 rstantools_2.2.0 ## [10] inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 ## [16] googlesheets4_1.0.1 tzdb_0.3.0 modelr_0.1.8 ## [19] RcppParallel_5.1.5 matrixStats_0.62.0 xts_0.12.1 ## [22] sandwich_3.0-2 prettyunits_1.1.1 colorspace_2.0-3 ## [25] rvest_1.0.2 ggdist_3.2.0 haven_2.5.1 ## [28] xfun_0.35 callr_3.7.3 crayon_1.5.2 ## [31] jsonlite_1.8.3 lme4_1.1-31 survival_3.4-0 ## [34] zoo_1.8-10 glue_1.6.2 gtable_0.3.1 ## [37] gargle_1.2.0 emmeans_1.8.0 distributional_0.3.1 ## [40] pkgbuild_1.3.1 rstan_2.21.7 abind_1.4-5 ## [43] scales_1.2.1 mvtnorm_1.1-3 DBI_1.1.3 ## [46] miniUI_0.1.1.1 xtable_1.8-4 HDInterval_0.2.2 ## [49] stats4_4.2.0 StanHeaders_2.21.0-7 DT_0.24 ## [52] htmlwidgets_1.5.4 httr_1.4.4 threejs_0.3.3 ## [55] arrayhelpers_1.1-0 posterior_1.3.1 ellipsis_0.3.2 ## [58] pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 ## [61] sass_0.4.2 dbplyr_2.2.1 utf8_1.2.2 ## [64] tidyselect_1.1.2 labeling_0.4.2 rlang_1.0.6 ## [67] reshape2_1.4.4 later_1.3.0 munsell_0.5.0 ## [70] cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [73] cli_3.5.0 generics_0.1.3 broom_1.0.1 ## [76] ggridges_0.5.3 evaluate_0.18 fastmap_1.1.0 ## [79] processx_3.8.0 knitr_1.40 fs_1.5.2 ## [82] nlme_3.1-159 projpred_2.2.1 mime_0.12 ## [85] xml2_1.3.3 compiler_4.2.0 bayesplot_1.9.0 ## [88] shinythemes_1.2.0 rstudioapi_0.13 gamm4_0.2-6 ## [91] curl_4.3.2 png_0.1-7 reprex_2.0.2 ## [94] bslib_0.4.0 stringi_1.7.8 highr_0.9 ## [97] ps_1.7.2 Brobdingnag_1.2-8 lattice_0.20-45 ## [100] Matrix_1.4-1 nloptr_2.0.3 markdown_1.1 ## [103] shinyjs_2.1.0 tensorA_0.36.2 vctrs_0.5.1 ## [106] pillar_1.8.1 lifecycle_1.0.3 jquerylib_0.1.4 ## [109] bridgesampling_1.1-2 estimability_1.4.1 httpuv_1.6.5 ## [112] R6_2.5.1 bookdown_0.28 promises_1.2.0.1 ## [115] gridExtra_2.3 codetools_0.2-18 boot_1.3-28 ## [118] MASS_7.3-58.1 colourpicker_1.1.1 gtools_3.9.3 ## [121] assertthat_0.2.1 withr_2.5.0 shinystan_2.6.0 ## [124] multcomp_1.4-20 mgcv_1.8-40 parallel_4.2.0 ## [127] hms_1.1.1 grid_4.2.0 minqa_1.2.5 ## [130] coda_0.19-4 rmarkdown_2.16 googledrive_2.0.0 ## [133] shiny_1.7.2 lubridate_1.8.0 base64enc_0.1-3 ## [136] dygraphs_1.1.1.6 References Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Miller, J. (2009). What is the probability of replicating a statistically significant effect? Psychonomic Bulletin &amp; Review, 16(4), 617–640. https://doi.org/10.3758/PBR.16.4.617 Nakagawa, S., &amp; Foster, T. M. (2004). The case against retrospective statistical power analyses with an introduction to power analysis. Acta Ethologica, 7(2), 103–108. https://doi.org/10.1007/s10211-004-0095-z O’Keefe, D. J. (2007). Brief report: Post hoc power, observed power, a priori power, retrospective power, prospective power, achieved power: Sorting out appropriate uses of statistical power analyses. Communication Methods and Measures, 1(4), 291–299. https://doi.org/10.1080/19312450701641375 Steidl, R. J., Hayes, J. P., &amp; Schauber, E. (1997). Statistical power analysis in wildlife research. The Journal of Wildlife Management, 61(2), 270. https://doi.org/10.2307/3802582 Sun, S., Pan, W., &amp; Wang, L. L. (2011). Rethinking observed power: Concept, practice, and implications. Methodology, 7(3), 81–87. https://doi.org/10.1027/1614-2241/a000025 Thomas, L. (1997). Retrospective power analysis. Conservation Biology, 11(1), 276–280. https://doi.org/10.1046/j.1523-1739.1997.96102.x "],["stan.html", "14 Stan 14.1 HMC sampling 14.2 Installing Stan 14.3 A Complete example 14.4 Specify models top-down in Stan 14.5 Limitations and extras Session info Footnote", " 14 Stan Stan is the name of a software package that creates representative samples of parameter values from a posterior distribution for complex hierarchical models, analogous to JAGS… According to the Stan reference manual, Stan is named after Stanislaw Ulam (1909–1984), who was a pioneer of Monte Carlo methods. (Stan is not named after the slang term referring to an overenthusiastic or psychotic fanatic, formed by a combination of the words “stalker” and “fan.”) The name of the software package has also been unpacked as the acronym, Sampling Through Adaptive Neighborhoods (Gelman et al., 2013, p. 307), but it is usually written as Stan not STAN. Stan uses a different method than JAGS for generating Monte Carlo steps. The method is called Hamiltonian Monte Carlo (HMC). HMC can be more effective than the various samplers in JAGS and BUGS, especially for large complex models. Moreover, Stan operates with compiled C++ and allows greater programming flexibility, which again is especially useful for unusual or complex models. For large data sets or complex models, Stan can provide solutions when JAGS (or BUGS) takes too long or fails. (pp. 399–400, emphasis in the original) To learn more about Stan from the Stan team themselves, check out the main website: https://mc-stan.org/. If you like to dive deep, bookmark the Stan user’s guide (Stan Development Team, 2022c) and the Stan reference manual (Stan Development Team, 2022b). We won’t be using Stan directly in this ebook. I prefer working with it indirectly through the interface of Bürkner’s brms package instead. If you haven’t already, bookmark the brms GitHub repository, CRAN page, and reference manual (Bürkner, 2022d). You can also view Bürkner’s talk from the useR! International R User 2017 Conference, brms: Bayesian multilevel models using Stan. Here’s how Bürkner described brms in its GitHub repo: The brms package provides an interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan, which is a C++ package for performing full Bayesian inference (see http://mc-stan.org/). The formula syntax is very similar to that of the package lme4 to provide a familiar and simple interface for performing regression analyses. A wide range of response distributions are supported, allowing users to fit – among others – linear, robust linear, count data, survival, response times, ordinal, zero-inflated, and even self-defined mixture models all in a multilevel context. Further modeling options include non-linear and smooth terms, auto-correlation structures, censored data, missing value imputation, and quite a few more. In addition, all parameters of the response distribution can be predicted in order to perform distributional regression. Multivariate models (i.e., models with multiple response variables) can be fit, as well. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. Model fit can easily be assessed and compared with posterior predictive checks, cross-validation, and Bayes factors. (emphasis in the original) 14.1 HMC sampling “Stan generates random representative samples from a posterior distribution by using a variation of the Metropolis algorithm called HMC” (p. 400). I’m not going to walk through the the details of HMC sampling, at this time. In addition to Kruschke’s explanation, you might check out McElreath’s lecture on HMC from January, 2019 or one of these lectures (here, here, or here) by Michael Betancourt. I’m also not sufficiently up on the math required to properly make the figures in this section. But we can at least get the ball rolling. library(tidyverse) library(patchwork) Here’s the primary data for the two upper left panels for Figure 14.1. d &lt;- tibble(theta = seq(from = -4, to = 4, by = 0.1)) %&gt;% mutate(density = dnorm(theta, mean = 0, sd = 1)) %&gt;% mutate(`-log(density)` = -log(density)) head(d) ## # A tibble: 6 × 3 ## theta density `-log(density)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -4 0.000134 8.92 ## 2 -3.9 0.000199 8.52 ## 3 -3.8 0.000292 8.14 ## 4 -3.7 0.000425 7.76 ## 5 -3.6 0.000612 7.40 ## 6 -3.5 0.000873 7.04 We need a couple more tibbles for the annotation. position &lt;- tibble(theta = -0.5, density = 0, `-log(density)` = 1.5) text &lt;- tibble(theta = -0.5, density = 0.2, `-log(density)` = 2.75, label1 = &quot;current position&quot;, label2 = &quot;random\\ninitial momentum&quot;) Plot. theme_set( theme_grey() + theme(panel.grid = element_blank()) ) p1 &lt;- d %&gt;% ggplot(aes(x = theta, y = density)) + geom_line(size = 2, color = &quot;grey67&quot;) + geom_point(data = position, size = 4) + geom_text(data = text, aes(label = label1)) + geom_segment(x = -0.5, xend = -0.5, y = 0.16, yend = 0.04, arrow = arrow(length = unit(0.2, &quot;cm&quot;)), size = 1/4, color = &quot;grey50&quot;) + ggtitle(&quot;Posterior Distrib.&quot;) + coord_cartesian(xlim = c(-3, 3)) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. p2 &lt;- d %&gt;% ggplot(aes(x = theta, y = `-log(density)`)) + geom_line(size = 2, color = &quot;grey67&quot;) + geom_point(data = position, size = 4) + geom_text(data = text, aes(label = label2)) + geom_segment(x = -1.1, xend = 0.1, y = 1.5, yend = 1.5, arrow = arrow(length = unit(0.275, &quot;cm&quot;), ends = &quot;both&quot;)) + ggtitle(&quot;Negative Log Posterior (&#39;Potential&#39;)&quot;) + coord_cartesian(xlim = c(-3, 3), ylim = c(0, 5)) (p1 / p2) &amp; scale_x_continuous(breaks = -3:3) For the plots in this chapter, we keep things simple and rely on the ggplot2 defaults with one exception: we omitted those unnecessary white gridlines with the theme_set() argument at the top of that block. You can undo that with theme_set(ggplot2::theme_grey()). Because I’m not sure how to make the dots and trajectories depicted in the third row, I also won’t be able to make proper histograms for the bottom rows. This will go for Figures 14.2 and 14.3, too. If you know how to reproduce them properly, please share your code in my GitHub issue #21. Let’s let Kruschke close this section out: Mathematical theories that accurately describe the dynamics of mechanical systems have been worked out by physicists. The formulation here, in terms of kinetic and potential energy, is named after William Rowan Hamilton (1805–1865). HMC was described in the physics literature by Duane et al. (1987) (who called it “hybrid” Monte Carlo), and HMC was applied to statistical problems by R. M. Neal (1994). A brief mathematical overview of HMC is presented by (MacKay, 2003, Chapter 30). A more thorough mathematical review of HMC is provided by (R. Neal, 2011). Details of how HMC is implemented in Stan can be found in the Stan reference manual and in the book by Gelman et al. (2013). (pp. 405–406) 14.2 Installing Stan You can learn about installing Stan at https://mc-stan.org/users/interfaces/. We, of course, have already been working with Stan via brms. Bürkner has some nice information on how to install brms in the FAQ section of the brms GitHub repository. To install the latest official release from CRAN, execute install.packages(\"brms\"). If you’d like to install the current developmental version, you can execute the following. if (!requireNamespace(&quot;remotes&quot;)) { install.packages(&quot;remotes&quot;) } remotes::install_github(&quot;paul-buerkner/brms&quot;) As Kruschke advised, it’s a good idea to “be sure that your versions of R and RStudio are up to date” (p. 407) when installing brms and/or Stan. People sometimes have difficulties installing Stan or brms after a they perform a software update. If you find yourself in that position, browse through some of the threads on the Stan forums at https://discourse.mc-stan.org/. 14.3 A Complete example If you’d like to learn how to fit models in Stan itself, you might consult the updated versions of the Stan user’s guide and Stan reference manual, which you can find at https://mc-stan.org/users/documentation/. You might also check out the Stan Case studies and other tutorials listed by the Stan team. We will continue using Stan via brms. The model Kruschke walked through in this section followed the form \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Bernoulli} (\\theta) \\\\ \\theta &amp; \\sim \\operatorname{Beta} (1, 1), \\end{align*}\\] where \\(\\theta\\) is the probability \\(y = 1\\). Kruschke showed how to simulate the data at the top of page 409. Here’s our tidyverse version. n &lt;- 50 z &lt;- 10 my_data &lt;- tibble(y = rep(1:0, times = c(z, n - z))) glimpse(my_data) ## Rows: 50 ## Columns: 1 ## $ y &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… Time to fire up brms. library(brms) In the absence of predictors, you might think of this as an intercept-only model. You can fit the simple intercept-only Bernoulli model with brms::brm() like this. fit14.1 &lt;- brm(data = my_data, family = bernoulli(link = identity), y ~ 1, prior(beta(1, 1), class = Intercept, lb = 0, ub = 1), iter = 1000, warmup = 200, chains = 3, cores = 3, seed = 14, file = &quot;fits/fit14.01&quot;) As Kruschke wrote, iter is the total number of steps per chain, including warmup steps in each chain. Thinning merely marks some steps as not to be used; thinning does not increase the number of steps taken. Thus, the total number of steps that Stan takes is chains·iter. Of those steps, the ones actually used as representative have a total count of chains·(iter−warmup)/thin. Therefore, if you know the desired total steps you want to keep, and you know the warm-up, chains, and thinning, then you can compute that the necessary iter equals the desired total multiplied by thin/chains+warmup. We did not specify the initial values of the chains in the example above, instead letting Stan randomly initialize the chains by default. The chains can be initialized by the user with the argument init, analogous to JAGS. (p. 409) Unlike what Kruschke showed on page 409, we did not use the thin argument, above, and will generally avoid thinning in this ebook. You just don’t tend to need to thin your chains when using Stan. I do, however, like to use the seed argument. Because computers use pseudorandom number generators to take random draws, I prefer to make my random draws reproducible by setting my seed. Others have argued against this. You do you. Kruschke mentioned trace plots and model summaries. Here’s our trace plot, which comes with a marginal density plot by brms default. plot(fit14.1, widths = c(2, 3)) Here’s the summary. print(fit14.1) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: my_data (Number of observations: 50) ## Draws: 3 chains, each with iter = 1000; warmup = 200; thin = 1; ## total post-warmup draws = 2400 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.21 0.05 0.11 0.32 1.00 789 928 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 14.3.1 Reusing the compiled model. “Because model compilation can take a while in Stan, it is convenient to store the DSO of a successfully compiled model and use it repeatedly for different data sets” (p. 410). This true for our brms paradigm, too. To reuse a compiled brm() model, we typically use the update() function. To demonstrate, we’ll first want some new data. Here we’ll increase our z value to 20. z &lt;- 20 my_data &lt;- tibble(y = rep(1:0, times = c(z, n - z))) glimpse(my_data) ## Rows: 50 ## Columns: 1 ## $ y &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,… For the first and most important argument, you need to tell update() what fit you’re reusing. We’ll use fit14.1. You also need to tell update() about your new data with the newdata argument. Because the model formula and priors are the same as before, we don’t need to use those arguments, here. fit14.2 &lt;- update(fit14.1, newdata = my_data, iter = 1000, warmup = 200, chains = 3, cores = 3, seed = 14, file = &quot;fits/fit14.02&quot;) ## The desired updates require recompiling the model Here’s the summary using the fixef() function. fixef(fit14.2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.4035677 0.06494748 0.2743692 0.5326829 14.3.2 General structure of Stan model specification. “The general structure of model specifications in Stan consist of six blocks” (p. 410). We don’t need to worry about this when using brms. Just use the brm() and update() functions. But if you’re curious about what the underlying Stan code is for your brms models, index the model fit with $model. fit14.2$model ## // generated with brms 2.17.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // total number of observations ## int Y[N]; // response variable ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## } ## parameters { ## real&lt;lower=0,upper=1&gt; Intercept; // temporary intercept for centered predictors ## } ## transformed parameters { ## real lprior = 0; // prior contributions to the log posterior ## lprior += beta_lpdf(Intercept | 1, 1); ## } ## model { ## // likelihood including constants ## if (!prior_only) { ## // initialize linear predictor term ## vector[N] mu = Intercept + rep_vector(0.0, N); ## target += bernoulli_lpmf(Y | mu); ## } ## // priors including constants ## target += lprior; ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = Intercept; ## } 14.3.3 Think log probability to think like Stan. The material in this subsection is outside of the scope of this ebook. 14.3.4 Sampling the prior in Stan. “There are several reasons why we might want to examine a sample from the prior distribution of a model” (p. 413). Happily, we can do this with brms with the sample_prior argument in the brm() function. By default, it is set to \"no\" and does not take prior samples. If you instead set sample_prior = \"yes\" or sample_prior = TRUE, samples are drawn solely from the prior. Here’s how to do that with an updated version of the model from fit14.2. fit14.3 &lt;- brm(data = my_data, family = bernoulli(link = identity), y ~ 1, prior(beta(1, 1), class = Intercept, lb = 0, ub = 1), iter = 1000, warmup = 200, chains = 3, cores = 3, sample_prior = &quot;yes&quot;, seed = 14, file = &quot;fits/fit14.03&quot;) Now we can gather the prior draws with the prior_draws() function. prior_draws(fit14.3) %&gt;% head() ## Intercept ## 1 0.2593548 ## 2 0.1327571 ## 3 0.2968221 ## 4 0.4340424 ## 5 0.3570039 ## 6 0.4815478 Here’s a look at the prior distribution. prior_draws(fit14.3) %&gt;% ggplot(aes(x = Intercept)) + geom_histogram(binwidth = 0.1, boundary = 0) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(&quot;Beta&quot;*(1*&quot;, &quot;*1)), x = expression(italic(p)(theta))) 14.3.5 Simplified scripts for frequently used analyses. This is not our approach when using brms. Throughout the chapters of this ebook, we will learn to make skillful use of the brms::brm() function to fit all our models. Once in a while we’ll take a shortcut and reuse a precompiled fit with update(). 14.4 Specify models top-down in Stan For humans, descriptive models begin, conceptually, with the data that are to be described. We first know the measurement scale of the data and their structure. Then we conceive of a likelihood function for the data. The likelihood function has meaningful parameters, which we might want to re-express in terms of other data (called covariates, predictors, or regressors). Then we build a meaningful hierarchical prior on the parameters. Finally, at the top level, we specify constants that express our prior knowledge, which might be vague or noncommittal. (p. 414) If you look at how I typically organize the arguments within brms::brm(), you’ll see this is generally the case there, too. Take another look at the code for fit14.1: fit14.1 &lt;- brm(data = my_data, family = bernoulli(link = identity), y ~ 1, prior(beta(1, 1), class = Intercept, lb = 0, ub = 1), iter = 1000, warmup = 200, chains = 3, cores = 3, seed = 14, file = &quot;fits/fit14.01&quot;) The first line within brm() defined the data. The second line defined the likelihood function and its link function. We haven’t talked much about link functions, yet, but that will start in Chapter 15. Likelihoods contain parameters and our third line within brm() defined the equation we wanted to use to predict/describe our parameter of interest, \\(\\theta\\). We defined our sole prior in the fourth line. The remaining arguments contain the unsexy technical specifications, such as how many MCMC chains we’d like to use and into what folder we’d like to save our fit as an external file. You do not need to arrange brm() arguments this way. For other arrangements, take a look at the examples in the brms reference manual or in some of Bürkner’s vignettes, such as his Estimating multivariate models with brms (2022b). However you go about fitting your models with brm(), I mainly recommend you find a general style and stick with it. Standardizing your approach will make your code more readable for others and yourself. 14.5 Limitations and extras At the time of this writing, one of the main limitations of Stan is that it does not allow discrete (i.e., categorical) parameters. The reason for this limitation is that Stan has HMC as its foundational sampling method, and HMC requires computing the gradient (i.e., derivative) of the posterior distribution with respect to the parameters. Of course, gradients are undefined for discrete parameters. (p. 415) To my knowledge this is still the case, which means brms has this limitation, too. As wildly powerful as it is, brms it not as flexible as working directly with Stan. However, Bürkner and others are constantly expanding its capabilities. Probably the best places keep track of the new and evolving features of brms are the issues and news sections in its GitHub repo, https://github.com/paul-buerkner/brms. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.18.0 Rcpp_1.0.9 patchwork_1.1.2 forcats_0.5.1 ## [5] stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 ## [9] tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 ## [4] igraph_1.3.4 splines_4.2.0 crosstalk_1.2.0 ## [7] TH.data_1.1-1 rstantools_2.2.0 inline_0.3.19 ## [10] digest_0.6.30 htmltools_0.5.3 fansi_1.0.3 ## [13] magrittr_2.0.3 checkmate_2.1.0 googlesheets4_1.0.1 ## [16] tzdb_0.3.0 modelr_0.1.8 RcppParallel_5.1.5 ## [19] matrixStats_0.62.0 xts_0.12.1 sandwich_3.0-2 ## [22] prettyunits_1.1.1 colorspace_2.0-3 rvest_1.0.2 ## [25] haven_2.5.1 xfun_0.35 callr_3.7.3 ## [28] crayon_1.5.2 jsonlite_1.8.3 lme4_1.1-31 ## [31] survival_3.4-0 zoo_1.8-10 glue_1.6.2 ## [34] gtable_0.3.1 gargle_1.2.0 emmeans_1.8.0 ## [37] distributional_0.3.1 pkgbuild_1.3.1 rstan_2.21.7 ## [40] abind_1.4-5 scales_1.2.1 mvtnorm_1.1-3 ## [43] DBI_1.1.3 miniUI_0.1.1.1 xtable_1.8-4 ## [46] stats4_4.2.0 StanHeaders_2.21.0-7 DT_0.24 ## [49] htmlwidgets_1.5.4 httr_1.4.4 threejs_0.3.3 ## [52] posterior_1.3.1 ellipsis_0.3.2 pkgconfig_2.0.3 ## [55] loo_2.5.1 farver_2.1.1 sass_0.4.2 ## [58] dbplyr_2.2.1 utf8_1.2.2 tidyselect_1.1.2 ## [61] labeling_0.4.2 rlang_1.0.6 reshape2_1.4.4 ## [64] later_1.3.0 munsell_0.5.0 cellranger_1.1.0 ## [67] tools_4.2.0 cachem_1.0.6 cli_3.5.0 ## [70] generics_0.1.3 broom_1.0.1 ggridges_0.5.3 ## [73] evaluate_0.18 fastmap_1.1.0 processx_3.8.0 ## [76] knitr_1.40 fs_1.5.2 nlme_3.1-159 ## [79] mime_0.12 projpred_2.2.1 xml2_1.3.3 ## [82] compiler_4.2.0 bayesplot_1.9.0 shinythemes_1.2.0 ## [85] rstudioapi_0.13 gamm4_0.2-6 reprex_2.0.2 ## [88] bslib_0.4.0 stringi_1.7.8 highr_0.9 ## [91] ps_1.7.2 Brobdingnag_1.2-8 lattice_0.20-45 ## [94] Matrix_1.4-1 nloptr_2.0.3 markdown_1.1 ## [97] shinyjs_2.1.0 tensorA_0.36.2 vctrs_0.5.1 ## [100] pillar_1.8.1 lifecycle_1.0.3 jquerylib_0.1.4 ## [103] bridgesampling_1.1-2 estimability_1.4.1 httpuv_1.6.5 ## [106] R6_2.5.1 bookdown_0.28 promises_1.2.0.1 ## [109] gridExtra_2.3 codetools_0.2-18 boot_1.3-28 ## [112] colourpicker_1.1.1 MASS_7.3-58.1 gtools_3.9.3 ## [115] assertthat_0.2.1 withr_2.5.0 shinystan_2.6.0 ## [118] multcomp_1.4-20 mgcv_1.8-40 parallel_4.2.0 ## [121] hms_1.1.1 grid_4.2.0 minqa_1.2.5 ## [124] coda_0.19-4 rmarkdown_2.16 googledrive_2.0.0 ## [127] shiny_1.7.2 lubridate_1.8.0 base64enc_0.1-3 ## [130] dygraphs_1.1.1.6 ## Warning in rm(d, position, text, p1, p2, n, z, my_data, fit14.1, fit14.2, : ## object &#39;fit14.4&#39; not found Footnote References Bürkner, P.-C. (2022b). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html Bürkner, P.-C. (2022d). brms reference manual, Version 2.17.0. https://CRAN.R-project.org/package=brms/brms.pdf Duane, S., Kennedy, A. D., Pendleton, B. J., &amp; Roweth, D. (1987). Hybrid Monte Carlo. Physics Letters B, 195(2), 216–222. https://doi.org/10.1016/0370-2693(87)91197-X Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). Bayesian data analysis (Third Edition). CRC press. https://stat.columbia.edu/~gelman/book/ MacKay, D. J. (2003). Information theory, inference and learning algorithms. Cambridge University Press. https://www.inference.org.uk/itprnn/book.pdf Neal, R. (2011). MCMC using Hamiltonian dynamics. In S. Brooks, A. Gelman, G. Jones, &amp; X.-L. Meng (Eds.), Handbook of Markov chain Monte Carlo (pp. 116–162). London, United Kingdom: Chapman &amp; Hall/CRC Press. https://arxiv.org/pdf/1206.1901.pdf Neal, R. M. (1994). An improved acceptance procedure for the hybrid Monte Carlo algorithm. Journal of Computational Physics, 111(1), 194–203. https://doi.org/10.1006/jcph.1994.1054 Stan Development Team. (2022b). Stan reference manual, Version 2.29. https://mc-stan.org/docs/2_29/reference-manual/ Stan Development Team. (2022c). Stan user’s guide, Version 2.29. https://mc-stan.org/docs/2_29/stan-users-guide/index.html "],["overview-of-the-generalized-linear-model.html", "15 Overview of the Generalized Linear Model 15.1 Types of variables 15.2 Linear combination of predictors 15.3 Linking from combined predictors to noisy predicted data 15.4 Formal expression of the GLM Session info", " 15 Overview of the Generalized Linear Model Along with Kruschke’s text, in this part if the project we’re moving away from simple Bernoulli coin flipping examples to more complicated analyses of the type we’d actually see in applied data analysis. As Kruschke explained, we’ll be using a versatile family of models known as the generalized linear model [GLM; Nelder &amp; Wedderburn (1972); Nelder &amp; Wedderburn (1972)]. This family of models comprises the traditional “off the shelf” analyses such as \\(t\\) tests, analysis of variance (ANOVA), multiple linear regression, logistic regression, log-linear models, etc. (Kruschke, 2015, p. 420) 15.1 Types of variables “To understand the GLM and its many specific cases, we must build up a variety of component concepts regarding relationships between variables and how variables are measured in the first place (p. 420).” 15.1.1 Predictor and predicted variables. It’s worth repeating the second paragraph of this subsection in its entirety. The key mathematical difference between predictor and predicted variables is that the likelihood function expresses the probability of values of the predicted variable as a function of values of the predictor variable. The likelihood function does not describe the probabilities of values of the predictor variable. The value of the predictor variable comes from outside the system being modeled, whereas the value of the predicted variable depends on the value of the predictor variable. (p. 420) This is one of those fine points that can be easy to miss when you’re struggling through the examples in this book or chest-deep in the murky waters of your own real-world data problem. But write this down on a sticky note and put it in your sock drawer or something. There are good reasons to fret about the distributional properties of your predictor variables–rules of thumb about the likelihood aren’t among them. 15.1.2 Scale types: metric, ordinal, nominal, and count. I don’t know that I’m interested in detailing the content of this section. But it’s worth while considering what Kruschke wrote in its close. Why we care: We care about the scale type because the likelihood function must specify a probability distribution on the appropriate scale. If the scale has two nominal values, then a Bernoulli likelihood function may be appropriate. If the scale is metric, then a normal distribution may be appropriate as a probability distribution to describe the data. Whenever we are choosing a model for data, we must answer the question, What kind of scale are we dealing with? (p. 423, emphasis in the original) 15.2 Linear combination of predictors “The core of the GLM is expressing the combined influence of predictors as their weighted sum. The following sections build this idea by scaffolding from the simplest intuitive cases” (p. 423). 15.2.1 Linear function of a single metric predictor. “A linear function is the generic, ‘vanilla,’ off-the-shelf dependency that is used in statistical models” (p. 424). Its basic form is \\[y = \\beta_0 + \\beta_1 x,\\] where \\(y\\) is the variable being predicted and \\(x\\) is the predictor. \\(\\beta_0\\) is the intercept (i.e., the expected value when \\(x\\) is zero) and \\(\\beta_1\\) is the expected increase in \\(y\\) after a one-unit increase in \\(x\\). Before we make our version of Figure 15.1, let’s talk about our theme and color palette. In this chapter, we’ll base our overall on ggplot2::theme_linedraw(). We’ll take selections from option = \"E\" from the viridis package to make our color palette. Based on our ggplot2 skills from the last few chapters, these moves are pretty mundane. We can go further. In this chapter, we’ll extend our skillset by practicing how to alter the default settings of our ggplot2 geoms. If you browse through the content of the text, you’ll see we’ll a lot of the upcomming plots will require lines of various sorts. We’ll make the bulk of those lines with geom_line(), geom_hline(), geom_vline(), and geom_segment(). as a first step, it’d be handy to see how their default aesthetic settings are defined. Here’s how to do that for geom_line(). ggplot2:::check_subclass(&quot;line&quot;, &quot;Geom&quot;)$default_aes ## Aesthetic mapping: ## * `colour` -&gt; &quot;black&quot; ## * `linewidth` -&gt; 0.5 ## * `linetype` -&gt; 1 ## * `alpha` -&gt; NA We can use the ggplot2::update_geom_defaults() function to change those default settings. Here’s how we might go about increasing the default size and color parameters. library(tidyverse) library(viridis) default_line &lt;- ggplot2:::check_subclass(&quot;line&quot;, &quot;Geom&quot;)$default_aes update_geom_defaults( geom = &quot;line&quot;, new = list( color = viridis_pal(option = &quot;E&quot;)(9)[2], size = 1) ) Now confirm our new defaults. ggplot2:::check_subclass(&quot;line&quot;, &quot;Geom&quot;)$default_aes ## $colour ## [1] &quot;#05366EFF&quot; ## ## $size ## [1] 1 ## ## $linewidth ## [1] 0.5 ## ## $linetype ## [1] 1 ## ## $alpha ## [1] NA Critically, notice how we saved the original default settings for geom_line() as default_line before we changed them. That way, all we have to do is execute this to change things back to normal. update_geom_defaults( geom = &quot;line&quot;, new = default_line ) ggplot2:::check_subclass(&quot;line&quot;, &quot;Geom&quot;)$default_aes ## $colour ## [1] &quot;black&quot; ## ## $linewidth ## [1] 0.5 ## ## $linetype ## [1] 1 ## ## $alpha ## [1] NA ## ## $size ## [1] 1 Now you see how this works, let’s save the default settings for the other three geoms. default_hline &lt;- ggplot2:::check_subclass(&quot;vline&quot;, &quot;Geom&quot;)$default_aes default_vline &lt;- ggplot2:::check_subclass(&quot;hline&quot;, &quot;Geom&quot;)$default_aes default_segment &lt;- ggplot2:::check_subclass(&quot;segment&quot;, &quot;Geom&quot;)$default_aes Here we’ll update the defaults of all four of our line-oriented geoms and change the settings for our global plot theme. update_geom_defaults( geom = &quot;line&quot;, new = list( color = viridis_pal(option = &quot;E&quot;)(9)[2], size = 1) ) update_geom_defaults( geom = &quot;hline&quot;, new = list( color = viridis_pal(option = &quot;E&quot;)(9)[8], size = 1/4, linetype = 2) ) update_geom_defaults( geom = &quot;vline&quot;, new = list( color = viridis_pal(option = &quot;E&quot;)(9)[8], size = 1/4, linetype = 2) ) update_geom_defaults( geom = &quot;segment&quot;, new = list( color = viridis_pal(option = &quot;E&quot;)(9)[5], size = 1/4) ) theme_set( theme_linedraw() + theme(panel.grid = element_blank()) ) You can learn more about these methods here and here. Let’s see what we’ve done by making the left panel of Figure 15.1. tibble(x = -3:7) %&gt;% mutate(y_1 = -5 + 2 * x, y_2 = 10 + 2 * x) %&gt;% ggplot(aes(x = x)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_line(aes(y = y_1)) + geom_line(aes(y = y_2)) + geom_text(data = tibble( x = 2.25, y = c(-5, 10), label = c(&quot;y = -5 + 2x&quot;, &quot;y = 10 + 2x&quot;) ), aes(y = y, label = label), size = 4.5) + labs(title = &quot;Different Intercepts&quot;, y = &quot;y&quot;) + coord_cartesian(xlim = c(-2, 6), ylim = c(-10, 25)) Did you notice how we followed the form of the basic linear function when we defined the values for y_1 and y_2? It’s that simple! Here’s the right panel. tibble(x = -3:7) %&gt;% mutate(y_1 = 10 + -0.5 * x, y_2 = 10 + 2 * x) %&gt;% ggplot(aes(x = x)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_line(aes(y = y_1)) + geom_line(aes(y = y_2)) + geom_text(data = tibble( x = 4, y = c(11, 13.75), label = c(&quot;y = 10 + -0.5x&quot;, &quot;y = 10 + 2x&quot;) ), aes(y = y, label = label), size = 4.5) + labs(title = &quot;Different Slopes&quot;, y = &quot;y&quot;) + coord_cartesian(xlim = c(-2, 6), ylim = c(-10, 25)) Summary of why we care. The likelihood function includes the form of the dependency of \\(y\\) on \\(x\\). When \\(y\\) and \\(x\\) are metric variables, the simplest form of dependency, both mathematically and intuitively, is one that preserves proportionality. The mathematical expression of this relation is a so-called linear function. The usual mathematical expression of a line is the \\(y\\)-intercept form, but sometimes a more intuitive expression is the \\(x\\) threshold form. Linear functions form the core of the GLM. (pp. 424–425, emphasis in the original) 15.2.2 Additive combination of metric predictors. The linear combination of \\(K\\) predictors has the general form \\[\\begin{align*} y &amp; = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_K x_K \\\\ &amp; = \\beta_0 + \\sum_{k = 1}^K \\beta_k x_k. \\end{align*}\\] In the special case where \\(K = 0\\), you have an intercept-only model, \\[y = \\beta_0,\\] in which \\(\\beta_0\\) simply models the mean of \\(y\\). We won’t be able to reproduce the wireframe plots of Figure 15.2, exactly. But we can use some geom_raster() tricks from back in Chapter 10 to express the third \\(y\\) dimension as fill gradients. crossing(x1 = seq(from = 0, to = 10, by = 0.5), x2 = seq(from = 0, to = 10, by = 0.5)) %&gt;% mutate(`y[1] == 0 + 1 * x[1] + 0 * x[2]` = 0 + 1 * x1 + 0 * x2, `y[2] == 0 + 0 * x[1] + 2 * x[2]` = 0 + 0 * x1 + 2 * x2, `y[3] == 0 + 1 * x[1] + 2 * x[2]` = 0 + 1 * x1 + 2 * x2, `y[4] == 10 + 1 * x[1] + 2 * x[2]` = 10 + 1 * x1 + 2 * x2) %&gt;% pivot_longer(cols = -c(x1, x2), values_to = &quot;y&quot;) %&gt;% ggplot(aes(x = x1, y = x2, fill = y)) + geom_raster() + scale_fill_viridis_c(option = &quot;E&quot;) + scale_x_continuous(expression(x[1]), expand = c(0, 0), breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(expression(x[2]), expand = c(0, 0), breaks = seq(from = 0, to = 10, by = 2)) + coord_equal() + facet_wrap(~ name, labeller = label_parsed) Here we’ve captured some of Kruschke’s grid aesthetic by keeping the by argument within seq() somewhat coarse and omitting the interpolate = T argument from geom_raster(). If you’d prefer smoother fill transitions for the \\(y\\)-values, set by = 0.1 and interpolate = T. 15.2.3 Nonadditive interaction of metric predictors. As it turns out, “the combined influence of two predictors does not have to be additive” (p. 427). Let’s explore what that can look like with our version of Figure 15.3. crossing(x1 = seq(from = 0, to = 10, by = 0.5), x2 = seq(from = 0, to = 10, by = 0.5)) %&gt;% mutate(`y[1] == 0 + 0 * x[1] + 0 * x[2] + 0.2 * x[1] * x[2]` = 0 + 0 * x1 + 0 * x2 + 0.2 * x1 * x2, `y[2] == 0 + 1 * x[1] + 1 * x[2] + 0 * x[1] * x[2]` = 0 + 1 * x1 + 1 * x2 + 0 * x1 * x2, `y[3] == 0 + 1 * x[1] + 1 * x[2] + -0.3 * x[1] * x[2]` = 0 + 1 * x1 + 1 * x2 + -0.3 * x1 * x2, `y[4] == 0 + -1 * x[1] + 1 * x[2] + 0.2 * x[1] * x[2]` = 0 + -1 * x1 + 1 * x2 + 0.2 * x1 * x2) %&gt;% pivot_longer(cols = -c(x1, x2), values_to = &quot;y&quot;) %&gt;% ggplot(aes(x = x1, y = x2, fill = y)) + geom_raster() + scale_fill_viridis_c(option = &quot;E&quot;) + scale_x_continuous(expression(x[1]), expand = c(0, 0), breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(expression(x[2]), expand = c(0, 0), breaks = seq(from = 0, to = 10, by = 2)) + coord_equal() + facet_wrap(~ name, labeller = label_parsed) Did you notice all those * operators in the mutate() code? We’re no longer restricting ourselves to additive functions. We’re square in the middle of multiplicative town! There is a subtlety in the use of the term “linear” that can sometimes cause confusion in this context. The interactions shown in Figure 15.3 are not linear on the two predictors \\(x_1\\) and \\(x_2\\). But if the product of the two predictors, \\(x_1 x_2\\), is thought of as a third predictor, then the model is linear on the three predictors, because the predicted value of \\(y\\) is a weighted additive combination of the three predictors. This reconceptualization can be useful for implementing nonlinear interactions in software for linear models, but we will not be making that semantic leap to a third predictor, and instead we will think of a nonadditive combination of two predictors. A nonadditive interaction of predictors does not have to be multiplicative. Other types of interaction are possible. (p. 428, emphasis in the original) In my experience, this “subtlety” is really easy to misunderstand. You might be surprised how often it pops up even among senior scholars. Which is all to say, this stuff is difficult, so give yourself a break. 15.2.4 Nominal predictors. 15.2.4.1 Linear model for a single nominal predictor. Instead of representing the value of the nominal predictor by a single scalar value \\(x\\), we will represent the nominal predictor by a vector \\(\\vec{x} = \\langle x_{[1]},...,x_{[J]} \\rangle\\) where \\(J\\) is the number of categories that the predictor has… We will denote the baseline value of the prediction as \\(\\beta_0\\). The deflection for the \\(j\\)th level of the predictor is denoted \\(\\beta_{[j]}\\). Then the predicted value is \\[\\begin{align*} y &amp; = \\beta_0 + \\beta_{[1]} x_{[1]} + \\dots + \\beta_{[J]} x_{[J]} \\\\ &amp; = \\beta_0 + \\vec{\\beta} \\cdot \\vec{x} \\end{align*}\\] where the notation \\(\\vec{\\beta} \\cdot \\vec{x}\\) is sometimes called the “dot product” of the vectors. (p. 429) 15.2.4.2 Additive combination of nominal predictors. When you have two additive nominal predictors, the model follows the form \\[\\begin{align*} y &amp; = \\beta_0 + \\vec \\beta_1 \\vec x_1 + \\vec \\beta_2 \\vec x_2 \\\\ &amp; = \\beta_0 + \\sum_{j} \\beta_{1[j]} x_{1[j]} + \\sum_{k} \\beta_{2[k]} x_{2[k]}, \\end{align*}\\] given the constraints \\[\\sum_{j} \\beta_{1[j]} = 0 \\;\\;\\; \\text{and} \\;\\;\\; \\sum_{k} \\beta_{2[k]} = 0.\\] Both panels in Figure 15.4 are going to require three separate data objects. Here’s the code for the top panel. arrows &lt;- tibble(x = c(0.1, 1.1, 2.1), y = c(1, 1.69, 1.69), yend = c(1.69, 1.69 + .07, 1.69 - .07)) text &lt;- tibble(x = c(0.44, 1.46, 2.51), y = c(1.68, 1.753, 1.625), label = c(&quot;beta[0] == 1.69&quot;, &quot;beta[&#39;[1]&#39;] == 0.07&quot;, &quot;beta[&#39;[2]&#39;] == -0.07&quot;)) tibble(x = 1:2, y = c(1.69 + .07, 1.69 - .07)) %&gt;% # plot! ggplot(aes(x = x, y = y)) + geom_hline(yintercept = 1.69) + geom_col(width = .075, fill = viridis_pal(option = &quot;E&quot;)(9)[2]) + geom_segment(data = arrows, aes(xend = x, yend = yend), arrow = arrow(length = unit(0.2, &quot;cm&quot;))) + geom_text(data = text, aes(label = label), size = 3.5, parse = T) + scale_x_continuous(breaks = 1:2, labels = c(&quot;&lt;1,0&gt;&quot;, &quot;&lt;0,1&gt;&quot;)) + coord_cartesian(xlim = c(0, 3), ylim = c(1.5, 1.75)) + theme(axis.ticks.x = element_blank()) Here’s the code for the bottom panel. arrows &lt;- tibble(x = c(0.1, 1.1, 2.1, 3.1, 4.1, 5.1), y = rep(c(50, 101), times = c(1, 5)), yend = c(101, 101 + 4, 101 - 3, 101 - 2, 101 + 6, 101 - 5)) text &lt;- tibble(x = c(0.41, 1.36, 2.41, 3.4, 4.35, 5.4), y = c(100.5, 104.5, 98.5, 99.5, 106.5, 96.5), label = c(&quot;beta[0] == 101&quot;, &quot;beta[&#39;[1]&#39;] == 4&quot;, &quot;beta[&#39;[2]&#39;] == -3&quot;, &quot;beta[&#39;[3]&#39;] == -2&quot;, &quot;beta[&#39;[4]&#39;] == 6&quot;, &quot;beta[&#39;[5]&#39;] == -5&quot;)) tibble(x = 1:5, y = c(101 + 4, 101 - 3, 101 - 2, 101 + 6, 101 - 5)) %&gt;% # the plot ggplot(aes(x = x, y = y)) + geom_hline(yintercept = 101) + geom_col(width = .075, fill = viridis_pal(option = &quot;E&quot;)(9)[2]) + geom_segment(data = arrows, aes(xend = x, yend = yend), arrow = arrow(length = unit(0.2, &quot;cm&quot;))) + geom_text(data = text, aes(label = label), size = 3.5, parse = T) + scale_x_continuous(breaks = 1:5, labels = c(&quot;&lt;1,0,0,0,0&gt;&quot;, &quot;&lt;0,1,0,0,0&gt;&quot;, &quot;&lt;0,0,1,0,0&gt;&quot;, &quot;&lt;0,0,0,1,0&gt;&quot;, &quot;&lt;0,0,0,0,1&gt;&quot;)) + coord_cartesian(xlim = c(0, 5.5), ylim = c(90, 106.5)) + theme(axis.ticks.x = element_blank()) Before we make our versions of the two panels in Figure 15.5, we should note that the values in the prose are at odds with the values implied in the figure. For simplicity, our versions of Figure 15.5 will match up with the values in the original figure, not the prose. For a look at what the figure might have looked like had it been based on the values in the prose, check out Kruschke’s Corrigenda. Here’s the code for the left panel of Figure 15.5. d &lt;- tibble(x_1 = rep(c(&quot; &lt;1,0&gt;&quot;, &quot;&lt;0,1&gt; &quot;), times = 4), x_2 = c(rep(0:2, each = 2), -0.25, 0.25), y = c(8, 10, 3, 5, 4, 6, 8.5, 10.5), type = rep(c(&quot;number&quot;, &quot;text&quot;), times = c(6, 2))) %&gt;% mutate(x_1 = factor(x_1, levels = c(&quot; &lt;1,0&gt;&quot;, &quot;&lt;0,1&gt; &quot;))) d %&gt;% filter(type == &quot;number&quot;) %&gt;% ggplot(aes(x = x_2, y = y, fill = x_1)) + geom_col(position = &quot;dodge&quot;) + geom_text(data = d %&gt;% filter(type == &quot;text&quot;), aes(label = x_1, color = x_1)) + scale_fill_viridis_d(NULL, option = &quot;E&quot;, begin = .25, end = .75) + scale_color_viridis_d(NULL, option = &quot;E&quot;, begin = .25, end = .75) + scale_x_continuous(breaks = 0:2, labels = c(&quot;&lt;1,0,0&gt;&quot;, &quot;&lt;0,1,0&gt;&quot;, &quot;&lt;0,0,1&gt;&quot;)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2), expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Additive (no interaction)&quot;) + theme(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;) Did you notice our filter() trick in the code? 15.2.4.3 Nonadditive interaction of nominal predictors. We need new notation to formalize the nonadditive influence of a combination of nominal values. Just as \\(\\vec x_1\\) refers to the value of predictor \\(1\\), and \\(\\vec x_2\\) refers to the value of predictor \\(2\\), the notation \\(\\vec x_{1 \\times 2}\\) will refer to a particular combination of values of predictors \\(1\\) and \\(2\\). If there are \\(J\\) levels of predictor \\(1\\) and \\(K\\) levels of predictor \\(2\\), then there are \\(J \\times K\\) combinations of the two predictors. To indicate a particular combination of levels from predictors \\(1\\) and \\(2\\), the corresponding component of \\(\\vec x_{1 \\times 2}\\) is set to \\(1\\) while all other components are set to \\(0\\). A nonadditive interaction of predictors is formally represented by including a term for the influence of combinations of predictors, beyond the additive influences, as follows: \\(y = \\beta_0 + \\vec \\beta_1 \\cdot \\vec x_1 + \\vec \\beta_2 \\cdot \\vec x_2 + \\vec \\beta_{1 \\times 2} \\cdot \\vec x_{1 \\times 2}\\). (pp. 432–433, emphasis in the original) Now we’re in nonadditive interaction land, here’s the code for the right panel of Figure 15.5. d &lt;- tibble(x_1 = rep(c(&quot; &lt;1,0&gt;&quot;, &quot;&lt;0,1&gt; &quot;), times = 4), x_2 = c(rep(0:2, each = 2), -0.25, 0.25), y = c(8, 10, 5, 3, 3, 7, 8.5, 10.5), type = rep(c(&quot;number&quot;, &quot;text&quot;), times = c(6, 2))) %&gt;% mutate(x_1 = factor(x_1, levels = c(&quot; &lt;1,0&gt;&quot;, &quot;&lt;0,1&gt; &quot;))) d %&gt;% filter(type == &quot;number&quot;) %&gt;% ggplot(aes(x = x_2, y = y, fill = x_1)) + geom_col(position = &quot;dodge&quot;) + geom_text(data = d %&gt;% filter(type == &quot;text&quot;), aes(label = x_1, color = x_1)) + scale_fill_viridis_d(NULL, option = &quot;E&quot;, begin = .25, end = .75) + scale_color_viridis_d(NULL, option = &quot;E&quot;, begin = .25, end = .75) + scale_x_continuous(breaks = 0:2, labels = c(&quot;&lt;1,0,0&gt;&quot;, &quot;&lt;0,1,0&gt;&quot;, &quot;&lt;0,0,1&gt;&quot;)) + scale_y_continuous(breaks = 0:5 * 2, expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Non-Additive Interaction&quot;) + theme(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;) “The main point to understand now is that the term ‘interaction’ refers to a nonadditive influence of the predictors on the predicted, regardless of whether the predictors are measured on a nominal scale or a metric scale” (p. 434, emphasis in the original). 15.3 Linking from combined predictors to noisy predicted data 15.3.1 From predictors to predicted central tendency. After the predictor variables are combined, they need to be mapped to the predicted variable. This mathematical mapping is called the (inverse) link function, and is denoted by \\(f()\\) in the following equation: \\[y = f(\\operatorname{lin}(x))\\] Until now, we have been assuming that the link function is merely the identity function, \\(f(\\operatorname{lin}(x)) = \\operatorname{lin}(x)\\). (p. 436, emphasis in the original) Yet as we’ll see, many models use links other than the identity function. 15.3.1.1 The logistic function. The logistic link function follows the form \\[y = \\operatorname{logistic}(x) = \\frac{1}{ \\big (1 + \\exp (-x) \\big )}.\\] We can write the logistic function for a univariable metric predictor as \\[y = \\operatorname{logistic}(x; \\beta_0, \\beta_1) = \\frac{1}{\\big (1 + \\exp (-[\\beta_0 + \\beta_1x]) \\big )}.\\] And if we prefer to parameterize it in terms of gain \\(\\gamma\\) and threshold \\(\\theta\\), it’d be \\[y = \\operatorname{logistic}(x; \\gamma, \\theta) = \\frac{1}{ \\big (1 + \\exp (-\\gamma [x - \\theta]) \\big )}.\\] We can make the sexy logistic curves of Figure 15.6 with stat_function(), into which we’ll plug our very own custom make_logistic() function. Here’s the left panel. make_logistic &lt;- function(x, gamma, theta) { 1 / (1 + exp(-gamma * (x - theta))) } # annotation text &lt;- tibble(x = c(-1, 3), y = c(.9, .1), label = str_c(&quot;list(gamma == 0.5, theta == &quot;, c(-1, 3), &quot;)&quot;)) # plot tibble(x = c(-11, 11)) %&gt;% ggplot(aes(x = x)) + geom_vline(xintercept = c(-1, 3)) + geom_hline(yintercept = .5) + stat_function(fun = make_logistic, args = list(gamma = .5, theta = -1), size = 1, color = viridis_pal(option = &quot;E&quot;)(9)[2]) + stat_function(fun = make_logistic, args = list(gamma = .5, theta = 3), size = 1, color = viridis_pal(option = &quot;E&quot;)(9)[2]) + geom_text(data = text, aes(y = y, label = label), size = 3.5, parse = T) + scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) + coord_cartesian(xlim = c(-10, 10)) + ggtitle(&quot;Different Thresholds&quot;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. For kicks, we’ll take a different approach for the right panel. Instead of pumping values through stat_function() within our plot code, we’ll use our make_logistic() function within mutate() before beginning the plot code. # define the annotation values text &lt;- tibble(x = c(2, -2), y = c(.92, .4), label = str_c(&quot;list(gamma == &quot;, c(2, 0.2), &quot;, theta == 4)&quot;)) # make the data crossing(gamma = c(2, .2), x = seq(from = -11, to = 11, by = 0.2)) %&gt;% mutate(y = make_logistic(x, gamma, theta = 4)) %&gt;% # plot! ggplot(aes(x = x, y = y)) + geom_vline(xintercept = 4) + geom_hline(yintercept = .5) + geom_line(aes(group = gamma), size = 1, color = viridis_pal(option = &quot;E&quot;)(9)[2]) + geom_text(data = text, aes(label = label), size = 3.5, parse = T) + scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) + coord_cartesian(xlim = c(-10, 10)) + ggtitle(&quot;Different Gains&quot;) I don’t know that one plotting approach is better than the other. It’s good to have options. To make our two-dimensional version of the wireframe plots of Figure 15.7, we’ll first want to define the logistic() function. logistic &lt;- function(x) { 1 / (1 + exp(-x)) } Now we’ll just extend the same method we used for Figures 15.2 and 15.3. crossing(x1 = seq(from = -6, to = 6, by = 0.5), x2 = seq(from = -6, to = 6, by = 0.5)) %&gt;% mutate(`y[1] == logistic(1 * ( 0 * x[1] + 1 * x[2] - 0))` = logistic(1 * ( 0 * x1 + 1 * x2 - 0)), `y[2] == logistic(1 * ( 0.71 * x[1] + 0.71 * x[2] - 0))` = logistic(1 * ( 0.71 * x1 + 0.71 * x2 - 0)), `y[3] == logistic(2 * ( 0 * x[1] + 1 * x[2] - -3))` = logistic(2 * ( 0 * x1 + 1 * x2 - -3)), `y[4] == logistic(2 * (-0.71 * x[1] + 0.71 * x[2] - 3))` = logistic(2 * (-0.71 * x1 + 0.71 * x2 - 3))) %&gt;% pivot_longer(cols = c(-x1, -x2), values_to = &quot;y&quot;) %&gt;% ggplot(aes(x = x1, y = x2, fill = y)) + geom_raster() + scale_fill_viridis_c(option = &quot;E&quot;, limits = c(0, 1)) + scale_x_continuous(expression(x[1]), expand = c(0, 0), breaks = seq(from = -6, to = 6, by = 2)) + scale_y_continuous(expression(x[2]), expand = c(0, 0), breaks = seq(from = -6, to = 6, by = 2), position = &quot;right&quot;) + coord_equal() + theme(legend.position = &quot;left&quot;, strip.text = element_text(size = 8)) + facet_wrap(~ name, labeller = label_parsed) “The threshold determines the position of the logistical cliff. In other words, the threshold determines the \\(x\\) values for which \\(y = 0.5\\)… The gain determines the steepness of the logistical cliff” (pp. 437–438, emphasis in the original). 15.3.1.2 The cumulative normal function. The cumulative normal is denoted \\(\\Phi (x; \\mu, \\sigma)\\), where \\(x\\) is a real number and where \\(\\mu\\) and \\(\\sigma\\) are parameter values, called the mean and standard deviation of the normal distribution. The parameter \\(\\mu\\) governs the point at which the cumulative normal, \\(\\Phi(x)\\), equals \\(0.5\\). In other words, \\(\\mu\\) plays the same role as the threshold in the logistic. The parameter \\(\\sigma\\) governs the steepness of the cumulative normal function at \\(x = \\mu\\), but inversely, such that a smaller value of \\(\\sigma\\) corresponds to a steeper cumulative normal. (p. 440, emphasis in the original) Here we plot the standard normal density in the top panel of Figure 15.8. tibble(x = seq(from = -4, to = 4, by = 0.05)) %&gt;% mutate(d = dnorm(x, mean = 0, sd = 1)) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(aes(fill = x &lt;= 1), show.legend = F) + geom_line(size = 1, color = viridis_pal(option = &quot;E&quot;)(9)[2]) + scale_fill_manual(values = c(&quot;white&quot;, alpha(viridis_pal(option = &quot;E&quot;)(9)[2], 2/3))) + scale_y_continuous(expression(p(x)), expand = expansion(mult = c(0, 0.05))) + coord_cartesian(xlim = c(-3, 3)) + ggtitle(&quot;Normal Density&quot;) Here’s the analogous cumulative normal function depicted in the bottom panel of Figure 15.8. tibble(x = seq(from = -4, to = 4, by = 0.05)) %&gt;% ggplot(aes(x = x)) + stat_function(fun = pnorm, args = list(mean = 0, sd = 1), size = 1, color = viridis_pal(option = &quot;E&quot;)(9)[2]) + geom_segment(aes(x = 1, xend = 1, y = 0, yend = pnorm(1, 0, 1)), arrow = arrow(length = unit(0.2, &quot;cm&quot;))) + scale_y_continuous(expression(Phi(x)), expand = expansion(mult = 0), limits = c(0, 1)) + coord_cartesian(xlim = c(-3, 3)) + ggtitle(&quot;Cumulative Normal&quot;) Terminology: The inverse of the cumulative normal is called the probit function. [“Probit” stands for “probability unit”; Bliss (1934)]. The probit function maps a value \\(p\\), for \\(0.0 \\leq p \\leq 1.0\\), onto the infinite real line, and a graph of the probit function looks very much like the logit function. (p. 439, emphasis in the original) 15.3.2 From predicted central tendency to noisy data. In the real world, there is always variation in \\(y\\) that we cannot predict from \\(x\\). This unpredictable “noise” in \\(y\\) might be deterministically caused by sundry factors we have neither measured nor controlled, or the noise might be caused by inherent non-determinism in \\(y\\). It does not matter either way because in practice the best we can do is predict the probability that \\(y\\) will have any particular value, dependent upon \\(x\\)… To make this notion of probabilistic tendency precise, we need to specify a probability distribution for \\(y\\) that depends on \\(f (\\operatorname{lin} (x))\\). To keep the notation tractable, first define \\(\\mu = f (\\operatorname{lin} (x))\\). The value \\(\\mu\\) represents the central tendency of the predicted \\(y\\) values, which might or might not be the mean. With this notation, we then denote the probability distribution of \\(y\\) as some to-be-specified probability density function, abbreviated as “pdf”: \\[y \\sim \\operatorname{pdf} \\big ( \\mu, [\\text{scale, shape, etc.}] \\big )\\] As indicated by the bracketed terms after \\(\\mu\\), the pdf might have various additional parameters that control the distribution’s scale (i.e., standard deviation), shape, etc. The form of the pdf depends on the measurement scale of the predicted variable. (pp. 440–441, emphasis in the original) The top panel of Figure 15.9 is tricky. One way to make those multiple densities tipped on their sides is with ggridges::geom_ridgeline() followed by coord_flip(). However, explore a different approach that’ll come in handy in many of the plots we’ll be making in later chapters. The method requires we make a data set with the necessary coordinates for the side-tipped densities. Let’s walk through that step slowly. curves &lt;- # define the 3 x-values we want the Gaussians to originate from tibble(x = seq(from = -7.5, to = 7.5, length.out = 4)) %&gt;% # use the formula 10 + 2x to compute the expected y-value for x mutate(y_mean = 10 + (2 * x)) %&gt;% # based on a Gaussian with `mean = y_mean` and `sd = 2`, compute the 99.5% intervals mutate(ll = qnorm(.0025, mean = y_mean, sd = 2), ul = qnorm(.9975, mean = y_mean, sd = 2)) %&gt;% # now use those interval bounds to make a sequence of y-values mutate(y = map2(ll, ul, seq, length.out = 100)) %&gt;% # since that operation returned a nested column, we need to `unnest()` unnest(y) %&gt;% # compute the density values mutate(density = map2_dbl(y, y_mean, dnorm, sd = 2)) %&gt;% # now rescale the density values to be wider. # since we want these to be our x-values, we&#39;ll # just redefine the x column with these results mutate(x = x - density * 2 / max(density)) str(curves) ## tibble [400 × 6] (S3: tbl_df/tbl/data.frame) ## $ x : num [1:400] -7.54 -7.55 -7.55 -7.56 -7.57 ... ## $ y_mean : num [1:400] -5 -5 -5 -5 -5 -5 -5 -5 -5 -5 ... ## $ ll : num [1:400] -10.6 -10.6 -10.6 -10.6 -10.6 ... ## $ ul : num [1:400] 0.614 0.614 0.614 0.614 0.614 ... ## $ y : num [1:400] -10.6 -10.5 -10.4 -10.3 -10.2 ... ## $ density: num [1:400] 0.00388 0.00454 0.0053 0.00617 0.00715 ... In case it’s not clear, we’ll be plotting with the x and y columns. Think of the other columns as showing our work. But now we’ve got those curves data, we’re ready to simulate the points and plot. # how many points would you like? n_samples &lt;- 750 # generate the points tibble(x = runif(n = n_samples, min = -10, max = 10)) %&gt;% mutate(y = rnorm(n = n_samples, mean = 10 + 2 * x, sd = 2)) %&gt;% # plot! ggplot(aes(x = x, y = y)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_point(size = 1/5) + geom_abline(intercept = 10, slope = 2, size = 1, color = viridis_pal(option = &quot;E&quot;)(9)[2]) + geom_path(data = curves, aes(group = y_mean), color = viridis_pal(option = &quot;E&quot;)(9)[2], size = 3/4) + labs(title = &quot;Normal PDF around Linear Function&quot;, y = &quot;y&quot;) + coord_cartesian(ylim = c(-10, 30)) We’ll revisit this method in Chapter 17. The wireframe plots at the bottom of Figure 15.9 and in Figure 15.10 are outside of our ggplot2 purview. However, we can continue on with our approach of expressing the third \\(y\\) dimension as fill gradients. Thus, here’s a version of the bottom of Figure 15.9. # how many points would you like? n_samples &lt;- 750 # generate and save the points set.seed(15) d &lt;- tibble(x1 = runif(n = n_samples, min = 0, max = 10), x2 = runif(n = n_samples, min = 0, max = 10)) %&gt;% mutate(y = rnorm(n = n(), mean = 10 + 1 * x1 + 2 * x2, sd = 4)) # make the grid crossing(x1 = seq(from = 0, to = 10, by = 0.5), x2 = seq(from = 0, to = 10, by = 0.5)) %&gt;% mutate(y = 10 + 1 * x1 + 2 * x2) %&gt;% # plot! ggplot(aes(x = x1, y = x2, fill = y)) + geom_raster() + geom_point(data = d, shape = 21, stroke = 1/10) + scale_color_viridis_c(option = &quot;E&quot;, limits = c(0, 50)) + scale_fill_viridis_c(option = &quot;E&quot;, limits = c(0, 50)) + scale_x_continuous(expression(x[1]), expand = c(0, 0), breaks = 0:5 * 2) + scale_y_continuous(expression(x[2]), expand = c(0, 0), breaks = 0:5 * 2, position = &quot;right&quot;) + coord_equal() + labs(subtitle = expression(&quot;y ~ N(m, sd=4), &quot;*m==10+1*x[1]+2*x[2])) + theme(legend.position = &quot;left&quot;) For Figure 15.10, we can simulate Bernoulli distributed data with the purrr::rbernoulli() function. But since rbernoulli() returns a logical vector of FALSE and TRUE values, we’ll need to transform the results into the 0/1 metric in a second step. # how many points would you like? n_samples &lt;- 750 # generate and save the points set.seed(15) d &lt;- tibble(x1 = runif(n = n_samples, min = -6, max = 6), x2 = runif(n = n_samples, min = -6, max = 6)) %&gt;% mutate(y = rbernoulli(n = n(), p = logistic(1 * (0.71 * x1 + 0.71 * x2 - 0)))) %&gt;% mutate(y = as.double(y)) # make the grid crossing(x1 = seq(from = -6, to = 6, by = 0.5), x2 = seq(from = -6, to = 6, by = 0.5)) %&gt;% mutate(y = logistic(1 * (0.71 * x1 + 0.71 * x2 - 0))) %&gt;% # plot! ggplot(aes(x = x1, y = x2, fill = y)) + geom_raster() + geom_text(data = d, aes(label = y, color = factor(y)), size = 2.5) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;), breaks = NULL) + scale_fill_viridis_c(expression(italic(p)(y==1)), option = &quot;E&quot;, limits = 0:1) + scale_x_continuous(expression(x[1]), expand = c(0, 0), breaks = -3:3 * 2) + scale_y_continuous(expression(x[2]), expand = c(0, 0), breaks = -3:3 * 2, position = &quot;right&quot;) + coord_equal() + labs(subtitle = expression(&quot;y ~ Bernoulli(m), &quot;*m==logistic(1*(0.71*x[1]+0.71*x[2]-0)))) + theme(legend.position = &quot;left&quot;) Because of their use of the logistic function, we typically refer to models of this kind as logistic regression. 15.4 Formal expression of the GLM We can write the GLM as \\[\\begin{align*} y &amp; \\sim \\operatorname{pdf} \\big (\\mu, [\\text{parameters}] \\big ), \\text{where} \\\\ \\mu &amp; = f \\big (\\operatorname{lin}(x), [\\text{parameters}] \\big ). \\end{align*}\\] As has been previously explained, the predictors \\(x\\) are combined in the linear function \\(\\operatorname{lin}(x)\\), and the function \\(f\\) in [the first equation] is called the inverse link function. The data, \\(y\\), are distributed around the central tendency \\(\\mu\\) according to the probability density function labeled “pdf.” (p. 444) 15.4.1 Cases of the GLM. When a client brings an application to a [statistical] consultant, one of the first things the consultant does is find out from the client which data are supposed to be predictors and which data are supposed to be predicted, and the measurement scales of the data… When you are considering how to analyze data, your first task is to be your own consultant and find out which data are predictors, which are predicted, and what measurement scales they are. (pp. 445–446) Soak this last bit in. It’s gold. I’ve found it to be true in my exchanges with other researchers and with my own data problems, too. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] viridis_0.6.2 viridisLite_0.4.1 forcats_0.5.1 stringr_1.4.1 ## [5] dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 ## [9] tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] lubridate_1.8.0 assertthat_0.2.1 digest_0.6.30 ## [4] utf8_1.2.2 R6_2.5.1 cellranger_1.1.0 ## [7] backports_1.4.1 reprex_2.0.2 evaluate_0.18 ## [10] highr_0.9 httr_1.4.4 pillar_1.8.1 ## [13] rlang_1.0.6 googlesheets4_1.0.1 readxl_1.4.1 ## [16] rstudioapi_0.13 jquerylib_0.1.4 rmarkdown_2.16 ## [19] labeling_0.4.2 googledrive_2.0.0 munsell_0.5.0 ## [22] broom_1.0.1 compiler_4.2.0 modelr_0.1.8 ## [25] xfun_0.35 pkgconfig_2.0.3 htmltools_0.5.3 ## [28] tidyselect_1.1.2 gridExtra_2.3 bookdown_0.28 ## [31] fansi_1.0.3 crayon_1.5.2 tzdb_0.3.0 ## [34] dbplyr_2.2.1 withr_2.5.0 grid_4.2.0 ## [37] jsonlite_1.8.3 gtable_0.3.1 lifecycle_1.0.3 ## [40] DBI_1.1.3 magrittr_2.0.3 scales_1.2.1 ## [43] cli_3.5.0 stringi_1.7.8 cachem_1.0.6 ## [46] farver_2.1.1 fs_1.5.2 xml2_1.3.3 ## [49] bslib_0.4.0 ellipsis_0.3.2 generics_0.1.3 ## [52] vctrs_0.5.1 tools_4.2.0 glue_1.6.2 ## [55] hms_1.1.1 fastmap_1.1.0 colorspace_2.0-3 ## [58] gargle_1.2.0 rvest_1.0.2 knitr_1.40 ## [61] haven_2.5.1 sass_0.4.2 References Bliss, C. I. (1934). The method of probits. Science. https://doi.org/10.1126/science.79.2037.38 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Nelder, J. A., &amp; Wedderburn, R. W. (1972). Generalized linear models. Journal of the Royal Statistical Society: Series A (General), 135(3), 370–384. https://doi.org/10.2307/2344614 "],["metric-predicted-variable-on-one-or-two-groups.html", "16 Metric-Predicted Variable on One or Two Groups 16.1 Estimating the mean and standard deviation of a normal distribution 16.2 Outliers and robust estimation: The \\(t\\) distribution 16.3 Two groups 16.4 Other noise distributions and transforming data Session info", " 16 Metric-Predicted Variable on One or Two Groups In the context of the generalized linear model (GLM) introduced in the previous chapter, this chapter’s situation involves the most trivial cases of the linear core of the GLM, as indicated in the left cells of Table 15.1 (p. 434), with a link function that is the identity along with a normal distribution for describing noise in the data, as indicated in the first row of Table 15.2 (p. 443). We will explore options for the prior distribution on parameters of the normal distribution, and methods for Bayesian estimation of the parameters. We will also consider alternative noise distributions for describing data that have outliers. (Kruschke, 2015, pp. 449–450) Although I agree this chapter covers the “most trivial cases of the linear core of the GLM”, Kruschke’s underselling himself a bit, here. In addition to “trivial” Gaussian models, Kruschke went well beyond and introduced robust Student’s \\(t\\) modeling. It’s a testament to Kruschke’s rigorous approach that he did so so early in the text. IMO, we could use more robust Student’s \\(t\\) models in the social sciences. So heed well, friends. 16.1 Estimating the mean and standard deviation of a normal distribution The Gaussian probability density function follows the form \\[p(y | \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left (-\\frac{1}{2} \\frac{(y - \\mu)^2}{\\sigma^2} \\right ),\\] where the two parameters to estimate are \\(\\mu\\) (i.e., the mean) and \\(\\sigma\\) (i.e., the standard deviation). If you prefer to think in terms of \\(\\sigma^2\\), that’s the variance. In case is wasn’t clear, \\(\\pi\\) is the actual number \\(\\pi\\), not a parameter to be estimated. We’ll divide Figure 16.1 into data and plot steps. I came up with the primary data like so: library(tidyverse) sequence_length &lt;- 100 d &lt;- crossing(y = seq(from = 50, to = 150, length.out = sequence_length), mu = c(87.8, 100, 112), sigma = c(7.35, 12.2, 18.4)) %&gt;% mutate(density = dnorm(y, mean = mu, sd = sigma), mu = factor(mu, labels = str_c(&quot;mu==&quot;, c(87.8, 100, 112))), sigma = factor(sigma, labels = str_c(&quot;sigma==&quot;, c(7.35, 12.2, 18.4)))) head(d) ## # A tibble: 6 × 4 ## y mu sigma density ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 50 mu==87.8 sigma==7.35 9.80e- 8 ## 2 50 mu==87.8 sigma==12.2 2.69e- 4 ## 3 50 mu==87.8 sigma==18.4 2.63e- 3 ## 4 50 mu==100 sigma==7.35 4.85e-12 ## 5 50 mu==100 sigma==12.2 7.37e- 6 ## 6 50 mu==100 sigma==18.4 5.40e- 4 Instead of putting the coordinates for the three data points in our d tibble, I just threw them into their own tibble in the geom_point() function. Okay, let’s talk color and theme. For this chapter, we’ll take our color palette from the beyonce package (D. L. Miller, 2021). As one might guess, the beyonce package provides an array of palettes based on pictures of Beyoncé. The origins of the palettes come from https://beyoncepalettes.tumblr.com/. Our palette will be #126. library(beyonce) beyonce_palette(126) bp &lt;- beyonce_palette(126)[] bp ## [1] &quot;#484D53&quot; &quot;#737A82&quot; &quot;#A67B6B&quot; &quot;#DABFAC&quot; &quot;#E7DDD3&quot; &quot;#D47DD2&quot; ## attr(,&quot;number&quot;) ## [1] 126 Our overall theme will be based on the default ggplot2::theme_grey(). theme_set( theme_grey() + theme(text = element_text(color = &quot;white&quot;), axis.text = element_text(color = beyonce_palette(126)[5]), axis.ticks = element_line(color = beyonce_palette(126)[5]), legend.background = element_blank(), legend.box.background = element_rect(fill = beyonce_palette(126)[5], color = &quot;transparent&quot;), legend.key = element_rect(fill = beyonce_palette(126)[5], color = &quot;transparent&quot;), legend.text = element_text(color = beyonce_palette(126)[1]), legend.title = element_text(color = beyonce_palette(126)[1]), panel.background = element_rect(fill = beyonce_palette(126)[5], color = beyonce_palette(126)[5]), panel.grid = element_blank(), plot.background = element_rect(fill = beyonce_palette(126)[1], color = beyonce_palette(126)[1]), strip.background = element_rect(fill = beyonce_palette(126)[4]), strip.text = element_text(color = beyonce_palette(126)[1])) ) Here’s Figure 16.1. d %&gt;% ggplot(aes(x = y)) + geom_area(aes(y = density), fill = bp[2]) + geom_vline(xintercept = c(85, 100, 115), linetype = 3, color = bp[5]) + geom_point(data = tibble(y = c(85, 100, 115)), aes(y = 0.002), size = 2, color = bp[6]) + scale_y_continuous(expression(italic(p)(italic(y)*&quot;|&quot;*mu*&quot;, &quot;*sigma)), expand = expansion(mult = c(0, 0.05)), breaks = NULL) + ggtitle(&quot;Competing Gaussian likelihoods given the same data&quot;) + coord_cartesian(xlim = c(60, 140)) + facet_grid(sigma ~ mu, labeller = label_parsed) Here’s how you might compute the \\(p(D | \\mu, \\sigma)\\) values and identify which combination of \\(\\mu\\) and \\(\\sigma\\) returns the maximum value for the data set. crossing(y = c(85, 100, 115), mu = c(87.8, 100, 112), sigma = c(7.35, 12.2, 18.4)) %&gt;% mutate(density = dnorm(y, mean = mu, sd = sigma)) %&gt;% group_by(mu, sigma) %&gt;% summarise(`p(D|mu, sigma)` = prod(density)) %&gt;% ungroup() %&gt;% mutate(maximum = `p(D|mu, sigma)` == max(`p(D|mu, sigma)`)) ## # A tibble: 9 × 4 ## mu sigma `p(D|mu, sigma)` maximum ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 87.8 7.35 0.0000000398 FALSE ## 2 87.8 12.2 0.00000172 FALSE ## 3 87.8 18.4 0.00000271 FALSE ## 4 100 7.35 0.00000248 FALSE ## 5 100 12.2 0.00000771 TRUE ## 6 100 18.4 0.00000524 FALSE ## 7 112 7.35 0.0000000456 FALSE ## 8 112 12.2 0.00000181 FALSE ## 9 112 18.4 0.00000277 FALSE 16.1.1 Solution by mathematical analysis Heads up on precision. Not much for us, here. But we might reiterate that sometimes we talk about the precision (see page 453), which is the reciprocal of the variance (i.e., \\(\\frac{1}{\\sigma^2}\\)). As we’ll see, the brms package doesn’t use priors parameterized in terms of precision. But JAGS does, which means we’ll need to be able to translate Kruschke’s precision-laden JAGS code into \\(\\sigma\\)-oriented brms code in many of the remaining chapters. Proceed with caution. 16.1.2 Approximation by MCMC in JAGS HMC in brms. Let’s load and glimpse() at the TwoGroupIQ.csv data. my_data &lt;- read_csv(&quot;data.R/TwoGroupIQ.csv&quot;) glimpse(my_data) ## Rows: 120 ## Columns: 2 ## $ Score &lt;dbl&gt; 102, 107, 92, 101, 110, 68, 119, 106, 99, 103, 90, 93, 79, 89, 137, 119, 126, 110, 7… ## $ Group &lt;chr&gt; &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, … The data file included values from two groups. my_data %&gt;% distinct(Group) ## # A tibble: 2 × 1 ## Group ## &lt;chr&gt; ## 1 Smart Drug ## 2 Placebo Kruschke clarified that for the following example, “the data are IQ (intelligence quotient) scores from a group of people who have consumed a ‘smart drug’” (p. 456). That means we’ll want to subset the data. my_data &lt;- my_data %&gt;% filter(Group == &quot;Smart Drug&quot;) It’s a good idea to take a look at the data before modeling. my_data %&gt;% ggplot(aes(x = Score)) + geom_histogram(fill = bp[2], binwidth = 5) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;The smart-drug group&quot;) Here are the mean and standard deviation of the Score values. (mean_y &lt;- mean(my_data$Score)) ## [1] 107.8413 (sd_y &lt;- sd(my_data$Score)) ## [1] 25.4452 The values of those sample statistics will come in handy in just a bit. But first, let’s load brms. library(brms) If we want to pass user-defined values into our brm() prior code, we’ll need to define them first in using the brms::stanvar() function. stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) It’s been a while since we used stanvar(), so we should review. Though we’ve saved the object as stanvars, you could name it whatever you want. The main trick is to tell brms about your values with the stanvars argument within brm(). Kruschke mentioned that the “the conventional noncommittal gamma prior [for the precision] has shape and rate constants that are close to zero, such as Sh = 0.01 and R = 0.01” (p. 456). Here’s what that looks like. tibble(x = seq(from = 0, to = 12, by = 0.025)) %&gt;% mutate(d = dgamma(x, shape = 0.01, rate = 0.01)) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = bp[3]) + geom_vline(xintercept = 1 / sd_y^2, linetype = 2, color = bp[5]) + labs(subtitle = &quot;The grey density in the background is the conventional gamma prior for precision.\\nThe dashed vertical line is our precision value.&quot;) + scale_x_continuous(expand = expansion(mult = c(0, 0.05))) + scale_y_continuous(expand = expansion(mult = c(0, 0.05)), breaks = NULL) + coord_cartesian(xlim = c(0, 8), ylim = c(0, 0.35)) ## Warning: Removed 1 rows containing non-finite values (`stat_align()`). The thing is, with brms we typically estimate \\(\\sigma\\) rather than precision. Though gamma is also a feasible prior distribution for \\(\\sigma\\), we won’t use it here. But we won’t be using Kruschke’s uniform prior, either. The Stan team discourages uniform priors for variance parameters, such as our \\(\\sigma\\). I’m not going to get into the details of why, but you’ve got that hyperlink above and the good old Stan user’s guide (Stan Development Team, 2022c) if you’d like to dive deeper. Here we’ll use the half normal. By “half normal,” we mean that the mean is zero and it’s bounded from zero to positive infinity–no negative \\(\\sigma\\) values for us! By the “half normal,” we also mean to suggest that smaller values are more credible than those approaching infinity. When working with unstandardized data, an easy default for a weakly-regularizing half normal is to set the \\(\\sigma\\) hyperparameter (i.e., S) to the standard deviation of the criterion variable (i.e., \\(s_Y\\)). Here’s that that looks like for this example. tibble(x = seq(from = 0, to = 110, by = 0.1)) %&gt;% mutate(d = dnorm(x, mean = 0, sd = sd_y)) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = bp[3]) + geom_vline(xintercept = sd_y, linetype = 2, color = bp[5]) + scale_x_continuous(expand = expansion(mult = c(0, 0.05))) + scale_y_continuous(expand = expansion(mult = c(0, 0.05)), breaks = NULL) + labs(subtitle = &quot;The gray density in the background is the half-normal prior for sigma.\\nThe dashed vertical line is our &#39;sd_y&#39; value.&quot;) + coord_cartesian(xlim = c(0, 100)) This prior isn’t quite as non-committal as the conventional gamma prior for precision. However, it will discourage the HMC algorithm from exploring \\(\\sigma\\) values much larger than two or three times the standard deviation in the data themselves. In practice, I’ve found it to have a minimal influence on the posterior. If you’d like to make it even less committal, try setting that \\(\\sigma\\) hyperparameter to some multiple of \\(s_Y\\) like \\(2 \\times s_Y\\) or \\(10 \\times s_Y\\). Compare this to Kruschke’s recommendations for setting a noncommittal uniform prior for \\(\\sigma\\). When using the uniform distribution, \\(\\operatorname{Uniform}(L, H)\\), we will set the high value \\(H\\) of the uniform prior on \\(\\sigma\\) to a huge multiple of the standard deviation in the data, and set the low value \\(L\\) to a tiny fraction of the standard deviation in the data. Again, this means that the prior is vague no matter what the scale of the data happens to be. (p. 455) On page 456, Kruschke gave an example of such a uniform prior with the code snip dunif( sdY/1000 , sdY*1000 ). Here’s what that would look like with our data. tibble(x = 0:(sd_y * 1000)) %&gt;% mutate(d = dunif(x, min = sd_y / 1000, max = sd_y * 1000)) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = bp[3]) + geom_vline(xintercept = sd_y, linetype = 2, color = bp[5]) + scale_x_continuous(expand = expansion(mult = c(0, 0.05))) + scale_y_continuous(expand = expansion(mult = c(0, 0.05)), breaks = NULL) + labs(subtitle = &quot;The gray density in the background is Kruschke&#39;s uniform prior for sigma.\\nThe dashed vertical line is our &#39;sd_y&#39; value.&quot;) + coord_cartesian() That’s really noncommittal. I’ll stick with my half normal. You do you. Kruschke had this to say about the prior for the mean: In this application we seek broad priors relative to typical data, so that the priors have minimal influence on the posterior. One way to discover the constants is by asking an expert in the domain being studied. But in lieu of that, we will use the data themselves to tell us what the typical scale of the data is. We will set \\(M\\) to the mean of the data, and set \\(S\\) to a huge multiple (e.g., \\(100\\)) of the standard deviation of the data. This way, no matter what the scale of the data is, the prior will be vague. (p. 455) In case you’re not following along closely in the text, we often use the normal distribution for the intercept and slope parameters in a simple regression model. By \\(M\\) and \\(S\\), Kruschke was referring to the \\(\\mu\\) and \\(\\sigma\\) parameters of the normal prior for our intercept. Here’s what that prior looks like in this data example. tibble(x = seq(from = -10000, to = 10000, by = 10)) %&gt;% mutate(d = dnorm(x, mean = mean_y, sd = sd_y * 100)) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = bp[3]) + geom_vline(xintercept = mean_y, linetype = 2, color = bp[5]) + scale_y_continuous(expand = expansion(mult = c(0, 0.05)), breaks = NULL) + labs(subtitle = &quot;The brown density in the background is the normal prior for mu.\\nThe dashed vertical line is our &#39;mean_y&#39; value.&quot;) Yep, Kruschke’s right. That is one noncommittal prior given our data. We could tighten that up by an order of magnitude and still have little influence on the posterior. Now we’ve decided on our parameterization (\\(\\sigma\\), not \\(\\tau\\)) and our priors (half-normal, not uniform or gamma), we are ready to make our version of the model diagram in Figure 16.2. library(patchwork) # normal density p1 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bp[5]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)&quot;, &quot;italic(S)[mu]&quot;), size = 7, color = bp[5], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[4])) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. # half-normal density p2 &lt;- tibble(x = seq(from = 0, to = 3, by = .01), d = (dnorm(x)) / max(dnorm(x))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7, color = bp[5]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma]&quot;, size = 7, color = bp[5], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[4])) ## two annotated arrows # save our custom arrow settings my_arrow &lt;- arrow(angle = 20, length = unit(0.35, &quot;cm&quot;), type = &quot;closed&quot;) p3 &lt;- tibble(x = c(.43, 1.5), y = c(1, 1), xend = c(.43, .8), yend = c(.2, .2)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bp[4]) + annotate(geom = &quot;text&quot;, x = c(.3, 1), y = .6, label = &quot;&#39;~&#39;&quot;, size = 10, color = bp[5], family = &quot;Times&quot;, parse = T) + xlim(0, 2) + theme_void() # a second normal density p4 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bp[5]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;mu&quot;, &quot;sigma&quot;), size = 7, color = bp[5], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[4])) # the final annotated arrow p5 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = bp[5], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, color = bp[4], arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p6 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = bp[5], parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 1, r = 2), area(t = 1, b = 2, l = 3, r = 4), area(t = 4, b = 5, l = 1, r = 2), area(t = 3, b = 4, l = 1, r = 4), area(t = 6, b = 6, l = 1, r = 2), area(t = 7, b = 7, l = 1, r = 2) ) # combine and plot! (p1 + p2 + p4 + p3 + p5 + p6) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Two things about the notation in our diagram: Because we have two \\(\\sigma\\) hyperparameters, we’ve denoted the one for the prior on \\(\\mu\\) as \\(S_\\mu\\) and the one for the prior on \\(\\sigma\\) as \\(S_\\sigma\\). Also, note that we fixed the \\(\\mu\\) hyperparameter for half-normal prior to zero. This won’t always be the case, but it’s so common within the brms ecosystem that I’m going to express it this way throughout most of this ebook. This is our default. Here’s how to put these priors to use with brms. fit16.1 &lt;- brm(data = my_data, family = gaussian, Score ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 16, file = &quot;fits/fit16.01&quot;) To be more explicit, the stanvars = stanvars argument at the bottom of our code is what allowed us to define our intercept prior as normal(mean_y, sd_y * 100) instead of requiring us to type in the parameters as normal(107.8413, 25.4452 * 100). The same basic point goes for our \\(\\sigma\\) prior. Also, notice our prior code for \\(\\sigma\\), prior(normal(0, sd_y), class = sigma). Nowhere in there did we actually say we wanted a half normal as opposed to a typical normal. That’s because the brms default is to set the lower bound for priors of class = sigma to zero. There’s no need for us to fiddle with it. Let’s examine the chains. plot(fit16.1, widths = c(2, 3)) They look good! The model summary looks sensible, too. print(fit16.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Score ~ 1 ## Data: my_data (Number of observations: 63) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 107.85 3.25 101.40 114.28 1.00 3675 2816 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 25.77 2.27 21.63 30.73 1.00 3277 3008 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Compare those values with mean_y and sd_y. mean_y ## [1] 107.8413 sd_y ## [1] 25.4452 Good times. Let’s extract the posterior draws and save them in a data frame draws. draws &lt;- as_draws_df(fit16.1) Here’s the leg work required to make our version of the three histograms in Figure 16.3. # we&#39;ll need this for `stat_pointinterval()` library(tidybayes) # we&#39;ll use this to mark off the ROPEs as white strips in the background rope &lt;- tibble(name = c(&quot;Mean&quot;, &quot;Standard Deviation&quot;, &quot;Effect Size&quot;), xmin = c(99, 14, -0.1), xmax = c(101, 16, 0.1)) # annotate the ROPE text &lt;- tibble(x = 0, y = 0.98, label = &quot;ROPE&quot;, name = &quot;Effect Size&quot;) # here are the primary data draws %&gt;% transmute(Mean = b_Intercept, `Standard Deviation` = sigma) %&gt;% mutate(`Effect Size` = (Mean - 100) / `Standard Deviation`) %&gt;% pivot_longer(everything()) %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = &quot;white&quot;) + stat_histinterval(aes(x = value, y = 0), point_interval = mode_hdi, .width = .95, fill = bp[2], color = bp[6], normalize = &quot;panels&quot;) + geom_text(data = text, aes(x = x, y = y, label = label), size = 2.5, color = bp[4]) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) If we wanted those exact 95% HDIs, we’d execute this. draws %&gt;% transmute(Mean = b_Intercept, `Standard Deviation` = sigma) %&gt;% mutate(`Effect Size` = (Mean - 100) / `Standard Deviation`) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mode_hdi(value) ## # A tibble: 3 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Effect Size 0.295 0.0603 0.565 0.95 mode hdi ## 2 Mean 108. 101. 114. 0.95 mode hdi ## 3 Standard Deviation 25.0 21.1 30.0 0.95 mode hdi For the next part, we should look at the structure of the posterior draws, draws. head(draws) ## # A draws_df: 6 iterations, 1 chains, and 4 variables ## b_Intercept sigma lprior lp__ ## 1 110 25 -13 -303 ## 2 110 25 -13 -303 ## 3 110 26 -13 -303 ## 4 108 22 -13 -304 ## 5 107 26 -13 -302 ## 6 109 25 -13 -302 ## # ... hidden reserved variables {&#39;.chain&#39;, &#39;.iteration&#39;, &#39;.draw&#39;} By default, head() returned six rows, each one corresponding to the credible parameter values from a given posterior draw. Following our model equation \\(\\text{Score}_i \\sim N(\\mu, \\sigma)\\), we might reformat the first two columns as: Score ~ \\(N\\)(110.18, 25.201) Score ~ \\(N\\)(110.18, 25.201) Score ~ \\(N\\)(110.12, 25.623) Score ~ \\(N\\)(108.03, 21.635) Score ~ \\(N\\)(106.759, 25.599) Score ~ \\(N\\)(109.443, 24.857) Each row of draws yields a full model equation with which we might credibly describe the data–or at least as credibly as we can within the limits of the model. We can give voice to a subset of these credible distributions with our version of the upper right panel of Figure 16.3. Before I show that plotting code, it might make sense to slow down on the preparatory data wrangling steps. There are several ways to overlay multiple posterior predictive density lines like those in our upcoming plots. We’ll practice a few over the next few chapters. For the method we’ll use in this chapter, it’s handy to first determine how many you’d like. Here we’ll follow Kruschke and choose 63, which we’ll save as n_lines. # how many credible density lines would you like? n_lines &lt;- 63 Now we’ve got our n_lines value, we’ll use it to subset the rows in draws with the slice() function. We’ll then use expand() to include a sequence of Score values to correspond to the formula implied in each of the remaining rows of draws. Notice how we also kept the .draw index in the game. That will help us with the plot in a bit. But the main event is how we used Score, b_Intercept, and sigma as the input for the arguments in the dnorm(). The output is a column of the corresponding density values. draws &lt;- draws %&gt;% slice(1:n_lines) %&gt;% expand(nesting(.draw, b_Intercept, sigma), Score = seq(from = 40, to = 250, by = 1)) %&gt;% mutate(density = dnorm(x = Score, mean = b_Intercept, sd = sigma)) str(draws) ## tibble [13,293 × 5] (S3: tbl_df/tbl/data.frame) ## $ .draw : int [1:13293] 1 1 1 1 1 1 1 1 1 1 ... ## $ b_Intercept: num [1:13293] 110 110 110 110 110 ... ## $ sigma : num [1:13293] 25.2 25.2 25.2 25.2 25.2 ... ## $ Score : num [1:13293] 40 41 42 43 44 45 46 47 48 49 ... ## $ density : num [1:13293] 0.000328 0.000366 0.000407 0.000453 0.000503 ... Note that after using expand(), we have a rather long data frame. Anyway, we’re ready to plot. draws %&gt;% ggplot(aes(x = Score)) + geom_histogram(data = my_data, aes(y = stat(density)), fill = bp[2], size = .2, binwidth = 5, boundary = 0) + geom_line(aes(y = density, group = .draw), size = 1/4, alpha = 1/3, color = bp[6]) + scale_x_continuous(&quot;y&quot;, limits = c(50, 210)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Data with Post. Pred.&quot;) Note the stat(density) argument in the geom_histogram() function. That’s what rescaled the histogram to the density metric. If you leave that part out, all the density lines will drop to the bottom of the plot. Also, did you see how we used .draw to group the density lines within the geom_line() function? That’s why we kept that information. Without that group = .draw argument, the resulting lines are a mess. Kruschke pointed out this last plot constitutes a form of posterior-predictive check, by which we check whether the model appears to be a reasonable description of the data. With such a small amount of data, it is difficult to visually assess whether normality is badly violated, but there appears to be a hint that the normal model is straining to accommodate some outliers: The peak of the data protrudes prominently above the normal curves, and there are gaps under the shoulders of the normal curves. (p. 458) We can perform a similar posterior-predictive check with the brms::pp_check() function. By default, it will return 10 simulated density lines. Like we did above, we’ll increase that by setting the ndraws argument to our n_lines value. library(bayesplot) color_scheme_set(scheme = bp[c(3, 1, 2, 5, 4, 6)]) pp_check(fit16.1, ndraws = n_lines) In principle, we didn’t need to load the bayesplot package to use the brms::pp_check() function. But doing so gave us access to the bayesplot::color_scheme_set(), which allowed us to apply the colors from our color palette to the plot. Before we move on, we should talk a little about effect sizes, which we all but glossed over in our code. Effect size is simply the amount of change induced by the treatment relative to the standard deviation: \\((\\mu - 100) / \\sigma\\). In other words, the effect size is the “standardized” change… A conventionally “small” effect size in psychological research is \\(0.2\\) (Cohen, 1988), and the ROPE limits are set at half that size for purposed of illustration. (p. 457, emphasis in the original). Another way to describe this kind of effect size is as a standardized mean difference. In addition to the seminal work by Cohen, you might brush up on effect sizes with Kelley and Preacher’s (2012) On effect size. 16.2 Outliers and robust estimation: The \\(t\\) distribution Here’s the code for our version of Figure 16.4. # wrangle crossing(nu = c(Inf, 4, 2, 1), y = seq(from = -8, to = 8, length.out = 500)) %&gt;% mutate(density = dt(x = y, df = nu)) %&gt;% # this line is unnecessary, but will help with the plot legend mutate(nu = factor(nu, levels = c(&quot;Inf&quot;, &quot;4&quot;, &quot;2&quot;, &quot;1&quot;))) %&gt;% # plot ggplot(aes(x = y, y = density, group = nu, color = nu)) + geom_line() + scale_color_manual(expression(paste(italic(t)[nu])), values = bp[c(6, 3:1)]) + scale_y_continuous(expression(p(y)), expand = expansion(mult = c(0, 0.05))) + coord_cartesian(xlim = c(-6, 6)) + theme(legend.position = c(.92, .75)) Although the \\(t\\) distribution is usually conceived as a sampling distribution for the NHST \\(t\\) test, we will use it instead as a convenient descriptive model of data with outliers… Outliers are simply data values that fall unusually far from a model’s expected value. Real data often contain outliers relative to a normal distribution. Sometimes the anomalous values can be attributed to extraneous influences that can be explicitly identified, in which case the affected data values can be corrected or removed. But usually we have no way of knowing whether a suspected outlying value was caused by an extraneous influence, or is a genuine representation of the target being measured. Instead of deleting suspected outliers from the data according to some arbitrary criterion, we retain all the data but use a noise distribution that is less affected by outliers than is the normal distribution. (p. 459) Here’s Figure 16.5.a. tibble(y = seq(from = -10, to = 20, length.out = 1e3)) %&gt;% ggplot(aes(x = y)) + geom_area(aes(y = dnorm(y, mean = 2.5, sd = 5.73)), fill = bp[2], alpha = 1/2) + geom_area(aes(y = metRology::dt.scaled(y, df = 1.14, mean = .12, sd = 1.47)), fill = bp[2], alpha = 1/2) + geom_vline(xintercept = c(.12, 2.5), color = bp[5], linetype = 3) + annotate(geom = &quot;point&quot;, x = c(-2:2, 15), y = 0.002, size = 2, color = bp[6]) + annotate(geom = &quot;text&quot;, x = c(1, 4), y = c(0.2, 0.08), label = c(&quot;italic(t)&quot;, &quot;italic(N)&quot;), color = bp[1], parse = T) + scale_y_continuous(expression(p(y)), expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Maximum Likelihood Estimates&quot;) + coord_cartesian(xlim = c(-5, 15)) I’m not aware that we have the data for the bottom panel of Figure 16.5. However, we can simulate similar data with the rt.scaled() function from the metRology package (Ellison., 2018). set.seed(145) # simulate the data d &lt;- tibble(y = metRology::rt.scaled(n = 177, df = 2.63, mean = 1.11, sd = 0.15)) # plot tibble(y = seq(from = -3, to = 12, length.out = 1e3)) %&gt;% ggplot(aes(y)) + geom_histogram(data = d, aes(y = stat(density)), fill = bp[3], size = .2, binwidth = .1) + geom_line(aes(y = dnorm(y, mean = 1.16, sd = 0.63)), color = bp[2]) + geom_line(aes(y = metRology::dt.scaled(y, df = 2.63, mean = 1.11, sd = 0.15)), color = bp[2]) + annotate(geom = &quot;text&quot;, x = c(1.5, 1.9), y = c(1.5, 0.6), label = c(&quot;italic(t)&quot;, &quot;italic(N)&quot;), color = bp[1], parse = T) + scale_x_continuous(breaks = seq(from = -2, to = 10, by = 2)) + scale_y_continuous(expression(p(y)), expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Maximum Likelihood Estimates&quot;) + coord_cartesian(xlim = c(-1.5, 10.25)) In case you were curious, this is how I selected the seed for the plot. Run the code yourself to get a sense of how it works. # in the R Notebook code block settings, I used: fig.width = 2, fig.height = 8 t_maker &lt;- function(seed) { set.seed(seed) tibble(y = metRology::rt.scaled(n = 177, df = 2.63, mean = 1.11, sd = 0.15)) %&gt;% summarise(min = min(y), max = max(y)) %&gt;% mutate(spread = max - min) } tibble(seed = 1:200) %&gt;% mutate(t = map(seed, t_maker)) %&gt;% unnest(t) %&gt;% ggplot(aes(x = reorder(seed, spread), ymin = min, ymax = max)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_linerange() + coord_flip() It is important to understand that the scale parameter \\(\\sigma\\) in the \\(t\\) distribution is not the standard deviation of the distribution. (Recall that the standard deviation is the square root of the variance, which is the expected value of the squared deviation from the mean, as defined back in Equation 4.8, p. 86.) The standard deviation is actually larger than \\(\\sigma\\) because of the heavy tails… While this value of the scale parameter is not the standard deviation of the distribution, it does have an intuitive relation to the spread of the data. Just as the range \\(\\pm \\sigma\\) covers the middle \\(68\\%\\) of a normal distribution, the range \\(\\pm \\sigma\\) covers the middle \\(58\\%\\) of a \\(t\\) distribution when \\(\\nu = 2\\), and the middle \\(50\\%\\) when \\(\\nu = 1\\). These areas are illustrated in the left column of Figure 16.6. The right column of Figure 16.6 shows the width under the middle of a \\(t\\) distribution that is needed to span \\(68.27\\%\\) of the distribution, which is the area under a normal distribution for \\(\\sigma = \\pm 1\\). (pp. 459–461, emphasis in the original) Speaking of which, here’s the code for the left column for Figure 16.6. # the primary data d &lt;- crossing(y = seq(from = -8, to = 8, length.out = 1e3), nu = c(Inf, 5, 2, 1)) %&gt;% mutate(label = str_c(&quot;nu == &quot;, nu) %&gt;% factor(., levels = c(&quot;nu == Inf&quot;, &quot;nu == 5&quot;, &quot;nu == 2&quot;, &quot;nu == 1&quot;))) # the subplot p1 &lt;- d %&gt;% ggplot(aes(x = y)) + geom_area(aes(y = dt(y, df = nu)), fill = bp[2]) + geom_area(data = . %&gt;% filter(y &gt;= -1 &amp; y &lt;= 1), aes(y = dt(y, df = nu)), fill = bp[1]) + # note how this function has its own data geom_text(data = tibble( y = 0, text = c(&quot;68%&quot;, &quot;64%&quot;, &quot;58%&quot;, &quot;50%&quot;), label = factor(c(&quot;nu == Inf&quot;, &quot;nu == 5&quot;, &quot;nu == 2&quot;, &quot;nu == 1&quot;))), aes(y = .175, label = text), color = &quot;white&quot;) + scale_y_continuous(expression(p(y)), expand = expansion(mult = c(0, 0.05)), breaks = c(0, .2, .4)) + labs(subtitle = &quot;Shaded from y = - 1 to y = 1&quot;) + coord_cartesian(xlim = c(-6, 6)) + facet_wrap(~ label, ncol = 1, labeller = label_parsed) Here’s the code for the right column. # the primary data d &lt;- tibble(nu = c(Inf, 5, 2, 1), ymin = c(-1, -1.11, -1.32, -1.84)) %&gt;% mutate(ymax = -ymin) %&gt;% expand(nesting(nu, ymin, ymax), y = seq(from = -8, to = 8, length.out = 1e3)) %&gt;% mutate(label = factor(str_c(&quot;nu==&quot;, nu), levels = str_c(&quot;nu==&quot;, c(Inf, 5, 2, 1)))) # the subplot p2 &lt;- d %&gt;% ggplot(aes(x = y)) + geom_area(aes(y = dt(y, df = nu)), fill = bp[2]) + geom_area(data = . %&gt;% # notice our `filter()` argument has changed filter(y &gt;= ymin &amp; y &lt;= ymax), aes(y = dt(y, df = nu)), fill = bp[1]) + annotate(geom = &quot;text&quot;, x = 0, y = .175, label = &quot;68%&quot;, color = &quot;white&quot;) + scale_y_continuous(expression(p(y)), expand = expansion(mult = c(0, 0.05)), breaks = c(0, .2, .4)) + labs(subtitle = &quot;Shaded for the middle 68.27%&quot;) + coord_cartesian(xlim = c(-6, 6)) + facet_wrap(~ label, ncol = 1, labeller = label_parsed) You may have noticed that we just pulled the values in the ymin column directly from Kruschke’s version of the figure on page 461. If you’d like a better understanding of where those values came from, you can reproduce them with the qt() function. qt(p = (1 - .6827) / 2, df = c(Inf, 5, 2, 1)) %&gt;% round(digits = 2) ## [1] -1.00 -1.11 -1.32 -1.84 Anyway, let’s bind the two ggplots together with the patchwork package to make the full version of Figure 16.6. p1 + p2 The use of a heavy-tailed distribution is often called robust estimation because the estimated value of the central tendency is stable, that is, “robust,” against outliers. The \\(t\\) distribution is useful as a likelihood function for modeling outliers at the level of observed data. But the \\(t\\) distribution is also useful for modeling outliers at higher levels in a hierarchical prior. We will encounter several applications. (p. 462, emphasis in the original) 16.2.1 Using the \\(t\\) distribution in JAGS brms. It’s easy to use Student’s \\(t\\) in brms. Just make sure to set family = student. By default, brms already sets the lower bound for \\(\\nu\\) to 1. But we do still need to use 1/29. To get a sense, let’s simulate exponential data using the rexp() function. Like Kruschke covered in the text (p. 462), the rexp() function has one parameter, rate, which is the reciprocal of the mean. Here we’ll set the mean to 29. n_draws &lt;- 1e7 mu &lt;- 29 set.seed(16) tibble(y = rexp(n = n_draws, rate = 1 / mu)) %&gt;% mutate(y_at_least_1 = ifelse(y &lt; 1, NA, y)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value, na.rm = T)) ## # A tibble: 2 × 2 ## name mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 y 29.0 ## 2 y_at_least_1 30.0 The simulation showed that when we define the exponential rate as 1/29 and use the typical boundary at 0, the mean of our samples converges to 29. But when we only consider the samples of 1 or greater, the mean converges to 30. Thus, our \\(\\operatorname{Exponential}(1/29)\\) prior with a boundary at 1 is how we get a shifted exponential distribution when we use it as our prior for \\(\\nu\\) in brms. Just make sure to remember that if you want the mean to be 30, you’ll need to specify the rate of 1/29. Also, Stan will bark if you simply try to set that exponential prior with the code prior(exponential(1/29), class = nu): DIAGNOSTIC(S) FROM PARSER: Info: integer division implicitly rounds to integer. Found int division: 1 / 29 Positive values rounded down, negative values rounded up or down in platform-dependent way. To avoid this, just do the division beforehand and save the results with stanvar(). stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) Here’s the brm() code. Note that we set the prior for our new \\(\\nu\\) parameter by specifying class = nu within the last prior() line. fit16.2 &lt;- brm(data = my_data, family = student, Score ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvars, seed = 16, file = &quot;fits/fit16.02&quot;) We can make the shifted exponential distribution (i.e., Figure 16.7) with simple addition. # how many draws would you like? n_draws &lt;- 1e6 # here are the data d &lt;- tibble(exp = rexp(n_draws, rate = 1/29)) %&gt;% transmute(exp_plus_1 = exp + 1, log_10_exp_plus_1 = log10(exp + 1)) # this is the plot in the top panel p1 &lt;- d %&gt;% ggplot(aes(x = exp_plus_1)) + geom_histogram(fill = bp[2], size = .2, binwidth = 5, boundary = 1) + stat_pointinterval(aes(y = 0), point_interval = mode_hdi, .width = .95, color = bp[6]) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(exponential(lambda==29)~shifted~+1), x = expression(nu)) + coord_cartesian(xlim = c(0, 150)) # the bottom panel plot p2 &lt;- d %&gt;% ggplot(aes(x = log_10_exp_plus_1)) + geom_histogram(fill = bp[2], size = .2, binwidth = .1, boundary = 0) + stat_pointinterval(aes(y = 0), point_interval = mode_hdi, .width = .95, color = bp[6]) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(log10(nu))) + coord_cartesian(xlim = c(0, 2.5)) # bind them together (p1 / p2) &amp; scale_x_continuous(expand = expansion(mult = c(0, 0.05))) Here are the scatter plots of Figure 16.8. pairs(fit16.2, off_diag_args = list(size = 1/3, alpha = 1/3)) I’m not aware of an easy way to use log10(nu) instead of nu with brms::pairs(). However, you can get those plots with the as_draws_df() function and a little wrangling. draws &lt;- as_draws_df(fit16.2) draws %&gt;% mutate(`log10(nu)` = log10(nu)) %&gt;% rename(mu = b_Intercept) %&gt;% select(mu, sigma, `log10(nu)`) %&gt;% pivot_longer(-`log10(nu)`) %&gt;% ggplot(aes(x = `log10(nu)`, y = value)) + geom_point(color = bp[6], size = 1/3, alpha = 1/3) + labs(x = expression(log10(nu)), y = NULL) + facet_grid(name ~ ., scales = &quot;free&quot;, switch = &quot;y&quot;, labeller = label_parsed) If you want the Pearson’s correlation coefficients, you can use base R cor(). draws %&gt;% mutate(`log10(nu)` = log10(nu)) %&gt;% select(b_Intercept, sigma, `log10(nu)`) %&gt;% cor() %&gt;% round(digits = 3) ## b_Intercept sigma log10(nu) ## b_Intercept 1.000 0.063 0.062 ## sigma 0.063 1.000 0.736 ## log10(nu) 0.062 0.736 1.000 The correlations among our parameters are a similar magnitude as those Kruschke presented in the text. Here are four of the panels for Figure 16.9. # we&#39;ll use this to mark off the ROPEs as white strips in the background rope &lt;- tibble(name = c(&quot;Mean&quot;, &quot;Scale&quot;, &quot;Effect Size&quot;), xmin = c(99, 14, -.1), xmax = c(101, 16, .1)) # here are the primary data draws %&gt;% transmute(Mean = b_Intercept, Scale = sigma, Normality = log10(nu)) %&gt;% mutate(`Effect Size` = (Mean - 100) / Scale) %&gt;% pivot_longer(everything()) %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = &quot;white&quot;) + stat_histinterval(aes(x = value, y = 0), point_interval = mode_hdi, .width = .95, fill = bp[2], color = bp[6], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) For the final panel of Figure 16.9, we’ll make our \\(t\\) lines in much the same way we did, earlier. But last time, we just took the first \\(\\mu\\) and \\(\\sigma\\) values from the first 63 rows of the post tibble. This time we’ll use dplyr::slice_sample() to take random draws from the post rows instead. We tell slice_sample() how many draws we’d like with the n argument. In addition to the change in our row selection strategy, this time we’ll slightly amend the code within the last mutate() line. Since we’d like to work with the \\(t\\) distribution, we specified metRology::dt.scaled() function instead of dnorm(). # how many credible density lines would you like? n_lines &lt;- 63 # setting the seed makes the results from `slice_sample()` reproducible set.seed(16) # wrangle draws %&gt;% tibble() %&gt;% slice_sample(n = n_lines) %&gt;% expand(nesting(.draw, b_Intercept, sigma, nu), Score = seq(from = 40, to = 250, by = 1)) %&gt;% mutate(density = metRology::dt.scaled(x = Score, df = nu, mean = b_Intercept, sd = sigma)) %&gt;% # plot ggplot(aes(x = Score)) + geom_histogram(data = my_data, aes(y = stat(density)), fill = bp[2], size = .2, binwidth = 5, boundary = 0) + geom_line(aes(y = density, group = .draw), size = 1/3, alpha = 1/3, color = bp[6]) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Data with Post. Pred.&quot;, x = &quot;y&quot;) + coord_cartesian(xlim = c(50, 210)) Much like Kruschke mused in the text, this plot shows that the posterior predictive \\(t\\) distributions appear to describe the data better than the normal distribution in Figure 16.3, insofar as the data histogram does not poke out at the mode and the gaps under the shoulders are smaller. (p. 464) In case you were wondering, here’s the model summary(). summary(fit16.2) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: Score ~ 1 ## Data: my_data (Number of observations: 63) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 107.11 2.82 101.80 112.70 1.00 2450 2136 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 19.55 3.44 13.33 26.44 1.00 1994 2218 ## nu 9.11 11.78 1.78 41.76 1.00 1803 1649 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It’s easy to miss how \\(\\sigma\\) in the robust estimate is much smaller than in the normal estimate. What we had interpreted as increased standard deviation induced by the smart drug might be better described as increased outliers. Both of these differences, that is, \\(\\mu\\) more tightly estimated and \\(\\sigma\\) smaller in magnitude, are a result of there being outliers in the data. The only way a normal distribution can accommodate the outliers is to use a large value for \\(\\sigma\\). In turn, that leads to “slop” in the estimate of \\(\\mu\\) because there is a wider range of \\(\\mu\\) values that reasonably fit the data when the standard deviation is large. (p. 464) We can use the brms::VarCorr() function to pull the summary statistics for \\(\\sigma\\) from both models. VarCorr(fit16.1)$residual__$sd ## Estimate Est.Error Q2.5 Q97.5 ## 25.76545 2.271015 21.6341 30.73192 VarCorr(fit16.2)$residual__$sd ## Estimate Est.Error Q2.5 Q97.5 ## 19.55234 3.441687 13.32592 26.4373 It is indeed the case that estimate for \\(\\sigma\\) is smaller in the \\(t\\) model. That smaller \\(\\sigma\\) resulted in a more precise estimate for \\(\\mu\\), as can be seen in the ‘Est.Error’ columns from the fixef() output. fixef(fit16.1) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 107.8495 3.247936 101.3962 114.2817 fixef(fit16.2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 107.1108 2.822999 101.7968 112.6969 Here that is in a coefficient plot using tidybayes::stat_interval(). bind_rows(as_draws_df(fit16.1) %&gt;% select(b_Intercept), as_draws_df(fit16.2) %&gt;% select(b_Intercept)) %&gt;% mutate(fit = rep(c(&quot;fit16.1&quot;, &quot;fit16.2&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = b_Intercept, y = fit)) + stat_interval(point_interval = mode_hdi, .width = c(.5, .8, .95)) + scale_color_manual(&quot;HDI&quot;, values = c(bp[c(4, 3, 1)]), labels = c(&quot;95%&quot;, &quot;80%&quot;, &quot;50%&quot;)) + labs(x = expression(mu), y = NULL) + theme(legend.key.size = unit(0.45, &quot;cm&quot;)) 16.2.2 Using the \\(t\\) distribution in Stan. Kruschke expressed concern about high autocorrelations in the chains of his JAGS model. Here are the results of our Stan/brms attempt. # rearrange the bayesplot color scheme color_scheme_set(scheme = bp[c(6, 2, 2, 2, 1, 2)]) draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_acf(pars = vars(b_Intercept:nu), lags = 35) For all three parameters, the autocorrelations were near zero by lag 3 or 4. Not bad. The \\(N_{eff}/N\\) ratios are okay. # rearrange the bayesplot color scheme again color_scheme_set(scheme = bp[c(2:1, 4:3, 5:6)]) neff_ratio(fit16.2) %&gt;% mcmc_neff(size = 2.5) + yaxis_text(hjust = 0) The trace plots look fine. # rearrange the bayesplot color scheme one more time color_scheme_set(scheme = c(&quot;white&quot;, bp[c(2, 2, 2, 6, 3)])) plot(fit16.2, widths = c(2, 3)) The values for nu are pretty skewed, but hopefully it makes sense to you why that might be the case. Here are the overlaid density plots. draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_dens_overlay(pars = vars(b_Intercept:nu)) The \\(\\widehat R\\) values are right where we like them. rhat(fit16.2)[1:3] ## b_Intercept sigma nu ## 1.0001770 0.9997878 1.0019868 If you peer into the contents of a brm() fit object (e.g., fit16.2 %&gt;% str()), you’ll discover it contains the Stan code. Here it is for our fit16.2. fit16.2$fit@stanmodel ## S4 class stanmodel &#39;7c10f5af16d42e79505bd3a119e9111b&#39; coded as follows: ## // generated with brms 2.17.0 ## functions { ## /* compute the logm1 link ## * Args: ## * p: a positive scalar ## * Returns: ## * a scalar in (-Inf, Inf) ## */ ## real logm1(real y) { ## return log(y - 1); ## } ## /* compute the inverse of the logm1 link ## * Args: ## * y: a scalar in (-Inf, Inf) ## * Returns: ## * a positive scalar ## */ ## real expp1(real y) { ## return exp(y) + 1; ## } ## } ## data { ## int&lt;lower=1&gt; N; // total number of observations ## vector[N] Y; // response variable ## int prior_only; // should the likelihood be ignored? ## real mean_y; ## real sd_y; ## real one_over_twentynine; ## } ## transformed data { ## } ## parameters { ## real Intercept; // temporary intercept for centered predictors ## real&lt;lower=0&gt; sigma; // dispersion parameter ## real&lt;lower=1&gt; nu; // degrees of freedom or shape ## } ## transformed parameters { ## real lprior = 0; // prior contributions to the log posterior ## lprior += normal_lpdf(Intercept | mean_y, sd_y * 100); ## lprior += normal_lpdf(sigma | 0, sd_y) ## - 1 * normal_lccdf(0 | 0, sd_y); ## lprior += exponential_lpdf(nu | one_over_twentynine) ## - 1 * exponential_lccdf(1 | one_over_twentynine); ## } ## model { ## // likelihood including constants ## if (!prior_only) { ## // initialize linear predictor term ## vector[N] mu = Intercept + rep_vector(0.0, N); ## target += student_t_lpdf(Y | nu, mu, sigma); ## } ## // priors including constants ## target += lprior; ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = Intercept; ## } ## Note the last line in the parameters block, “real&lt;lower=1&gt; nu; // degrees of freedom or shape.” By default, brms set the lower bound for \\(\\nu\\) to 1. Just for kicks and giggles, the pp_check() offers us a handy way to compare the performance of our Gaussian fit16.2 and our Student’s \\(t\\) fit16.2. If we set type = \"ecdf_overlay\" within pp_check(), we’ll get the criterion Score displayed as a cumulative distribution function (CDF) rather than a typical density. Then, pp_check() presents CDF’s based on draws from the posterior for comparison. Just like with the default pp_check() plots, we like it when those simulated distributions mimic the one from the original data. color_scheme_set(scheme = bp[c(1, 1, 1, 1, 1, 6)]) # fit16.1 with Gaus set.seed(16) p1 &lt;- pp_check(fit16.1, ndraws = n_lines, type = &quot;ecdf_overlay&quot;) + labs(subtitle = &quot;fit16.1 with `family = gaussian`&quot;) + coord_cartesian(xlim = range(my_data$Score)) + theme(legend.position = &quot;none&quot;) # fit16.2 with Student&#39;s t p2 &lt;- pp_check(fit16.2, ndraws = n_lines, type = &quot;ecdf_overlay&quot;) + labs(subtitle = &quot;fit16.2 with `family = student`&quot;) + coord_cartesian(xlim = range(my_data$Score)) # combine the subplots p1 + p2 It’s subtle, but you might notice that the simulated CDFs from fit16.1 have shallower slopes in the middle when compared to the original data in the pink. However, the fit16.2-based simulated CDFs match up more closely with the original data. This suggests an edge for fit16.2. Revisiting our skills from Chapter 10, we might also compare their model weights. model_weights(fit16.1, fit16.2) %&gt;% round(digits = 6) ## fit16.1 fit16.2 ## 0.000022 0.999978 Almost all the stacking weight (see Yao et al., 2018) went to fit16.2, our robust Student’s \\(t\\) model. 16.3 Two groups When there are two groups, we estimate the mean and scale for each group. When using \\(t\\) distributions for robust estimation, we could also estimate the normality of each group separately. But because there usually are relatively few outliers, we will use a single normality parameter to describe both groups, so that the estimate of the normality is more stably estimated. (p. 468) To get a sense of what this looks like, here’s our version of the model diagram in Figure 16.11. # exponential density p1 &lt;- tibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;exp&quot;, size = 7, color = bp[5]) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(K)&quot;, size = 7, color = bp[5], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[4])) # normal density p2 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bp[5]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)&quot;, &quot;italic(S)[mu]&quot;), size = 7, color = bp[5], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[4])) # half-normal density p3 &lt;- tibble(x = seq(from = 0, to = 3, by = .01), d = (dnorm(x)) / max(dnorm(x))) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7, color = bp[5]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma]&quot;, size = 7, color = bp[5], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[4])) # four annotated arrows p4 &lt;- tibble(x = c(.43, .43, 1.5, 2.5), y = c(1, .55, 1, 1), xend = c(.43, 1.15, 1.5, 1.8), yend = c(.8, .15, .2, .2)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bp[4]) + annotate(geom = &quot;text&quot;, x = c(.3, .65, 1.38, 1.62, 2, 2.3), y = c(.92, .25, .6, .6, .6, .6), label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;=&#39;&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;), size = c(10, 10, 10, 7, 10, 7), color = bp[5], family = &quot;Times&quot;, parse = T) + annotate(geom = &quot;text&quot;, x = .43, y = .7, label = &quot;nu*minute+1&quot;, size = 7, color = bp[5], family = &quot;Times&quot;, parse = T) + xlim(0, 3) + theme_void() # student-t density p5 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;student t&quot;, size = 7, color = bp[5], family = &quot;Times&quot;) + annotate(geom = &quot;text&quot;, x = 0, y = .6, label = &quot;nu~~~mu[italic(j)]~~~sigma[italic(j)]&quot;, size = 7, color = bp[5], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[4])) # the final annotated arrow p6 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)*&#39;|&#39;*italic(j)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = bp[5], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, color = bp[4], arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p7 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y)[italic(i)*&#39;|&#39;*italic(j)]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = bp[5], parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 1, r = 2), area(t = 1, b = 2, l = 3, r = 4), area(t = 1, b = 2, l = 5, r = 6), area(t = 4, b = 5, l = 3, r = 4), area(t = 3, b = 4, l = 1, r = 6), area(t = 6, b = 6, l = 3, r = 4), area(t = 7, b = 7, l = 3, r = 4) ) # combine and plot! (p1 + p2 + p3 + p5 + p4 + p6 + p7) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Since we subset the data, earlier, we’ll just reload it to get the full data set. my_data &lt;- read_csv(&quot;data.R/TwoGroupIQ.csv&quot;) This time, we’ll compute mean_y and sd_y from the full data. (mean_y &lt;- mean(my_data$Score)) ## [1] 104.1333 (sd_y &lt;- sd(my_data$Score)) ## [1] 22.43532 stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) Within the brms framework, Bürkner calls it distributional modeling when you model more than the mean. Since we’re now modeling both \\(\\mu\\) and \\(\\sigma\\), we’re fitting a distributional model. When doing so with brms, you typically wrap your formula syntax into the bf() function. It’s also important to know that when modeling \\(\\sigma\\), brms defaults to modeling its log. So we’ll use log(sd_y) in its prior. For more on all this, see Bürkner’s (2022a) vignette, Estimating distributional models with brms. fit16.3 &lt;- brm(data = my_data, family = student, bf(Score ~ 0 + Group, sigma ~ 0 + Group), prior = c(prior(normal(mean_y, sd_y * 100), class = b), prior(normal(0, log(sd_y)), class = b, dpar = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvars, seed = 16, file = &quot;fits/fit16.03&quot;) Did you catch our ~ 0 + Group syntax? That suppressed the usual intercept for our estimates of both \\(\\mu\\) and \\(\\log (\\sigma)\\). Since Group is a categorical variable, that results in brm() fitting separate intercepts for each category. This is our brms analogue to the x[i] syntax Kruschke mentioned on page 468. It’s what allowed us to estimate \\(\\mu_j\\) and \\(\\log (\\sigma_j)\\). Let’s look at the model summary. print(fit16.3) ## Family: student ## Links: mu = identity; sigma = log; nu = identity ## Formula: Score ~ 0 + Group ## sigma ~ 0 + Group ## Data: my_data (Number of observations: 120) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## GroupPlacebo 99.25 1.73 95.75 102.69 1.00 4198 2859 ## GroupSmartDrug 107.13 2.57 102.02 112.20 1.00 4558 2894 ## sigma_GroupPlacebo 2.38 0.15 2.08 2.66 1.00 3261 2667 ## sigma_GroupSmartDrug 2.83 0.16 2.52 3.14 1.00 3289 2711 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## nu 3.56 1.47 1.85 6.91 1.00 3014 2933 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Remember that the \\(\\sigma\\)’s are now in the log scale. If you want a quick conversion, you might just exponentiate the point estimates from fixef(). fixef(fit16.3)[3:4, 1] %&gt;% exp() ## sigma_GroupPlacebo sigma_GroupSmartDrug ## 10.75589 17.02897 But please don’t stop there. Get your hands dirty with the full posterior. Speaking of which, if we want to make the histograms in Figure 16.12, we’ll need to first extract the posterior draws. draws &lt;- as_draws_df(fit16.3) glimpse(draws) ## Rows: 4,000 ## Columns: 10 ## $ b_GroupPlacebo &lt;dbl&gt; 98.84107, 98.95692, 97.60022, 101.50527, 101.42407, 103.80839, 99.3… ## $ b_GroupSmartDrug &lt;dbl&gt; 108.2899, 107.8557, 106.7158, 105.1145, 105.7819, 105.5277, 109.091… ## $ b_sigma_GroupPlacebo &lt;dbl&gt; 2.558770, 2.379741, 2.445698, 2.376179, 2.355777, 2.530996, 2.42473… ## $ b_sigma_GroupSmartDrug &lt;dbl&gt; 2.788483, 2.702594, 2.677785, 2.789920, 2.930548, 3.085090, 2.59061… ## $ nu &lt;dbl&gt; 2.476186, 3.641985, 2.323852, 4.152640, 2.527162, 3.368567, 3.37226… ## $ lprior &lt;dbl&gt; -25.53535, -25.50549, -25.46959, -25.54700, -25.52753, -25.64883, -… ## $ lp__ &lt;dbl&gt; -550.6470, -548.9934, -550.1101, -549.9440, -550.4131, -553.9229, -… ## $ .chain &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, … ## $ .draw &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, … Along with transforming the metrics of a few of the parameters, we may as well rename them to match those in the text. draws &lt;- draws %&gt;% mutate(`Placebo Mean` = b_GroupPlacebo, `Smart Drug Mean` = b_GroupSmartDrug, # we need to transform the next three parameters `Placebo Scale` = b_sigma_GroupPlacebo %&gt;% exp(), `Smart Drug Scale` = b_sigma_GroupSmartDrug %&gt;% exp(), Normality = nu %&gt;% log10()) %&gt;% mutate(`Difference of Means` = `Smart Drug Mean` - `Placebo Mean`, `Difference of Scales` = `Smart Drug Scale` - `Placebo Scale`, `Effect Size` = (`Smart Drug Mean` - `Placebo Mean`) / sqrt((`Smart Drug Scale`^2 + `Placebo Scale`^2) / 2)) %&gt;% select(.draw, `Placebo Mean`:`Effect Size`) glimpse(draws) ## Rows: 4,000 ## Columns: 9 ## $ .draw &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, … ## $ `Placebo Mean` &lt;dbl&gt; 98.84107, 98.95692, 97.60022, 101.50527, 101.42407, 103.80839, 99.3… ## $ `Smart Drug Mean` &lt;dbl&gt; 108.2899, 107.8557, 106.7158, 105.1145, 105.7819, 105.5277, 109.091… ## $ `Placebo Scale` &lt;dbl&gt; 12.919914, 10.802100, 11.538605, 10.763693, 10.546318, 12.566015, 1… ## $ `Smart Drug Scale` &lt;dbl&gt; 16.25634, 14.91838, 14.55282, 16.27972, 18.73789, 21.86943, 13.3379… ## $ Normality &lt;dbl&gt; 0.3937832, 0.5613382, 0.3662085, 0.6183243, 0.4026331, 0.5274451, 0… ## $ `Difference of Means` &lt;dbl&gt; 9.448791, 8.898769, 9.115588, 3.609216, 4.357816, 1.719300, 9.72004… ## $ `Difference of Scales` &lt;dbl&gt; 3.3364298, 4.1162786, 3.0142176, 5.5160320, 8.1915750, 9.3034187, 2… ## $ `Effect Size` &lt;dbl&gt; 0.64351019, 0.68326507, 0.69412541, 0.26153519, 0.28661982, 0.09640… Now we’re ready for the bulk of Figure 16.12. # save the factor levels levels &lt;- c(&quot;Placebo Mean&quot;, &quot;Smart Drug Mean&quot;, &quot;Placebo Scale&quot;, &quot;Difference of Means&quot;, &quot;Smart Drug Scale&quot;, &quot;Difference of Scales&quot;, &quot;Normality&quot;, &quot;Effect Size&quot;) # we&#39;ll use this to mark off the ROPEs as white strips in the background rope &lt;- tibble(name = factor(c(&quot;Difference of Means&quot;, &quot;Difference of Scales&quot;, &quot;Effect Size&quot;), levels = levels), xmin = c(-1, -1, -.1), xmax = c(1, 1, .1)) # here are the primary data draws %&gt;% pivot_longer(-.draw) %&gt;% # this isn&#39;t necessary, but it arranged our subplots like those in the text mutate(name = factor(name, levels = levels)) %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = &quot;white&quot;) + stat_histinterval(aes(x = value, y = 0), point_interval = mode_hdi, .width = .95, fill = bp[2], color = bp[6], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Now let’s make the upper two panels in the right column of Figure 16.12. You might note a few things in our wrangling code. First, in the mutate() function, we defined the density values for the two Group conditions one at a time. If you look carefully within those definitions, you’ll see we used 10^Normality for the df argument, rather than just Normality. Why? Remember how a few code blocks up we transformed the original nu column to Normality by placing it within log10()? Well, if you want to undo that, you have to take 10 to the power of Normality. Next, notice that we subset the draws tibble with select(). This wasn’t technically necessary, but it made the next line easier. Finally, with the pivot_longer() function, we transformed the Placebo and Smart Drug columns into an index column strategically named Group, which matched up with the original my_data data, and a density column, which contained the actual density values for the lines. I know. That’s a lot to take in. If you’re confused, run the lines in the code one at a time to see how they work. But anyway, here’s the final result! # how many credible density lines would you like? n_lines &lt;- 63 # setting the seed makes the results from `sample_n()` reproducible set.seed(16) # wrangle draws %&gt;% tibble() %&gt;% slice_sample(n = n_lines) %&gt;% expand(nesting(.draw, `Placebo Mean`, `Smart Drug Mean`, `Placebo Scale`, `Smart Drug Scale`, Normality), Score = seq(from = 40, to = 250, by = 1)) %&gt;% mutate(Placebo = metRology::dt.scaled(x = Score, df = 10^Normality, mean = `Placebo Mean`, sd = `Placebo Scale`), `Smart Drug` = metRology::dt.scaled(x = Score, df = 10^Normality, mean = `Smart Drug Mean`, sd = `Smart Drug Scale`)) %&gt;% select(.draw, Score:`Smart Drug`) %&gt;% pivot_longer(cols = c(-.draw, -Score), names_to = &quot;Group&quot;, values_to = &quot;density&quot;) %&gt;% # plot ggplot(aes(x = Score)) + geom_histogram(data = my_data, aes(y = stat(density)), fill = bp[2], size = .2, binwidth = 5, boundary = 0) + geom_line(aes(y = density, group = .draw), size = 1/3, alpha = 1/3, color = bp[6]) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Data with Post. Pred.&quot;, x = &quot;y&quot;) + coord_cartesian(xlim = c(50, 210)) + facet_wrap(~ Group) 16.3.0.1 Bonus: Pooled standard deviation. It was easy to miss it, but you might have noticed we changed how we computed our effect size in our version of Figure 16.12. Earlier in the chapter, we simply computed our standardized mean difference by dividing by \\(\\sigma\\), which is a fine thing to do when you have a well-defined population standard deviation. However, when you want to compute a standardized mean difference between two groups, you might not have a well-defined population standard deviation. In such a case, a common approach is to standardize the effect using a pooled standard deviation. In the case where you have two groups with equal sample sizes, the pooled standard deviation, \\(\\sigma_p\\), is defined as \\[\\sigma_p = \\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}},\\] which is the equation we used in Figure 16.12. When you don’t have equal sample sizes in groups, a popular alternative formula is \\[\\sigma_p^* = \\sqrt{\\frac{(n_1 - 1)\\sigma_1^2 + (n_2 - 1)\\sigma_2^2}{n_1 + n_2 - 2}},\\] which uses the group-based sample sizes to weight their influence on \\(\\sigma_p^*\\). Thus, the complete formulas for computing standardized mean differences for two-group experimental data are \\[d = \\frac{\\mu_2 - \\mu_1}{\\sqrt{(\\sigma_1^2 + \\sigma_2^2) / 2}},\\] when \\(n_1 = n_2\\), or \\[d = \\frac{\\mu_2 - \\mu_1}{\\sqrt{ \\left[(n_1 - 1)\\sigma_1^2 + (n_2 - 1)\\sigma_2^2 \\right] / (n_1 + n_2 - 2)}},\\] when \\(n_1 \\neq n_2\\). Another key insight with this notation is that there are many formulas one can use to compute a standardized mean difference, often called a Cohen’s \\(d\\). Thus, when you are computing one for a scientific presentation, it’s imperative that you tell your audience exactly how you computed your effect size. Show them the formula. Otherwise, your ambiguity can muddle how people interpret your work. 16.3.1 Analysis by NHST. Here’s how we might perform a \\(t\\)-test. t.test(data = my_data, Score ~ Group) ## ## Welch Two Sample t-test ## ## data: Score by Group ## t = -1.958, df = 111.44, p-value = 0.05273 ## alternative hypothesis: true difference in means between group Placebo and group Smart Drug is not equal to 0 ## 95 percent confidence interval: ## -15.70602585 0.09366161 ## sample estimates: ## mean in group Placebo mean in group Smart Drug ## 100.0351 107.8413 To help with the comparison, notice how the 95% CIs from the \\(t\\)-test range from -15.7 to 0.1. That indicates that the \\(t\\)-test subtracted the Smart Drug from Placebo. I point this out because the difference scores we computed along with Kruschke for Figure 16.12 did the subtraction in the other direction. But don’t fret. The magnitude of the difference stays the same either way. Only the signs changed. For example: fixef(fit16.3)[&quot;GroupPlacebo&quot;, 1] - fixef(fit16.3)[&quot;GroupSmartDrug&quot;, 1] ## [1] -7.880447 fixef(fit16.3)[&quot;GroupSmartDrug&quot;, 1] - fixef(fit16.3)[&quot;GroupPlacebo&quot;, 1] ## [1] 7.880447 But anyway, in addition to the difference distributions from Figure 16.12, we might also use the brms::hypothesis() function to do something of a Bayesian \\(t\\)-test. hypothesis(fit16.3, &quot;GroupPlacebo - GroupSmartDrug = 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star ## 1 (GroupPlacebo-Gro... = 0 -7.88 3.07 -13.91 -1.85 NA NA * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. See how our 95% ETIs are both negative and how we have that little * in the Star column? If you like the visual approach, you can even feed the hypothesis() code into plot(). hypothesis(fit16.3, &quot;GroupPlacebo - GroupSmartDrug = 0&quot;) %&gt;% plot() Here’s how one might customize that plot. h &lt;- hypothesis(fit16.3, &quot;GroupPlacebo - GroupSmartDrug = 0&quot;) h$samples %&gt;% ggplot(aes(x = H1, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, breaks = 40, fill = bp[2], color = bp[6]) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;GroupPlacebo - GroupSmartDrug = 0&quot;) Execute str(h) for more insights. Back to the text: The reason that the \\(t\\) test is less sensitive than the Bayesian estimation in this example is that the \\(t\\) test assumes normality and therefore its estimate of the within-group variances is too large when there are outliers. The \\(t\\) test has other problems. Unlike the Bayesian analysis, the \\(t\\) test provides only a test of the equality of means, without a test of the equality of variances. To test equality of variances, we need to run an additional test, namely an \\(F\\) test of the ratio of variances, which would inflate the \\(p\\) values of both tests. Moreover, both tests compute \\(p\\) values based on hypothetical normally distributed data, and the \\(F\\) test is particularly sensitive to violations of this assumption. Therefore it would be better to use resampling methods to compute the \\(p\\) values (and correcting them for multiple tests). (pp. 471–472) Don’t do all that. Use robust Bayes, instead. Just for kicks, we can compare the \\(\\sigma\\)’s with hypothesis(), too. hypothesis(fit16.3, &quot;exp(sigma_GroupPlacebo) - exp(sigma_GroupSmartDrug) = 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star ## 1 (exp(sigma_GroupP... = 0 -6.36 2.77 -12.08 -1.14 NA NA * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. See? We don’t need an \\(F\\) test. 16.4 Other noise distributions and transforming data It’s worth repeating a portion of this section. If the initially assumed noise distribution does not match the data distribution, there are two ways to pursue a better description. The preferred way is to use a better noise distribution. The other way is to transform the data to a new scale so that they tolerably match the shape of the assumed noise distribution. In other words, we can either change the shoe to fit the foot, or we can squeeze the foot to fit in the shoe. Changing the shoe is preferable to squeezing the foot. In traditional statistical software, users were stuck with the pre-packaged noise distribution, and had no way to change it, so they transformed their data and squeezed them into the software. This practice can lead to confusion in interpreting the parameters because they are describing the transformed data, not the data on the original scale. In software such as [brms, we can spend less time squeezing our feet into ill-fitting shoes]. (p. 472) We’ll have more practice with the robust Student’s \\(t\\) in some of the following chapters. But if you’d like even more, you might check out my blog post on the topic, Robust linear regression with Student’s \\(t\\)-distribution, and the companion post, Bayesian robust correlations with brms (and why you should love Student’s \\(t\\)). Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bayesplot_1.9.0 tidybayes_3.0.2 patchwork_1.1.2 brms_2.18.0 Rcpp_1.0.9 beyonce_0.1 ## [7] forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 ## [13] tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 metRology_0.9-28-1 plyr_1.8.7 ## [5] igraph_1.3.4 svUnit_1.0.6 splines_4.2.0 crosstalk_1.2.0 ## [9] TH.data_1.1-1 rstantools_2.2.0 inline_0.3.19 digest_0.6.30 ## [13] htmltools_0.5.3 fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 ## [17] googlesheets4_1.0.1 tzdb_0.3.0 modelr_0.1.8 RcppParallel_5.1.5 ## [21] matrixStats_0.62.0 vroom_1.5.7 xts_0.12.1 sandwich_3.0-2 ## [25] prettyunits_1.1.1 colorspace_2.0-3 rvest_1.0.2 ggdist_3.2.0 ## [29] haven_2.5.1 xfun_0.35 callr_3.7.3 crayon_1.5.2 ## [33] jsonlite_1.8.3 lme4_1.1-31 survival_3.4-0 zoo_1.8-10 ## [37] glue_1.6.2 gtable_0.3.1 gargle_1.2.0 emmeans_1.8.0 ## [41] distributional_0.3.1 pkgbuild_1.3.1 rstan_2.21.7 DEoptimR_1.0-11 ## [45] abind_1.4-5 scales_1.2.1 mvtnorm_1.1-3 DBI_1.1.3 ## [49] miniUI_0.1.1.1 xtable_1.8-4 HDInterval_0.2.2 bit_4.0.4 ## [53] stats4_4.2.0 StanHeaders_2.21.0-7 DT_0.24 htmlwidgets_1.5.4 ## [57] httr_1.4.4 threejs_0.3.3 arrayhelpers_1.1-0 posterior_1.3.1 ## [61] ellipsis_0.3.2 pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 ## [65] sass_0.4.2 dbplyr_2.2.1 utf8_1.2.2 tidyselect_1.1.2 ## [69] labeling_0.4.2 rlang_1.0.6 reshape2_1.4.4 later_1.3.0 ## [73] munsell_0.5.0 cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [77] cli_3.5.0 generics_0.1.3 broom_1.0.1 ggridges_0.5.3 ## [81] evaluate_0.18 fastmap_1.1.0 processx_3.8.0 knitr_1.40 ## [85] bit64_4.0.5 fs_1.5.2 robustbase_0.95-0 nlme_3.1-159 ## [89] projpred_2.2.1 mime_0.12 xml2_1.3.3 compiler_4.2.0 ## [93] shinythemes_1.2.0 rstudioapi_0.13 gamm4_0.2-6 reprex_2.0.2 ## [97] bslib_0.4.0 stringi_1.7.8 highr_0.9 ps_1.7.2 ## [101] Brobdingnag_1.2-8 lattice_0.20-45 Matrix_1.4-1 nloptr_2.0.3 ## [105] markdown_1.1 shinyjs_2.1.0 tensorA_0.36.2 vctrs_0.5.1 ## [109] pillar_1.8.1 lifecycle_1.0.3 jquerylib_0.1.4 bridgesampling_1.1-2 ## [113] estimability_1.4.1 httpuv_1.6.5 R6_2.5.1 bookdown_0.28 ## [117] promises_1.2.0.1 gridExtra_2.3 codetools_0.2-18 boot_1.3-28 ## [121] colourpicker_1.1.1 MASS_7.3-58.1 gtools_3.9.3 assertthat_0.2.1 ## [125] withr_2.5.0 shinystan_2.6.0 multcomp_1.4-20 mgcv_1.8-40 ## [129] parallel_4.2.0 hms_1.1.1 grid_4.2.0 minqa_1.2.5 ## [133] coda_0.19-4 rmarkdown_2.16 googledrive_2.0.0 numDeriv_2016.8-1.1 ## [137] shiny_1.7.2 lubridate_1.8.0 base64enc_0.1-3 dygraphs_1.1.1.6 References Bürkner, P.-C. (2022a). Estimating distributional models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd Edition). Routledge. https://doi.org/10.4324/9780203771587 Ellison., S. L. R. (2018). metRology: Support for metrological applications. https://CRAN.R-project.org/package=metRology Kelley, K., &amp; Preacher, K. J. (2012). On effect size. Psychological Methods, 17(2), 137. https://doi.org/10.1037/a0028086 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Miller, D. L. (2021). beyonce: Beyoncé colour palettes for R. https://github.com/dill/beyonce Stan Development Team. (2022c). Stan user’s guide, Version 2.29. https://mc-stan.org/docs/2_29/stan-users-guide/index.html Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A. (2018). Using stacking to average Bayesian predictive distributions (with discussion). Bayesian Analysis, 13(3), 917–1007. https://doi.org/10.1214/17-BA1091 "],["metric-predicted-variable-with-one-metric-predictor.html", "17 Metric Predicted Variable with One Metric Predictor 17.1 Simple linear regression 17.2 Robust linear regression 17.3 Hierarchical regression on individuals within groups 17.4 Quadratic trend and weighted data 17.5 Procedure and perils for expanding a model Session info", " 17 Metric Predicted Variable with One Metric Predictor We will initially describe the relationship between the predicted variable, \\(y\\) and predictor, \\(x\\), with a simple linear model and normally distributed residual randomness in \\(y\\). This model is often referred to as ‘simple linear regression.’ We will generalize the model in three ways. First, we will give it a noise distribution that accommodates outliers, which is to say that we will replace the normal distribution with a \\(t\\) distribution as we did in the previous chapter. The model will be implemented in [brms]. Next, we will consider differently shaped relations between the predictor and the predicted, such as quadratic trend. Finally, we will consider hierarchical models of situations in which every individual has data that can be described by an individual trend, and we also want to estimate group-level typical trends across individuals. (Kruschke, 2015, p. 478) 17.1 Simple linear regression It wasn’t entirely clear how Kruschke simulated the bimodal data on the right panel of Figure 17.1. I figured an even split of two Gaussians would suffice and just sighted their \\(\\mu\\)’s and \\(\\sigma\\)’s. library(tidyverse) # how many draws per panel would you like? n_draw &lt;- 1000 set.seed(17) d &lt;- tibble(panel = rep(letters[1:2], each = n_draw), x = c(runif(n = n_draw, min = -10, max = 10), rnorm(n = n_draw / 2, mean = -7, sd = 2), rnorm(n = n_draw / 2, mean = 3, sd = 2))) %&gt;% mutate(y = 10 + 2 * x + rnorm(n = n(), mean = 0, sd = 2)) head(d) ## # A tibble: 6 × 3 ## panel x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a -6.90 -4.09 ## 2 a 9.37 28.6 ## 3 a -0.635 11.8 ## 4 a 5.54 23.3 ## 5 a -1.84 10.9 ## 6 a 0.776 10.9 In case you missed it, Kruschke defied the formula for these data in Figure 17.1. It is \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma = 2), \\text{where} \\\\ \\mu_i &amp; = 10 + 2 x_i. \\end{align*}\\] “Note that the model only specifies the dependency of \\(y\\) on \\(x\\). The model does not say anything about what generates \\(x\\), and there is no probability distribution assumed for describing \\(x\\)” (p. 479). Let this sink into your soul. It took a long time, for me. E.g., a lot of people fret over the distributions of their \\(x\\) variables. Now one might should examine them to make sure nothing looks off, such as for data coding mistakes. But if they’re not perfectly or even approximately Gaussian, that isn’t necessarily an issue. The typical linear model makes no presumption about the distribution of the predictors. Often times, the largest issue is whether the \\(x\\) variables are categorical or continuous. Before we make our Figure 17.1, we’ll want to make a separate tibble of the values necessary to plot those sideways Gaussians. Here are the steps. curves &lt;- # define the 3 x-values we want the Gaussians to originate from tibble(x = seq(from = -7.5, to = 7.5, length.out = 4)) %&gt;% # use the formula 10 + 2x to compute the expected y-value for x mutate(y_mean = 10 + (2 * x)) %&gt;% # based on a Gaussian with `mean = y_mean` and `sd = 2`, compute the 99% intervals mutate(ll = qnorm(.005, mean = y_mean, sd = 2), ul = qnorm(.995, mean = y_mean, sd = 2)) %&gt;% # now use those interval bounds to make a sequence of y-values mutate(y = map2(ll, ul, seq, length.out = 100)) %&gt;% # since that operation returned a nested column, we need to `unnest()` unnest(y) %&gt;% # compute the density values mutate(density = map2_dbl(y, y_mean, dnorm, sd = 2)) %&gt;% # now rescale the density values to be wider. # since we want these to be our x-values, we&#39;ll # just redefine the x column with these results mutate(x = x - density * 2 / max(density)) str(curves) ## tibble [400 × 6] (S3: tbl_df/tbl/data.frame) ## $ x : num [1:400] -7.57 -7.58 -7.59 -7.61 -7.62 ... ## $ y_mean : num [1:400] -5 -5 -5 -5 -5 -5 -5 -5 -5 -5 ... ## $ ll : num [1:400] -10.2 -10.2 -10.2 -10.2 -10.2 ... ## $ ul : num [1:400] 0.152 0.152 0.152 0.152 0.152 ... ## $ y : num [1:400] -10.15 -10.05 -9.94 -9.84 -9.74 ... ## $ density: num [1:400] 0.00723 0.00826 0.0094 0.01068 0.01209 ... Before we make Figure 17.1, let’s talk color. Like last chapter, we’ll take our color palette from the beyonce package. Our palette will be a nine-point version of #41. library(beyonce) bp &lt;- beyonce_palette(41, n = 9, type = &quot;continuous&quot;) bp The global theme will be ggplot2::theme_linedraw() with the grid lines removed. Make Figure 17.1. theme_set( theme_linedraw() + theme(panel.grid = element_blank()) ) d %&gt;% ggplot(aes(x = x, y = y)) + geom_vline(xintercept = 0, size = 1/3, linetype = 2, color = bp[9]) + geom_hline(yintercept = 0, size = 1/3, linetype = 2, color = bp[9]) + geom_point(size = 1/3, alpha = 1/3, color = bp[5]) + stat_smooth(method = &quot;lm&quot;, se = F, fullrange = T, color = bp[1]) + geom_path(data = curves, aes(group = y_mean), color = bp[2], size = 1) + labs(title = &quot;Normal PDF around Linear Function&quot;, subtitle = &quot;We simulated x from a uniform distribution in the left panel and simulated it from a mixture of\\n two Gaussians on the right.&quot;) + coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 30)) + theme(strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(~ panel) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. Concerning causality, the simple linear model makes no claims about causal connections between \\(x\\) and \\(y\\). The simple linear model merely describes a tendency for \\(y\\) values to be linearly related to \\(x\\) values, hence “predictable” from the \\(x\\) values. When describing data with this model, we are starting with a scatter plot of points generated by an unknown process in the real world, and estimating parameter values that would produce a smattering of points that might mimic the real data. Even if the descriptive model mimics the data well (and it might not), the mathematical “process” in the model may have little if anything to do with the real-world process that created the data. Nevertheless, the parameters in the descriptive model are meaningful because they describe tendencies in the data. (p. 479, emphasis added) I emphasized these points because I’ve heard and seen a lot of academics conflate linear regression models with causal models. For sure, it might well be preferable if your regression model was also a causal model. But good old prediction is fine, too. 17.2 Robust linear regression There is no requirement to use a normal distribution for the noise distribution. The normal distribution is traditional because of its relative simplicity in mathematical derivations. But real data may have outliers, and the use of (optionally) heavy-tailed noise distributions is straight forward in contemporary Bayesian software[, like brms]. (pp. 479–480) Let’s make our version of the model diagram in Figure 17.2 to get a sense of where we’re going. library(patchwork) # normal density p1 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. # a second normal density p2 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)[1]&quot;, &quot;italic(S)[1]&quot;), size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) ## two annotated arrows # save our custom arrow settings my_arrow &lt;- arrow(angle = 20, length = unit(0.35, &quot;cm&quot;), type = &quot;closed&quot;) p3 &lt;- tibble(x = c(.33, 1.67), y = c(1, 1), xend = c(.75, 1.1), yend = c(0, 0)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bp[1]) + annotate(geom = &quot;text&quot;, x = c(.4, 1.25), y = .5, label = &quot;&#39;~&#39;&quot;, size = 10, color = bp[1], family = &quot;Times&quot;, parse = T) + xlim(0, 2) + theme_void() # exponential density p4 &lt;- tibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;exp&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(K)&quot;, size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # likelihood formula p5 &lt;- tibble(x = .5, y = .25, label = &quot;beta[0]+beta[1]*italic(x)[italic(i)]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = bp[1], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # half-normal density p6 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma]&quot;, size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # four annotated arrows p7 &lt;- tibble(x = c(.43, .43, 1.5, 2.5), y = c(1, .55, 1, 1), xend = c(.43, 1.225, 1.5, 1.75), yend = c(.8, .15, .2, .2)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bp[1]) + annotate(geom = &quot;text&quot;, x = c(.3, .7, 1.38, 2), y = c(.92, .22, .65, .6), label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;=&#39;&quot;, &quot;&#39;=&#39;&quot;, &quot;&#39;~&#39;&quot;), size = 10, color = bp[1], family = &quot;Times&quot;, parse = T) + annotate(geom = &quot;text&quot;, x = .43, y = .7, label = &quot;nu*minute+1&quot;, size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + xlim(0, 3) + theme_void() # student-t density p8 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;student t&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = 0, y = .6, label = &quot;nu~~~mu[italic(i)]~~~sigma&quot;, size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # the final annotated arrow p9 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = bp[1], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, color = bp[1], arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p10 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = bp[1], parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 3, r = 5), area(t = 1, b = 2, l = 7, r = 9), area(t = 4, b = 5, l = 1, r = 3), area(t = 4, b = 5, l = 5, r = 7), area(t = 4, b = 5, l = 9, r = 11), area(t = 3, b = 4, l = 3, r = 9), area(t = 7, b = 8, l = 5, r = 7), area(t = 6, b = 7, l = 1, r = 11), area(t = 9, b = 9, l = 5, r = 7), area(t = 10, b = 10, l = 5, r = 7) ) # combine and plot! (p1 + p2 + p4 + p5 + p6 + p3 + p8 + p7 + p9 + p10) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Here’s Kruschke’s HtWtDataGenerator() code. HtWtDataGenerator &lt;- function(nSubj, rndsd = NULL, maleProb = 0.50) { # Random height, weight generator for males and females. Uses parameters from # Brainard, J. &amp; Burmaster, D. E. (1992). Bivariate distributions for height and # weight of men and women in the United States. Risk Analysis, 12(2), 267-275. # Kruschke, J. K. (2011). Doing Bayesian data analysis: # A Tutorial with R and BUGS. Academic Press / Elsevier. # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition: # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier. # require(MASS) # Specify parameters of multivariate normal (MVN) distributions. # Men: HtMmu &lt;- 69.18 HtMsd &lt;- 2.87 lnWtMmu &lt;- 5.14 lnWtMsd &lt;- 0.17 Mrho &lt;- 0.42 Mmean &lt;- c(HtMmu, lnWtMmu) Msigma &lt;- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd, Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2) # Women cluster 1: HtFmu1 &lt;- 63.11 HtFsd1 &lt;- 2.76 lnWtFmu1 &lt;- 5.06 lnWtFsd1 &lt;- 0.24 Frho1 &lt;- 0.41 prop1 &lt;- 0.46 Fmean1 &lt;- c(HtFmu1, lnWtFmu1) Fsigma1 &lt;- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1, Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2) # Women cluster 2: HtFmu2 &lt;- 64.36 HtFsd2 &lt;- 2.49 lnWtFmu2 &lt;- 4.86 lnWtFsd2 &lt;- 0.14 Frho2 &lt;- 0.44 prop2 &lt;- 1 - prop1 Fmean2 &lt;- c(HtFmu2, lnWtFmu2) Fsigma2 &lt;- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2, Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2) # Randomly generate data values from those MVN distributions. if (!is.null(rndsd)) {set.seed(rndsd)} datamatrix &lt;- matrix(0, nrow = nSubj, ncol = 3) colnames(datamatrix) &lt;- c(&quot;male&quot;, &quot;height&quot;, &quot;weight&quot;) maleval &lt;- 1; femaleval &lt;- 0 # arbitrary coding values for (i in 1:nSubj) { # Flip coin to decide sex sex &lt;- sample(c(maleval, femaleval), size = 1, replace = TRUE, prob = c(maleProb, 1 - maleProb)) if (sex == maleval) {datum = MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)} if (sex == femaleval) { Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2)) if (Fclust == 1) {datum = MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)} if (Fclust == 2) {datum = MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)} } datamatrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1)) } return(datamatrix) } Let’s take this baby for a spin to simulate our data. d &lt;- HtWtDataGenerator(nSubj = 300, rndsd = 17, maleProb = .50) %&gt;% as_tibble() %&gt;% # this will allow us to subset 30 of the values into their own group mutate(subset = rep(0:1, times = c(9, 1)) %&gt;% rep(., 30)) head(d) ## # A tibble: 6 × 4 ## male height weight subset ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0 63.3 167. 0 ## 2 0 62.4 126. 0 ## 3 1 66.4 124 0 ## 4 0 62.9 148. 0 ## 5 1 65.5 151. 0 ## 6 1 71.4 234. 0 Note how we set the seed for the the pseudorandom number generator with the rndsd argument, which makes the results in this ebook reproducible. But since we do not know what seed value Kruschke used to simulate the data in this section of the test, the results of our models in the next section will differ a little from those in the text. However, if you want to more closely reproduce Kruschke’s examples, load the HtWtData30.csv and HtWtData300.csv data files and fit the models to those, instead. Anyway, fortunately, we do not have to worry much about analytical derivations because we can let JAGS or Stan generate a high resolution picture of the posterior distribution. Our job, therefore, is to specify sensible priors and to make sure that the MCMC process generates a trustworthy posterior sample that is converged and well mixed. (p. 483) 17.2.1 Robust linear regression in JAGS brms. Presuming a data set with a sole standardized predictor x_z for a sole standardized criterion y_z, the basic brms code corresponding to the JAGS code Kruschke showed on page 483 looks like this. fit &lt;- brm(data = my_data, family = student, y_z ~ 1 + x_z, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;)) Like we discussed in Chapter 16, we won’t be using the uniform prior for \\(\\sigma\\). Since we’re presuming standardized data, a half-unit normal is a fine choice. But do note this is much tighter than Kruschke’s \\(\\operatorname{Uniform} (0.001, 1000)\\) and it will have down-the-road consequences for our results versus those in the text. Also, look at how we just pumped the definition of our sole stanvar(1/29, name = \"one_over_twentynine\") operation right into the stanvar argument. If we were defining multiple values this way, I’d prefer to save this as an object first and then just pump that object into stanvars. But in this case, it was simple enough to just throw directly into the brm() function. 17.2.1.1 Standardizing the data for MCMC sampling. “Standardizing simply means re-scaling the data relative to their mean and standard deviation” (p. 485). For any variable \\(x\\), that follows the formula \\[z_i = \\frac{x_i - \\bar x}{s},\\] where \\(x_i\\) is the \\(i\\)th row in the vector of \\(x\\) values, \\(\\bar x\\) is the sample mean for \\(x\\), \\(s\\) is the sample standard deviation for \\(x\\), and the product of the standardizing procedure is \\(z_i\\) (i.e., the \\(z\\)-score). Kruschke mentioned how standardizing your data before feeding it into JAGS often helps the algorithm operate smoothly. The same basic principle holds for brms and Stan. Stan can often handle unstandardized data just fine. But if you ever run into estimation difficulties, consider standardizing your data and trying again. We’ll make a simple function to standardize the height and weight values. standardize &lt;- function(x) { (x - mean(x)) / sd(x) } d &lt;- d %&gt;% mutate(height_z = standardize(height), weight_z = standardize(weight)) Somewhat analogous to how Kruschke standardized his data within the JAGS code, you could standardize the data within the brm() function. That would look something like this. fit &lt;- brm(data = d %&gt;% # the standardizing occurs in the next two lines mutate(height_z = standardize(height), weight_z = standardize(weight)), family = student, weight_z ~ 1 + height_z) But anyway, let’s open brms. library(brms) We’ll fit the two models at once. fit1 will be of the total data sample. fit2 is of the \\(n = 30\\) subset. fit17.1 &lt;- brm(data = d, family = student, weight_z ~ 1 + height_z, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 17, file = &quot;fits/fit17.01&quot;) fit17.2 &lt;- update(fit17.1, newdata = d %&gt;% filter(subset == 1), chains = 4, cores = 4, seed = 17, file = &quot;fits/fit17.02&quot;) ## The desired updates require recompiling the model Here are the results. print(fit17.1) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: weight_z ~ 1 + height_z ## Data: d (Number of observations: 300) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.03 0.05 -0.13 0.07 1.00 3465 2645 ## height_z 0.46 0.05 0.35 0.56 1.00 3893 2697 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.84 0.05 0.74 0.94 1.00 2948 2733 ## nu 25.01 20.46 6.11 83.28 1.00 2741 2778 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit17.2) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: weight_z ~ 1 + height_z ## Data: d %&gt;% filter(subset == 1) (Number of observations: 30) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.11 0.12 -0.35 0.12 1.00 4055 2855 ## height_z 0.61 0.11 0.41 0.82 1.00 3790 2757 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.63 0.09 0.48 0.84 1.00 3610 2469 ## nu 38.97 31.30 5.45 121.75 1.00 3781 2603 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Based on Kruschke’s Equation 17.2, we can convert the standardized coefficients back to their original metric using the formulas \\[\\begin{align*} \\beta_0 &amp; = \\zeta_0 \\operatorname{SD}_y + M_y - \\frac{\\zeta_1 M_x \\operatorname{SD}_y}{\\operatorname{SD}_x} \\;\\; \\text{and} \\\\ \\beta_1 &amp; = \\frac{\\zeta_1 \\operatorname{SD}_y}{\\operatorname{SD}_x}, \\end{align*}\\] where \\(\\zeta_0\\) and \\(\\zeta_1\\) denote the intercept and slope for the model of the standardized data, respectively, and that model follows the familiar form \\[z_{\\hat y} = \\zeta_0 + \\zeta_1 z_x.\\] To implement those equations, we’ll first extract the posterior draws. We begin with fit17.1, the model for which \\(N = 300\\). draws &lt;- as_draws_df(fit17.1) head(draws) ## # A draws_df: 6 iterations, 1 chains, and 6 variables ## b_Intercept b_height_z sigma nu lprior lp__ ## 1 0.033 0.46 0.77 5.1 -10 -405 ## 2 0.091 0.47 0.73 10.8 -11 -408 ## 3 0.154 0.41 0.81 6.3 -11 -411 ## 4 -0.194 0.48 0.99 51.9 -12 -410 ## 5 -0.143 0.48 1.00 40.0 -12 -408 ## 6 -0.168 0.49 0.93 49.4 -12 -406 ## # ... hidden reserved variables {&#39;.chain&#39;, &#39;.iteration&#39;, &#39;.draw&#39;} Let’s wrap the consequences of Equation 17.2 into two functions. make_beta_0 &lt;- function(zeta_0, zeta_1, sd_x, sd_y, m_x, m_y) { zeta_0 * sd_y + m_y - zeta_1 * m_x * sd_y / sd_x } make_beta_1 &lt;- function(zeta_1, sd_x, sd_y) { zeta_1 * sd_y / sd_x } After saving a few values, we’re ready to use our custom functions to convert our posteriors for b_Intercept and b_height_z to their natural metric. sd_x &lt;- sd(d$height) sd_y &lt;- sd(d$weight) m_x &lt;- mean(d$height) m_y &lt;- mean(d$weight) draws &lt;- draws %&gt;% mutate(b_0 = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_height_z, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = b_height_z, sd_x = sd_x, sd_y = sd_y)) glimpse(draws) ## Rows: 4,000 ## Columns: 11 ## $ b_Intercept &lt;dbl&gt; 0.033449878, 0.091239362, 0.154258692, -0.193823671, -0.142858016, -0.16759966… ## $ b_height_z &lt;dbl&gt; 0.4630762, 0.4714897, 0.4064655, 0.4757892, 0.4848149, 0.4896576, 0.5275564, 0… ## $ sigma &lt;dbl&gt; 0.7678310, 0.7267777, 0.8082075, 0.9882686, 1.0003481, 0.9328399, 0.9104863, 0… ## $ nu &lt;dbl&gt; 5.109545, 10.798497, 6.279290, 51.899674, 39.956352, 49.405875, 55.019777, 10.… ## $ lprior &lt;dbl&gt; -10.47370, -10.63927, -10.54572, -12.28095, -11.88108, -12.14174, -12.31494, -… ## $ lp__ &lt;dbl&gt; -404.6587, -407.6282, -410.7937, -409.9921, -408.3061, -406.4314, -408.0862, -… ## $ .chain &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ .iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,… ## $ .draw &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,… ## $ b_0 &lt;dbl&gt; -123.86501, -127.08109, -84.63352, -139.58365, -143.41445, -147.26777, -171.38… ## $ b_1 &lt;dbl&gt; 4.297515, 4.375596, 3.772148, 4.415497, 4.499259, 4.544201, 4.895916, 3.510447… Now we’re finally ready to make the top panel of Figure 17.4. # how many posterior lines would you like? n_lines &lt;- 100 d %&gt;% ggplot(aes(x = height, y = weight)) + geom_abline(data = draws %&gt;% slice(1:n_lines), aes(intercept = b_0, slope = b_1, group = .draw), color = bp[2], size = 1/4, alpha = 1/3) + geom_point(alpha = 1/2, color = bp[5]) + labs(subtitle = eval(substitute(paste(&quot;Data with&quot;, n_lines, &quot;credible regression lines&quot;))), x = &quot;height&quot;, y = &quot;weight&quot;) + coord_cartesian(xlim = c(50, 80), ylim = c(-50, 470)) We’ll want to open the tidybayes package to help make the histograms. library(tidybayes) # we&#39;ll use this to mark off the ROPEs as white strips in the background rope &lt;- tibble(name = &quot;Slope&quot;, xmin = -.5, xmax = .5) # annotate the ROPE text &lt;- tibble(x = 0, y = 0.98, label = &quot;ROPE&quot;, name = &quot;Slope&quot;) # here are the primary data draws %&gt;% transmute(Intercept = b_0, Slope = b_1, Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% pivot_longer(everything()) %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = bp[9]) + stat_histinterval(aes(x = value, y = 0), point_interval = mode_hdi, .width = .95, fill = bp[6], color = bp[1], slab_color = bp[5], breaks = 40, normalize = &quot;panels&quot;) + geom_text(data = text, aes(x = x, y = y, label = label), size = 2.75, color = &quot;white&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) Here’s the scatter plot for the slope and intercept. draws %&gt;% ggplot(aes(x = b_1, y = b_0)) + geom_point(color = bp[3], size = 1/3, alpha = 1/3) + labs(x = expression(beta[1]), y = expression(beta[0])) That is one strong correlation! Finally, here’s the scatter plot for \\(\\operatorname{log10}(\\nu)\\) and \\(\\sigma_{\\text{transformed back to its raw metric}}\\). draws %&gt;% transmute(Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% ggplot(aes(x = Normality, y = Scale)) + geom_point(color = bp[3], size = 1/3, alpha = 1/3) + labs(x = expression(log10(nu)), y = expression(sigma)) Let’s back track and make the plots for Figure 17.3 with fit17.2. We’ll need to extract the posterior draws and wrangle, as before. draws &lt;- as_draws_df(fit17.2) draws &lt;- draws %&gt;% mutate(b_0 = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_height_z, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = b_height_z, sd_x = sd_x, sd_y = sd_y)) glimpse(draws) ## Rows: 4,000 ## Columns: 11 ## $ b_Intercept &lt;dbl&gt; -0.1019127271, -0.1602236195, -0.2453014631, -0.0767535353, -0.1667942551, -0.… ## $ b_height_z &lt;dbl&gt; 0.5989544, 0.6491469, 0.5190576, 0.5046634, 0.6864314, 0.7096647, 0.7160394, 0… ## $ sigma &lt;dbl&gt; 0.7103285, 0.5990408, 0.6441226, 0.7320464, 0.4938717, 0.8033633, 0.8161114, 0… ## $ nu &lt;dbl&gt; 42.477685, 45.532409, 56.704386, 27.973545, 31.975895, 29.216210, 15.016777, 7… ## $ lprior &lt;dbl&gt; -11.72048, -11.75331, -12.16595, -11.23548, -11.22862, -11.33463, -10.85538, -… ## $ lp__ &lt;dbl&gt; -36.70185, -36.15062, -37.40014, -37.47648, -37.73622, -39.23489, -40.09140, -… ## $ .chain &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ .iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,… ## $ .draw &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,… ## $ b_0 &lt;dbl&gt; -212.69271, -245.79182, -168.15852, -153.42572, -269.11056, -288.95231, -293.2… ## $ b_1 &lt;dbl&gt; 5.558515, 6.024319, 4.817043, 4.683460, 6.370333, 6.585946, 6.645106, 6.573859… Here’s the top panel of Figure 17.3. # how many posterior lines would you like? n_lines &lt;- 100 ggplot(data = d %&gt;% filter(subset == 1), aes(x = height, y = weight)) + geom_vline(xintercept = 0, color = bp[9]) + geom_abline(data = draws %&gt;% slice(1:n_lines), aes(intercept = b_0, slope = b_1, group = .draw), color = bp[6], size = 1/4, alpha = 1/3) + geom_point(alpha = 1/2, color = bp[3]) + scale_y_continuous(breaks = seq(from = -300, to = 200, by = 100)) + labs(subtitle = eval(substitute(paste(&quot;Data with&quot;, n_lines, &quot;credible regression lines&quot;))), x = &quot;height&quot;, y = &quot;weight&quot;) + coord_cartesian(xlim = c(0, 80), ylim = c(-350, 250)) Next we’ll make the histograms. # here are the primary data draws %&gt;% transmute(Intercept = b_0, Slope = b_1, Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% pivot_longer(everything()) %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = bp[9]) + stat_histinterval(aes(x = value, y = 0), point_interval = mode_hdi, .width = .95, fill = bp[6], color = bp[1], slab_color = bp[5], breaks = 40, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) And we’ll finish up with the scatter plots. draws %&gt;% ggplot(aes(x = b_1, y = b_0)) + geom_point(color = bp[4], size = 1/3, alpha = 1/3) + labs(x = expression(beta[1]), y = expression(beta[0])) draws %&gt;% transmute(Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% ggplot(aes(x = Normality, y = Scale)) + geom_point(color = bp[4], size = 1/3, alpha = 1/3) + labs(x = expression(log10(nu)), y = expression(sigma)) 17.2.2 Robust linear regression in Stan. Recall from Section 14.1 (p. 400) that Stan uses Hamiltonian dynamics to find proposed positions in parameter space. The trajectories use the gradient of the posterior distribution to move large distances even in narrow distributions. Thus, HMC by itself, without data standardization, should be able to efficiently generate a representative sample from the posterior distribution. (p. 487) To be clear, we’re going to fit the models with Stan/brms twice. Above, we used the standardized data like Kruschke did with his JAGS code. Now we’re getting ready to follow along with the text and use Stan/brms to fit the models with the unstandardized data. 17.2.2.1 Constants for vague priors. The issues is we want a system where we can readily specify vague priors on our regression models when the data are not standardized. As it turns out, a regression slope can take on a maximum value of \\(\\operatorname{SD}_y / \\operatorname{SD}_x\\) for data that are perfectly correlated. Therefore, the prior on the slope will be given a standard deviation that is large compared to that maximum. The biggest that an intercept could be, for data that are perfectly correlated, is \\(M_x \\operatorname{SD}_y / \\operatorname{SD}_x\\). Therefore, the prior on the intercept will have a standard deviation that is large compared to that maximum. (p. 487) With that in mind, we’ll specify our stanvars as follows. beta_0_sigma &lt;- 10 * abs(m_x * sd_y / sd_x) beta_1_sigma &lt;- 10 * abs(sd_y / sd_x) stanvars &lt;- stanvar(beta_0_sigma, name = &quot;beta_0_sigma&quot;) + stanvar(beta_1_sigma, name = &quot;beta_1_sigma&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) As in Chapter 16, “set the priors to be extremely broad relative to the data” (p. 487). With our stanvars defined, we’re ready to fit fit17.3. fit17.3 &lt;- brm(data = d, family = student, weight ~ 1 + height, prior = c(prior(normal(0, beta_0_sigma), class = Intercept), prior(normal(0, beta_1_sigma), class = b), prior(normal(0, sd_y), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvars, seed = 17, file = &quot;fits/fit17.03&quot;) Here’s the model summary. print(fit17.3) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: weight ~ 1 + height ## Data: d (Number of observations: 300) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -120.60 33.49 -184.21 -56.13 1.00 3068 2594 ## height 4.22 0.50 3.25 5.17 1.00 3119 2647 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 29.13 1.70 25.81 32.46 1.00 2484 2282 ## nu 25.43 20.98 6.19 82.85 1.00 2059 2553 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now compare the histograms for these posterior draws to those we made, above, from those fit17.1. You’ll see they’re quite similar. # here are the primary data as_draws_df(fit17.3) %&gt;% transmute(Intercept = b_Intercept, Slope = b_height, Scale = sigma, Normality = nu %&gt;% log10()) %&gt;% pivot_longer(everything()) %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = bp[9]) + stat_histinterval(aes(x = value, y = 0), point_interval = mode_hdi, .width = .95, fill = bp[6], color = bp[1], slab_color = bp[5], breaks = 40, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) 17.2.3 Stan or JAGS? In this ebook we only fit the models with brms, which uses Stan under the hood. But since we fit the \\(N = 300\\) model with both standardized and unstandardized data, we can compare their performance. For that, we’ll want bayesplot. library(bayesplot) They had equally impressive autocorrelation plots. # set the bayesplot color scheme color_scheme_set(scheme = bp[c(1, 3, 8, 7, 5, 5)]) as_draws_df(fit17.1) %&gt;% mutate(chain = .chain) %&gt;% mcmc_acf(pars = vars(b_Intercept:nu), lags = 10) + ggtitle(&quot;fit17.1&quot;) as_draws_df(fit17.3) %&gt;% mutate(chain = .chain) %&gt;% mcmc_acf(pars = vars(b_Intercept:nu), lags = 10) + ggtitle(&quot;fit17.3&quot;) Their \\(N_{eff}/N\\) ratios were pretty similar. Both were reasonable. You’d probably want to run a simulation to contrast them with any rigor. # change the bayesplot color scheme color_scheme_set(scheme = bp[c(1, 3, 4, 6, 7, 9)]) p1 &lt;- neff_ratio(fit17.1) %&gt;% mcmc_neff() + yaxis_text(hjust = 0) + ggtitle(&quot;fit17.1&quot;) p2 &lt;- neff_ratio(fit17.3) %&gt;% mcmc_neff() + yaxis_text(hjust = 0) + ggtitle(&quot;fit17.3&quot;) p1 / p2 + plot_layout(guide = &quot;collect&quot;) 17.2.4 Interpreting the posterior distribution. Halfway through the prose, Kruschke mentioned how the models provide entire posteriors for the weight of a 50-inch-tall person. brms offers a few ways to do so. In some applications, there is interest in extrapolating or interpolating trends at \\(x\\) values sparsely represented in the current data. For instance, we might want to predict the weight of a person who is \\(50\\) inches tall. A feature of Bayesian analysis is that we get an entire distribution of credible predicted values, not only a point estimate. (p. 489) Since this is such a simple model, one way is to work directly with the posterior draws Here we use the model formula \\(y_i = \\beta_0 + \\beta_1 x_i\\) by adding the transformed intercept b_0 to the product of 50 and the transformed coefficient for height, b_1. draws %&gt;% mutate(weight_at_50 = b_0 + b_1 * 50) %&gt;% ggplot(aes(x = weight_at_50, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = bp[6], color = bp[1], slab_color = bp[5], breaks = 40, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;lbs&quot;) ## Warning: Unknown or uninitialised column: `linewidth`. Looks pretty wide, doesn’t it? Hopefully this isn’t a surprise. Recall that this draws is from fit17.2, the posterior based on the \\(n = 30\\) data. With so few cases, most predictions from that model are uncertain. But also, 50 inches is way out of the bounds of the data the model was based on, so we should be uncertain in this range. Let’s practice a second method. With the brms::fitted() function, we can specify the desired height value into a tibble, which we’ll then feed into the newdata argument. Fitted will then return the model-implied criterion value for that predictor variable. To warm up, we’ll first to it with fit17.3, the model based on the untransformed data. nd &lt;- tibble(height = 50) fitted(fit17.3, newdata = nd) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 90.2715 8.668269 73.33173 107.1422 The code returned a typical brms-style summary of the posterior mean, standard deviation, and 95% percentile-based intervals. The same basic method will work for the standardized models, fit17.1 or fit17.2. But that will take a little more wrangling. First, we’ll need to transform our desired value 50 into its standardized version. nd &lt;- tibble(height_z = (50 - mean(d$height)) / sd(d$height)) When we feed this value into fitted(), it will return the corresponding posterior within the standardized metric. But we want unstandardized, so we’ll need to transform. That’ll be a few-step process. First, to do the transformation properly, we’ll want to work with the poster draws themselves, rather than summary values. So we’ll set summary = F. We’ll then convert the draws into a tibble format. Then we’ll use the transmute() function to do the conversion. In the final step, we’ll use mean_qi() to compute the summary values. fitted(fit17.1, newdata = nd, summary = F) %&gt;% as_tibble() %&gt;% transmute(weight = V1 * sd(d$weight) + mean(d$weight)) %&gt;% mean_qi() ## # A tibble: 1 × 6 ## weight .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 90.0 73.1 107. 0.95 mean qi If you look above, you’ll see the results are within rounding error of those from fit3. 17.3 Hierarchical regression on individuals within groups In the previous applications, the \\(j\\)th individual contributed a single \\(x_j, y_j\\) pair. But suppose instead that every individual, \\(j\\), contributes multiple observations of \\(x_{i|j}, y_{i|j}\\) pairs. (The subscript notation \\(i|j\\) means the \\(i\\)th observation within the \\(j\\)th individual.) With these data, we can estimate a regression curve for every individual. If we also assume that the individuals are mutually representative of a common group, then we can estimate group-level parameters too. (p. 490) Load the fictitious data and take a glimpse(). my_data &lt;- read_csv(&quot;data.R/HierLinRegressData.csv&quot;) glimpse(my_data) ## Rows: 132 ## Columns: 3 ## $ Subj &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 5, 5, 5, 5, 5… ## $ X &lt;dbl&gt; 60.2, 61.5, 61.7, 62.3, 67.6, 69.2, 53.7, 60.1, 60.5, 62.3, 63.0, 64.0, 64.1, 66.7, 6… ## $ Y &lt;dbl&gt; 145.6, 157.3, 165.6, 158.8, 196.1, 183.9, 165.0, 166.9, 179.0, 196.2, 192.3, 200.7, 1… Our goal is to describe each individual with a linear regression, and simultaneously to estimate the typical slope and intercept of the group overall. A key assumption for our analysis is that each individual is representative of the group. Therefore, every individual informs the estimate of the group slope and intercept, which in turn inform the estimates of all the individual slopes and intercepts. Thereby we get sharing of information across individuals, and shrinkage of individual estimates toward the overarching mode. (p. 491) 17.3.1 The model and implementation in JAGS brms. Kruschke described the model diagram in Figure 17.6 as “a bit daunting” (p. 491). The code to make our version of the diagram is “a bit daunting,” too. Just like the code for any other diagram, it’s modular. If you’re following along with me and making these on your own, just build it up, step by step. # normal density p1 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # half-normal density p2 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma][0]&quot;, size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # a second normal density p3 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)[1]&quot;, &quot;italic(S)[1]&quot;), size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # a second half-normal density p4 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma][1]&quot;, size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # four annotated arrows p5 &lt;- tibble(x = c(.05, .35, .65, .95), y = c(1, 1, 1, 1), xend = c(.32, .4, .65, .72), yend = c(.2, .2, .2, .2)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bp[1]) + annotate(geom = &quot;text&quot;, x = c(.15, .35, .625, .78), y = .55, label = &quot;&#39;~&#39;&quot;, size = 10, color = bp[1], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # third normal density p6 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;mu[0]&quot;, &quot;sigma[0]&quot;), size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # fourth normal density p7 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;mu[1]&quot;, &quot;sigma[1]&quot;), size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # two annotated arrows p8 &lt;- tibble(x = c(.18, .82), y = c(1, 1), xend = c(.36, .55), yend = c(0, 0)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bp[1]) + annotate(geom = &quot;text&quot;, x = c(.18, .33, .64, .77), y = .55, label = c(&quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;), size = c(10, 7, 10, 7), color = bp[1], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # exponential density p9 &lt;- tibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;exp&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(K)&quot;, size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # likelihood formula p10 &lt;- tibble(x = .5, y = .25, label = &quot;beta[0][italic(j)]+beta[1][italic(j)]*italic(x)[italic(i)*&#39;|&#39;*italic(j)]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = bp[1], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # half-normal density p11 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma]&quot;, size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # four annotated arrows p12 &lt;- tibble(x = c(.43, .43, 1.5, 2.5), y = c(1, .55, 1, 1), xend = c(.43, 1.225, 1.5, 1.75), yend = c(.8, .15, .2, .2)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bp[1]) + annotate(geom = &quot;text&quot;, x = c(.3, .7, 1.38, 2), y = c(.92, .22, .65, .6), label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;=&#39;&quot;, &quot;&#39;=&#39;&quot;, &quot;&#39;~&#39;&quot;), size = 10, color = bp[1], family = &quot;Times&quot;, parse = T) + annotate(geom = &quot;text&quot;, x = .43, y = .7, label = &quot;nu*minute+1&quot;, size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + xlim(0, 3) + theme_void() # student-t density p13 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) + geom_area(fill = bp[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;student t&quot;, size = 7, color = bp[1]) + annotate(geom = &quot;text&quot;, x = 0, y = .6, label = &quot;nu~~mu[italic(i)*&#39;|&#39;*italic(j)]~~sigma&quot;, size = 7, color = bp[1], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bp[1])) # the final annotated arrow p14 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)*&#39;|&#39;*italic(j)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = bp[1], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, color = bp[1], arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p15 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y)[italic(i)*&#39;|&#39;*italic(j)]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = bp[1], parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 1, r = 3), area(t = 1, b = 2, l = 5, r = 7), area(t = 1, b = 2, l = 9, r = 11), area(t = 1, b = 2, l = 13, r = 15), area(t = 4, b = 5, l = 5, r = 7), area(t = 4, b = 5, l = 9, r = 11), area(t = 3, b = 4, l = 1, r = 15), area(t = 7, b = 8, l = 3, r = 5), area(t = 7, b = 8, l = 7, r = 9), area(t = 7, b = 8, l = 11, r = 13), area(t = 6, b = 7, l = 5, r = 11), area(t = 10, b = 11, l = 7, r = 9), area(t = 9, b = 10, l = 3, r = 13), area(t = 12, b = 12, l = 7, r = 9), area(t = 13, b = 13, l = 7, r = 9) ) # combine and plot! (p1 + p2 + p3 + p4 + p6 + p7 + p5 + p9 + p10 + p11 + p8 + p13 + p12 + p14 + p15) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Just look at that sweet thing! If you made you version, here; have a piece of cake. 🍰 You earned it. Now let’s standardize the data and define our stanvars. I should note that standardizing and mean centering, more generally, becomes complicated with multilevel models. Here we’re just standardizing based on the grand mean and grand standard deviation. But there are other ways to standardize, such as within groups. Craig Enders has a good (2013) book chapter that touched on the topic, as well as an earlier (2007) paper with Tofighi. my_data &lt;- my_data %&gt;% mutate(x_z = standardize(X), y_z = standardize(Y)) In my experience, you typically use the (|) syntax when fitting a hierarchical model with thebrm() function. The terms before the | are those varying by group and you tell brm() what the grouping variable is after the |. In the case of multiple group-level parameters–which is the case with this model (i.e., both intercept and the x_z slope)–, this syntax also estimates correlations among the group-level parameters. Kruschke’s model doesn’t appear to include such a correlation. Happily, we can use the (||) syntax instead, which omits correlations among the group-level parameters. If you’re curious about the distinction, fit the model both ways and explore the differences in the print() output. For more on the topic, see the Group-level terms subsection of the brmsformula section of the brms reference manual (Bürkner, 2022d). fit17.4 &lt;- brm(data = my_data, family = student, y_z ~ 1 + x_z + (1 + x_z || Subj), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 1), class = sigma), # the next line is new prior(normal(0, 1), class = sd), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, seed = 17, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), file = &quot;fits/fit17.04&quot;) Did you catch that prior(normal(0, 1), class = sd) line in the code? That’s the prior we used for our hierarchical variance parameters, \\(\\sigma_0\\) and \\(\\sigma_1\\). Just like with the scale parameter, \\(\\sigma\\), we used the zero-mean half-normal distribution. By default, brms sets their left boundary to zero, which keeps the HMC algorithm from exploring negative variance values. Anyway, here’s the model summary(). summary(fit17.4) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: y_z ~ 1 + x_z + (1 + x_z || Subj) ## Data: my_data (Number of observations: 132) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~Subj (Number of levels: 25) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.02 0.18 0.72 1.44 1.00 1182 2090 ## sd(x_z) 0.22 0.12 0.02 0.47 1.00 969 1609 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.07 0.22 -0.36 0.49 1.00 874 1376 ## x_z 0.70 0.10 0.51 0.90 1.00 2991 3100 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.59 0.05 0.49 0.70 1.00 1764 2273 ## nu 39.63 30.88 6.23 125.68 1.00 3813 2203 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 17.3.2 The posterior distribution: Shrinkage and prediction. Keeping in the same spirit of Section 17.2.4, we’ll make the plots of Figure 17.5 in two ways. First, we’ll use our make_beta_0() and make_beta_1() functions to transform the model coefficients. draws &lt;- as_draws_df(fit17.4) sd_x &lt;- sd(my_data$X) sd_y &lt;- sd(my_data$Y) m_x &lt;- mean(my_data$X) m_y &lt;- mean(my_data$Y) draws &lt;- draws %&gt;% mutate(b_0 = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_x_z, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = b_x_z, sd_x = sd_x, sd_y = sd_y)) %&gt;% select(.draw, b_0, b_1) head(draws) ## # A tibble: 6 × 3 ## .draw b_0 b_1 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -90.8 3.59 ## 2 2 -64.0 3.20 ## 3 3 -51.2 2.94 ## 4 4 -84.8 3.51 ## 5 5 -21.6 2.54 ## 6 6 -82.9 3.49 Here’s the top panel of Figure 17.4. # how many posterior lines would you like? n_lines &lt;- 250 my_data %&gt;% mutate(Subj = factor(Subj, levels = 25:1)) %&gt;% ggplot(aes(x = X, y = Y)) + geom_abline(data = draws %&gt;% slice(1:n_lines), aes(intercept = b_0, slope = b_1, group = .draw), color = &quot;grey50&quot;, size = 1/4, alpha = 1/5) + geom_point(aes(color = Subj), alpha = 1/2) + geom_line(aes(group = Subj, color = Subj), size = 1/4) + scale_color_manual(values = beyonce_palette(41, n = 25, type = &quot;continuous&quot;), breaks = NULL) + scale_y_continuous(breaks = seq(from = 50, to = 250, by = 50)) + labs(subtitle = eval(substitute(paste(&quot;Data from all units with&quot;, n_lines, &quot;credible population-level\\nregression lines&quot;)))) + coord_cartesian(xlim = c(40, 95), ylim = c(30, 270)) Recall how we can use coef() to extract the Subj-specific parameters. But we’ll want posterior draws rather than summaries, which requires summary = F. It’ll take a bit of wrangling to get the output in a tidy format. Once we’re there, the plot code will be fairly simple. c &lt;- # first collect and wrangle the draws for the Subj-level intercept and slopes rbind(coef(fit17.4, summary = F)$Subj[, , &quot;Intercept&quot;], coef(fit17.4, summary = F)$Subj[, , &quot;x_z&quot;]) %&gt;% data.frame() %&gt;% set_names(1:25) %&gt;% mutate(draw = rep(1:4000, times = 2), param = rep(c(&quot;Intercept&quot;, &quot;Slope&quot;), each = 4000)) %&gt;% pivot_longer(`1`:`25`, names_to = &quot;Subj&quot;) %&gt;% pivot_wider(names_from = param, values_from = value) %&gt;% # now we&#39;re ready to un-standardize the standardized coefficients mutate(b_0 = make_beta_0(zeta_0 = Intercept, zeta_1 = Slope, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = Slope, sd_x = sd_x, sd_y = sd_y)) # how many lines would you like? n_lines &lt;- 250 # the plot: my_data %&gt;% mutate(Subj = factor(Subj, levels = 25:1)) %&gt;% ggplot(aes(x = X, y = Y)) + geom_abline(data = c %&gt;% filter(draw &lt;= n_lines), aes(intercept = b_0, slope = b_1), color = &quot;grey50&quot;, size = 1/4, alpha = 1/5) + geom_point(aes(color = Subj)) + scale_color_manual(values = beyonce_palette(41, n = 25, type = &quot;continuous&quot;), breaks = NULL) + scale_x_continuous(breaks = seq(from = 50, to = 90, by = 20)) + scale_y_continuous(breaks = seq(from = 50, to = 250, by = 100)) + labs(subtitle = &quot;Each unit now has its own bundle of credible regression lines&quot;) + coord_cartesian(xlim = c(45, 90), ylim = c(50, 270)) + facet_wrap(~ Subj %&gt;% factor(., levels = 1:25)) There’s some good pedagogy in that method. But I like having options and in this case fitted() affords a simpler workflow. Here’s the preparatory data wrangling step. # how many posterior lines would you like? n_lines &lt;- 250 nd &lt;- # since we&#39;re working with straight lines, we only need two x-values tibble(x_z = c(-5, 5)) %&gt;% mutate(X = x_z * sd(my_data$X) + mean(my_data$X), name = str_c(&quot;V&quot;, 1:n())) f &lt;- fitted(fit17.4, newdata = nd, # since we only want the fixed effects, we&#39;ll use `re_formula` # to maginalize over the random effects re_formula = Y_z ~ 1 + X_z, summary = F, # here we use `ndraws` to subset right from the get go ndraws = n_lines) %&gt;% as_tibble() %&gt;% mutate(draw = 1:n()) %&gt;% pivot_longer(-draw) %&gt;% # transform the `y_z` values back into the `Y` metric mutate(Y = value * sd(my_data$Y) + mean(my_data$Y)) %&gt;% # now attach the predictor values to the output left_join(nd, by = &quot;name&quot;) head(f) ## # A tibble: 6 × 6 ## draw name value Y x_z X ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 V1 -2.78 64.2 -5 31.4 ## 2 1 V2 2.62 237. 5 103. ## 3 2 V1 -3.47 42.1 -5 31.4 ## 4 2 V2 4.14 286. 5 103. ## 5 3 V1 -3.33 46.6 -5 31.4 ## 6 3 V2 3.99 281. 5 103. For the second time, here’s the top panel of Figure 17.4, this time based off of fitted(). p1 &lt;- my_data %&gt;% mutate(Subj = factor(Subj, levels = 25:1)) %&gt;% ggplot(aes(x = X, y = Y)) + geom_line(data = f, aes(group = draw), color = &quot;grey50&quot;, size = 1/4, alpha = 1/5) + geom_point(aes(color = Subj), alpha = 1/2) + geom_line(aes(group = Subj, color = Subj), size = 1/4) + scale_color_manual(values = beyonce_palette(41, n = 25, type = &quot;continuous&quot;)) + scale_y_continuous(breaks = seq(from = 50, to = 250, by = 50)) + labs(subtitle = eval(substitute(paste(&quot;Data from all units with&quot;, n_lines, &quot;credible population-level\\nregression lines&quot;)))) + coord_cartesian(xlim = c(40, 95), ylim = c(30, 270)) + theme(legend.position = &quot;none&quot;) p1 The whole process is quite similar for the Subj-specific lines. There are two main differences. First, we need to specify which Subj values we’d like to get fitted() points for. That goes into our nd tibble. Second, we omit the re_formula argument. There are other subtleties, like with the contents of the bind_cols() function. But hopefully those are self-evident. # how many posterior lines would you like? n_lines &lt;- 250 nd &lt;- tibble(x_z = c(-5, 5)) %&gt;% mutate(X = x_z * sd(my_data$X) + mean(my_data$X)) %&gt;% expand(nesting(x_z, X), Subj = distinct(my_data, Subj) %&gt;% pull()) %&gt;% mutate(name = str_c(&quot;V&quot;, 1:n())) f &lt;- fitted(fit17.4, newdata = nd, summary = F, ndraws = n_lines) %&gt;% as_tibble() %&gt;% mutate(draw = 1:n()) %&gt;% pivot_longer(-draw) %&gt;% mutate(Y = value * sd(my_data$Y) + mean(my_data$Y)) %&gt;% left_join(nd, by = &quot;name&quot;) head(f) ## # A tibble: 6 × 7 ## draw name value Y x_z X Subj ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 V1 -4.04 23.9 -5 31.4 1 ## 2 1 V2 -0.881 125. -5 31.4 2 ## 3 1 V3 -4.70 2.69 -5 31.4 3 ## 4 1 V4 -4.12 21.4 -5 31.4 4 ## 5 1 V5 -3.83 30.6 -5 31.4 5 ## 6 1 V6 -10.1 -169. -5 31.4 6 And now for the second time, here’s the bottom panel of Figure 17.4, this time based off of fitted(). p2 &lt;- my_data %&gt;% mutate(Subj = factor(Subj, levels = 25:1)) %&gt;% ggplot(aes(x = X, y = Y)) + geom_line(data = f, aes(group = draw), color = &quot;grey50&quot;, size = 1/4, alpha = 1/5) + geom_point(aes(color = Subj)) + scale_color_manual(values = beyonce_palette(41, n = 25, type = &quot;continuous&quot;), breaks = NULL) + scale_x_continuous(breaks = seq(from = 50, to = 90, by = 20)) + scale_y_continuous(breaks = seq(from = 50, to = 250, by = 100)) + labs(subtitle = &quot;Each unit now has its own bundle of credible regression lines&quot;) + coord_cartesian(xlim = c(45, 90), ylim = c(50, 270)) + facet_wrap(~ Subj %&gt;% factor(., levels = 1:25)) # combine with patchwork p3 &lt;- plot_spacer() p4 &lt;- (p3 | p1 | p3) + plot_layout(widths = c(1, 4, 1)) (p4 / p2) + plot_layout(heights = c(0.6, 1)) Especially if you’re new to these kinds of models, it’s easy to get lost in all that code. And for real–the wrangling required for those plots was no joke. The primary difficulty was that we had to convert standardized solutions to unstandardized solutions, which leads to an important distinction. When we used the first method of working with the as_draws_df() and coef() output, we focused on transforming the model parameters. In contrast, when we used the second method of working with the fitted() output, we focused instead on transforming the model predictions and predictor values. This distinction can be really confusing, at first. Stick with it! There will be times one method is more convenient or intuitive than the other. It’s good to have both methods in your repertoire. 17.4 Quadratic trend and weighted data Quadratic models follow the general form \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2,\\] where \\(\\beta_2\\) is the quadratic term which, when 0, reduces the results to a simple linear model. That’s right; the linear model is a special case of the quadratic. This time the data come from the American Community Survey and Puerto Rico Community Survey. In his footnote #3, Kruschke indicated “Data are from http://www.census.gov/hhes/www/income/data/Fam_Inc_SizeofFam1.xls, retrieved December 11, 2013. Median family income for years 2009-2011.” As to our read.csv() code, note the comment.char argument. my_data &lt;- read.csv(&quot;data.R/IncomeFamszState3yr.csv&quot;, comment.char = &quot;#&quot;) glimpse(my_data) ## Rows: 312 ## Columns: 4 ## $ FamilySize &lt;int&gt; 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2, 3,… ## $ State &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;… ## $ MedianIncome &lt;int&gt; 48177, 53323, 64899, 59417, 54099, 47655, 73966, 82276, 87726, 87216, 84488, … ## $ SampErr &lt;int&gt; 581, 1177, 1170, 2446, 3781, 3561, 1858, 3236, 3722, 6127, 6761, 5754, 590, 1… Here we’ll standardize all variables but State, our grouping variable. It’d be silly to try to standardize that. my_data &lt;- my_data %&gt;% mutate(family_size_z = standardize(FamilySize), median_income_z = standardize(MedianIncome), se_z = SampErr / (mean(SampErr))) glimpse(my_data) ## Rows: 312 ## Columns: 7 ## $ FamilySize &lt;int&gt; 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2,… ## $ State &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alaska&quot;… ## $ MedianIncome &lt;int&gt; 48177, 53323, 64899, 59417, 54099, 47655, 73966, 82276, 87726, 87216, 8448… ## $ SampErr &lt;int&gt; 581, 1177, 1170, 2446, 3781, 3561, 1858, 3236, 3722, 6127, 6761, 5754, 590… ## $ family_size_z &lt;dbl&gt; -1.4615023, -0.8769014, -0.2923005, 0.2923005, 0.8769014, 1.4615023, -1.46… ## $ median_income_z &lt;dbl&gt; -1.26216763, -0.91386251, -0.13034520, -0.50139236, -0.86133924, -1.297499… ## $ se_z &lt;dbl&gt; 0.2242541, 0.4542979, 0.4515961, 0.9441060, 1.4593886, 1.3744731, 0.717150… With brms, there are a couple ways to handle measurement error on a variable (e.g., see Chapter 14 of my ebook, Statistical rethinking with brms, ggplot2, and the tidyverse (Kurz, 2020). Here we’ll use the se() syntax, following the form response | se(se_response, sigma = TRUE). In this form, se stands for standard error, the loose frequentist analogue to the Bayesian posterior \\(\\textit{SD}\\). Unless you’re fitting a meta-analysis on summary information, make sure to specify sigma = TRUE. Without that you’ll have no estimate for \\(\\sigma\\)! For more information on the se() method, go to the brms reference manual and find the Additional response information subsection of the brmsformula section (Bürkner, 2022d, p. 42). fit17.5 &lt;- brm(data = my_data, family = student, median_income_z | se(se_z, sigma = TRUE) ~ 1 + family_size_z + I(family_size_z^2) + (1 + family_size_z + I(family_size_z^2) || State), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 1), class = sigma), prior(normal(0, 1), class = sd), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 17, file = &quot;fits/fit17.05&quot;) Did you notice the I(family_size_z^2) part of the formula? The brms package follows a typical convention in R statistical functions in that if you want to multiply a variable by itself as in a quadratic model, you nest the family_size_z^2 part within the I() function. Take a look at the model summary. print(fit17.5) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: median_income_z | se(se_z, sigma = TRUE) ~ 1 + family_size_z + I(family_size_z^2) + (1 + family_size_z + I(family_size_z^2) || State) ## Data: my_data (Number of observations: 312) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~State (Number of levels: 52) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.75 0.09 0.60 0.93 1.00 732 1488 ## sd(family_size_z) 0.07 0.04 0.00 0.16 1.01 778 1098 ## sd(Ifamily_size_zE2) 0.05 0.03 0.00 0.13 1.01 695 1350 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.39 0.12 0.17 0.62 1.01 373 826 ## family_size_z 0.12 0.05 0.02 0.22 1.00 3352 2759 ## Ifamily_size_zE2 -0.44 0.04 -0.52 -0.36 1.00 3436 3041 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.05 0.04 0.00 0.13 1.00 2110 1921 ## nu 68.97 36.04 22.04 158.27 1.00 6673 3217 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Do see that Ifamily_size_zE2 row? That’s the summary of our quadratic term. 17.4.1 Results and interpretation. A new model type requires a different approach to un-standardizing our standardized coefficients. Based on Equation 17.3, we can convert our coefficients using the formulas \\[\\begin{align*} \\beta_0 &amp; = \\zeta_0 \\operatorname{SD}_y + M_y - \\frac{\\zeta_1 M_x \\operatorname{SD}_y}{\\operatorname{SD}_x} + \\frac{\\zeta_2 M^{2}_x \\operatorname{SD}_y}{\\operatorname{SD}^{2}_x}, \\\\ \\beta_1 &amp; = \\frac{\\zeta_1 \\operatorname{SD}_y}{\\operatorname{SD}_x} - \\frac{2 \\zeta_2 M_x \\operatorname{SD}_y}{\\operatorname{SD}^{2}_x}, \\text{and} \\\\ \\beta_2 &amp; = \\frac{\\zeta_2 \\operatorname{SD}_y}{\\operatorname{SD}^{2}_x}. \\end{align*}\\] We’ll make new custom functions to use them. make_beta_0 &lt;- function(zeta_0, zeta_1, zeta_2, sd_x, sd_y, m_x, m_y) { zeta_0 * sd_y + m_y - zeta_1 * m_x * sd_y / sd_x + zeta_2 * m_x^2 * sd_y / sd_x^2 } make_beta_1 &lt;- function(zeta_1, zeta_2, sd_x, sd_y, m_x) { zeta_1 * sd_y / sd_x - 2 * zeta_2 * m_x * sd_y / sd_x^2 } make_beta_2 &lt;- function(zeta_2, sd_x, sd_y) { zeta_2 * sd_y / sd_x^2 } # may as well respecify these, too m_x &lt;- mean(my_data$FamilySize) m_y &lt;- mean(my_data$MedianIncome) sd_x &lt;- sd(my_data$FamilySize) sd_y &lt;- sd(my_data$MedianIncome) Now we’ll extract our posterior draws and make the conversions. draws &lt;- as_draws_df(fit17.5) %&gt;% mutate(b_0 = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_family_size_z, zeta_2 = b_Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = b_family_size_z, zeta_2 = b_Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y, m_x = m_x), b_2 = make_beta_2(zeta_2 = b_Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y)) %&gt;% select(.draw, b_0:b_2) Our geom_abline() approach from before won’t work with curves. We’ll have to resort to geom_line(). With the geom_line() approach, we’ll need many specific values of model-implied MedianIncome across a densely-packed range of FamilySize. We want to use a lot of FamilySize values, like 30 or 50 or so, to make sure the curves look smooth. Below, we’ll use 50 (i.e., length.out = 50). But if it’s still not clear why, try plugging in a lesser value, like 5 or so. You’ll see. # how many posterior lines would you like? n_lines &lt;- 200 set.seed(17) draws &lt;- draws %&gt;% slice_sample(n = n_lines) %&gt;% rownames_to_column(var = &quot;draw&quot;) %&gt;% expand(nesting(.draw, b_0, b_1, b_2), FamilySize = seq(from = 1, to = 9, length.out = 50)) %&gt;% mutate(MedianIncome = b_0 + b_1 * FamilySize + b_2 * FamilySize^2) head(draws) ## # A tibble: 6 × 6 ## .draw b_0 b_1 b_2 FamilySize MedianIncome ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 20901. 21997. -2364. 1 40533. ## 2 5 20901. 21997. -2364. 1.16 43290. ## 3 5 20901. 21997. -2364. 1.33 45920. ## 4 5 20901. 21997. -2364. 1.49 48424. ## 5 5 20901. 21997. -2364. 1.65 50803. ## 6 5 20901. 21997. -2364. 1.82 53055. Now we’re ready to make the top panel of Figure 17.7. my_data %&gt;% ggplot(aes(x = FamilySize, y = MedianIncome)) + geom_line(data = draws, aes(group = .draw), size = 1/4, alpha = 1/5, color = &quot;grey67&quot;) + geom_line(aes(group = State, color = State), alpha = 2/3, size = 1/4) + geom_point(aes(color = State), alpha = 2/3, size = 1/2) + scale_color_manual(values = beyonce_palette(41, n = 52, type = &quot;continuous&quot;), breaks = NULL) + scale_x_continuous(&quot;Family size&quot;, breaks = 1:8) + labs(title = &quot;All states&quot;, y = &quot;Median income&quot;) + coord_cartesian(xlim = c(1, 8), ylim = c(0, 150000)) Like before, we’ll extract the group-level coefficients (i.e., those specific to the States) with the coef() function. And also like before, the coef() output will require a little wrangling. c &lt;- # first collect and wrangle the draws for the State-level intercept and slopes rbind(coef(fit17.5, summary = F)$State[, , &quot;Intercept&quot;], coef(fit17.5, summary = F)$State[, , &quot;family_size_z&quot;], coef(fit17.5, summary = F)$State[, , &quot;Ifamily_size_zE2&quot;]) %&gt;% data.frame() %&gt;% mutate(draw = rep(1:4000, times = 3), param = rep(c(&quot;Intercept&quot;, &quot;family_size_z&quot;, &quot;Ifamily_size_zE2&quot;), each = 4000)) %&gt;% pivot_longer(Alabama:Wyoming, names_to = &quot;State&quot;) %&gt;% pivot_wider(names_from = param, values_from = value) %&gt;% # let&#39;s go ahead and make the standardized-to-unstandardized conversions, here mutate(b_0 = make_beta_0(zeta_0 = Intercept, zeta_1 = family_size_z, zeta_2 = Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = family_size_z, zeta_2 = Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y, m_x = m_x), b_2 = make_beta_2(zeta_2 = Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y)) %&gt;% # we just want the first 25 states, from Alabama through Mississippi, so we&#39;ll `filter()` filter(State &lt;= &quot;Mississippi&quot;) str(c) ## tibble [100,000 × 8] (S3: tbl_df/tbl/data.frame) ## $ draw : int [1:100000] 1 1 1 1 1 1 1 1 1 1 ... ## $ State : chr [1:100000] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... ## $ Intercept : num [1:100000] 0.0546 0.6689 0.1472 -0.0836 0.6011 ... ## $ family_size_z : num [1:100000] 0.149 0.107 0.155 0.114 0.107 ... ## $ Ifamily_size_zE2: num [1:100000] -0.513 -0.468 -0.4 -0.524 -0.415 ... ## $ b_0 : num [1:100000] 9407 24709 22033 7598 29127 ... ## $ b_1 : num [1:100000] 24594 22182 19533 24790 19777 ... ## $ b_2 : num [1:100000] -2590 -2362 -2021 -2645 -2095 ... Now we’ll subset by n_lines, expand() by FamilySize, and use the model formula to compute the expected MedianIncome values. # how many posterior lines would you like? n_lines &lt;- 200 set.seed(17) c &lt;- c %&gt;% group_by(State) %&gt;% slice_sample(n = n_lines) %&gt;% ungroup() %&gt;% expand(nesting(draw, State, b_0, b_1, b_2), FamilySize = seq(from = 1, to = 9, length.out = 50)) %&gt;% mutate(MedianIncome = b_0 + b_1 * FamilySize + b_2 * FamilySize^2) head(c) ## # A tibble: 6 × 7 ## draw State b_0 b_1 b_2 FamilySize MedianIncome ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Idaho -951. 25134. -2719. 1 21464. ## 2 1 Idaho -951. 25134. -2719. 1.16 24607. ## 3 1 Idaho -951. 25134. -2719. 1.33 27605. ## 4 1 Idaho -951. 25134. -2719. 1.49 30458. ## 5 1 Idaho -951. 25134. -2719. 1.65 33166. ## 6 1 Idaho -951. 25134. -2719. 1.82 35729. Finally, we’re ready for the State-specific miniatures in Figure 17.7. my_data %&gt;% filter(State &lt;= &quot;Mississippi&quot;) %&gt;% ggplot(aes(x = FamilySize, y = MedianIncome)) + geom_line(data = c, aes(group = draw), size = 1/4, alpha = 1/5, color = &quot;grey67&quot;) + geom_point(aes(color = State)) + geom_line(aes(color = State)) + scale_color_manual(values = beyonce_palette(41, n = 52, type = &quot;continuous&quot;), breaks = NULL) + scale_x_continuous(&quot;Family size&quot;, breaks = 1:8) + labs(subtitle = &quot;Each State now has its own bundle of credible regression curves.&quot;, y = &quot;Median income&quot;) + coord_cartesian(xlim = c(1, 8), ylim = c(0, 150000)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ State) Magic! As our model coefficients proliferate, the fitted() approach from above starts to look more and more appetizing. Check it out for yourself. Although “almost all of the posterior distribution [was] below \\(\\nu = 4\\)” in the text (p. 500), the bulk of our \\(\\nu\\) distribution spanned across much larger values. as_draws_df(fit17.5) %&gt;% ggplot(aes(x = nu, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = bp[6], color = bp[1], slab_color = bp[5], breaks = 40, normalize = &quot;panels&quot;) + scale_x_continuous(expand = expansion(mult = c(0, 0.05))) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(Our~big~nu), x = NULL) ## Warning: Unknown or uninitialised column: `linewidth`. I’m guessing the distinction in our \\(\\nu\\) distribution and that in the text is our use of the se() syntax in the brm() formula. If you have a better explanation, share it. 17.4.2 Further extensions. Kruschke discussed the ease with which users of Bayesian software might specify nonlinear models. Check out Bürkner’s (2022) vignette, Estimating non-linear models with brms, for more on the topic. Though I haven’t used it, I believe it is also possible to use the \\(t\\) distribution to model group-level variation in brms (see this GitHub discussion for details). 17.5 Procedure and perils for expanding a model Across several chapters, we’ve already dipped our toes into posterior predictive checks. For more on the PPC “double dipping” issue, check out Gelman’s Discussion with Sander Greenland on posterior predictive checks or Simpson’s Touch me, I want to feel your data, which is itself connected to Gabry et al. (2019), Visualization in Bayesian workflow. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bayesplot_1.9.0 tidybayes_3.0.2 brms_2.18.0 Rcpp_1.0.9 patchwork_1.1.2 beyonce_0.1 ## [7] forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 ## [13] tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 igraph_1.3.4 ## [5] svUnit_1.0.6 splines_4.2.0 crosstalk_1.2.0 TH.data_1.1-1 ## [9] rstantools_2.2.0 inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 googlesheets4_1.0.1 ## [17] tzdb_0.3.0 modelr_0.1.8 RcppParallel_5.1.5 matrixStats_0.62.0 ## [21] vroom_1.5.7 xts_0.12.1 sandwich_3.0-2 prettyunits_1.1.1 ## [25] colorspace_2.0-3 rvest_1.0.2 ggdist_3.2.0 haven_2.5.1 ## [29] xfun_0.35 callr_3.7.3 crayon_1.5.2 jsonlite_1.8.3 ## [33] lme4_1.1-31 survival_3.4-0 zoo_1.8-10 glue_1.6.2 ## [37] gtable_0.3.1 gargle_1.2.0 emmeans_1.8.0 distributional_0.3.1 ## [41] pkgbuild_1.3.1 rstan_2.21.7 abind_1.4-5 scales_1.2.1 ## [45] mvtnorm_1.1-3 emo_0.0.0.9000 DBI_1.1.3 miniUI_0.1.1.1 ## [49] xtable_1.8-4 HDInterval_0.2.2 bit_4.0.4 stats4_4.2.0 ## [53] StanHeaders_2.21.0-7 DT_0.24 htmlwidgets_1.5.4 httr_1.4.4 ## [57] threejs_0.3.3 arrayhelpers_1.1-0 posterior_1.3.1 ellipsis_0.3.2 ## [61] pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 sass_0.4.2 ## [65] dbplyr_2.2.1 utf8_1.2.2 tidyselect_1.1.2 labeling_0.4.2 ## [69] rlang_1.0.6 reshape2_1.4.4 later_1.3.0 munsell_0.5.0 ## [73] cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 cli_3.5.0 ## [77] generics_0.1.3 broom_1.0.1 ggridges_0.5.3 evaluate_0.18 ## [81] fastmap_1.1.0 bit64_4.0.5 processx_3.8.0 knitr_1.40 ## [85] fs_1.5.2 nlme_3.1-159 mime_0.12 projpred_2.2.1 ## [89] xml2_1.3.3 compiler_4.2.0 shinythemes_1.2.0 rstudioapi_0.13 ## [93] gamm4_0.2-6 reprex_2.0.2 bslib_0.4.0 stringi_1.7.8 ## [97] highr_0.9 ps_1.7.2 Brobdingnag_1.2-8 lattice_0.20-45 ## [101] Matrix_1.4-1 nloptr_2.0.3 markdown_1.1 shinyjs_2.1.0 ## [105] tensorA_0.36.2 vctrs_0.5.1 pillar_1.8.1 lifecycle_1.0.3 ## [109] jquerylib_0.1.4 bridgesampling_1.1-2 estimability_1.4.1 httpuv_1.6.5 ## [113] R6_2.5.1 bookdown_0.28 promises_1.2.0.1 gridExtra_2.3 ## [117] codetools_0.2-18 boot_1.3-28 colourpicker_1.1.1 MASS_7.3-58.1 ## [121] gtools_3.9.3 assertthat_0.2.1 withr_2.5.0 shinystan_2.6.0 ## [125] multcomp_1.4-20 mgcv_1.8-40 parallel_4.2.0 hms_1.1.1 ## [129] grid_4.2.0 minqa_1.2.5 coda_0.19-4 rmarkdown_2.16 ## [133] googledrive_2.0.0 shiny_1.7.2 lubridate_1.8.0 base64enc_0.1-3 ## [137] dygraphs_1.1.1.6 References Bürkner, P.-C. (2022). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html Bürkner, P.-C. (2022d). brms reference manual, Version 2.17.0. https://CRAN.R-project.org/package=brms/brms.pdf Enders, C. (2013). Centering predictors and contextual effects. In M. Scott, J. Simonoff, &amp; B. Marx (Eds.), The SAGE Handbook of Multilevel Modeling (pp. 89–108). SAGE Publications Ltd. https://doi.org/10.4135/9781446247600.n6 Enders, C. K., &amp; Tofighi, D. (2007). Centering predictor variables in cross-sectional multilevel models: A new look at an old issue. Psychological Methods, 12(2), 121. https://doi.org/10.1037/1082-989X.12.2.121 Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., &amp; Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Kurz, A. S. (2020). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 "],["metric-predicted-variable-with-multiple-metric-predictors.html", "18 Metric Predicted Variable with Multiple Metric Predictors 18.1 Multiple linear regression 18.2 Multiplicative interaction of metric predictors 18.3 Shrinkage of regression coefficients 18.4 Variable selection Session info Footnote", " 18 Metric Predicted Variable with Multiple Metric Predictors We will consider models in which the predicted variable is an additive combination of predictors, all of which have proportional influence on the prediction. This kind of model is called multiple linear regression. We will also consider nonadditive combinations of predictors, which are called interactions. (Kruschke, 2015, p. 509, emphasis in the original) 18.1 Multiple linear regression Say we have one criterion \\(y\\) and two predictors, \\(x_1\\) and \\(x_2\\). If \\(y \\sim \\operatorname{Normal}(\\mu, \\sigma)\\) and \\(\\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\), then it’s also the case that we can rewrite the formula for \\(y\\) as \\[y \\sim \\operatorname{Normal}(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2, \\sigma).\\] As Kruschke pointed out, the basic model “assumes homogeneity of variance, which means that at all values of \\(x_1\\) and \\(x_2\\), the variance \\(\\sigma^2\\) of \\(y\\) is the same” (p. 510). If we presume the data for the two \\(x\\) variables are uniformly distributed within 0 and 10, we can make the data for Figure 18.1 like this. library(tidyverse) n &lt;- 300 set.seed(18) d &lt;- tibble(x_1 = runif(n = n, min = 0, max = 10), x_2 = runif(n = n, min = 0, max = 10)) %&gt;% mutate(y = rnorm(n = n, mean = 10 + x_1 + 2 * x_2, sd = 2)) head(d) ## # A tibble: 6 × 3 ## x_1 x_2 y ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.23 8.62 38.4 ## 2 7.10 1.33 20.8 ## 3 9.66 1.08 17.1 ## 4 0.786 7.09 25.2 ## 5 0.536 6.67 27.3 ## 6 5.75 4.72 26.1 Before we plot those d data, we’ll want to make a data object containing the information necessary to make the grid lines for Kruschke’s 3D regression plane. To my mind, this will be easier to do in stages. If you look at the top upper panel of Figure 18.1 as a reference, our first step will be to make the vertical lines. Save them as d1. theme_set( theme_linedraw() + theme(panel.grid = element_blank()) ) d1 &lt;- tibble(index = 1:21, x_1 = seq(from = 0, to = 10, length.out = 21)) %&gt;% expand(nesting(index, x_1), x_2 = c(0, 10)) %&gt;% mutate(y = 10 + 1 * x_1 + 2 * x_2) d1 %&gt;% ggplot(aes(x = x_1, y = y, group = index)) + geom_path(color = &quot;grey85&quot;) + ylim(0, 50) You may have noticed our theme_set() lines at the top. Though we’ll be using a different default theme later in the project, this is the best theme to use for these initial few plots. Okay, now let’s make the more horizontally-oriented grid lines and save them as d2. d2 &lt;- tibble(index = 1:21 + 21, x_2 = seq(from = 0, to = 10, length.out = 21)) %&gt;% expand(nesting(index, x_2), x_1 = c(0, 10)) %&gt;% mutate(y = 10 + 1 * x_1 + 2 * x_2) d2 %&gt;% ggplot(aes(x = x_1, y = y, group = index)) + geom_path(color = &quot;grey85&quot;) + ylim(0, 50) Now combine the two and save them as grid. grid &lt;- bind_rows(d1, d2) grid %&gt;% ggplot(aes(x = x_1, y = y, group = index)) + geom_path(color = &quot;grey85&quot;) + ylim(0, 50) grid %&gt;% ggplot(aes(x = x_2, y = y, group = index)) + geom_path(color = &quot;grey85&quot;) + ylim(0, 50) grid %&gt;% ggplot(aes(x = x_1, y = x_2, group = index)) + geom_path(color = &quot;grey85&quot;) We’re finally ready combine d and grid to make the three 2D scatter plots from Figure 18.1. d %&gt;% ggplot(aes(x = x_1, y = y)) + geom_path(data = grid, aes(group = index), color = &quot;grey85&quot;) + geom_segment(aes(xend = x_1, yend = 10 + x_1 + 2 * x_2), size = 1/4, linetype = 3) + geom_point(shape = 21, stroke = 1/10, color = &quot;white&quot;, fill = &quot;steelblue4&quot;) + scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) + scale_y_continuous(limits = c(0, 50), expand = c(0, 0)) d %&gt;% ggplot(aes(x = x_2, y = y)) + geom_path(data = grid, aes(group = index), color = &quot;grey85&quot;) + geom_segment(aes(xend = x_2, yend = 10 + x_1 + 2 * x_2), size = 1/4, linetype = 3) + geom_point(shape = 21, stroke = 1/10, color = &quot;white&quot;, fill = &quot;steelblue4&quot;) + scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) + scale_y_continuous(limits = c(0, 50), expand = c(0, 0)) d %&gt;% ggplot(aes(x = x_1, y = x_2)) + geom_path(data = grid, aes(group = index), color = &quot;grey85&quot;) + geom_point(shape = 21, stroke = 1/10, color = &quot;white&quot;, fill = &quot;steelblue4&quot;) + scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) + scale_y_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) As in previous chapters, I’m not aware that ggplot2 allows for three-dimensional wireframe plots of the kind in the upper left panel. If you’d like to make one in base R, have at it. For Figure 18.2, the \\(x\\) variables look to be multivariate normal with a correlation of about -.95. We can simulate such data with help from the MASS package (Ripley, 2021; Venables &amp; Ripley, 2002). Sven Hohenstein’s answer to this stats.stackexchange.com question provides the steps for simulating the data. First, we’ll need to specify the desired means and standard deviations for our variables. Then we’ll make a correlation matrix with 1s on the diagonal and the desired correlation coefficient, \\(\\rho\\) on the off-diagonal. Since the correlation matrix is symmetric, both off-diagonal positions are the same. Then we convert the correlation matrix to a covariance matrix. mus &lt;- c(5, 5) sds &lt;- c(2, 2) cors &lt;- matrix(c(1, -.95, -.95, 1), ncol = 2) cors ## [,1] [,2] ## [1,] 1.00 -0.95 ## [2,] -0.95 1.00 covs &lt;- sds %*% t(sds) * cors covs ## [,1] [,2] ## [1,] 4.0 -3.8 ## [2,] -3.8 4.0 Now we’ve defined our means, standard deviations, and covariance matrix, we’re ready to simulate the data with the MASS::mvrnorm() function. # how many data points would you like to simulate? n &lt;- 300 set.seed(18.2) d &lt;- MASS::mvrnorm(n = n, mu = mus, Sigma = covs, empirical = T) %&gt;% as_tibble() %&gt;% set_names(&quot;x_1&quot;, &quot;x_2&quot;) %&gt;% mutate(y = rnorm(n = n, mean = 10 + x_1 + 2 * x_2, sd = 2)) Now we have our simulated data in hand, we’re ready for three of the four panels of Figure 18.2. p1 &lt;- d %&gt;% ggplot(aes(x = x_1, y = y)) + geom_path(data = grid, aes(group = index), color = &quot;grey85&quot;) + geom_segment(aes(xend = x_1, yend = 10 + x_1 + 2 * x_2), size = 1/4, linetype = 3) + geom_point(shape = 21, stroke = 1/10, color = &quot;white&quot;, fill = &quot;steelblue4&quot;) + scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) + scale_y_continuous(limits = c(0, 50), expand = c(0, 0)) p2 &lt;- d %&gt;% ggplot(aes(x = x_2, y = y)) + geom_path(data = grid, aes(group = index), color = &quot;grey85&quot;) + geom_segment(aes(xend = x_2, yend = 10 + x_1 + 2 * x_2), size = 1/4, linetype = 3) + geom_point(shape = 21, stroke = 1/10, color = &quot;white&quot;, fill = &quot;steelblue4&quot;) + scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) + scale_y_continuous(limits = c(0, 50), expand = c(0, 0)) p3 &lt;- d %&gt;% ggplot(aes(x = x_1, y = x_2)) + geom_path(data = grid, aes(group = index), color = &quot;grey85&quot;) + geom_point(shape = 21, stroke = 1/10, color = &quot;white&quot;, fill = &quot;steelblue4&quot;) + scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) + scale_y_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) # bind them together with patchwork library(patchwork) plot_spacer() + p1 + p2 + p3 We came pretty close. 18.1.1 The perils of correlated predictors. Figures 18.1 and 18.2 show data generated from the same model. In both figures, \\(\\sigma = 2\\), \\(\\beta_0 = 10\\), \\(\\beta_1 = 1\\), \\(\\beta_2 = 2\\). All that differs between the two figures is the distribution of the \\(\\langle x_1, x_2 \\rangle\\) values, which is not specified by the model. In Figure 18.1, the \\(\\langle x_1, x_2 \\rangle\\) values are distributed independently. In Figure 18.2, the \\(\\langle x_1, x_2 \\rangle\\) values are negatively correlated: When \\(x_1\\) is small, \\(x_2\\) tends to be large, and when \\(x_1\\) is large, \\(x_2\\) tends to be small. (p. 510) If you look closely at our simulation code from above, you’ll see we have done so, too. Real data often have correlated predictors. For example, consider trying to predict a state’s average high-school SAT score on the basis of the amount of money the state spends per pupil. If you plot only mean SAT against money spent, there is actually a decreasing trend… (p. 513, emphasis in the original) Before we remake Figure 18.3 to examine that decreasing trend, we’ll need to load the data from (Guber, 1999). my_data &lt;- read_csv(&quot;data.R/Guber1999data.csv&quot;) glimpse(my_data) ## Rows: 50 ## Columns: 8 ## $ State &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;C… ## $ Spend &lt;dbl&gt; 4.405, 8.963, 4.778, 4.459, 4.992, 5.443, 8.817, 7.030, 5.71… ## $ StuTeaRat &lt;dbl&gt; 17.2, 17.6, 19.3, 17.1, 24.0, 18.4, 14.4, 16.6, 19.1, 16.3, … ## $ Salary &lt;dbl&gt; 31.144, 47.951, 32.175, 28.934, 41.078, 34.571, 50.045, 39.0… ## $ PrcntTake &lt;dbl&gt; 8, 47, 27, 6, 45, 29, 81, 68, 48, 65, 57, 15, 13, 58, 5, 9, … ## $ SATV &lt;dbl&gt; 491, 445, 448, 482, 417, 462, 431, 429, 420, 406, 407, 468, … ## $ SATM &lt;dbl&gt; 538, 489, 496, 523, 485, 518, 477, 468, 469, 448, 482, 511, … ## $ SATT &lt;dbl&gt; 1029, 934, 944, 1005, 902, 980, 908, 897, 889, 854, 889, 979… Before we get all excited and try to plot those data as in Figure 18.3, we’ll need to redefine the 3D grid of our regression plane, this time based on the equation at the top of Figure 18.3. d1 &lt;- tibble(index = 1:21, Spend = seq(from = 3.4, to = 10.1, length.out = 21)) %&gt;% expand(nesting(index, Spend), PrcntTake = c(0, 85)) d2 &lt;- tibble(index = 1:21 + 21, PrcntTake = seq(from = 0, to = 85, length.out = 21)) %&gt;% expand(nesting(index, PrcntTake), Spend = c(3.4, 10.1)) grid &lt;- bind_rows(d1, d2) %&gt;% mutate(SATT = 993.8 + -2.9 * PrcntTake + 12.3 * Spend) grid %&gt;% glimpse() ## Rows: 84 ## Columns: 4 ## $ index &lt;dbl&gt; 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10… ## $ Spend &lt;dbl&gt; 3.400, 3.400, 3.735, 3.735, 4.070, 4.070, 4.405, 4.405, 4.74… ## $ PrcntTake &lt;dbl&gt; 0, 85, 0, 85, 0, 85, 0, 85, 0, 85, 0, 85, 0, 85, 0, 85, 0, 8… ## $ SATT &lt;dbl&gt; 1035.6200, 789.1200, 1039.7405, 793.2405, 1043.8610, 797.361… Now we have our updated grid object, we’re ready to plot the data in our version of Figure 18.3. p1 &lt;- my_data %&gt;% ggplot(aes(x = Spend, y = SATT)) + geom_path(data = grid, aes(group = index), color = &quot;grey85&quot;) + geom_segment(aes(xend = Spend, yend = 993.8 + -2.9 * PrcntTake + 12.3 * Spend), size = 1/4, linetype = 3) + geom_point(shape = 21, stroke = 1/10, color = &quot;white&quot;, fill = &quot;steelblue4&quot;) + scale_x_continuous(limits = c(3.4, 10.1), expand = c(0, 0), breaks = 2:5 * 2) + scale_y_continuous(limits = c(785, 1120)) p2 &lt;- my_data %&gt;% ggplot(aes(x = PrcntTake, y = SATT)) + geom_path(data = grid, aes(group = index), color = &quot;grey85&quot;) + geom_segment(aes(xend = PrcntTake, yend = 993.8 + -2.9 * PrcntTake + 12.3 * Spend), size = 1/4, linetype = 3) + geom_point(shape = 21, stroke = 1/10, color = &quot;white&quot;, fill = &quot;steelblue4&quot;) + scale_x_continuous(&quot;% Take&quot;, limits = c(0, 85), expand = c(0, 0)) + scale_y_continuous(limits = c(785, 1120)) p3 &lt;- my_data %&gt;% ggplot(aes(x = PrcntTake, y = Spend)) + geom_path(data = grid, aes(group = index), color = &quot;grey85&quot;) + geom_point(shape = 21, stroke = 1/10, color = &quot;white&quot;, fill = &quot;steelblue4&quot;) + scale_x_continuous(&quot;% Take&quot;, limits = c(0, 85), expand = c(0, 0)) + scale_y_continuous(limits = c(3.4, 10.1), expand = c(0, 0)) # bind them together and add a title wrap_elements(grid::textGrob(&#39;No 3D wireframe plots for us&#39;)) + p1 + p2 + p3 + plot_annotation(title = &quot;SATT ~ N(m,sd=31.5), m = 993.8 + −2.9 %Take + 12.3 Spend&quot;) You can learn more about how we added that title to our plot ensemble from Pedersen’s (2020) vignette, Adding annotation and style, and more about how we added that text in place of a wireframe plot from another of his (2020) vignettes, Plot assembly. The separate influences of the two predictors could be assessed in this example because the predictors had only mild correlation with each other. There was enough independent variation of the two predictors that their distinct relationships to the outcome variable could be detected. In some situations, however, the predictors are so tightly correlated that their distinct effects are difficult to tease apart. Correlation of predictors causes the estimates of their regression coefficients to trade-off, as we will see when we examine the posterior distribution. (p. 514) 18.1.2 The model and implementation. Let’s make our version of the model diagram in Figure 18.4 to get a sense of where we’re going. If you look back to Section 17.2, you’ll see this is just a minor reworking of the code from Figure 17.2. # normal density p1 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = &quot;steelblue4&quot;, color = &quot;steelblue4&quot;, alpha = .6) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. # a second normal density p2 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = &quot;steelblue4&quot;, color = &quot;steelblue4&quot;, alpha = .6) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)[italic(j)]&quot;, &quot;italic(S)[italic(j)]&quot;), size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) ## two annotated arrows # save our custom arrow settings my_arrow &lt;- arrow(angle = 20, length = unit(0.35, &quot;cm&quot;), type = &quot;closed&quot;) p3 &lt;- tibble(x = c(.33, 1.67), y = c(1, 1), xend = c(.67, 1.2), yend = c(0, 0)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow) + annotate(geom = &quot;text&quot;, x = c(.35, 1.3), y = .5, label = &quot;&#39;~&#39;&quot;, size = 10, family = &quot;Times&quot;, parse = T) + xlim(0, 2) + theme_void() # exponential density p4 &lt;- tibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) + geom_area(fill = &quot;steelblue4&quot;, color = &quot;steelblue4&quot;, alpha = .6) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;exp&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(K)&quot;, size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # likelihood formula p5 &lt;- tibble(x = .5, y = .25, label = &quot;beta[0]+sum()[italic(j)]*beta[italic(j)]*italic(x)[italic(ji)]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # half-normal density p6 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = &quot;steelblue4&quot;, color = &quot;steelblue4&quot;, alpha = .6) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma]&quot;, size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # four annotated arrows p7 &lt;- tibble(x = c(.43, .43, 1.5, 2.5), y = c(1, .55, 1, 1), xend = c(.43, 1.225, 1.5, 1.75), yend = c(.8, .15, .2, .2)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow) + annotate(geom = &quot;text&quot;, x = c(.3, .7, 1.38, 2), y = c(.92, .22, .65, .6), label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;=&#39;&quot;, &quot;&#39;=&#39;&quot;, &quot;&#39;~&#39;&quot;), size = 10, family = &quot;Times&quot;, parse = T) + annotate(geom = &quot;text&quot;, x = .43, y = .7, label = &quot;nu*minute+1&quot;, size = 7, family = &quot;Times&quot;, parse = T) + xlim(0, 3) + theme_void() # student-t density p8 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) + geom_area(fill = &quot;steelblue4&quot;, color = &quot;steelblue4&quot;, alpha = .6) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;student t&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = 0, y = .6, label = &quot;nu~~~mu[italic(i)]~~~sigma&quot;, size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # the final annotated arrow p9 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p10 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 3, r = 5), area(t = 1, b = 2, l = 7, r = 9), area(t = 4, b = 5, l = 1, r = 3), area(t = 4, b = 5, l = 5, r = 7), area(t = 4, b = 5, l = 9, r = 11), area(t = 3, b = 4, l = 3, r = 9), area(t = 7, b = 8, l = 5, r = 7), area(t = 6, b = 7, l = 1, r = 11), area(t = 9, b = 9, l = 5, r = 7), area(t = 10, b = 10, l = 5, r = 7) ) # combine and plot! (p1 + p2 + p4 + p5 + p6 + p3 + p8 + p7 + p9 + p10) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) “As with the model for simple linear regression, the Markov Chain Monte Carlo (MCMC) sampling can be more efficient if the data are mean-centered or standardized” (p. 515). We’ll make a custom function to standardize the criterion and predictor values. standardize &lt;- function(x) { (x - mean(x)) / sd(x) } my_data &lt;- my_data %&gt;% mutate(prcnt_take_z = standardize(PrcntTake), spend_z = standardize(Spend), satt_z = standardize(SATT)) Let’s open brms. library(brms) Now we’re ready to fit the model. As Kruschke pointed out, the priors on the standardized predictors are set with an arbitrary standard deviation of \\(2.0\\). This value was chosen because standardized regression coefficients are algebraically constrained to fall between \\(−1\\) and \\(+1\\) in least-squares regression6, and therefore, the regression coefficients will not exceed those limits by much. A normal distribution with standard deviation of \\(2.0\\) is reasonably flat over the range from \\(−1\\) to \\(+1\\). (p. 516) With data like this, even a prior(normal(0, 1), class = b) would be only mildly regularizing. This is a good place to emphasize how priors in brms are given classes. If you’d like all parameters within a given class to have the prior, you can just specify one prior argument within that class. For our fit8.1, both parameters of class = b have a normal(0, 2) prior. So we can just include one statement to handle both. Had we wanted different priors for the coefficients for spend_z and prcnt_take_z, we’d need to include two prior() arguments with at least one including a coef argument. fit18.1 &lt;- brm(data = my_data, family = student, satt_z ~ 1 + spend_z + prcnt_take_z, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 18, file = &quot;fits/fit18.01&quot;) Check the model summary. print(fit18.1) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: satt_z ~ 1 + spend_z + prcnt_take_z ## Data: my_data (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.06 -0.13 0.12 1.00 3960 2676 ## spend_z 0.24 0.08 0.09 0.40 1.00 3135 3103 ## prcnt_take_z -1.03 0.08 -1.19 -0.88 1.00 3231 3156 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.42 0.05 0.32 0.53 1.00 3243 2544 ## nu 32.66 28.46 4.25 108.83 1.00 3133 2672 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). So when we use a multivariable model, increases in spending now appear associated with increases in SAT scores. 18.1.3 The posterior distribution. Based on Equation 18.1, we can convert the standardized coefficients from our multivariable model back to their original metric as follows: \\[\\begin{align*} \\beta_0 &amp; = \\operatorname{SD}_y \\zeta_0 + M_y - \\operatorname{SD}_y \\sum_j \\frac{\\zeta_j M_{x_j}}{\\operatorname{SD}_{x_j}} \\;\\;\\; \\text{and} \\\\ \\beta_j &amp; = \\frac{\\operatorname{SD}_y \\zeta_j}{\\operatorname{SD}_{x_j}}. \\end{align*}\\] To use them, we’ll first extract the posterior draws draws &lt;- as_draws_df(fit18.1) head(draws) ## # A draws_df: 6 iterations, 1 chains, and 7 variables ## b_Intercept b_spend_z b_prcnt_take_z sigma nu lprior lp__ ## 1 -0.0027 0.265 -1.05 0.41 15.1 -9.1 -35 ## 2 0.0420 0.118 -0.95 0.45 98.4 -12.0 -37 ## 3 0.1076 0.250 -1.01 0.44 16.1 -9.2 -37 ## 4 0.0997 0.168 -0.96 0.41 10.3 -9.0 -37 ## 5 -0.0577 0.099 -0.93 0.44 120.0 -12.7 -38 ## 6 0.1263 0.209 -0.99 0.41 6.9 -8.8 -38 ## # ... hidden reserved variables {&#39;.chain&#39;, &#39;.iteration&#39;, &#39;.draw&#39;} Like we did in Chapter 17, let’s wrap the consequences of Equation 18.1 into two functions. make_beta_0 &lt;- function(zeta_0, zeta_1, zeta_2, sd_x_1, sd_x_2, sd_y, m_x_1, m_x_2, m_y) { sd_y * zeta_0 + m_y - sd_y * ((zeta_1 * m_x_1 / sd_x_1) + (zeta_2 * m_x_2 / sd_x_2)) } make_beta_j &lt;- function(zeta_j, sd_j, sd_y) { sd_y * zeta_j / sd_j } After saving a few values, we’re ready to use our custom functions. sd_x_1 &lt;- sd(my_data$Spend) sd_x_2 &lt;- sd(my_data$PrcntTake) sd_y &lt;- sd(my_data$SATT) m_x_1 &lt;- mean(my_data$Spend) m_x_2 &lt;- mean(my_data$PrcntTake) m_y &lt;- mean(my_data$SATT) draws &lt;- draws %&gt;% mutate(b_0 = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_y = m_y), b_1 = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), b_2 = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) glimpse(draws) ## Rows: 4,000 ## Columns: 13 ## $ b_Intercept &lt;dbl&gt; -0.002697924, 0.042041763, 0.107571901, 0.099739103, -0… ## $ b_spend_z &lt;dbl&gt; 0.26475590, 0.11795547, 0.24994698, 0.16762745, 0.09880… ## $ b_prcnt_take_z &lt;dbl&gt; -1.0502024, -0.9532420, -1.0056834, -0.9577257, -0.9265… ## $ sigma &lt;dbl&gt; 0.4095773, 0.4518504, 0.4420941, 0.4108245, 0.4441622, … ## $ nu &lt;dbl&gt; 15.105647, 98.430670, 16.097243, 10.343269, 119.990594,… ## $ lprior &lt;dbl&gt; -9.146251, -12.006651, -9.183343, -8.955325, -12.740062… ## $ lp__ &lt;dbl&gt; -35.12580, -37.26377, -36.94507, -37.32173, -38.33067, … ## $ .chain &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, … ## $ .draw &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, … ## $ b_0 &lt;dbl&gt; 983.3494, 1024.7382, 992.0149, 1013.3928, 1020.8516, 10… ## $ b_1 &lt;dbl&gt; 14.535580, 6.475970, 13.722544, 9.203052, 5.424804, 11.… ## $ b_2 &lt;dbl&gt; -2.936085, -2.665010, -2.811622, -2.677545, -2.590445, … Before we make the figure, we’ll update our overall plot theme to cowplot::theme_minimal_grid(). Our overall color scheme and plot aesthetic will be based on some of the plots in Chapter 16, Visualizing uncertainty, of Wilke (2019). As we’ll be making a lot of customized density plots in this chapter, we may as well save those settings, here. We’ll call the function with those settings stat_wilke(). library(tidybayes) library(ggdist) library(cowplot) # update the default theme setting theme_set(theme_minimal_grid()) # define the function stat_wilke &lt;- function(height = 1.25, point_size = 5, ...) { list( # for the graded fill stat_slab(aes(fill_ramp = stat( cut_cdf_qi(cdf, .width = c(.8, .95, .99), labels = scales::percent_format(accuracy = 1)))), height = height, slab_alpha = .75, fill = &quot;steelblue4&quot;, ...), # for the top outline and the mode dot stat_halfeye(.width = 0, point_interval = mode_qi, height = height, size = point_size, slab_size = 1/3, slab_color = &quot;steelblue4&quot;, fill = NA, color = &quot;chocolate3&quot;, ...), # fill settings scale_fill_ramp_discrete(range = c(1, .4), na.translate = F), # adjust the guide_legend() settings guides(fill_ramp = guide_legend( direction = &quot;horizontal&quot;, keywidth = unit(0.925, &quot;cm&quot;), label.hjust = 0.5, label.position = &quot;bottom&quot;, title = &quot;posterior prob.&quot;, title.hjust = 0.5, title.position = &quot;top&quot;)), # ensure we&#39;re using `cowplot::theme_minimal_hgrid()` as a base theme theme_minimal_hgrid(), # adjust the legend settings theme(legend.background = element_rect(fill = &quot;white&quot;), legend.text = element_text(margin = margin(-0.2, 0, -0.2, 0, &quot;cm&quot;)), legend.title = element_text(margin = margin(-0.2, 0, -0.2, 0, &quot;cm&quot;))) ) } Here’s the top panel of Figure 18.5. # here are the primary data draws %&gt;% transmute(Intercept = b_0, Spend = b_1, `Percent Take` = b_2, Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% pivot_longer(everything()) %&gt;% # the plot ggplot(aes(x = value)) + stat_wilke(normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(ylim = c(-0.01, NA)) + panel_border() + theme(legend.position = c(.72, .2)) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) The slope on spending has a mode of about \\(13\\), which suggests that SAT scores rise by about \\(13\\) points for every extra \\(\\$1000\\) spent per pupil. The slope on percentage taking the exam (PrcntTake) is also credibly non-zero, with a mode around \\(−2.8\\), which suggests that SAT scores fall by about \\(2.8\\) points for every additional \\(1\\%\\) of students who take the test. (p. 517) If you want those exact modes and, say, 50% intervals around them, you can just use tidybayes::mode_hdi(). draws %&gt;% transmute(Spend = b_1, `Percent Take` = b_2) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mode_hdi(value, .width = .5) ## # A tibble: 2 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Percent Take -2.88 -3.04 -2.74 0.5 mode hdi ## 2 Spend 12.6 9.81 15.6 0.5 mode hdi The brms::bayes_R2() function makes it easy to compute a Bayesian \\(R^2\\). Simply feed a brm() fit object into bayes_R2() and you’ll get back the posterior mean, \\(\\textit{SD}\\), and 95% intervals. bayes_R2(fit18.1) ## Estimate Est.Error Q2.5 Q97.5 ## R2 0.8138893 0.02282763 0.7575688 0.8431136 I’m not going to go into the technical details here, but you should be aware that the Bayeisan \\(R^2\\) returned from the bayes_R2() function is not calculated the same as it is with OLS. If you want to dive in, check out the paper by Gelman et al. (2019), R-squared for Bayesian regression models. Anyway, if you’d like to view the Bayesian \\(R^2\\) distribution rather than just get the summaries, specify summary = F, convert the output to a tibble, and plot as usual. bayes_R2(fit18.1, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = R2, y = 0)) + stat_wilke() + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(paste(&quot;Bayesian &quot;, italic(R)^2)), x = NULL) + coord_cartesian(xlim = c(0, 1), ylim = c(-0.01, NA)) + theme(legend.position = c(.01, .8)) Since the brms::bayes_R2() function is not identical with Kruschke’s method in the text, the results might differ a bit. We can get a sense of the scatter plots with bayesplot::mcmc_pairs(). library(bayesplot) color_scheme_set(c(&quot;steelblue4&quot;, &quot;steelblue4&quot;, &quot;steelblue4&quot;, &quot;steelblue4&quot;, &quot;steelblue4&quot;, &quot;steelblue4&quot;)) draws %&gt;% transmute(Intercept = b_0, Spend = b_1, `Percent Take` = b_2, Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% mcmc_pairs(diag_fun = &quot;dens&quot;, off_diag_args = list(size = 1/8, alpha = 1/8)) One way to get the Pearson’s correlation coefficients among the parameters is with psych::lowerCor(). draws %&gt;% transmute(Intercept = b_0, Spend = b_1, `Percent Take` = b_2, Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% psych::lowerCor(digits = 3) ## Intrc Spend PrcnT Scale Nrmlt ## Intercept 1.000 ## Spend -0.936 1.000 ## Percent Take 0.335 -0.597 1.000 ## Scale 0.036 -0.051 0.080 1.000 ## Normality 0.099 -0.125 0.137 0.384 1.000 If you like more control for customizing your pairs plots, you’ll find a friend in the ggpairs() function from the GGally package (Schloerke et al., 2021). We’re going to blow past the default settings and customize the format for the plots in the upper triangle, the diagonal, and the lower triangle. library(GGally) my_upper &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_point(size = 1/2, shape = 21, stroke = 1/10, color = &quot;white&quot;, fill = &quot;steelblue4&quot;) + panel_border() } my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + stat_wilke(point_size = 2) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(ylim = c(-0.01, NA)) + panel_border() } my_lower &lt;- function(data, mapping, ...) { # get the x and y data to use the other code x &lt;- eval_data_col(data, mapping$x) y &lt;- eval_data_col(data, mapping$y) # compute the correlations corr &lt;- cor(x, y, method = &quot;p&quot;, use = &quot;pairwise&quot;) # plot the cor value ggally_text( label = formatC(corr, digits = 2, format = &quot;f&quot;) %&gt;% str_replace(., &quot;0\\\\.&quot;, &quot;.&quot;), mapping = aes(), color = &quot;black&quot;, size = 4) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + panel_border() } Let’s see what we’ve done. draws %&gt;% transmute(`Intercept~(beta[0])` = b_0, `Spend~(beta[1])` = b_1, `Percent~Take~(beta[2])` = b_2, sigma = sigma * sd_y, `log10(nu)` = nu %&gt;% log10()) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = label_parsed) + theme(strip.text = element_text(size = 8)) For more ideas on customizing a ggpairs() plot, go here or here or here. Kruschke finished the subsection with the observation: “Sometimes we are interested in using the linear model to predict \\(y\\) values for \\(x\\) values of interest. It is straight forward to generate a large sample of credible \\(y\\) values for specified \\(x\\) values” (p. 519). Like we practiced with in the last chapter, the simplest way to do so in brms is with the fitted() function. For a quick example, say we wanted to know what the model would predict if we were to have a standard-score increase in spending and a simultaneous standard-score decrease in the percent taking the exam. We’d just specify those values in a tibble and feed that tibble into fitted() along with the model. nd &lt;- tibble(prcnt_take_z = -1, spend_z = 1) fitted(fit18.1, newdata = nd) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 1.266984 0.1543101 0.9738981 1.564561 18.1.4 Redundant predictors. As a simplified example of correlated predictors, think of just two data points: Suppose \\(y = 1\\) for \\(\\langle x_1, x_2 \\rangle = \\langle 1, 1 \\rangle\\) and \\(y = 2\\) for \\(\\langle x_1, x_2 \\rangle = \\langle 2, 2 \\rangle\\). The linear model, \\(y = \\beta_1 x_1 + \\beta_2 x_2\\) is supposed to satisfy both data points, and in this case both are satisfied by \\(1 = \\beta_1 + \\beta_2\\). Therefore, many different combinations of \\(\\beta_1\\) and \\(\\beta_2\\) satisfy the data. For example, it could be that \\(\\beta_1 = 2\\) and \\(\\beta_2 = -1\\), or \\(\\beta_1 = 0.5\\) and \\(\\beta_2 = 0.5\\), or \\(\\beta_1 = 0\\) and \\(\\beta_2 = 1\\). In other words, the credible values of \\(\\beta_1\\) and \\(\\beta_2\\) are anticorrelated and trade-off to fit the data. (p. 519) Here are what those data look like. You would not want to fit a regression model with these data. tibble(x_1 = 1:2, x_2 = 1:2, y = 1:2) ## # A tibble: 2 × 3 ## x_1 x_2 y ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 ## 2 2 2 2 We can take percentages and turn them into their inverse re-expressed as a proportion. percent_take &lt;- 37 (100 - percent_take) / 100 ## [1] 0.63 Let’s make a redundant predictor and then standardize() it. my_data &lt;- my_data %&gt;% mutate(prop_not_take = (100 - PrcntTake) / 100) %&gt;% mutate(prop_not_take_z = standardize(prop_not_take)) glimpse(my_data) ## Rows: 50 ## Columns: 13 ## $ State &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Californi… ## $ Spend &lt;dbl&gt; 4.405, 8.963, 4.778, 4.459, 4.992, 5.443, 8.817, 7.030… ## $ StuTeaRat &lt;dbl&gt; 17.2, 17.6, 19.3, 17.1, 24.0, 18.4, 14.4, 16.6, 19.1, … ## $ Salary &lt;dbl&gt; 31.144, 47.951, 32.175, 28.934, 41.078, 34.571, 50.045… ## $ PrcntTake &lt;dbl&gt; 8, 47, 27, 6, 45, 29, 81, 68, 48, 65, 57, 15, 13, 58, … ## $ SATV &lt;dbl&gt; 491, 445, 448, 482, 417, 462, 431, 429, 420, 406, 407,… ## $ SATM &lt;dbl&gt; 538, 489, 496, 523, 485, 518, 477, 468, 469, 448, 482,… ## $ SATT &lt;dbl&gt; 1029, 934, 944, 1005, 902, 980, 908, 897, 889, 854, 88… ## $ prcnt_take_z &lt;dbl&gt; -1.0178453, 0.4394222, -0.3078945, -1.0925770, 0.36469… ## $ spend_z &lt;dbl&gt; -1.10086058, 2.24370805, -0.82716069, -1.06123647, -0.… ## $ satt_z &lt;dbl&gt; 0.8430838, -0.4266207, -0.2929676, 0.5223163, -0.85431… ## $ prop_not_take &lt;dbl&gt; 0.92, 0.53, 0.73, 0.94, 0.55, 0.71, 0.19, 0.32, 0.52, … ## $ prop_not_take_z &lt;dbl&gt; 1.0178453, -0.4394222, 0.3078945, 1.0925770, -0.364690… Here’s the correlation matrix for Spend, PrcntTake and prop_not_take, as seen on page 520. my_data %&gt;% select(Spend, PrcntTake, prop_not_take) %&gt;% cor() ## Spend PrcntTake prop_not_take ## Spend 1.0000000 0.5926274 -0.5926274 ## PrcntTake 0.5926274 1.0000000 -1.0000000 ## prop_not_take -0.5926274 -1.0000000 1.0000000 We’re ready to fit the redundant-predictor model. fit18.2 &lt;- brm(data = my_data, family = student, satt_z ~ 0 + Intercept + spend_z + prcnt_take_z + prop_not_take_z, prior = c(prior(normal(0, 2), class = b, coef = &quot;Intercept&quot;), prior(normal(0, 2), class = b, coef = &quot;spend_z&quot;), prior(normal(0, 2), class = b, coef = &quot;prcnt_take_z&quot;), prior(normal(0, 2), class = b, coef = &quot;prop_not_take_z&quot;), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 18, # this will let us use `prior_samples()` later on sample_prior = &quot;yes&quot;, file = &quot;fits/fit18.02&quot;) You might notice a few things about the brm() code. First, we have used the ~ 0 + Intercept + ... syntax instead of the default syntax for intercepts. In normal situations, we would have been in good shape using the typical ~ 1 + ... syntax for the intercept, especially given our use of standardized data. However, since brms version 2.5.0, using the sample_prior argument to draw samples from the prior distribution will no longer allow us to return samples from the typical brms intercept. Bürkner addressed the issue on the Stan forums. As he pointed out, if you want to get prior samples from an intercept, you’ll have to use the alternative syntax. The other thing to point out is that even though we used the same prior on all the predictors, including the intercept, we still explicitly spelled each out with the coef argument. If we hadn’t been explicit like this, we would only get a single b vector from the prior_samples() function. But since we want separate vectors for each of our predictors, we used the verbose code. If you’re having a difficult time understanding these two points, experiment. Fit the model in a few different ways with either the typical or the alternative intercept syntax and with either the verbose prior code or the simplified prior(normal(0, 2), class = b) code. And after each, execute prior_samples(fit18.2). You’ll see. Let’s move on. Kruschke mentioned high autocorrelations in the prose. Here are the autocorrelation plots for our \\(\\beta\\)’s. color_scheme_set(c(&quot;steelblue4&quot;, &quot;steelblue4&quot;, &quot;chocolate3&quot;, &quot;steelblue4&quot;, &quot;steelblue4&quot;, &quot;steelblue4&quot;)) draws &lt;- as_draws_df(fit18.2) draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_acf(pars = vars(b_Intercept:b_prop_not_take_z), lags = 10) Looks like HMC made a big difference. The \\(N_{eff}/N\\) ratios weren’t terrible, either. color_scheme_set(c(&quot;steelblue4&quot;, &quot;steelblue4&quot;, &quot;chocolate3&quot;, &quot;steelblue4&quot;, &quot;chocolate3&quot;, &quot;chocolate3&quot;)) neff_ratio(fit18.2)[1:6] %&gt;% mcmc_neff() + yaxis_text(hjust = 0) Earlier we computed the correlations among the correlation matrix for the predictors, as Kruschke displayed on page 520. Here we’ll compute the correlations among their coefficients in the model. The brms::vcov() function returns a variance/covariance matrix–or a correlation matrix when you set correlation = T–of the population-level parameters (i.e., the fixed effects). It returns the values to a decadent level of precision, so we’ll simplify the output with round(). vcov(fit18.2, correlation = T) %&gt;% round(digits = 3) ## Intercept spend_z prcnt_take_z prop_not_take_z ## Intercept 1.000 -0.033 -0.002 -0.002 ## spend_z -0.033 1.000 -0.013 0.022 ## prcnt_take_z -0.002 -0.013 1.000 0.998 ## prop_not_take_z -0.002 0.022 0.998 1.000 The correlations among the redundant predictors were still very high. If any of the nondiagonal correlations are high (i.e., close to \\(+1\\) or close to \\(−1\\)), be careful when interpreting the posterior distribution. Here, we can see that the correlation of PrcntTake and PropNotTake is \\(−1.0\\), which is an immediate sign of redundant predictors. (p. 520) You can really get a sense of the silliness of the parameters if you plot them. We’ll use stat_wilke() to get a sense of densities and summaries of the \\(\\beta\\)’s. draws %&gt;% pivot_longer(b_Intercept:b_prop_not_take_z) %&gt;% # this line isn&#39;t necessary, but it does allow us to arrange the parameters on the y-axis mutate(name = factor(name, levels = c(&quot;b_prop_not_take_z&quot;, &quot;b_prcnt_take_z&quot;, &quot;b_spend_z&quot;, &quot;b_Intercept&quot;))) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + stat_wilke(normalize = &quot;xy&quot;, point_size = 3) + labs(x = NULL, y = NULL) + coord_cartesian(xlim = c(-5, 5), ylim = c(1.4, NA)) + theme(axis.text.y = element_text(hjust = 0), legend.position = c(.76, .8)) Yeah, on the standardized scale those are some ridiculous estimates. Let’s update our make_beta_0() function. make_beta_0 &lt;- function(zeta_0, zeta_1, zeta_2, zeta_3, sd_x_1, sd_x_2, sd_x_3, sd_y, m_x_1, m_x_2, m_x_3, m_y) { sd_y * zeta_0 + m_y - sd_y * ((zeta_1 * m_x_1 / sd_x_1) + (zeta_2 * m_x_2 / sd_x_2) + (zeta_3 * m_x_3 / sd_x_3)) } sd_x_1 &lt;- sd(my_data$Spend) sd_x_2 &lt;- sd(my_data$PrcntTake) sd_x_3 &lt;- sd(my_data$prop_not_take) sd_y &lt;- sd(my_data$SATT) m_x_1 &lt;- mean(my_data$Spend) m_x_2 &lt;- mean(my_data$PrcntTake) m_x_3 &lt;- mean(my_data$prop_not_take) m_y &lt;- mean(my_data$SATT) draws &lt;- draws %&gt;% transmute(Intercept = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, zeta_3 = b_prop_not_take_z, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_x_3 = sd_x_3, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_x_3 = m_x_3, m_y = m_y), Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), `Proportion not Take` = make_beta_j(zeta_j = b_prop_not_take_z, sd_j = sd_x_3, sd_y = sd_y), Scale = sigma * sd_y, Normality = nu %&gt;% log10()) glimpse(draws) ## Rows: 4,000 ## Columns: 6 ## $ Intercept &lt;dbl&gt; 1286.0814, 676.4512, 1027.5406, 1050.0106, 870.6… ## $ Spend &lt;dbl&gt; 13.245560, 15.358896, 15.489605, 6.154676, 8.823… ## $ `Percent Take` &lt;dbl&gt; -6.01820783, 0.08713037, -3.65264566, -3.1232467… ## $ `Proportion not Take` &lt;dbl&gt; -298.694047, 313.684035, -47.605008, -9.001090, … ## $ Scale &lt;dbl&gt; 28.00306, 27.52761, 35.59712, 30.65278, 32.54666… ## $ Normality &lt;dbl&gt; 0.9288994, 1.1999304, 1.6110071, 1.4414806, 1.05… Now we’ve done the conversions, here are the histograms of Figure 18.6. draws %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_wilke(normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(ylim = c(-0.01, NA)) + panel_border() + theme(axis.text.x = element_text(size = 8), legend.position = &quot;none&quot;) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Their scatter plots are as follows. draws %&gt;% set_names(&quot;Intercept~(beta[0])&quot;, &quot;Spend~(beta[1])&quot;, &quot;Percent~Take~(beta[2])&quot;, &quot;Percent~Not~Take~(beta[3])&quot;, &quot;sigma&quot;, &quot;log10(nu)&quot;) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = label_parsed) + theme(strip.text = element_text(size = 7)) Figure 18.7 is all about the prior predictive distribution. Here we’ll extract the priors with prior_samples() and wrangle all in one step. prior_draws &lt;- prior_draws(fit18.2) %&gt;% transmute(Intercept = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, zeta_3 = b_prop_not_take_z, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_x_3 = sd_x_3, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_x_3 = m_x_3, m_y = m_y), Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), `Proportion not Take` = make_beta_j(zeta_j = b_prop_not_take_z, sd_j = sd_x_3, sd_y = sd_y), Scale = sigma * sd_y, Normality = nu %&gt;% log10()) glimpse(prior_draws) ## Rows: 4,000 ## Columns: 6 ## $ Intercept &lt;dbl&gt; 1357.56702, 2162.89345, 2399.94017, -180.89588, … ## $ Spend &lt;dbl&gt; -76.87285, -198.75804, -132.41235, 163.27256, -9… ## $ `Percent Take` &lt;dbl&gt; -6.0704337, 1.2110818, -2.9404931, 4.3357744, -8… ## $ `Proportion not Take` &lt;dbl&gt; 601.4546878, 77.7307853, -788.2675110, -189.2042… ## $ Scale &lt;dbl&gt; 27.211411, 2.214812, 196.404525, 71.740578, 66.8… ## $ Normality &lt;dbl&gt; 1.7340393, 1.5508948, 1.4500064, 1.7267028, 0.76… Now we’ve wrangled the priors, we’re ready to make the histograms at the top of Figure 18.7. prior_draws %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_wilke(normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(ylim = c(-0.01, NA)) + panel_border() + theme(axis.text.x = element_text(size = 8), legend.position = &quot;none&quot;) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Since we used the half-Gaussian prior for our \\(\\sigma\\), our Scale histogram looks different from Kruschke’s. Otherwise, everything’s on the up and up. Here are the pairs plots at the bottom of Figure 18.7. prior_draws %&gt;% set_names(&quot;Intercept~(beta[0])&quot;, &quot;Spend~(beta[1])&quot;, &quot;Percent~Take~(beta[2])&quot;, &quot;Percent~Not~Take~(beta[3])&quot;, &quot;sigma&quot;, &quot;log10(nu)&quot;) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = label_parsed) + theme(strip.text = element_text(size = 7)) At the top of page 523, Kruschke asked us to “notice that the posterior distribution in Figure 18.6 has ranges for the redundant parameters that are only a little smaller than their priors.” With a little wrangling, we can compare the prior/posterior distributions for our redundant parameters more directly. draws %&gt;% pivot_longer(everything(), names_to = &quot;parameter&quot;, values_to = &quot;posterior&quot;) %&gt;% bind_cols( prior_draws %&gt;% pivot_longer(everything()) %&gt;% transmute(prior = value) ) %&gt;% pivot_longer(-parameter) %&gt;% filter(parameter %in% c(&quot;Percent Take&quot;, &quot;Proportion not Take&quot;)) %&gt;% ggplot(aes(x = value, y = 0)) + stat_wilke(normalize = &quot;panels&quot;) + scale_fill_viridis_d(option = &quot;D&quot;, begin = .35, end = .65) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(ylim = c(-0.01, NA)) + panel_border() + theme(legend.position = &quot;none&quot;) + facet_grid(name ~ parameter, scales = &quot;free&quot;) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Kruschke was right. The posterior distributions are only slightly narrower than the priors for those two. With our combination of data and model, we learned virtually nothing beyond the knowledge we encoded in those priors. Kruschke mentioned SEM as a possible solution to multicollinearity. brms isn’t fully capable of SEM, at the moment (see issue #304), but its multivariate syntax (Bürkner, 2022b) does allow for path analysis and IRT models. However, you can currently fit a variety of Bayesian SEMs with the blavaan package (Merkle et al., 2021; Merkle &amp; Rosseel, 2018). I’m not aware of any textbooks highlighting blavaan. If you know of any, please share. 18.1.5 Informative priors, sparse data, and correlated predictors. It’s worth reproducing some of Kruschke’s prose from this subsection. The examples in this book tend to use mildly informed priors (e.g., using information about the rough magnitude and range of the data). But a benefit of Bayesian analysis is the potential for cumulative scientific progress by using priors that have been informed from previous research. Informed priors can be especially useful when the amount of data is small compared to the parameter space. A strongly informed prior essentially reduces the scope of the credible parameter space, so that a small amount of new data implies a narrow zone of credible parameter values. (p. 523) 18.2 Multiplicative interaction of metric predictors From page 526: Formally, interactions can have many different specific functional forms. We will consider multiplicative interaction. This means that the nonadditive interaction is expressed by multiplying the predictors. The predicted value is a weighted combination of the individual predictors and, additionally, the multiplicative product of the predictors. For two metric predictors, regression with multiplicative interaction has these algebraically equivalent expressions: \\[\\begin{align*} \\mu &amp; = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{1 \\times 2} x_1 x_2 \\\\ &amp; = \\beta_0 + \\underbrace{(\\beta_1 + \\beta_{1 \\times 2} x_2)}_{\\text{slope of } x_1} x_1 + \\beta_2 x_2 \\\\ &amp; = \\beta_0 + \\beta_1 x_1 + \\underbrace{(\\beta_2 + \\beta_{1 \\times 2} x_1)}_{\\text{slope of } x_2} x_2. \\end{align*}\\] We can’t quite reproduce Figure 18.8 with our ggplot2 repertoire. But we can capture some of Figure 18.8 with the geom_raster() approach we used in Chapter 15. d &lt;- crossing(x1 = seq(from = 0, to = 10, by = 0.5), x2 = seq(from = 0, to = 10, by = 0.5)) %&gt;% mutate(y = 10 + -1 * x1 + 2 * x2 + (0.2) * x1 * x2) p1 &lt;- d %&gt;% ggplot(aes(x = x1, y = x2)) + geom_raster(aes(fill = y)) + # these are all variants of steelblue4 scale_fill_gradient2(low = &quot;#0a141b&quot;, mid = &quot;#36648B&quot;, high = &quot;#d6e0e7&quot;, midpoint = 20, limits = c(0, 40)) + scale_x_continuous(expression(x[1]), expand = c(0, 0), breaks = 0:5 * 2) + scale_y_continuous(expression(x[2]), expand = c(0, 0), breaks = 0:5 * 2, position = &quot;right&quot;) + labs(subtitle = expression(y==10+-1*x[1]+2*x[2]+(0.2)*x[1]*x[2])) + coord_equal() + theme(legend.position = &quot;left&quot;, plot.subtitle = element_text(size = 8)) p2 &lt;- d %&gt;% ggplot(aes(x = x1, y = x2)) + geom_raster(aes(fill = y)) + geom_hline(yintercept = c(0:5 * 2), size = 1, color = &quot;chocolate3&quot;) + # these are all variants of steelblue4 scale_fill_gradient2(low = &quot;#0a141b&quot;, mid = &quot;#36648B&quot;, high = &quot;#d6e0e7&quot;, midpoint = 20, limits = c(0, 40)) + scale_x_continuous(expression(x[1]), expand = c(0, 0), breaks = 0:5 * 2) + scale_y_continuous(expression(x[2]), expand = c(0, 0), breaks = 0:5 * 2, position = &quot;right&quot;) + labs(subtitle = expression(y==10+(-1+0.2*x[2])*x[1]+2*x[2])) + coord_equal() + theme(legend.position = &quot;none&quot;, plot.subtitle = element_text(size = 8)) p3 &lt;- d %&gt;% ggplot(aes(x = x1, y = x2)) + geom_raster(aes(fill = y)) + geom_vline(xintercept = c(0:5 * 2), size = 1, color = &quot;chocolate3&quot;) + # these are all variants of steelblue4 scale_fill_gradient2(low = &quot;#0a141b&quot;, mid = &quot;#36648B&quot;, high = &quot;#d6e0e7&quot;, midpoint = 20, limits = c(0, 40)) + scale_x_continuous(expression(x[1]), expand = c(0, 0), breaks = 0:5 * 2) + scale_y_continuous(expression(x[2]), expand = c(0, 0), breaks = 0:5 * 2, position = &quot;right&quot;) + labs(subtitle = expression(y==10+-1*x[1]+(2+0.2*x[1])*x[2])) + coord_equal() + theme(legend.position = &quot;none&quot;, plot.subtitle = element_text(size = 8)) p1 + p2 + p3 As Kruschke wrote: “Great care must be taken when interpreting the coefficients of a model that includes interaction terms (Braumoeller, 2004). In particular, low-order terms are especially difficult to interpret when higher-order interactions are present” (p. 526). When in doubt, plot. 18.2.1 An example. Presuming we’re still just modeling \\(\\mu\\) with two predictors, we can express the formula with the interaction term as \\[ \\mu = \\beta_0 + \\beta_1+ \\beta_2 x_2 + \\underbrace{\\beta_{1 \\times 2}}_{\\beta_3} \\underbrace{x_2 x_1 }_{x_3}. \\] With brms, you can specify an interaction with either the x_i*x_j syntax or the x_i:x_j syntax. I typically use x_i:x_j. It’s often the case that you can just make the interaction term right in the model formula. But since we’re fitting the model with standardized predictors and then using Kruschke’s equations to convert the parameters back to the unstandardized metric, it seems easier to make the interaction term in the data, first. my_data &lt;- my_data %&gt;% # make x_3 mutate(interaction = Spend * PrcntTake) %&gt;% mutate(interaction_z = standardize(interaction)) Now we’ll fit the model. fit18.3 &lt;- brm(data = my_data, family = student, satt_z ~ 1 + spend_z + prcnt_take_z + interaction_z, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 18, file = &quot;fits/fit18.03&quot;) Note that even though an interaction term might seem different kind from other regression terms, it’s just another coefficient of class = b as far as the prior() statements are concerned. Anyway, let’s inspect the summary(). summary(fit18.3) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: satt_z ~ 1 + spend_z + prcnt_take_z + interaction_z ## Data: my_data (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.00 0.06 -0.11 0.12 1.00 2834 2360 ## spend_z 0.04 0.14 -0.23 0.33 1.00 1928 2436 ## prcnt_take_z -1.46 0.29 -2.01 -0.88 1.00 1719 2147 ## interaction_z 0.57 0.37 -0.16 1.28 1.00 1625 1955 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.41 0.05 0.32 0.52 1.00 2509 2045 ## nu 35.69 30.43 4.63 115.70 1.00 2371 2168 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Like Kruschke reported on page 528, here’s the correlation matrix for the (unstandardized) predictors. my_data %&gt;% select(Spend, PrcntTake, interaction) %&gt;% cor() ## Spend PrcntTake interaction ## Spend 1.0000000 0.5926274 0.7750251 ## PrcntTake 0.5926274 1.0000000 0.9511463 ## interaction 0.7750251 0.9511463 1.0000000 The correlations among the \\(\\zeta\\) coefficients are about as severe. vcov(fit18.3, correlation = T) %&gt;% round(digits = 3) ## Intercept spend_z prcnt_take_z interaction_z ## Intercept 1.000 0.058 0.041 -0.052 ## spend_z 0.058 1.000 0.706 -0.829 ## prcnt_take_z 0.041 0.706 1.000 -0.963 ## interaction_z -0.052 -0.829 -0.963 1.000 We can see that the interaction variable is strongly correlated with both predictors. Therefore, we know that there will be strong trade-offs among the regression coefficients, and the marginal distributions of single regression coefficients might be much wider than when there was no interaction included. (p. 528) Let’s convert the posterior draws to the unstandardized metric. sd_x_3 &lt;- sd(my_data$interaction) m_x_3 &lt;- mean(my_data$interaction) draws &lt;- as_draws_df(fit18.3) %&gt;% transmute(Intercept = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, zeta_3 = b_interaction_z, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_x_3 = sd_x_3, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_x_3 = m_x_3, m_y = m_y), Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), `Spend : Percent Take` = make_beta_j(zeta_j = b_interaction_z, sd_j = sd_x_3, sd_y = sd_y), Scale = sigma * sd_y, Normality = nu %&gt;% log10()) glimpse(draws) ## Rows: 4,000 ## Columns: 6 ## $ Intercept &lt;dbl&gt; 1052.8407, 1089.7440, 1014.5478, 1065.8655, 108… ## $ Spend &lt;dbl&gt; -0.2380493, -4.4463594, 8.0726366, -0.7320065, … ## $ `Percent Take` &lt;dbl&gt; -4.235056, -4.348027, -4.017688, -3.967700, -3.… ## $ `Spend : Percent Take` &lt;dbl&gt; 0.271935765, 0.245176318, 0.160051920, 0.193318… ## $ Scale &lt;dbl&gt; 32.21732, 30.62781, 31.34580, 25.56230, 24.6994… ## $ Normality &lt;dbl&gt; 1.6576920, 1.2469875, 1.3987112, 1.7882973, 1.4… Now we’ve done the conversions, here are our versions of the histograms of Figure 18.9. draws %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_wilke(normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(ylim = c(-0.01, NA)) + panel_border() + theme(legend.position = &quot;none&quot;) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. “To properly understand the credible slopes on the two predictors, we must consider the credible slopes on each predictor as a function of the value of the other predictor” (p. 528). This is our motivation for the middle panel of Figure 18.9. To make it, we’ll need to expand() our post, wrangle a bit, and plot with geom_pointrange(). # this will come in handy in `expand()` bounds &lt;- range(my_data$PrcntTake) p1 &lt;- # wrangle draws %&gt;% expand(nesting(Spend, `Spend : Percent Take`), PrcntTake = seq(from = bounds[1], to = bounds[2], length.out = 20)) %&gt;% mutate(slope = Spend + `Spend : Percent Take` * PrcntTake) %&gt;% group_by(PrcntTake) %&gt;% median_hdi(slope) %&gt;% # plot ggplot(aes(x = PrcntTake, y = slope, ymin = .lower, ymax = .upper)) + geom_hline(yintercept = 0, color = &quot;grey25&quot;, linetype = 2) + geom_pointrange(shape = 21, stroke = 1/10, fatten = 6, color = &quot;steelblue4&quot;, fill = &quot;chocolate3&quot;) + labs(title = expression(&quot;Slope on spend is &quot;~beta[1]+beta[3]%.%prcnt_take), x = &quot;Value of prcnt_take&quot;, y = &quot;Slope on spend&quot;) We’ll follow the same basic order of operations for the final panel and then bind them together with patchwork. # this will come in handy in `expand()` bounds &lt;- range(my_data$Spend) p2 &lt;- # wrangle draws %&gt;% expand(nesting(`Percent Take`, `Spend : Percent Take`), Spend = seq(from = bounds[1], to = bounds[2], length.out = 20)) %&gt;% mutate(slope = `Percent Take` + `Spend : Percent Take` * Spend) %&gt;% group_by(Spend) %&gt;% median_hdi(slope) %&gt;% # plot ggplot(aes(x = Spend, y = slope, ymin = .lower, ymax = .upper)) + geom_pointrange(shape = 21, stroke = 1/10, fatten = 6, color = &quot;steelblue4&quot;, fill = &quot;chocolate3&quot;) + labs(title = expression(&quot;Slope on prcnt_take is &quot;~beta[2]+beta[3]%.%spend), x = &quot;Value of spend&quot;, y = &quot;Slope on prcnt_take&quot;) p1 / p2 Kruschke outlined all this in the opening paragraphs of page 530. His parting words of this subsection warrant repeating: “if you include an interaction term, you cannot ignore it even if its marginal posterior distribution includes zero” (p. 530). 18.3 Shrinkage of regression coefficients In some research, there are many candidate predictors which we suspect could possibly be informative about the predicted variable. For example, when predicting college GPA, we might include high-school GPA, high-school SAT score, income of student, income of parents, years of education of the parents, spending per pupil at the student’s high school, student IQ, student height, weight, shoe size, hours of sleep per night, distance from home to school, amount of caffeine consumed, hours spent studying, hours spent earning a wage, blood pressure, etc. We can include all the candidate predictors in the model, with a regression coefficient for every predictor. And this is not even considering interactions, which we will ignore for now. With so many candidate predictors of noisy data, there may be some regression coefficients that are spuriously estimated to be non-zero. We would like some protection against accidentally nonzero regression coefficients. (p. 530) That’s what this section is all about. Figure 18.10 will give us a sense of what a model like this might look like. # brackets p1 &lt;- tibble(x = .5, y = .5, label = &quot;{_} {_}&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # two annotated arrows p2 &lt;- tibble(x = c(.15, .85), y = c(1, 1), xend = c(.25, .75), yend = c(.2, .2)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow) + xlim(0, 1) + theme_void() # normal density p3 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = &quot;steelblue4&quot;, color = &quot;steelblue4&quot;, alpha = .6) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # a student-t density p4 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) + geom_area(fill = &quot;steelblue4&quot;, color = &quot;steelblue4&quot;, alpha = .6) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;student t&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = 0, y = .6, label = &quot;nu[beta]~~~0~~~sigma[beta]&quot;, size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # two more annotated arrows p5 &lt;- tibble(x = c(.33, 1.67), y = c(1, 1), xend = c(.63, 1.2), yend = c(0, 0)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow) + annotate(geom = &quot;text&quot;, x = c(.35, 1.35, 1.54), y = .5, label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;), size = c(10, 10, 7), family = &quot;Times&quot;, parse = T) + xlim(0, 2) + theme_void() # exponential density p6 &lt;- tibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) + geom_area(fill = &quot;steelblue4&quot;, color = &quot;steelblue4&quot;, alpha = .6) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;exp&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(K)&quot;, size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # likelihood formula p7 &lt;- tibble(x = .5, y = .25, label = &quot;beta[0]+sum()[italic(j)]*beta[italic(j)]*italic(x)[italic(ji)]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # half-normal density p8 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = &quot;steelblue4&quot;, color = &quot;steelblue4&quot;, alpha = .6) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma]&quot;, size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # four annotated arrows p9 &lt;- tibble(x = c(.43, .43, 1.5, 2.5), y = c(1, .55, 1, 1), xend = c(.43, 1.225, 1.5, 1.75), yend = c(.8, .15, .2, .2)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow) + annotate(geom = &quot;text&quot;, x = c(.3, .7, 1.38, 2), y = c(.92, .22, .65, .6), label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;=&#39;&quot;, &quot;&#39;=&#39;&quot;, &quot;&#39;~&#39;&quot;), size = 10, family = &quot;Times&quot;, parse = T) + annotate(geom = &quot;text&quot;, x = .43, y = .7, label = &quot;nu*minute+1&quot;, size = 7, family = &quot;Times&quot;, parse = T) + xlim(0, 3) + theme_void() # a second student-t density p10 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) + geom_area(fill = &quot;steelblue4&quot;, color = &quot;steelblue4&quot;, alpha = .6) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;student t&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = 0, y = .6, label = &quot;nu~~~mu[italic(i)]~~~sigma&quot;, size = 7, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5)) # the final annotated arrow p11 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p12 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 1, l = 7, r = 9), area(t = 3, b = 4, l = 3, r = 5), area(t = 3, b = 4, l = 7, r = 9), area(t = 2, b = 3, l = 7, r = 9), area(t = 6, b = 7, l = 1, r = 3), area(t = 6, b = 7, l = 5, r = 7), area(t = 6, b = 7, l = 9, r = 11), area(t = 5, b = 6, l = 3, r = 9), area(t = 9, b = 10, l = 5, r = 7), area(t = 8, b = 9, l = 1, r = 11), area(t = 11, b = 11, l = 5, r = 7), area(t = 12, b = 12, l = 5, r = 7) ) # combine and plot! (p1 + p3 + p4 + p2 + p6 + p7 + p8 + p5 + p10 + p9 + p11 + p12) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Make our random noise predictors with rnorm(). set.seed(18) my_data &lt;- my_data %&gt;% mutate(x_rand_1 = rnorm(n = n(), mean = 0, sd = 1), x_rand_2 = rnorm(n = n(), mean = 0, sd = 1), x_rand_3 = rnorm(n = n(), mean = 0, sd = 1), x_rand_4 = rnorm(n = n(), mean = 0, sd = 1), x_rand_5 = rnorm(n = n(), mean = 0, sd = 1), x_rand_6 = rnorm(n = n(), mean = 0, sd = 1), x_rand_7 = rnorm(n = n(), mean = 0, sd = 1), x_rand_8 = rnorm(n = n(), mean = 0, sd = 1), x_rand_9 = rnorm(n = n(), mean = 0, sd = 1), x_rand_10 = rnorm(n = n(), mean = 0, sd = 1), x_rand_11 = rnorm(n = n(), mean = 0, sd = 1), x_rand_12 = rnorm(n = n(), mean = 0, sd = 1)) glimpse(my_data) ## Rows: 50 ## Columns: 27 ## $ State &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Californi… ## $ Spend &lt;dbl&gt; 4.405, 8.963, 4.778, 4.459, 4.992, 5.443, 8.817, 7.030… ## $ StuTeaRat &lt;dbl&gt; 17.2, 17.6, 19.3, 17.1, 24.0, 18.4, 14.4, 16.6, 19.1, … ## $ Salary &lt;dbl&gt; 31.144, 47.951, 32.175, 28.934, 41.078, 34.571, 50.045… ## $ PrcntTake &lt;dbl&gt; 8, 47, 27, 6, 45, 29, 81, 68, 48, 65, 57, 15, 13, 58, … ## $ SATV &lt;dbl&gt; 491, 445, 448, 482, 417, 462, 431, 429, 420, 406, 407,… ## $ SATM &lt;dbl&gt; 538, 489, 496, 523, 485, 518, 477, 468, 469, 448, 482,… ## $ SATT &lt;dbl&gt; 1029, 934, 944, 1005, 902, 980, 908, 897, 889, 854, 88… ## $ prcnt_take_z &lt;dbl&gt; -1.0178453, 0.4394222, -0.3078945, -1.0925770, 0.36469… ## $ spend_z &lt;dbl&gt; -1.10086058, 2.24370805, -0.82716069, -1.06123647, -0.… ## $ satt_z &lt;dbl&gt; 0.8430838, -0.4266207, -0.2929676, 0.5223163, -0.85431… ## $ prop_not_take &lt;dbl&gt; 0.92, 0.53, 0.73, 0.94, 0.55, 0.71, 0.19, 0.32, 0.52, … ## $ prop_not_take_z &lt;dbl&gt; 1.0178453, -0.4394222, 0.3078945, 1.0925770, -0.364690… ## $ interaction &lt;dbl&gt; 35.240, 421.261, 129.006, 26.754, 224.640, 157.847, 71… ## $ interaction_z &lt;dbl&gt; -0.94113720, 0.93111798, -0.48635915, -0.98229547, -0.… ## $ x_rand_1 &lt;dbl&gt; 0.92645924, 1.82282117, -1.61056690, -0.28510975, -0.3… ## $ x_rand_2 &lt;dbl&gt; -0.90258025, -1.13163679, 0.49708131, -0.54771876, -0.… ## $ x_rand_3 &lt;dbl&gt; 0.51576102, 0.30710965, 0.66199996, 2.21990655, -2.041… ## $ x_rand_4 &lt;dbl&gt; 1.08730491, -1.23909473, 0.43161390, 1.06733141, -0.78… ## $ x_rand_5 &lt;dbl&gt; -0.23846777, 0.15702031, -1.02132795, 0.75395217, -2.3… ## $ x_rand_6 &lt;dbl&gt; 0.06014956, 1.00555800, 1.47981871, -0.82827890, -0.58… ## $ x_rand_7 &lt;dbl&gt; 1.46961709, 0.51790320, -2.33110353, 0.11339996, 1.726… ## $ x_rand_8 &lt;dbl&gt; 0.03463437, -1.48737599, -0.01528284, 0.48480309, 0.20… ## $ x_rand_9 &lt;dbl&gt; -0.4556078, -0.7035475, -0.5001913, -0.6526022, 0.7742… ## $ x_rand_10 &lt;dbl&gt; 1.2858586, -0.7474640, -0.3107255, -1.1037468, 0.33136… ## $ x_rand_11 &lt;dbl&gt; 0.17236599, -0.37956084, 0.31982301, 0.29678108, 1.220… ## $ x_rand_12 &lt;dbl&gt; -0.53048519, 0.92465424, 0.66876661, 0.30935146, 1.474… Here’s how to fit the naïve model, which does not impose shrinkage on the coefficients. fit18.4 &lt;- update(fit18.1, newdata = my_data, formula = satt_z ~ 1 + prcnt_take_z + spend_z + x_rand_1 + x_rand_2 + x_rand_3 + x_rand_4 + x_rand_5 + x_rand_6 + x_rand_7 + x_rand_8 + x_rand_9 + x_rand_10 + x_rand_11 + x_rand_12, seed = 18, file = &quot;fits/fit18.04&quot;) ## The desired updates require recompiling the model Examine the posterior with posterior_summary(). posterior_summary(fit18.4)[1:17, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -0.03 0.07 -0.16 0.11 ## b_prcnt_take_z -1.12 0.09 -1.28 -0.94 ## b_spend_z 0.32 0.09 0.16 0.50 ## b_x_rand_1 0.03 0.06 -0.08 0.15 ## b_x_rand_2 0.02 0.09 -0.16 0.20 ## b_x_rand_3 0.09 0.07 -0.05 0.23 ## b_x_rand_4 -0.09 0.07 -0.23 0.05 ## b_x_rand_5 0.00 0.06 -0.12 0.13 ## b_x_rand_6 -0.02 0.08 -0.17 0.13 ## b_x_rand_7 -0.03 0.07 -0.17 0.12 ## b_x_rand_8 -0.18 0.07 -0.32 -0.04 ## b_x_rand_9 0.13 0.06 0.02 0.25 ## b_x_rand_10 0.00 0.05 -0.10 0.10 ## b_x_rand_11 0.05 0.09 -0.14 0.22 ## b_x_rand_12 -0.08 0.06 -0.20 0.04 ## sigma 0.38 0.07 0.23 0.52 ## nu 25.02 26.94 2.08 99.25 Before we can make Figure 18.11, we’ll need to update our make_beta_0() function to accommodate this model. make_beta_0 &lt;- function(zeta_0, zeta_1, zeta_2, zeta_3, zeta_4, zeta_5, zeta_6, zeta_7, zeta_8, zeta_9, zeta_10, zeta_11, zeta_12, zeta_13, zeta_14, sd_x_1, sd_x_2, sd_x_3, sd_x_4, sd_x_5, sd_x_6, sd_x_7, sd_x_8, sd_x_9, sd_x_10, sd_x_11, sd_x_12, sd_x_13, sd_x_14, sd_y, m_x_1, m_x_2, m_x_3, m_x_4, m_x_5, m_x_6, m_x_7, m_x_8, m_x_9, m_x_10, m_x_11, m_x_12, m_x_13, m_x_14, m_y) { sd_y * zeta_0 + m_y - sd_y * ((zeta_1 * m_x_1 / sd_x_1) + (zeta_2 * m_x_2 / sd_x_2) + (zeta_3 * m_x_3 / sd_x_3) + (zeta_4 * m_x_4 / sd_x_4) + (zeta_5 * m_x_5 / sd_x_5) + (zeta_6 * m_x_6 / sd_x_6) + (zeta_7 * m_x_7 / sd_x_7) + (zeta_8 * m_x_8 / sd_x_8) + (zeta_9 * m_x_9 / sd_x_9) + (zeta_10 * m_x_10 / sd_x_10) + (zeta_11 * m_x_11 / sd_x_11) + (zeta_12 * m_x_12 / sd_x_12) + (zeta_13 * m_x_13 / sd_x_13) + (zeta_14 * m_x_14 / sd_x_14)) } Sigh, our poor make_beta_0() and make_beta_1() code is getting obscene. I don’t have the energy to think of how to wrap this into a simpler function. Someone probably should. If that ends up as you, do share your code. sd_x_1 &lt;- sd(my_data$Spend) sd_x_2 &lt;- sd(my_data$PrcntTake) sd_x_3 &lt;- sd(my_data$x_rand_1) sd_x_4 &lt;- sd(my_data$x_rand_2) sd_x_5 &lt;- sd(my_data$x_rand_3) sd_x_6 &lt;- sd(my_data$x_rand_4) sd_x_7 &lt;- sd(my_data$x_rand_5) sd_x_8 &lt;- sd(my_data$x_rand_6) sd_x_9 &lt;- sd(my_data$x_rand_7) sd_x_10 &lt;- sd(my_data$x_rand_8) sd_x_11 &lt;- sd(my_data$x_rand_9) sd_x_12 &lt;- sd(my_data$x_rand_10) sd_x_13 &lt;- sd(my_data$x_rand_11) sd_x_14 &lt;- sd(my_data$x_rand_12) sd_y &lt;- sd(my_data$SATT) m_x_1 &lt;- mean(my_data$Spend) m_x_2 &lt;- mean(my_data$PrcntTake) m_x_3 &lt;- mean(my_data$x_rand_1) m_x_4 &lt;- mean(my_data$x_rand_2) m_x_5 &lt;- mean(my_data$x_rand_3) m_x_6 &lt;- mean(my_data$x_rand_4) m_x_7 &lt;- mean(my_data$x_rand_5) m_x_8 &lt;- mean(my_data$x_rand_6) m_x_9 &lt;- mean(my_data$x_rand_7) m_x_10 &lt;- mean(my_data$x_rand_8) m_x_11 &lt;- mean(my_data$x_rand_9) m_x_12 &lt;- mean(my_data$x_rand_10) m_x_13 &lt;- mean(my_data$x_rand_11) m_x_14 &lt;- mean(my_data$x_rand_12) m_y &lt;- mean(my_data$SATT) draws &lt;- as_draws_df(fit18.4) %&gt;% transmute(Intercept = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, zeta_3 = b_x_rand_1, zeta_4 = b_x_rand_2, zeta_5 = b_x_rand_3, zeta_6 = b_x_rand_4, zeta_7 = b_x_rand_5, zeta_8 = b_x_rand_6, zeta_9 = b_x_rand_7, zeta_10 = b_x_rand_8, zeta_11 = b_x_rand_9, zeta_12 = b_x_rand_10, zeta_13 = b_x_rand_11, zeta_14 = b_x_rand_12, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_x_3 = sd_x_3, sd_x_4 = sd_x_4, sd_x_5 = sd_x_5, sd_x_6 = sd_x_6, sd_x_7 = sd_x_7, sd_x_8 = sd_x_8, sd_x_9 = sd_x_9, sd_x_10 = sd_x_10, sd_x_11 = sd_x_11, sd_x_12 = sd_x_12, sd_x_13 = sd_x_13, sd_x_14 = sd_x_14, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_x_3 = m_x_3, m_x_4 = m_x_4, m_x_5 = m_x_5, m_x_6 = m_x_6, m_x_7 = m_x_7, m_x_8 = m_x_8, m_x_9 = m_x_9, m_x_10 = m_x_10, m_x_11 = m_x_11, m_x_12 = m_x_12, m_x_13 = m_x_13, m_x_14 = m_x_14, m_y = m_y), Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), x_rand_1 = make_beta_j(zeta_j = b_x_rand_1, sd_j = sd_x_3, sd_y = sd_y), x_rand_2 = make_beta_j(zeta_j = b_x_rand_2, sd_j = sd_x_4, sd_y = sd_y), x_rand_3 = make_beta_j(zeta_j = b_x_rand_3, sd_j = sd_x_5, sd_y = sd_y), x_rand_4 = make_beta_j(zeta_j = b_x_rand_4, sd_j = sd_x_6, sd_y = sd_y), x_rand_5 = make_beta_j(zeta_j = b_x_rand_5, sd_j = sd_x_7, sd_y = sd_y), x_rand_6 = make_beta_j(zeta_j = b_x_rand_6, sd_j = sd_x_8, sd_y = sd_y), x_rand_7 = make_beta_j(zeta_j = b_x_rand_7, sd_j = sd_x_9, sd_y = sd_y), x_rand_8 = make_beta_j(zeta_j = b_x_rand_8, sd_j = sd_x_10, sd_y = sd_y), x_rand_9 = make_beta_j(zeta_j = b_x_rand_9, sd_j = sd_x_11, sd_y = sd_y), x_rand_10 = make_beta_j(zeta_j = b_x_rand_10, sd_j = sd_x_12, sd_y = sd_y), x_rand_11 = make_beta_j(zeta_j = b_x_rand_11, sd_j = sd_x_13, sd_y = sd_y), x_rand_12 = make_beta_j(zeta_j = b_x_rand_12, sd_j = sd_x_14, sd_y = sd_y), Scale = sigma * sd_y, Normality = nu %&gt;% log10()) glimpse(draws) ## Rows: 4,000 ## Columns: 17 ## $ Intercept &lt;dbl&gt; 950.7077, 969.4177, 955.6650, 957.3323, 979.0691, 904.1… ## $ Spend &lt;dbl&gt; 21.337416, 19.647029, 19.676473, 20.433890, 14.925248, … ## $ `Percent Take` &lt;dbl&gt; -3.286047, -3.473662, -3.293917, -3.168405, -3.199506, … ## $ x_rand_1 &lt;dbl&gt; -0.7750959, 1.2345310, 2.2533082, 7.9075852, -6.3989120… ## $ x_rand_2 &lt;dbl&gt; 4.3716598, 5.3214159, 0.2139152, -2.3934613, 0.9786895,… ## $ x_rand_3 &lt;dbl&gt; 13.9225315, 8.7596922, 12.3683049, 1.9136425, 1.9405810… ## $ x_rand_4 &lt;dbl&gt; -8.833918, -4.325065, -5.970432, -1.095080, -8.476883, … ## $ x_rand_5 &lt;dbl&gt; 0.2862657, 0.0327725, -1.8087232, 7.4595729, -2.4899133… ## $ x_rand_6 &lt;dbl&gt; -7.58014923, -10.39285061, -10.38175497, -1.85296607, -… ## $ x_rand_7 &lt;dbl&gt; -8.1876266, 6.9171116, 3.1819146, -6.7840152, -1.746752… ## $ x_rand_8 &lt;dbl&gt; -10.748734, -20.558489, -24.303320, -9.397539, -13.0822… ## $ x_rand_9 &lt;dbl&gt; 10.897684, 10.371134, 7.273580, 9.937170, 2.848720, 5.1… ## $ x_rand_10 &lt;dbl&gt; 4.36641286, 1.51521683, -1.71910237, 2.30166117, -0.417… ## $ x_rand_11 &lt;dbl&gt; -2.1425651, -6.5159347, 0.9490695, -2.5980596, 9.091885… ## $ x_rand_12 &lt;dbl&gt; -6.0867734, -3.1446508, -8.0939326, -2.2343898, 0.70466… ## $ Scale &lt;dbl&gt; 22.81950, 26.20729, 29.09542, 24.65083, 20.92236, 27.80… ## $ Normality &lt;dbl&gt; 0.6783691, 0.6442554, 1.4320776, 0.8167508, 0.5223094, … Okay, here are our versions of the histograms of Figure 18.11. draws %&gt;% pivot_longer(cols = c(Intercept:x_rand_3, x_rand_10:Normality)) %&gt;% mutate(name = factor(name, levels = c(&quot;Intercept&quot;, &quot;Spend&quot;, &quot;Percent Take&quot;, &quot;x_rand_1&quot;, &quot;x_rand_2&quot;, &quot;x_rand_3&quot;, &quot;x_rand_10&quot;, &quot;x_rand_11&quot;, &quot;x_rand_12&quot;, &quot;Scale&quot;, &quot;Normality&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_wilke(normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(ylim = c(-0.01, NA)) + panel_border() + theme(axis.text.x = element_text(size = 9), legend.position = c(.74, .09)) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. And here’s the final density plot depicting the Bayesian \\(R^2\\). bayes_R2(fit18.4, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = R2, y = 0)) + stat_wilke() + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(paste(&quot;Bayesian &quot;, italic(R)^2)), x = NULL) + coord_cartesian(xlim = c(0, 1), ylim = c(-0.01, NA)) + theme(legend.position = c(.01, .8)) Note that unlike the one Kruschke displayed in the text, our brms::bayes_R2()-based \\(R^2\\) distribution did not exceed the logical right bound of 1. Sometimes when you have this many parameters you’d like to compare, it’s better to display their summaries with an ordered coefficient plot. draws %&gt;% pivot_longer(Spend:x_rand_12) %&gt;% mutate(name = fct_reorder(name, value)) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = 0, color = &quot;grey25&quot;, linetype = 2) + stat_pointinterval(point_interval = mode_hdi, .width = .95, point_size = 4, color = &quot;steelblue4&quot;, point_color = &quot;chocolate3&quot;) + labs(x = NULL, y = NULL) + theme(axis.text.y = element_text(hjust = 0)) Now we can see that by chance alone, the coefficients for x_rand_8 and x_rand_9 are clearly distinct from zero. Our stat_wilke() function can be informative, too. draws %&gt;% pivot_longer(Spend:x_rand_12) %&gt;% ggplot(aes(x = value, y = reorder(name, value), group = reorder(name, value))) + geom_vline(xintercept = 0, color = &quot;grey25&quot;, linetype = 2) + stat_wilke(height = 10, point_size = 3) + labs(x = NULL, y = NULL) + coord_cartesian(ylim = c(1.25, 14.5)) + theme(axis.text.y = element_text(hjust = 0), legend.position = c(.785, .13)) With brms, we can fit something like the model Kruschke displayed in Figure 18.12 with the horseshoe() prior. From the horseshoe section of the brms reference manual: The horseshoe prior is a special shrinkage prior initially proposed by (Carvalho et al., 2009). It is symmetric around zero with fat tails and an infinitely large spike at zero. This makes it ideal for sparse models that have many regression coefficients, although only a minority of them is non-zero. The horseshoe prior can be applied on all population-level effects at once (excluding the intercept) by using set_prior(\"horseshoe(1)\"). The 1 implies that the student-t prior of the local shrinkage parameters has 1 degrees of freedom (Bürkner, 2022d, p. 105) Based on the quote, here’s how to fit our horseshoe-prior model with brm(). fit18.5 &lt;- update(fit18.4, newdata = my_data, formula = satt_z ~ 1 + prcnt_take_z + spend_z + x_rand_1 + x_rand_2 + x_rand_3 + x_rand_4 + x_rand_5 + x_rand_6 + x_rand_7 + x_rand_8 + x_rand_9 + x_rand_10 + x_rand_11 + x_rand_12, prior = c(prior(normal(0, 2), class = Intercept), prior(horseshoe(1), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), seed = 18, control = list(adapt_delta = .9), file = &quot;fits/fit18.05&quot;) Check the parameter summary. posterior_summary(fit18.5)[1:17, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -0.02 0.06 -0.14 0.11 ## b_prcnt_take_z -1.02 0.10 -1.20 -0.82 ## b_spend_z 0.20 0.11 0.00 0.39 ## b_x_rand_1 0.01 0.04 -0.05 0.10 ## b_x_rand_2 0.01 0.04 -0.09 0.11 ## b_x_rand_3 0.02 0.04 -0.05 0.13 ## b_x_rand_4 -0.03 0.05 -0.15 0.04 ## b_x_rand_5 0.00 0.03 -0.07 0.08 ## b_x_rand_6 -0.02 0.04 -0.12 0.06 ## b_x_rand_7 -0.01 0.04 -0.10 0.07 ## b_x_rand_8 -0.09 0.07 -0.23 0.01 ## b_x_rand_9 0.05 0.06 -0.02 0.18 ## b_x_rand_10 0.00 0.03 -0.06 0.07 ## b_x_rand_11 0.01 0.05 -0.08 0.12 ## b_x_rand_12 -0.02 0.04 -0.12 0.05 ## sigma 0.40 0.06 0.25 0.52 ## nu 30.92 28.58 2.99 109.39 Our make_beta_0() and make_beta_1() code remains obscene. draws &lt;- as_draws_df(fit18.5) %&gt;% transmute(Intercept = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, zeta_3 = b_x_rand_1, zeta_4 = b_x_rand_2, zeta_5 = b_x_rand_3, zeta_6 = b_x_rand_4, zeta_7 = b_x_rand_5, zeta_8 = b_x_rand_6, zeta_9 = b_x_rand_7, zeta_10 = b_x_rand_8, zeta_11 = b_x_rand_9, zeta_12 = b_x_rand_10, zeta_13 = b_x_rand_11, zeta_14 = b_x_rand_12, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_x_3 = sd_x_3, sd_x_4 = sd_x_4, sd_x_5 = sd_x_5, sd_x_6 = sd_x_6, sd_x_7 = sd_x_7, sd_x_8 = sd_x_8, sd_x_9 = sd_x_9, sd_x_10 = sd_x_10, sd_x_11 = sd_x_11, sd_x_12 = sd_x_12, sd_x_13 = sd_x_13, sd_x_14 = sd_x_14, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_x_3 = m_x_3, m_x_4 = m_x_4, m_x_5 = m_x_5, m_x_6 = m_x_6, m_x_7 = m_x_7, m_x_8 = m_x_8, m_x_9 = m_x_9, m_x_10 = m_x_10, m_x_11 = m_x_11, m_x_12 = m_x_12, m_x_13 = m_x_13, m_x_14 = m_x_14, m_y = m_y), Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), x_rand_1 = make_beta_j(zeta_j = b_x_rand_1, sd_j = sd_x_3, sd_y = sd_y), x_rand_2 = make_beta_j(zeta_j = b_x_rand_2, sd_j = sd_x_4, sd_y = sd_y), x_rand_3 = make_beta_j(zeta_j = b_x_rand_3, sd_j = sd_x_5, sd_y = sd_y), x_rand_4 = make_beta_j(zeta_j = b_x_rand_4, sd_j = sd_x_6, sd_y = sd_y), x_rand_5 = make_beta_j(zeta_j = b_x_rand_5, sd_j = sd_x_7, sd_y = sd_y), x_rand_6 = make_beta_j(zeta_j = b_x_rand_6, sd_j = sd_x_8, sd_y = sd_y), x_rand_7 = make_beta_j(zeta_j = b_x_rand_7, sd_j = sd_x_9, sd_y = sd_y), x_rand_8 = make_beta_j(zeta_j = b_x_rand_8, sd_j = sd_x_10, sd_y = sd_y), x_rand_9 = make_beta_j(zeta_j = b_x_rand_9, sd_j = sd_x_11, sd_y = sd_y), x_rand_10 = make_beta_j(zeta_j = b_x_rand_10, sd_j = sd_x_12, sd_y = sd_y), x_rand_11 = make_beta_j(zeta_j = b_x_rand_11, sd_j = sd_x_13, sd_y = sd_y), x_rand_12 = make_beta_j(zeta_j = b_x_rand_12, sd_j = sd_x_14, sd_y = sd_y), Scale = sigma * sd_y, Normality = nu %&gt;% log10()) glimpse(draws) ## Rows: 4,000 ## Columns: 17 ## $ Intercept &lt;dbl&gt; 1043.4842, 955.1105, 997.7826, 960.3109, 1009.5597, 990… ## $ Spend &lt;dbl&gt; 3.466299, 18.465108, 9.246108, 16.303882, 11.150742, 14… ## $ `Percent Take` &lt;dbl&gt; -2.637569, -2.876726, -2.489912, -2.703482, -3.139547, … ## $ x_rand_1 &lt;dbl&gt; 2.72503654, -0.48380911, -1.21854153, 0.26639377, 3.568… ## $ x_rand_2 &lt;dbl&gt; -2.138543765, 5.538560141, 0.332987840, 0.219574019, 0.… ## $ x_rand_3 &lt;dbl&gt; 1.75638920, 1.60000395, 3.24765170, -1.43170057, 1.0854… ## $ x_rand_4 &lt;dbl&gt; -0.222482772, 0.200751027, -0.116239310, 1.779126024, -… ## $ x_rand_5 &lt;dbl&gt; -0.36486960, 1.13923859, -1.08890670, -2.20912245, 0.02… ## $ x_rand_6 &lt;dbl&gt; -0.09521246, -2.04746521, -5.59629491, -0.08087280, -1.… ## $ x_rand_7 &lt;dbl&gt; -3.208171846, 2.454125289, 1.809477083, -0.171674689, -… ## $ x_rand_8 &lt;dbl&gt; 0.6524765, -11.3271844, -6.8373678, -14.3736269, -1.623… ## $ x_rand_9 &lt;dbl&gt; 3.75898992, 4.45866480, 2.13479111, 5.58475417, 2.51534… ## $ x_rand_10 &lt;dbl&gt; -0.38315913, -0.53013225, -2.26906586, 0.13986268, 0.79… ## $ x_rand_11 &lt;dbl&gt; 0.03762051, -1.99494054, 3.81382431, 0.39868220, -8.240… ## $ x_rand_12 &lt;dbl&gt; -1.45354785, -1.09171882, -2.94307027, 1.72021426, 0.04… ## $ Scale &lt;dbl&gt; 36.03939, 27.98800, 36.53762, 27.37404, 32.54841, 33.36… ## $ Normality &lt;dbl&gt; 1.3218879, 1.3955561, 1.6572943, 1.4741560, 1.0952938, … And here are our stat_wilke() plot versions of the majority of the histograms of Figure 18.12. draws %&gt;% pivot_longer(cols = c(Intercept:x_rand_3, x_rand_10:Normality)) %&gt;% mutate(name = factor(name, levels = c(&quot;Intercept&quot;, &quot;Spend&quot;, &quot;Percent Take&quot;, &quot;x_rand_1&quot;, &quot;x_rand_2&quot;, &quot;x_rand_3&quot;, &quot;x_rand_10&quot;, &quot;x_rand_11&quot;, &quot;x_rand_12&quot;, &quot;Scale&quot;, &quot;Normality&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_wilke(normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(ylim = c(-0.01, NA)) + panel_border() + theme(axis.text.x = element_text(size = 10), legend.position = c(.74, .09)) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Based on the distributions for the random predictors, it looks like our brms horseshoe prior regularized more aggressively than Kruschke’s hierarchical prior in the text. And interestingly, look how our marginal posterior for Spend is bimodal. For kicks and giggles, here’s the corresponding coefficient plot for \\(\\beta_1\\) through \\(\\beta_{14}\\). draws %&gt;% pivot_longer(Spend:x_rand_12) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_vline(xintercept = 0, color = &quot;grey25&quot;, linetype = 2) + stat_pointinterval(point_interval = mode_hdi, .width = .95, point_size = 4, color = &quot;steelblue4&quot;, point_color = &quot;chocolate3&quot;) + labs(x = NULL, y = NULL) + theme(axis.text.y = element_text(hjust = 0)) But anyways, here’s that final Bayesian \\(R^2\\) density for Figure 18.12. bayes_R2(fit18.5, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = R2, y = 0)) + stat_wilke() + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(paste(&quot;Bayesian &quot;, italic(R)^2)), x = NULL) + coord_cartesian(xlim = c(0, 1), ylim = c(-0.01, NA)) + theme(legend.position = c(.01, .8)) Just recall, though, that our fit18.5 was not exactly like Kruschke’s model. Whereas we hard coded the scale of our Student-\\(t\\) horseshoe prior to be 1, Kruschke estimated it with help from the gamma distribution. I’m not aware that’s currently possible in brms. If I’m at fault and you know how to do it, please share your code. 18.4 Variable selection We can rewrite the linear regression model to accommodate whether it includes a predictor as \\[\\mu_i = \\beta_0 + \\sum_j \\delta_j \\beta_j x_{j, i},\\] where \\(\\delta\\) is a dummy for which 0 = not included 1 = included. I’m not aware of a way to use \\(\\delta\\) as an inclusion indicator in brms the way Kruschke implemented it in JAGS. And in fact, it appears this might be unfeasible within the Stan framework. But if you know of a way, please share your code. However, this issue can lead to a similar approach: information criteria. To do so, let’s follow Kruschke’s basic flow and use the first model from way back in Section 18.1.1 as a starting point. The model formula was as follows. fit18.1$formula ## satt_z ~ 1 + spend_z + prcnt_take_z Taking interactions off the table for a moment, we can specify four model types with various combinations of the two predictors, prcnt_take_z and spend_z. fit18.1 was the first, which we might denote as \\(\\langle 1, 1 \\rangle\\). That leads to the remaining possibilities as \\(\\langle 1, 0 \\rangle:\\) satt_z ~ 1 + spend_z \\(\\langle 0, 1 \\rangle:\\) satt_z ~ 1 + prcnt_take_z \\(\\langle 0, 0 \\rangle:\\) satt_z ~ 1 Let’s fit those models. fit18.6 &lt;- update(fit18.1, formula = satt_z ~ 1 + spend_z, seed = 18, file = &quot;fits/fit18.06&quot;) fit18.7 &lt;- update(fit18.1, formula = satt_z ~ 1 + prcnt_take_z, seed = 18, file = &quot;fits/fit18.07&quot;) fit18.8 &lt;- brm(data = my_data, family = student, satt_z ~ 1, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 18, file = &quot;fits/fit18.08&quot;) We’ll compare our models with the LOO information criterion. Like other information criteria, the LOO values aren’t of interest in and of themselves. However, the values of one model’s LOO relative to that of another is of great interest. We generally prefer models with lower estimates. fit18.1 &lt;- add_criterion(fit18.1, &quot;loo&quot;) fit18.6 &lt;- add_criterion(fit18.6, &quot;loo&quot;) fit18.7 &lt;- add_criterion(fit18.7, &quot;loo&quot;) fit18.8 &lt;- add_criterion(fit18.8, &quot;loo&quot;) loo_compare(fit18.1, fit18.6, fit18.7, fit18.8) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit18.1 0.0 0.0 -31.9 5.4 4.0 0.7 63.8 10.7 ## fit18.7 -3.2 2.9 -35.1 4.4 2.7 0.5 70.3 8.8 ## fit18.6 -37.7 6.3 -69.6 4.0 2.1 0.3 139.2 8.1 ## fit18.8 -40.8 5.7 -72.7 3.1 1.3 0.2 145.4 6.1 In this case, fit18.1 and fit18.7 clearly have the lowest estimates, but the standard error of their difference score is about the same size as their difference. So the LOO difference score puts them on similar footing. Recall that you can do a similar analysis with the waic() function. Let’s compare that with the insights from the model_weights() function. (mw &lt;- model_weights(fit18.1, fit18.6, fit18.7, fit18.8)) ## fit18.1 fit18.6 fit18.7 fit18.8 ## 8.858096e-01 1.308897e-08 1.141904e-01 4.281579e-08 If you don’t like scientific notation, you can always wrangle and plot. mw %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% set_names(&quot;fit&quot;, &quot;estimate&quot;) %&gt;% ggplot(aes(x = estimate, y = reorder(fit, estimate))) + geom_text(aes(label = estimate %&gt;% round(3) %&gt;% as.character())) + scale_x_continuous(&quot;stacking weight&quot;, limits = c(0, 1)) + ylab(NULL) Based on this weighting scheme, almost all the weight went to the full model, fit18.1. But note, in the intro of their vignette on the topic, Vehtari &amp; Gabry (2022b) opined: Ideally, we would avoid the Bayesian model combination problem by extending the model to include the separate models as special cases, and preferably as a continuous expansion of the model space. For example, instead of model averaging over different covariate combinations, all potentially relevant covariates should be included in a predictive model (for causal analysis more care is needed) and a prior assumption that only some of the covariates are relevant can be presented with regularized horseshoe prior (Piironen &amp; Vehtari, 2017). For variable selection we recommend projective predictive variable selection (Piironen and Vehtari, 2017; projpred package). Perhaps unsurprisingly, their thoughts on the topic are similar with the Gelman et al quotation Kruschke provided on page 536: Some prominent authors eschew the variable-selection approach for typical applications in their fields. For example, (Gelman et al., 2013, p. 369) said, “For the regressions we typically see, we do not believe any coefficients to be truly zero and we do not generally consider it a conceptual (as opposed to computational) advantage to get point estimates of zero—but regularized estimates such as obtained by lasso can be much better than those resulting from simple least squares and flat prior distributions …we are not comfortable with an underlying model in which the coefficients can be exactly zero.” For more on some of these methods, check out Vehtari’s GitHub repository, Tutorial on model assessment, model selection and inference after model selection. But anyways, our model weighting methods cohered with Kruschke’s \\(\\delta\\)-inclusion-indicator method in that both suggested the full model, fit1, and the model with prcnt_take as the sole predictor, fit7, were given the greatest weight. I’m not aware that our information criteria weighting/model stacking methods provide probability distributions of the type Kruschke displayed in the left portions of Figure 18.13. But we can at least recreate the plots in the other panels. # first we&#39;ll get the posterior draws from `fit18.1` and wrangle them as_draws_df(fit18.1) %&gt;% transmute(Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% pivot_longer(everything()) %&gt;% # within `bind_rows()`, we extract and wrangle the posterior draws from `fit18.7` and them insert them below those from `fit18.1` bind_rows( as_draws_df(fit18.7) %&gt;% mutate(value = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), name = &quot;PrcntTake&quot;) %&gt;% select(name, value) ) %&gt;% # now we just need a little indexing and factor ordering mutate(model = rep(c(&quot;fit18.1&quot;, &quot;fit18.7&quot;), times = c(8000, 4000)), name = factor(name, levels = c(&quot;Spend&quot;, &quot;PrcntTake&quot;))) %&gt;% # we finally plot! ggplot(aes(x = value, y = 0)) + stat_wilke(normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(ylim = c(-0.01, NA)) + panel_border() + theme(legend.position = c(.13, .23)) + facet_grid(model ~ name, scales = &quot;free&quot;) 18.4.1 Inclusion probability is strongly affected by vagueness of prior. To follow along, let’s fit the models with the updated \\(\\textit{SD} = 1\\) on the \\(\\beta_{1+}\\) priors code. fit18.9 &lt;- update(fit18.1, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 18, file = &quot;fits/fit18.09&quot;) fit18.10 &lt;- update(fit18.9, formula = satt_z ~ 1 + spend_z, seed = 18, file = &quot;fits/fit18.10&quot;) fit18.11 &lt;- update(fit18.9, formula = satt_z ~ 1 + prcnt_take_z, seed = 18, file = &quot;fits/fit18.11&quot;) fit18.12 &lt;- update(fit18.8, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), seed = 18, file = &quot;fits/fit18.12&quot;) And now we’ll fit the models with the updated \\(\\textit{SD} = 10\\). fit18.13 &lt;- update(fit18.9, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 10), class = sigma), prior(exponential(one_over_twentynine), class = nu)), seed = 18, file = &quot;fits/fit18.13&quot;) fit18.14 &lt;- update(fit18.13, formula = satt_z ~ 1 + spend_z, seed = 18, file = &quot;fits/fit18.14&quot;) fit18.15 &lt;- update(fit18.13, formula = satt_z ~ 1 + prcnt_take_z, seed = 18, file = &quot;fits/fit18.15&quot;) fit18.16 &lt;- update(fit18.12, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = sigma), prior(exponential(one_over_twentynine), class = nu)), seed = 18, file = &quot;fits/fit18.16&quot;) Now we’ve fit the models, we’re ready to examine how altering the \\(\\textit{SD}\\)’s on the \\(\\beta_j\\) priors influenced the model comparisons via model_weights(). Here we’ll use the default stacking method. mw %&gt;% rbind(model_weights(fit18.9, fit18.10, fit18.11, fit18.12), model_weights(fit18.13, fit18.14, fit18.15, fit18.16)) %&gt;% as_tibble() %&gt;% set_names(&quot;1, 1&quot;, &quot;1, 0&quot;, &quot;0, 1&quot;, &quot;0, 0&quot;) %&gt;% pivot_longer(everything()) %&gt;% mutate(prior = rep(str_c(&quot;SD = &quot;, c(10, 2, 1)), times = 4) %&gt;% factor(., levels = str_c(&quot;SD = &quot;, c(10, 2, 1)))) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_text(aes(label = value %&gt;% round(3) %&gt;% as.character())) + labs(x = &quot;Stacking weight&quot;, y = expression(paste(&quot;Models defined by Kruschke&#39;s &quot;, delta, &quot; notation&quot;))) + coord_cartesian(xlim = c(0, 1)) + theme(panel.grid = element_blank()) + panel_border() + facet_grid(prior ~ .) So unlike in the depictions in Figure 18.14, the stacking method was insensitive to the \\(\\textit{SD}\\)’s on our \\(\\beta_j\\) priors. We might compare LOO difference scores, too. fit18.9 &lt;- add_criterion(fit18.9, &quot;loo&quot;) fit18.10 &lt;- add_criterion(fit18.10, &quot;loo&quot;) fit18.11 &lt;- add_criterion(fit18.11, &quot;loo&quot;) fit18.12 &lt;- add_criterion(fit18.12, &quot;loo&quot;) fit18.13 &lt;- add_criterion(fit18.13, &quot;loo&quot;) fit18.14 &lt;- add_criterion(fit18.14, &quot;loo&quot;) fit18.15 &lt;- add_criterion(fit18.15, &quot;loo&quot;) fit18.16 &lt;- add_criterion(fit18.16, &quot;loo&quot;) loo_compare(fit18.1, fit18.6, fit18.7, fit18.8) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit18.1 0.0 0.0 -31.9 5.4 4.0 0.7 63.8 10.7 ## fit18.7 -3.2 2.9 -35.1 4.4 2.7 0.5 70.3 8.8 ## fit18.6 -37.7 6.3 -69.6 4.0 2.1 0.3 139.2 8.1 ## fit18.8 -40.8 5.7 -72.7 3.1 1.3 0.2 145.4 6.1 loo_compare(fit18.9, fit18.10, fit18.11, fit18.12) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit18.9 0.0 0.0 -32.0 5.4 4.0 0.8 63.9 10.7 ## fit18.11 -3.2 2.8 -35.1 4.4 2.7 0.5 70.3 8.8 ## fit18.10 -37.6 6.3 -69.6 4.0 2.1 0.3 139.1 8.1 ## fit18.12 -40.7 5.7 -72.7 3.0 1.4 0.2 145.4 6.1 loo_compare(fit18.13, fit18.14, fit18.15, fit18.16) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit18.13 0.0 0.0 -32.0 5.4 4.1 0.8 64.1 10.7 ## fit18.15 -3.1 2.9 -35.2 4.4 2.7 0.5 70.3 8.7 ## fit18.14 -37.5 6.3 -69.6 4.0 2.0 0.3 139.1 8.0 ## fit18.16 -40.7 5.7 -72.8 3.0 1.4 0.2 145.5 6.0 The LOO difference score patterns were also about the same across the \\(\\textit{SD}\\)’s on our \\(\\beta_j\\) priors. Let’s finish up with our version of the histograms comparing the model predictors. Here’s the code for those in the top portion of Figure 18.14. # first we&#39;ll get the posterior draws from `fit18.9` and wrangle them as_draws_df(fit18.9) %&gt;% transmute(Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% pivot_longer(everything()) %&gt;% # within `bind_rows()`, we extract and wrangle the posterior draws from `fit18.11` and # then insert them below those from `fit18.9` bind_rows( as_draws_df(fit18.11) %&gt;% transmute(value = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), name = &quot;PrcntTake&quot;) %&gt;% select(name, value) ) %&gt;% # now we just need a little indexing and factor ordering mutate(model = rep(c(&quot;fit18.9&quot;, &quot;fit18.11&quot;), times = c(8000, 4000)) %&gt;% factor(., levels = c(&quot;fit18.9&quot;, &quot;fit18.11&quot;)), name = factor(name, levels = c(&quot;Spend&quot;, &quot;PrcntTake&quot;))) %&gt;% # we finally plot! ggplot(aes(x = value, y = 0)) + stat_wilke(normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(ylim = c(-0.01, NA)) + panel_border() + theme(legend.position = c(.13, .23)) + facet_grid(model ~ name, scales = &quot;free&quot;) And now we’ll do our version of the histograms for the bottom portion of Figure 18.14. as_draws_df(fit18.13) %&gt;% transmute(Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% pivot_longer(everything()) %&gt;% bind_rows( as_draws_df(fit18.15) %&gt;% transmute(value = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), name = &quot;PrcntTake&quot;) %&gt;% select(name, value) ) %&gt;% mutate(model = rep(c(&quot;fit18.13&quot;, &quot;fit18.15&quot;), times = c(8000, 4000)) %&gt;% factor(., levels = c(&quot;fit18.13&quot;, &quot;fit18.15&quot;)), name = factor(name, levels = c(&quot;Spend&quot;, &quot;PrcntTake&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_wilke(normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(ylim = c(-0.01, NA)) + theme(legend.position = c(.13, .23)) + panel_border() + facet_grid(model ~ name, scales = &quot;free&quot;) Kruschke concluded this subsection with Bayesian model comparison can be strongly affected by the degree of vagueness in the priors, even though explicit estimates of the parameter values may be minimally affected. Therefore, be very cautious when interpreting the results of Bayesian variable selection. The next section discusses a way to inform the prior by using concurrent data instead of previous data. (p. 542) We should note that while the method in text was “strongly affected by the degree of vagueness in the priors”, the information-criteria and model weighting methods, above, were not. If you’re interested in comparing models within the Bayesian paradigm, choose your method with care. 18.4.2 Variable selection with hierarchical shrinkage. Kruschke opened the subsection with a few good points: If you have strong previous research that can inform the prior, then it should be used. But if previous knowledge is weak, then the uncertainty should be expressed in the prior. This is an underlying mantra of the Bayesian approach: Any uncertainty should be expressed in the prior. (p. 543) Here we’ll standardize our new predictors, StuTeaRat and Salary. my_data &lt;- my_data %&gt;% mutate(stu_tea_rat_z = standardize(StuTeaRat), salary_z = standardize(Salary)) We can use Kruschke’s gamma_s_and_r_from_mode_sd() function to return the exact shape and rate parameters to make a gamma with a mode of 1 and an \\(\\textit{SD}\\) of 10. gamma_s_and_r_from_mode_sd &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } Here are the values. (p &lt;- gamma_s_and_r_from_mode_sd(mode = 1, sd = 10) %&gt;% as.numeric()) ## [1] 1.1051249 0.1051249 That gamma distribution looks like this. tibble(x = seq(from = 0, to = 65, length.out = 1e3)) %&gt;% ggplot(aes(x = x, y = dgamma(x, shape = p[1], rate = p[2]))) + geom_area(color = &quot;steelblue4&quot;, fill = &quot;steelblue4&quot;, alpha = .6) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Our gamma prior&quot;) + coord_cartesian(xlim = c(0, 60)) We can code those values in with arbitrary precision with the stanvar() function. stanvars &lt;- stanvar(1/29, name = &quot;one_over_twentynine&quot;) + stanvar(p[1], name = &quot;my_shape&quot;) + stanvar(p[2], name = &quot;my_rate&quot;) + stanvar(scode = &quot; real&lt;lower=0&gt; tau;&quot;, block = &quot;parameters&quot;) Note that last stanvar() line. Bürkner recently posted an exemplar of how to set a hierarchical prior on a regression coefficient in a brm() model: # define a hierarchical prior on the regression coefficients bprior &lt;- set_prior(&quot;normal(0, tau)&quot;, class = &quot;b&quot;) + set_prior(&quot;target += normal_lpdf(tau | 0, 10)&quot;, check = FALSE) stanvars &lt;- stanvar(scode = &quot; real&lt;lower=0&gt; tau;&quot;, block = &quot;parameters&quot;) make_stancode(count ~ Trt + log_Base4_c, epilepsy, prior = bprior, stanvars = stanvars) Following the method, we tell brm() we’d like to estimate the \\(\\textit{SD}\\) of our \\(\\beta_{1+}\\) priors with prior(normal(0, tau), class = b), where the tau is a stand-in for the \\(\\textit{SD}\\). In the next line, set_prior(\"target += gamma_lpdf(tau | my_shape, my_rate)\", check = FALSE), we tell brm() we’d like to estimate tau with a gamma(my_shape, my_rate), the values for which were saved in our stanvars object, above. And it’s that stanvar() line in that code wherein we told brm() we’d like that parameter to have a lower bound of 0. Let’s put it to use. fit18.1111 &lt;- brm(data = my_data, family = student, satt_z ~ 1 + spend_z + prcnt_take_z + stu_tea_rat_z + salary_z, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, tau), class = b), set_prior(&quot;target += gamma_lpdf(tau | my_shape, my_rate)&quot;, check = FALSE), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, control = list(adapt_delta = .99), stanvars = stanvars, seed = 18, file = &quot;fits/fit18.1111&quot;) fit18.0111 &lt;- update(fit18.1111, formula = satt_z ~ 1 + prcnt_take_z + stu_tea_rat_z + salary_z, file = &quot;fits/fit18.0111&quot;) fit18.1011 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + stu_tea_rat_z + salary_z, file = &quot;fits/fit18.1011&quot;) fit18.1101 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + prcnt_take_z + salary_z, file = &quot;fits/fit18.1101&quot;) fit18.1110 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + prcnt_take_z + stu_tea_rat_z , file = &quot;fits/fit18.1110&quot;) fit18.0011 &lt;- update(fit18.1111, formula = satt_z ~ 1 + stu_tea_rat_z + salary_z, file = &quot;fits/fit18.0011&quot;) fit18.0101 &lt;- update(fit18.1111, formula = satt_z ~ 1 + prcnt_take_z + salary_z, file = &quot;fits/fit18.0101&quot;) fit18.0110 &lt;- update(fit18.1111, formula = satt_z ~ 1 + prcnt_take_z + stu_tea_rat_z , file = &quot;fits/fit18.0110&quot;) fit18.1001 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + salary_z, file = &quot;fits/fit18.1001&quot;) fit18.1010 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + stu_tea_rat_z , file = &quot;fits/fit18.1010&quot;) fit18.1100 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + prcnt_take_z , file = &quot;fits/fit18.1100&quot;) fit18.0001 &lt;- update(fit18.1111, formula = satt_z ~ 1 + salary_z, file = &quot;fits/fit18.0001&quot;) fit18.0010 &lt;- update(fit18.1111, formula = satt_z ~ 1 + stu_tea_rat_z , control = list(adapt_delta = .99999), seed = 18, file = &quot;fits/fit18.0010&quot;) fit18.0100 &lt;- update(fit18.1111, formula = satt_z ~ 1 + prcnt_take_z , file = &quot;fits/fit18.0100&quot;) fit18.1000 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z , file = &quot;fits/fit18.1000&quot;) fit18.0000 &lt;- update(fit18.1111, formula = satt_z ~ 1 , file = &quot;fits/fit18.0000&quot;) In order to keep track of the next 16 models, we switched our usual naming convention. Instead of continuing on keeping on calling them fit18.17 through fit18.33, we used Kruschke’s \\(\\delta\\) 0/1 convention. If we set the formula for the full model as satt_z ~ 1 + spend_z + prcnt_take_z + stu_tea_rat_z + salary_z, the name becomes fit18.1111. Accordingly, we called the model omitting spend_z, the first predictor, fit18.0111, and so on. Before we go any further, here are the correlations among the predictor variables in the full model, fit18.1111. my_data %&gt;% select(Spend, PrcntTake, StuTeaRat, Salary) %&gt;% cor() %&gt;% round(digits = 3) ## Spend PrcntTake StuTeaRat Salary ## Spend 1.000 0.593 -0.371 0.870 ## PrcntTake 0.593 1.000 -0.213 0.617 ## StuTeaRat -0.371 -0.213 1.000 -0.001 ## Salary 0.870 0.617 -0.001 1.000 Notice above that Salary is strongly correlated with Spend, and therefore, a model that includes both Salary and Spend will show a strong trade-off between those predictors and consequently will show inflated uncertainty in the regression coefficients for either one. Should only one or the other be included, or both, or neither? (p. 544) Behold the model weights. ( mw &lt;- model_weights(fit18.1111, fit18.0111, fit18.1011, fit18.1101, fit18.1110, fit18.0011, fit18.0101, fit18.0110, fit18.1001, fit18.1010, fit18.1100, fit18.0001, fit18.0010, fit18.0100, fit18.1000, fit18.0000) ) ## fit18.1111 fit18.0111 fit18.1011 fit18.1101 fit18.1110 fit18.0011 ## 2.964385e-06 2.535276e-01 3.007985e-05 4.983780e-06 3.880511e-06 3.467826e-05 ## fit18.0101 fit18.0110 fit18.1001 fit18.1010 fit18.1100 fit18.0001 ## 6.332967e-03 2.317225e-02 4.617908e-07 6.120252e-07 2.250000e-01 7.676098e-02 ## fit18.0010 fit18.0100 fit18.1000 fit18.0000 ## 2.407980e-07 3.302637e-01 8.486454e-02 5.330482e-09 We’ll plot our model weights like before. mw %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% set_names(&quot;fit&quot;, &quot;weight&quot;) %&gt;% mutate(fit = str_remove(fit, &quot;fit18.&quot;)) %&gt;% ggplot(aes(x = weight, y = reorder(fit, weight))) + geom_text(aes(label = weight %&gt;% round(3) %&gt;% as.character()), size = 3) + coord_cartesian(xlim = c(0, 1)) + labs(x = &quot;Stacking weight&quot;, y = &quot;fit18.[xxxx]&quot;) + theme(panel.grid = element_blank()) + panel_border() As you might notice, pattern among model weights is similar with but not identical to the one among the model probabilities Kruschke displayed in Figure 18.15. Here we’ll plot the histograms for our top six. # first, we need to redefine `sd_x_3` and `sd_x_4` in terms of our two new predictors sd_x_3 &lt;- sd(my_data$StuTeaRat) sd_x_4 &lt;- sd(my_data$Salary) ## Now we&#39;ll start extracting our posterior draws and wrangling them, by model # fit18.0100 as_draws_df(fit18.0100) %&gt;% transmute(Intercept = b_Intercept, PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% pivot_longer(everything()) %&gt;% mutate(fit = &quot;fit18.0100&quot;) %&gt;% # fit18.0111 bind_rows( as_draws_df(fit18.0111) %&gt;% transmute(Intercept = b_Intercept, PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), StuTeaRat = make_beta_j(zeta_j = b_stu_tea_rat_z, sd_j = sd_x_3, sd_y = sd_y), Salary = make_beta_j(zeta_j = b_salary_z, sd_j = sd_x_4, sd_y = sd_y)) %&gt;% pivot_longer(everything()) %&gt;% mutate(fit = &quot;fit18.0111&quot;) ) %&gt;% # fit18.1100 bind_rows( as_draws_df(fit18.1100) %&gt;% transmute(Intercept = b_Intercept, Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% pivot_longer(everything()) %&gt;% mutate(fit = &quot;fit18.1100&quot;) ) %&gt;% # fit18.1000 bind_rows( as_draws_df(fit18.1000) %&gt;% transmute(Intercept = b_Intercept, Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y)) %&gt;% pivot_longer(everything()) %&gt;% mutate(fit = &quot;fit18.1000&quot;) ) %&gt;% # fit18.0001 bind_rows( as_draws_df(fit18.0001) %&gt;% transmute(Intercept = b_Intercept, Salary = make_beta_j(zeta_j = b_salary_z, sd_j = sd_x_4, sd_y = sd_y)) %&gt;% pivot_longer(everything()) %&gt;% mutate(fit = &quot;fit18.0001&quot;) ) %&gt;% # fit18.0110 # spend_z + prcnt_take_z + stu_tea_rat_z bind_rows( as_draws_df(fit18.0110) %&gt;% transmute(Intercept = b_Intercept, PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), StuTeaRat = make_beta_j(zeta_j = b_stu_tea_rat_z, sd_j = sd_x_3, sd_y = sd_y)) %&gt;% pivot_longer(everything()) %&gt;% mutate(fit = &quot;fit18.0110&quot;) ) %&gt;% # the next two lines just help order the grid the plots appear in mutate(name = factor(name, levels = c(&quot;Intercept&quot;, &quot;Spend&quot;, &quot;PrcntTake&quot;, &quot;StuTeaRat&quot;, &quot;Salary&quot;)), fit = factor(fit, levels = c(&quot;fit18.0100&quot;, &quot;fit18.0111&quot;, &quot;fit18.1100&quot;, &quot;fit18.1000&quot;, &quot;fit18.0001&quot;, &quot;fit18.0110&quot;))) %&gt;% # finally, the plot! ggplot(aes(x = value, y = 0)) + stat_wilke(normalize = &quot;panels&quot;, point_size = 4) + scale_fill_viridis_d(option = &quot;D&quot;, begin = .2, end = .8) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(ylim = c(-0.01, NA)) + panel_border() + theme(legend.position = &quot;none&quot;) + facet_grid(fit ~ name, scales = &quot;free&quot;) Like Kruschke’s results in the text, PrcntTake was the most prominent predictor. 18.4.3 What to report and what to conclude. Kruschke made a great point in the opening paragraph of this subsection. It might make sense to use the single most credible model, especially if it is notably more credible than the runner up, and if the goal is to have a parsimonious explanatory description of the data. But it is important to recognize that using the single best model, when it excludes some predictors, is concluding that the regression coefficients on the excluded predictors are exactly zero. (p. 546) Later he added “A forthright report should state the posterior probabilities of the several top models. Additionally, it can be useful to report, for each model, the ratio of its posterior probability relative to that of the best model” (p. 546). With our information criteria and model weights approach, we don’t have posterior probabilities for the models themselves. But we can report on their information criteria comparisons and weights. In the final paragraph of the subsection, Kruschke wrote: When the goal is prediction of \\(y\\) for interesting values of the predictors, as opposed to parsimonious explanation, then it is usually not appropriate to use only the single most probable model. Instead, predictions should be based on as much information as possible, using all models to the extent that they are credible. This approach is called Bayesian model averaging (BMA). (p. 547) It’s worth it to walk this out a bit. With brms, one can use brms::pp_average() to get the weighted posterior distributions for the model parameters. This is a natural extension of our model weights comparisons. # how many points on the x-axis? n_points &lt;- 30 # what vales of the predictors would we like to evaluate the weighted posterior over? nd &lt;- tibble(spend_z = seq(from = -3, to = 3, length.out = n_points), prcnt_take_z = 0, stu_tea_rat_z = 0, salary_z = 0) pp &lt;- # the first things we feed into `pp_average()` are the `brm()` fits we&#39;d like to average over pp_average(fit18.1111, fit18.0111, fit18.1011, fit18.1101, fit18.1110, fit18.0011, fit18.0101, fit18.0110, fit18.1001, fit18.1010, fit18.1100, fit18.0001, fit18.0010, fit18.0100, fit18.1000, fit18.0000, # here we tell it to evaluate the posterior over these predictor values newdata = nd, # we can get the mean trends using the &quot;fitted&quot; method method = &quot;fitted&quot;, # by `robust`, we mean we&#39;d like the Estimate in terms of posterior medians, rather than means robust = T) str(pp) ## num [1:30, 1:4] -0.0183 -0.0183 -0.0183 -0.0183 -0.0183 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## - attr(*, &quot;weights&quot;)= Named num [1:16] 2.96e-06 2.54e-01 3.01e-05 4.98e-06 3.88e-06 ... ## ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;fit18.1111&quot; &quot;fit18.0111&quot; &quot;fit18.1011&quot; &quot;fit18.1101&quot; ... ## - attr(*, &quot;ndraws&quot;)= Named num [1:16] 0 1014 0 0 0 ... ## ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;fit18.1111&quot; &quot;fit18.0111&quot; &quot;fit18.1011&quot; &quot;fit18.1101&quot; ... The pp object will require a little wrangling before it’s of use for ggplot2. pp %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = spend_z, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = 0, color = &quot;grey25&quot;, linetype = 2) + geom_pointrange(shape = 21, stroke = 1/10, fatten = 6, color = &quot;steelblue4&quot;, fill = &quot;chocolate3&quot;) + labs(x = &quot;Value of Spend_z&quot;, y = &quot;Standardized SATT&quot;) We can build on this to make a plot considering each of the four predictors. But first that requires we make a new nd tibble to feed into pp_average(). # how many points on the x-axis? n_points &lt;- 30 # what vales of the predictors would we like to evaluate the weighted posterior over? nd &lt;- tibble(spend_z = c(seq(from = -3, to = 3, length.out = n_points), rep(0, times = n_points * 3)), prcnt_take_z = c(rep(0, times = n_points), seq(from = -3, to = 3, length.out = n_points), rep(0, times = n_points * 2)), stu_tea_rat_z = c(rep(0, times = n_points * 2), seq(from = -3, to = 3, length.out = n_points), rep(0, times = n_points)), salary_z = c(rep(0, times = n_points * 3), seq(from = -3, to = 3, length.out = n_points))) pp &lt;- pp_average(fit18.1111, fit18.0111, fit18.1011, fit18.1101, fit18.1110, fit18.0011, fit18.0101, fit18.0110, fit18.1001, fit18.1010, fit18.1100, fit18.0001, fit18.0010, fit18.0100, fit18.1000, fit18.0000, newdata = nd, method = &quot;fitted&quot;, robust = T, # note the `probs` argument probs = c(.025, .975, .1, .9, .25, .75)) str(pp) ## num [1:120, 1:8] -0.0186 -0.0186 -0.0186 -0.0186 -0.0186 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:8] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ... ## - attr(*, &quot;weights&quot;)= Named num [1:16] 2.96e-06 2.54e-01 3.01e-05 4.98e-06 3.88e-06 ... ## ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;fit18.1111&quot; &quot;fit18.0111&quot; &quot;fit18.1011&quot; &quot;fit18.1101&quot; ... ## - attr(*, &quot;ndraws&quot;)= Named num [1:16] 0 1014 0 0 0 ... ## ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;fit18.1111&quot; &quot;fit18.0111&quot; &quot;fit18.1011&quot; &quot;fit18.1101&quot; ... In each panel of the plot, below, we focus on one predictor. For that predictor, we hold all other three at their mean, which, since they are all standardized, is zero. We consider the posterior predictions for standardized SAT scores across a range of values each focal predictor. The posterior predictions are depicted in terms of 95%, 80%, and 50% percentile-based interval bands and a line at the median. pp %&gt;% as_tibble() %&gt;% mutate(x = seq(from = -3, to = 3, length.out = n_points) %&gt;% rep(., times = 4), predictor = rep(c(&quot;Spend_z&quot;, &quot;PrcntTake_z&quot;, &quot;StuTeaRat_z&quot;, &quot;Salary_z&quot;), each = n_points)) %&gt;% ggplot(aes(x = x)) + geom_hline(yintercept = 0, color = &quot;grey25&quot;, linetype = 2) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/5, fill = &quot;steelblue4&quot;) + geom_ribbon(aes(ymin = Q10, ymax = Q90), alpha = 1/4, fill = &quot;steelblue4&quot;) + geom_ribbon(aes(ymin = Q25, ymax = Q75), alpha = 1/3, fill = &quot;steelblue4&quot;) + geom_line(aes(y = Estimate), size = 1, color = &quot;chocolate3&quot;) + labs(x = &quot;Standardized value of the focal predictor&quot;, y = &quot;Standardized SATT&quot;) + theme_minimal_vgrid() + panel_border() + facet_grid(predictor ~ ., scales = &quot;free&quot;) Based on the weighted average across the models, the PrcntTake_z predictor was the most potent. 18.4.4 Caution: Computational methods. To conclude this section regarding variable selection, it is appropriate to recapitulate the considerations at the beginning of the section. Variable selection is a reasonable approach only if it is genuinely plausible and meaningful that candidate predictors have zero relation to the predicted variable. The results can be surprisingly sensitive to the seemingly innocuous choice of prior for the regression coefficients, and, of course, the prior for the inclusion probability. Because of these limitations, hierarchical shrinkage priors may be a more meaningful approach. (p. 548) 18.4.5 Caution: Interaction variables. When interaction terms are included in a model that also has hierarchical shrinkage on regression coefficients, the interaction coefficients should not be put under the same higher-level prior distribution as the individual component coefficients, because interaction coefficients are conceptually from a different class of variables than individual components. (pp. 548–549) Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] GGally_2.1.2 bayesplot_1.9.0 cowplot_1.1.1 ggdist_3.2.0 ## [5] tidybayes_3.0.2 brms_2.18.0 Rcpp_1.0.9 patchwork_1.1.2 ## [9] forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 ## [13] readr_2.1.2 tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 ## [17] tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 ## [4] igraph_1.3.4 svUnit_1.0.6 splines_4.2.0 ## [7] crosstalk_1.2.0 TH.data_1.1-1 rstantools_2.2.0 ## [10] inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 ## [16] googlesheets4_1.0.1 tzdb_0.3.0 modelr_0.1.8 ## [19] RcppParallel_5.1.5 matrixStats_0.62.0 vroom_1.5.7 ## [22] xts_0.12.1 sandwich_3.0-2 prettyunits_1.1.1 ## [25] colorspace_2.0-3 rvest_1.0.2 haven_2.5.1 ## [28] xfun_0.35 callr_3.7.3 crayon_1.5.2 ## [31] jsonlite_1.8.3 lme4_1.1-31 survival_3.4-0 ## [34] zoo_1.8-10 glue_1.6.2 gtable_0.3.1 ## [37] gargle_1.2.0 emmeans_1.8.0 distributional_0.3.1 ## [40] pkgbuild_1.3.1 rstan_2.21.7 abind_1.4-5 ## [43] scales_1.2.1 mvtnorm_1.1-3 DBI_1.1.3 ## [46] miniUI_0.1.1.1 xtable_1.8-4 HDInterval_0.2.2 ## [49] bit_4.0.4 stats4_4.2.0 StanHeaders_2.21.0-7 ## [52] DT_0.24 htmlwidgets_1.5.4 httr_1.4.4 ## [55] threejs_0.3.3 RColorBrewer_1.1-3 arrayhelpers_1.1-0 ## [58] posterior_1.3.1 ellipsis_0.3.2 reshape_0.8.9 ## [61] pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 ## [64] sass_0.4.2 dbplyr_2.2.1 utf8_1.2.2 ## [67] tidyselect_1.1.2 labeling_0.4.2 rlang_1.0.6 ## [70] reshape2_1.4.4 later_1.3.0 munsell_0.5.0 ## [73] cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [76] cli_3.5.0 generics_0.1.3 broom_1.0.1 ## [79] ggridges_0.5.3 evaluate_0.18 fastmap_1.1.0 ## [82] processx_3.8.0 knitr_1.40 bit64_4.0.5 ## [85] fs_1.5.2 nlme_3.1-159 mime_0.12 ## [88] projpred_2.2.1 xml2_1.3.3 compiler_4.2.0 ## [91] shinythemes_1.2.0 rstudioapi_0.13 gamm4_0.2-6 ## [94] reprex_2.0.2 bslib_0.4.0 stringi_1.7.8 ## [97] highr_0.9 ps_1.7.2 Brobdingnag_1.2-8 ## [100] lattice_0.20-45 Matrix_1.4-1 psych_2.2.5 ## [103] nloptr_2.0.3 markdown_1.1 shinyjs_2.1.0 ## [106] tensorA_0.36.2 vctrs_0.5.1 pillar_1.8.1 ## [109] lifecycle_1.0.3 jquerylib_0.1.4 bridgesampling_1.1-2 ## [112] estimability_1.4.1 httpuv_1.6.5 R6_2.5.1 ## [115] bookdown_0.28 promises_1.2.0.1 gridExtra_2.3 ## [118] codetools_0.2-18 boot_1.3-28 colourpicker_1.1.1 ## [121] MASS_7.3-58.1 gtools_3.9.3 assertthat_0.2.1 ## [124] withr_2.5.0 mnormt_2.1.0 shinystan_2.6.0 ## [127] multcomp_1.4-20 mgcv_1.8-40 parallel_4.2.0 ## [130] hms_1.1.1 grid_4.2.0 minqa_1.2.5 ## [133] coda_0.19-4 rmarkdown_2.16 googledrive_2.0.0 ## [136] shiny_1.7.2 lubridate_1.8.0 base64enc_0.1-3 ## [139] dygraphs_1.1.1.6 Footnote References Braumoeller, B. F. (2004). Hypothesis testing and multiplicative interaction terms. International Organization, 58(4), 807–820. https://doi.org/10.1017/S0020818304040251 Bürkner, P.-C. (2022b). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html Bürkner, P.-C. (2022d). brms reference manual, Version 2.17.0. https://CRAN.R-project.org/package=brms/brms.pdf Carvalho, C. M., Polson, N. G., &amp; Scott, J. G. (2009). Handling sparsity via the horseshoe. Artificial Intelligence and Statistics, 73–80. http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). Bayesian data analysis (Third Edition). CRC press. https://stat.columbia.edu/~gelman/book/ Gelman, A., Goodrich, B., Gabry, J., &amp; Vehtari, A. (2019). R-squared for Bayesian regression models. The American Statistician, 73(3), 307–309. https://doi.org/10.1080/00031305.2018.1549100 Guber, L., Deborah. (1999). Getting what you pay for: The debate over equity in public school expenditures. Journal of Statistics Education, 7(2). https://www.semanticscholar.org/paper/Getting-What-You-Pay-For-The-Debate-Over-Equity-in-Guber/29c30e9dc77b56340faa5e6ad35e0741a5a83d49 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Merkle, E. C., &amp; Rosseel, Y. (2018). blavaan: Bayesian structural equation models via parameter expansion. Journal of Statistical Software, 85(4), 1–30. https://doi.org/10.18637/jss.v085.i04 Merkle, E. C., Rosseel, Y., &amp; Goodrich, B. (2021). blavaan: Bayesian latent variable analysis. https://CRAN.R-project.org/package=blavaan Pedersen, Thomas L. (2020). Adding annotation and style. https://patchwork.data-imaginist.com/articles/guides/annotation.html Pedersen, Thomas L. (2020). Plot assembly. https://patchwork.data-imaginist.com/articles/guides/assembly.html Piironen, J., &amp; Vehtari, A. (2017). Sparsity information and regularization in the horseshoe and other shrinkage priors. Electronic Journal of Statistics, 11(2), 5018–5051. https://doi.org/10.1214/17-EJS1337SI Ripley, B. (2021). MASS: Support functions and datasets for venables and Ripley’s MASS. https://CRAN.R-project.org/package=MASS Schloerke, B., Crowley, J., Di Cook, Briatte, F., Marbach, M., Thoen, E., Elberg, A., &amp; Larmarange, J. (2021). GGally: Extension to ’ggplot2’. https://CRAN.R-project.org/package=GGally Vehtari, A., &amp; Gabry, J. (2022b, March 23). Bayesian stacking and pseudo-BMA weights using the loo package. https://CRAN.R-project.org/package=loo/vignettes/loo2-weights.html Venables, W. N., &amp; Ripley, B. D. (2002). Modern applied statistics with S (Fourth Edition). Springer. http://www.stats.ox.ac.uk/pub/MASS4 Wilke, C. O. (2019). Fundamentals of data visualization. https://clauswilke.com/dataviz/ In his Corrigenda, Kruschke further clarified: “that is true only in case there is a single predictor, not for multiple predictors. The statement could have said that ‘R^2 is algebraically constrained to fall between −1 and +1 in least-squares regression’. More relevantly, replace the statement with the following: ‘In multiple linear regression, standardized regression coefficients tend to fall between -2 and +2 unless the predictors are very strongly correlated and have strongly opposing effects. If your data have strongly correlated predictors, consider widening the prior.’”↩︎ "],["metric-predicted-variable-with-one-nominal-predictor.html", "19 Metric Predicted Variable with One Nominal Predictor 19.1 Describing multiple groups of metric data 19.2 Traditional analysis of variance 19.3 Hierarchical Bayesian approach 19.4 Including a metric predictor 19.5 Heterogeneous variances and robustness against outliers 19.6 Exercises Walk out an effect size Session info", " 19 Metric Predicted Variable with One Nominal Predictor This chapter considers data structures that consist of a metric predicted variable and a nominal predictor…. This type of data structure can arise from experiments or from observational studies. In experiments, the researcher assigns the categories (at random) to the experimental subjects. In observational studies, both the nominal predictor value and the metric predicted value are generated by processes outside the direct control of the researcher. In either case, the same mathematical description can be applied to the data (although causality is best inferred from experimental intervention). The traditional treatment of this sort of data structure is called single-factor analysis of variance (ANOVA), or sometimes one-way ANOVA. Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter will also consider the situation in which there is also a metric predictor that accompanies the primary nominal predictor. The metric predictor is sometimes called a covariate, and the traditional treatment of this data structure is called analysis of covariance (ANCOVA). The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups, etc. (Kruschke, 2015, pp. 553–554) 19.1 Describing multiple groups of metric data Figure 19.1 illustrates the conventional description of grouped metric data. Each group is represented as a position on the horizontal axis. The vertical axis represents the variable to be predicted by group membership. The data are assumed to be normally distributed within groups, with equal standard deviation in all groups. The group means are deflections from overall baseline, such that the deflections sum to zero. Figure 19.1 provides a specific numerical example, with data that were randomly generated from the model. (p. 554) We’ll want a custom data-generating function for our primary group data. library(tidyverse) generate_data &lt;- function(seed, mean) { set.seed(seed) rnorm(n, mean = grand_mean + mean, sd = 2) } n &lt;- 20 grand_mean &lt;- 101 d &lt;- tibble(group = 1:5, deviation = c(4, -5, -2, 6, -3)) %&gt;% mutate(d = map2(group, deviation, generate_data)) %&gt;% unnest(d) %&gt;% mutate(iteration = rep(1:n, times = 5)) glimpse(d) ## Rows: 100 ## Columns: 4 ## $ group &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,… ## $ deviation &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, -5, -5, -5, -5, -5, … ## $ d &lt;dbl&gt; 103.74709, 105.36729, 103.32874, 108.19056, 105.65902, 103.35906, 105.97486, 106… ## $ iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, … Here we’ll make a tibble containing the necessary data for the rotated Gaussians. As far as I can tell, Kruschke’s Gaussians only span to the bounds of percentile-based 98% intervals. We partition off those bounds for each group by the ll and ul columns in the first mutate() function. In the second mutate(), we expand the dataset to include a sequence of 100 values between those lower- and upper-limit points. In the third mutate(), we feed those points into the dnorm() function, with group-specific means and a common sd. densities &lt;- d %&gt;% distinct(group, deviation) %&gt;% mutate(ll = qnorm(.01, mean = grand_mean + deviation, sd = 2), ul = qnorm(.99, mean = grand_mean + deviation, sd = 2)) %&gt;% mutate(d = map2(ll, ul, seq, length.out = 100)) %&gt;% mutate(density = map2(d, grand_mean + deviation, dnorm, sd = 2)) %&gt;% unnest(c(d, density)) head(densities) ## # A tibble: 6 × 6 ## group deviation ll ul d density ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 100. 110. 100. 0.0133 ## 2 1 4 100. 110. 100. 0.0148 ## 3 1 4 100. 110. 101. 0.0165 ## 4 1 4 100. 110. 101. 0.0183 ## 5 1 4 100. 110. 101. 0.0203 ## 6 1 4 100. 110. 101. 0.0224 We’ll need two more supplementary tibbles to add the flourishes to the plot. The arrow tibble will specify our light-gray arrows. The text tibble will contain our annotation information. arrow &lt;- tibble(d = grand_mean, group = 1:5, deviation = c(4, -5, -2, 6, -3), offset = .1) head(arrow) ## # A tibble: 5 × 4 ## d group deviation offset ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 101 1 4 0.1 ## 2 101 2 -5 0.1 ## 3 101 3 -2 0.1 ## 4 101 4 6 0.1 ## 5 101 5 -3 0.1 text &lt;- tibble(d = grand_mean, group = c(0:5, 0), deviation = c(0, 4, -5, -2, 6, -3, 10), offset = rep(c(1/4, 0), times = c(6, 1)), angle = rep(c(90, 0), times = c(6, 1)), label = c(&quot;beta[0]==101&quot;, &quot;beta[&#39;[1]&#39;]==4&quot;,&quot;beta[&#39;[2]&#39;]==-5&quot;, &quot;beta[&#39;[3]&#39;]==-2&quot;, &quot;beta[&#39;[4]&#39;]==6&quot;, &quot;beta[&#39;[5]&#39;]==3&quot;, &quot;sigma[&#39;all&#39;]==2&quot;)) head(text) ## # A tibble: 6 × 6 ## d group deviation offset angle label ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 101 0 0 0.25 90 beta[0]==101 ## 2 101 1 4 0.25 90 beta[&#39;[1]&#39;]==4 ## 3 101 2 -5 0.25 90 beta[&#39;[2]&#39;]==-5 ## 4 101 3 -2 0.25 90 beta[&#39;[3]&#39;]==-2 ## 5 101 4 6 0.25 90 beta[&#39;[4]&#39;]==6 ## 6 101 5 -3 0.25 90 beta[&#39;[5]&#39;]==3 We’re almost ready to plot. Before we do, let’s talk color and theme. For this chapter, we’ll take our color palette from the palettetown package (Lucas, 2016), which provides an array of color palettes inspired by Pokémon. Our color palette will be #17, which is based on Pidgeotto. library(palettetown) scales::show_col(pokepal(pokemon = 17)) pp &lt;- pokepal(pokemon = 17) pp ## [1] &quot;#785848&quot; &quot;#E0B048&quot; &quot;#D03018&quot; &quot;#202020&quot; &quot;#A87858&quot; &quot;#F8E858&quot; &quot;#583828&quot; &quot;#E86040&quot; &quot;#F0F0A0&quot; ## [10] &quot;#F8A870&quot; &quot;#C89878&quot; &quot;#F8F8F8&quot; &quot;#A0A0A0&quot; Our overall plot theme will be based on the default theme_grey() with a good number of adjustments. theme_set( theme_grey() + theme(text = element_text(color = pp[4]), axis.text = element_text(color = pp[4]), axis.ticks = element_line(color = pp[4]), legend.background = element_blank(), legend.box.background = element_blank(), legend.key = element_rect(fill = pp[9]), panel.background = element_rect(fill = pp[9], color = pp[9]), panel.grid = element_blank(), plot.background = element_rect(fill = pp[12], color = pp[12]), strip.background = element_rect(fill = alpha(pp[2], 1/3), color = &quot;transparent&quot;), strip.text = element_text(color = pp[4])) ) Now make Figure 19.1. library(ggridges) d %&gt;% ggplot(aes(x = d, y = group, group = group)) + geom_vline(xintercept = grand_mean, color = pp[12]) + geom_jitter(height = .05, alpha = 4/4, shape = 1, color = pp[10]) + # the Gaussians geom_ridgeline(data = densities, aes(height = -density), min_height = NA, scale = 3/2, size = 3/4, fill = &quot;transparent&quot;, color = pp[7]) + # the small arrows geom_segment(data = arrow, aes(xend = d + deviation, y = group + offset, yend = group + offset), color = pp[5], size = 1, arrow = arrow(length = unit(.2, &quot;cm&quot;))) + # the large arrow on the left geom_segment(aes(x = 80, xend = grand_mean, y = 0, yend = 0), color = pp[5], size = 3/4, arrow = arrow(length = unit(.2, &quot;cm&quot;))) + # the text geom_text(data = text, aes(x = grand_mean + deviation, y = group + offset, label = label, angle = angle), size = 4, color = pp[4], parse = T) + scale_y_continuous(NULL, breaks = 1:5, labels = c(&quot;&lt;1,0,0,0,0&gt;&quot;, &quot;&lt;0,1,0,0,0&gt;&quot;, &quot;&lt;0,0,1,0,0&gt;&quot;, &quot;&lt;0,0,0,1,0&gt;&quot;, &quot;&lt;0,0,0,0,1&gt;&quot;)) + xlab(NULL) + coord_flip(xlim = c(90, 112), ylim = c(-0.2, 5.5)) The descriptive model presented in Figure 19.1 is the traditional one used by classical ANOVA (which is described a bit more in the next section). More general models are straight forward to implement in Bayesian software. For example, outliers could be accommodated by using heavy-tailed noise distributions (such as a \\(t\\) distribution) instead of a normal distribution, and different groups could be given different standard deviations. (p. 556) 19.2 Traditional analysis of variance The terminology, “analysis of variance,” comes from a decomposition of overall data variance into within-group variance and between-group variance (Fisher, 1925). Algebraically, the sum of squared deviations of the scores from their overall mean equals the sum of squared deviations of the scores from their respective group means plus the sum of squared deviations of the group means from the overall mean. In other words, the total variance can be partitioned into within-group variance plus between-group variance. Because one definition of the word “analysis” is separation into constituent parts, the term ANOVA accurately describes the underlying algebra in the traditional methods. That algebraic relation is not used in the hierarchical Bayesian approach presented here. The Bayesian method can estimate component variances, however. Therefore, the Bayesian approach is not ANOVA, but is analogous to ANOVA. (p. 556) 19.3 Hierarchical Bayesian approach “Our goal is to estimate its parameters in a Bayesian framework. Therefore, all the parameters need to be given a meaningfully structured prior distribution” (p. 557). However, our approach will depart a little from the one in the text. All our parameters will not “have generic noncommittal prior distributions” (p. 557). Most importantly, we will not follow the example in (Gelman, 2006) of putting a broad uniform prior on \\(\\sigma_y\\). Rather, we will continue using the half-Gaussian prior, as recommended by the Stan team. However, we will follow Kruschke’s lead for the overall intercept and use a Gaussian prior “made broad on the scale of the data” (p. 557). And like Kruschke, we will estimate \\(\\sigma_\\beta\\) from the data. To further get a sense of this, let’s make our version of the hierarchical model diagram of Figure 19.2. library(patchwork) # bracket p1 &lt;- tibble(x = .99, y = .5, label = &quot;{_}&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, hjust = 1, color = pp[8], family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() ## plain arrow # save our custom arrow settings my_arrow &lt;- arrow(angle = 20, length = unit(0.35, &quot;cm&quot;), type = &quot;closed&quot;) p2 &lt;- tibble(x = .72, y = 1, xend = .68, yend = .25) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # normal density p3 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. # second normal density p4 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;0&quot;, &quot;sigma[beta]&quot;), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # two annotated arrows p5 &lt;- tibble(x = c(.16, .81), y = c(1, 1), xend = c(.47, .77), yend = c(0, 0)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(.25, .74, .83), y = .5, label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;), size = c(10, 10, 7), color = pp[4], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # likelihood formula p6 &lt;- tibble(x = .99, y = .25, label = &quot;beta[0]+sum()[italic(j)]*beta[&#39;[&#39;*italic(j)*&#39;]&#39;]*italic(x)[&#39;[&#39;*italic(j)*&#39;]&#39;](italic(i))&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(hjust = 1, size = 7, color = pp[4], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # half-normal density p7 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma]&quot;, size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # annotated arrow p8 &lt;- tibble(x = .38, y = .65, label = &quot;&#39;=&#39;&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, color = pp[4], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = .25, arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # the third normal density p9 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;mu[italic(i)]&quot;, &quot;sigma[italic(y)]&quot;), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # another annotated arrow p10 &lt;- tibble(x = .55, y = .6, label = &quot;&#39;~&#39;&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, color = pp[4], parse = T, family = &quot;Times&quot;) + geom_segment(x = .82, xend = .38, y = 1, yend = .2, arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # the final annotated arrow p11 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = pp[4], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # some text p12 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pp[4], parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 1, l = 6, r = 7), area(t = 2, b = 3, l = 6, r = 7), area(t = 3, b = 4, l = 1, r = 3), area(t = 3, b = 4, l = 5, r = 7), area(t = 6, b = 7, l = 1, r = 7), area(t = 5, b = 6, l = 1, r = 7), area(t = 6, b = 7, l = 9, r = 11), area(t = 9, b = 10, l = 5, r = 7), area(t = 8, b = 9, l = 5, r = 7), area(t = 8, b = 9, l = 5, r = 11), area(t = 11, b = 11, l = 5, r = 7), area(t = 12, b = 12, l = 5, r = 7) ) # combine and plot! (p1 + p2 + p3 + p4 + p6 + p5 + p7 + p9 + p8 + p10 + p11 + p12) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Later on in the text, Kruschke opined: A crucial pre-requisite for estimating \\(\\sigma_\\beta\\) from all the groups is an assumption that all the groups are representative and informative for the estimate. It only makes sense to influence the estimate of one group with data from the other groups if the groups can be meaningfully described as representative of a shared higher-level distribution. (p. 559) Although I agree with him in spirit, this doesn’t appear to strictly be the case. As odd and paradoxical as this sounds, partial pooling can be of use even when the some of the cases are of a different kind. For more on the topic, see Efron and Morris’s classic (1977) paper, Stein’s paradox in statistics, and my blog post walking out one of their examples in brms. 19.3.1 Implementation in JAGS brms. The brms setup, of course, differs a bit from JAGS. fit &lt;- brm(data = my_data, family = gaussian, y ~ 1 + (1 | categirical_variable), prior = c(prior(normal(0, x), class = Intercept), prior(normal(0, x), class = b), prior(cauchy(0, x), class = sd), prior(cauchy(0, x), class = sigma))) The noise standard deviation \\(\\sigma_y\\) is depicted in the prior statement including the argument class = sigma. The grand mean is depicted by the first 1 in the model formula and its prior is indicated by the class = Intercept argument. We indicate we’d like group-based deviations from the grand mean with the (1 | categirical_variable) syntax, where the 1 on the left side of the bar indicates we’d like our intercepts to vary by group and the categirical_variable part simply represents the name of a given categorical variable we’d like those intercepts to vary by. The brms default is to do this with deviance scores, the mean for which will be zero. Although it’s not obvious in the formula syntax, the model presumes the group-based deviations are normally distributed with a mean of zero and a standard deviation, which Kruschke termed \\(\\sigma_\\beta\\). There is no prior for the mean. It’s set at zero. But there is a prior for \\(\\sigma_\\beta\\), which is denoted by the argument class = sd. We, of course, are not using a uniform prior on any of our variance parameters. But in order to be weakly informative, we will use the half-Cauchy. Recall that since the brms default is to set the lower bound for any variance parameter to 0, there’s no need to worry about doing so ourselves. So even though the syntax only indicates cauchy, it’s understood to mean Cauchy with a lower bound at zero; since the mean is usually 0, that makes is a half-Cauchy. Kruschke set the upper bound for his \\(\\sigma_y\\) to 10 times the standard deviation of the criterion variable. The tails of the half-Cauchy are sufficiently fat that, in practice, I’ve found it doesn’t matter much what you set the \\(\\textit{SD}\\) of its prior to. One is often a sensible default for reasonably-scaled data. But if we want to take a more principled approach, we can set it to the size of the criterion’s \\(\\textit{SD}\\) or perhaps even 10 times that value. Kruschke suggested using a gamma on \\(\\sigma_\\beta\\), which is a sensible alternative to half-Cauchy often used within the Stan universe. Especially in situations in which you would like to (a) keep the variance parameter above zero, but (b) still allow it to be arbitrarily close to zero, and also (c) let the likelihood dominate the posterior, the Stan team recommends the \\(\\operatorname{Gamma}(2, 0)\\) prior, based on the paper by Chung and colleagues (2013, click here). But you should note that I don’t mean a literal 0 for the second parameter in the gamma distribution, but rather some small value like 0.1 or so. This is all clarified in Chung et al. (2013). Here’s what \\(\\operatorname{Gamma}(2, 0.1)\\) looks like. tibble(x = seq(from = 0, to = 110, by = .1)) %&gt;% ggplot(aes(x = x, y = dgamma(x, 2, 0.1))) + geom_area(fill = pp[10]) + annotate(geom = &quot;text&quot;, x = 14.25, y = 0.015, label = &quot;&#39;Gamma&#39;*(2*&#39;, &#39;*0.1)&quot;, parse = T, color = pp[1], size = 4.25) + scale_x_continuous(expand = c(0, 0), limits = c(0, 110)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) If you’d like that prior be even less informative, just reduce it to like \\(\\operatorname{Gamma}(2, 0.01)\\) or so. Kruschke goes further to recommend “the shape and rate parameters of the gamma distribution are set so its mode is sd(y)/2 and its standard deviation is 2*sd(y), using the function gammaShRaFromModeSD explained in Section 9.2.2.” (pp. 560–561). Let’s make that function. gamma_a_b_from_omega_sigma &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } So in the case of standardized data where sd(1) = 1, we’d use our gamma_a_b_from_omega_sigma() function like so. sd_y &lt;- 1 omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y (s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)) ## $shape ## [1] 1.283196 ## ## $rate ## [1] 0.5663911 And that produces the following gamma distribution. tibble(x = seq(from = 0, to = 21, by = .01)) %&gt;% ggplot(aes(x = x, y = dgamma(x, s_r$shape, s_r$rate))) + geom_area(fill = pp[8]) + annotate(geom = &quot;text&quot;, x = 2.75, y = 0.02, label = &quot;&#39;Gamma&#39;*(1.283196*&#39;, &#39;*0.5663911)&quot;, parse = T, color = pp[7], size = 2.75) + scale_x_continuous(breaks = c(0, 1, 5, 10, 20), expand = c(0, 0), limits = c(0, 21)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) In the parameter space that matters, from zero to one, that gamma is pretty noninformative. It peaks between the two, slopes very gently rightward, but has the nice steep slope on the left keeping the estimates off the zero boundary. And even though that right slope is very gentle given the scale of the data, it’s aggressive enough that it should keep the MCMC chains from spending a lot of time in ridiculous parts of the parameter space. I.e., when working with finite numbers of iterations, we want our MCMC chains wasting exactly zero iterations investigating what the density might be for \\(\\sigma_\\beta \\approx 1e10\\) for standardized data. 19.3.2 Example: Sex and death. Let’s load and glimpse() at Hanley and Shapiro’s (1994) fruit-fly data. my_data &lt;- read_csv(&quot;data.R/FruitflyDataReduced.csv&quot;) glimpse(my_data) ## Rows: 125 ## Columns: 3 ## $ Longevity &lt;dbl&gt; 35, 37, 49, 46, 63, 39, 46, 56, 63, 65, 56, 65, 70, 63, 65, 70, 77, 81, 86… ## $ CompanionNumber &lt;chr&gt; &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant… ## $ Thorax &lt;dbl&gt; 0.64, 0.68, 0.68, 0.72, 0.72, 0.76, 0.76, 0.76, 0.76, 0.76, 0.80, 0.80, 0.… We can use geom_density_ridges() to help get a sense of how our criterion Longevity is distributed across groups of CompanionNumber. my_data %&gt;% group_by(CompanionNumber) %&gt;% mutate(group_mean = mean(Longevity)) %&gt;% ungroup() %&gt;% mutate(CompanionNumber = fct_reorder(CompanionNumber, group_mean)) %&gt;% ggplot(aes(x = Longevity, y = CompanionNumber, fill = group_mean)) + geom_density_ridges(scale = 3/2, size = .2, color = pp[9]) + scale_fill_gradient(low = pp[4], high = pp[2]) + scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + scale_y_discrete(NULL, expand = expansion(mult = c(0, 0.4))) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) Let’s fire up brms. library(brms) We’ll want to do the preparatory work to define our stanvars. (mean_y &lt;- mean(my_data$Longevity)) ## [1] 57.44 (sd_y &lt;- sd(my_data$Longevity)) ## [1] 17.56389 omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y (s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)) ## $shape ## [1] 1.283196 ## ## $rate ## [1] 0.03224747 With the prep work is done, here are our stanvars. stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Now fit the model, our hierarchical Bayesian alternative to an ANOVA. fit19.1 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 1 + (1 | CompanionNumber), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, control = list(adapt_delta = 0.99), stanvars = stanvars, file = &quot;fits/fit19.01&quot;) Much like Kruschke’s JAGS chains, our brms chains are well behaved, but only after fiddling with the adapt_delta setting. library(bayesplot) color_scheme_set(scheme = pp[c(10, 8, 12, 5, 1, 4)]) plot(fit19.1, widths = c(2, 3)) Also like Kruschke, our chains appear moderately autocorrelated. # extract the posterior draws draws &lt;- as_draws_df(fit19.1) # plot draws %&gt;% mutate(chain = .chain) %&gt;% mcmc_acf(pars = vars(b_Intercept:sigma), lags = 10) Here’s the model summary. print(fit19.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 1 + (1 | CompanionNumber) ## Data: my_data (Number of observations: 125) ## Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup draws = 12000 ## ## Group-Level Effects: ## ~CompanionNumber (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 14.92 7.57 6.27 35.82 1.00 2515 3486 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 57.33 7.59 41.70 72.92 1.00 2250 2245 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 14.88 0.95 13.18 16.86 1.00 6571 6625 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With the ranef() function, we can get the summaries of the group-specific deflections. ranef(fit19.1) ## $CompanionNumber ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## None0 5.8334936 7.942217 -9.693236 22.207219 ## Pregnant1 7.0048914 7.901202 -9.030515 23.002838 ## Pregnant8 5.6482612 7.906738 -10.335139 21.786660 ## Virgin1 -0.5423866 7.836639 -16.720255 14.976009 ## Virgin8 -17.4580865 7.947826 -33.885667 -2.249241 And with the coef() function, we can get those same group-level summaries in a non-deflection metric. coef(fit19.1) ## $CompanionNumber ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## None0 63.16238 2.933578 57.39000 68.96544 ## Pregnant1 64.33378 2.951117 58.50962 70.17887 ## Pregnant8 62.97715 2.921190 57.26046 68.76300 ## Virgin1 56.78650 2.896133 51.16755 62.52056 ## Virgin8 39.87080 3.032215 33.83380 45.73514 Those are all estimates of the group-specific means. Since it wasn’t modeled, all have the same parameter estimates for \\(\\sigma_y\\). posterior_summary(fit19.1)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 14.8816737 0.9459512 13.1836374 16.8615943 To prepare for our version of the top panel of Figure 19.3, we’ll use slice_sample() to randomly sample from the posterior draws, saving the subset as draws_20. # how many random draws from the posterior would you like? n_draws &lt;- 20 # subset set.seed(19) draws_20 &lt;- draws %&gt;% slice_sample(n = n_draws, replace = F) glimpse(draws_20) ## Rows: 20 ## Columns: 13 ## $ b_Intercept &lt;dbl&gt; 59.79337, 68.08343, 65.87654, 49.78539, 61.36588,… ## $ sd_CompanionNumber__Intercept &lt;dbl&gt; 9.746157, 21.206450, 12.517377, 22.193778, 13.077… ## $ sigma &lt;dbl&gt; 13.96830, 14.94739, 15.45839, 14.55226, 15.96590,… ## $ `r_CompanionNumber[None0,Intercept]` &lt;dbl&gt; 0.5159277, -5.8309287, -2.9403040, 12.0782556, 2.… ## $ `r_CompanionNumber[Pregnant1,Intercept]` &lt;dbl&gt; 4.9800818, -1.4330721, -3.5383401, 10.5176174, 2.… ## $ `r_CompanionNumber[Pregnant8,Intercept]` &lt;dbl&gt; 6.158075, -7.063172, -1.633937, 10.781149, 1.1145… ## $ `r_CompanionNumber[Virgin1,Intercept]` &lt;dbl&gt; -2.6577022, -12.3264394, -5.4096473, 7.5111997, -… ## $ `r_CompanionNumber[Virgin8,Intercept]` &lt;dbl&gt; -17.697159, -28.331720, -21.037003, -13.096651, -… ## $ lprior &lt;dbl&gt; -13.17332, -13.38440, -13.27956, -13.37765, -13.3… ## $ lp__ &lt;dbl&gt; -528.6157, -525.9077, -529.4408, -526.6029, -528.… ## $ .chain &lt;int&gt; 3, 4, 2, 3, 1, 2, 2, 4, 1, 4, 4, 4, 2, 4, 2, 2, 3… ## $ .iteration &lt;int&gt; 1677, 910, 1483, 1557, 2745, 1803, 2677, 1255, 26… ## $ .draw &lt;int&gt; 7677, 9910, 4483, 7557, 2745, 4803, 5677, 10255, … Before we make our version of the top panel, let’s make a corresponding plot of the fixed intercept, the grand mean. The most important lines in the code, below are the ones where we used stat_function() within mapply(). tibble(x = c(0, 150)) %&gt;% ggplot(aes(x = x)) + mapply(function(mean, sd) { stat_function(fun = dnorm, args = list(mean = mean, sd = sd), alpha = 2/3, size = 1/3, color = pp[4]) }, # enter means and standard deviations here mean = draws_20 %&gt;% pull(b_Intercept), sd = draws_20 %&gt;% pull(sigma) ) + geom_jitter(data = my_data, aes(x = Longevity, y = -0.001), height = .001, alpha = 3/4, color = pp[10]) + scale_x_continuous(&quot;Longevity&quot;, breaks = 0:4 * 25, limits = c(0, NA), expand = expansion(mult = c(0, 0.05))) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Posterior Predictive Distribution&quot;, subtitle = &quot;The jittered dots are the ungrouped Longevity data. The\\nGaussians are posterior draws depicting the overall\\ndistribution, the grand mean.&quot;) + coord_cartesian(xlim = c(0, 110)) Unfortunately, we can’t extend our mapply(stat_function()) method to the group-level estimates. To my knowledge, there isn’t a way to show the group estimates at different spots along the y-axis. And our mapply(stat_function()) approach has other limitations, too. Happily, we have some great alternatives. To use them, we’ll need a little help from tidybayes. library(tidybayes) For the first part, we’ll take tidybayes::add_epred_draws() for a whirl. densities &lt;- my_data %&gt;% distinct(CompanionNumber) %&gt;% add_epred_draws(fit19.1, ndraws = 20, seed = 19, dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) glimpse(densities) ## Rows: 100 ## Columns: 8 ## Groups: CompanionNumber, .row [5] ## $ CompanionNumber &lt;chr&gt; &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .draw &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, … ## $ .epred &lt;dbl&gt; 63.46305, 61.49345, 68.17744, 62.48045, 61.84531, 63.64695, 64.24260, 60.6… ## $ mu &lt;dbl&gt; 63.46305, 61.49345, 68.17744, 62.48045, 61.84531, 63.64695, 64.24260, 60.6… ## $ sigma &lt;dbl&gt; 13.93148, 13.77736, 13.20789, 15.96590, 14.70948, 14.86626, 15.45839, 14.8… With the first two lines, we made a \\(5 \\times 1\\) tibble containing the five levels of the experimental grouping variable, CompanionNumber. The add_epred_draws() function comes from tidybayes (see the Posterior fits section of Kay, 2021). The first argument of the add_epred_draws() is newdata, which works much like it does in brms::fitted(); it took our \\(5 \\times 1\\) tibble. The next argument took our brms model fit, fit19.1. With the ndraws argument, we indicated we just wanted 20 random draws from the posterior. The seed argument makes those random draws reproducible. With dpar, we requested distributional regression parameters in the output. In our case, those were the \\(\\mu\\) and \\(\\sigma\\) values for each level of CompanionNumber. Since we took 20 draws across 5 groups, we ended up with a 100-row tibble. The next steps are a direct extension of the method we used to make our Gaussians for our version of Figure 19.1. densities &lt;- densities %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Longevity = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Longevity) %&gt;% mutate(density = dnorm(Longevity, mu, sigma)) glimpse(densities) ## Rows: 10,000 ## Columns: 12 ## Groups: CompanionNumber, .row [5] ## $ CompanionNumber &lt;chr&gt; &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .draw &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ .epred &lt;dbl&gt; 63.46305, 63.46305, 63.46305, 63.46305, 63.46305, 63.46305, 63.46305, 63.4… ## $ mu &lt;dbl&gt; 63.46305, 63.46305, 63.46305, 63.46305, 63.46305, 63.46305, 63.46305, 63.4… ## $ sigma &lt;dbl&gt; 13.93148, 13.93148, 13.93148, 13.93148, 13.93148, 13.93148, 13.93148, 13.9… ## $ ll &lt;dbl&gt; 36.15784, 36.15784, 36.15784, 36.15784, 36.15784, 36.15784, 36.15784, 36.1… ## $ ul &lt;dbl&gt; 90.76826, 90.76826, 90.76826, 90.76826, 90.76826, 90.76826, 90.76826, 90.7… ## $ Longevity &lt;dbl&gt; 36.15784, 36.70947, 37.26109, 37.81271, 38.36433, 38.91595, 39.46757, 40.0… ## $ density &lt;dbl&gt; 0.004195179, 0.004530160, 0.004884226, 0.005257716, 0.005650899, 0.0060639… If you look at the code we used to make ll and ul, you’ll see we used 95% intervals, this time. Our second mutate() function is basically the same. After unnesting the tibble, we just needed to plug in the Longevity, mu, and sigma values into the dnorm() function to compute the corresponding density values. densities %&gt;% ggplot(aes(x = Longevity, y = CompanionNumber)) + # here we make our density lines geom_ridgeline(aes(height = density, group = interaction(CompanionNumber, .draw)), fill = NA, color = adjustcolor(pp[4], alpha.f = 2/3), size = 1/3, scale = 25) + # the original data with little jitter thrown in geom_jitter(data = my_data, height = .04, alpha = 3/4, color = pp[10]) + # pretty much everything below this line is aesthetic fluff scale_x_continuous(breaks = 0:4 * 25, limits = c(0, 110), expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Data with Posterior Predictive Distrib.&quot;, y = NULL) + coord_cartesian(ylim = c(1.25, 5.25)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) Do be aware that when you use this method, you may have to fiddle around with the geom_ridgeline() scale argument to get the Gaussian’s heights on reasonable-looking relative heights. Stick in different numbers to get a sense of what I mean. I also find that I’m often not a fan of the way the spacing on the y axis ends up with default geom_ridgeline(). It’s easy to overcome this with a little ylim fiddling. To return to the more substantive interpretation, the top panel of Figure 19.3 suggests that the normal distributions with homogeneous variances appear to be reasonable descriptions of the data. There are no dramatic outliers relative to the posterior predicted curves, and the spread of the data within each group appears to be reasonably matched by the width of the posterior normal curves. (Be careful when making visual assessments of homogeneity of variance because the visual spread of the data depends on the sample size; for a reminder see the [see the right panel of Figure 17.1, p. 478].) The range of credible group means, indicated by the peaks of the normal curves, suggests that the group Virgin8 is clearly lower than the others, and the group Virgin1 might be lower than the controls. To find out for sure, we need to examine the differences of group means, which we do in the next section. (p. 564) For clarity, the “see the right panel of Figure 17.1, p. 478” part was changed following Kruschke’s Corrigenda. 19.3.3 Contrasts. It is straight forward to examine the posterior distribution of credible differences. Every step in the MCMC chain provides a combination of group means that are jointly credible, given the data. Therefore, every step in the MCMC chain provides a credible difference between groups… To construct the credible differences of group \\(1\\) and group \\(2\\), at every step in the MCMC chain we compute \\[\\begin{align*} \\mu_1 - \\mu_2 &amp; = (\\beta_0 + \\beta_1) - (\\beta_0 + \\beta_2) \\\\ &amp; = (+1) \\cdot \\beta_1 + (-1) \\cdot \\beta_2 \\end{align*}\\] In other words, the baseline cancels out of the calculation, and the difference is a sum of weighted group deflections. Notice that the weights sum to zero. To construct the credible differences of the average of groups \\(1\\)-\\(3\\) and the average of groups \\(4\\)-\\(5\\), at every step in the MCMC chain we compute \\[\\begin{align*} \\small{(\\mu_1 + \\mu_2 + \\mu_3) / 3 - (\\mu_4 + \\mu_5) / 2} &amp; = \\small{((\\beta_0 + \\beta_1) + (\\beta_0 + \\beta_2) + (\\beta_0 + \\beta_3) ) / 3 - ((\\beta_0 + \\beta_4) + (\\beta_0 + \\beta_5) ) / 2} \\\\ &amp; = \\small{(\\beta_1 + \\beta_2 + \\beta_3) / 3 - (\\beta_4 + \\beta_5) / 2} \\\\ &amp; = \\small{(+ 1/3) \\cdot \\beta_1 + (+ 1/3) \\cdot \\beta_2 + (+ 1/3) \\cdot \\beta_3 + (- 1/2) \\cdot \\beta_4 + (- 1/2) \\cdot \\beta_5} \\end{align*}\\] Again, the difference is a sum of weighted group deflections. The coefficients on the group deflections have the properties that they sum to zero, with the positive coefficients summing to \\(+1\\) and the negative coefficients summing to \\(−1\\). Such a combination is called a contrast. The differences can also be expressed in terms of effect size, by dividing the difference by \\(\\sigma_y\\) at each step in the chain. (pp. 565–566) To warm up, here’s how to compute the first contrast shown in the lower portion of Kruschke’s Figure 19.3–the contrast between the two pregnant conditions and the none-control condition. draws %&gt;% transmute(c = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant8,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) %&gt;% ggplot(aes(x = c, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_color = pp[5], slab_fill = pp[5], color = pp[4]) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Pregnant1.Pregnant8 vs None0&quot;, x = expression(Difference~(mu[1]+mu[2])/2-mu[3])) Up to this point, our primary mode of showing marginal posterior distributions has either been minute variations on Kruschke’s typical histogram approach or with densities. We’ll use those again in the future, too. In this chapter and the next, we’ll veer a little further from the source material and depict our marginal posteriors with dot plots and their very close relatives, quantile plots. In the dot plot, above, each of the 4,000 posterior draws is depicted by one of the stacked brown dots. To stack the dots in neat columns like that, tidybayes has to round a little. Though we lose something in the numeric precision, we gain a lot in interpretability. We’ll have more to say in just a moment. In case you were curious, here are the HMC-based posterior mode and 95% HDIs. draws %&gt;% transmute(difference = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant8,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) %&gt;% mode_hdi(difference) ## # A tibble: 1 × 6 ## difference .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.0876 -6.69 7.25 0.95 mode hdi Little difference, there. Now let’s quantify the same contrast as a standardized mean difference effect size. draws %&gt;% transmute(es = ((`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant8,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) / sigma) %&gt;% ggplot(aes(x = es, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Pregnant1.Pregnant8 vs None0&quot;, x = expression(Effect~Size~(Difference/sigma[italic(y)]))) From a standardized-mean-difference perspective, that’s tiny. Also note that because our model fit19.1 did not allow the standard deviation parameter \\(\\sigma_y\\) to vary across groups, \\(\\sigma_y\\) is effectively a pooled standard deviation (\\(\\sigma_p\\)). Did you notice the quantiles = 100 argument within stat_dotsinterval()? Instead of a dot plot with 4,000 tiny little dots, that argument converted the output to a quantile plot. The 4,000 posterior draws are now summarized by 100 dots, each of which represents \\(1\\%\\) of the total sample Fernandes et al. (2018). This quantile dot-plot method will be our main approach for the rest of the chapter. Okay, now let’s do the rest in bulk. First we’ll do the difference scores. differences &lt;- draws %&gt;% transmute(`Pregnant1.Pregnant8.None0 vs Virgin1` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant8,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - `r_CompanionNumber[Virgin1,Intercept]`, `Virgin1 vs Virgin8` = `r_CompanionNumber[Virgin1,Intercept]` - `r_CompanionNumber[Virgin8,Intercept]`, `Pregnant1.Pregnant8.None0 vs Virgin1.Virgin8` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant8,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - (`r_CompanionNumber[Virgin1,Intercept]` + `r_CompanionNumber[Virgin8,Intercept]`) / 2) differences %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + facet_wrap(~ name, scales = &quot;free&quot;) Because we save our data wrangling labor from above as differences, it won’t take much more effort to compute and plot the corresponding effect sizes as displayed in the bottom row of Figure 19.3. differences %&gt;% mutate_all(.funs = ~ . / draws$sigma) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Effect Size (Standardized mean difference)&quot;) + facet_wrap(~ name, scales = &quot;free_x&quot;) In traditional ANOVA, analysts often perform a so-called omnibus test that asks whether it is plausible that all the groups are simultaneously exactly equal. I find that the omnibus test is rarely meaningful, however…. In the hierarchical Bayesian estimation used here, there is no direct equivalent to an omnibus test in ANOVA, and the emphasis is on examining all the meaningful contrasts. (p. 567) Speaking of all meaningful contrasts, if you’d like to make all pairwise comparisons in a hierarchical model of this form, tidybayes offers a convenient way to do so (see the Comparing levels of a factor section of Kay, 2021). Here we’ll demonstrate with stat_dotsinterval(). fit19.1 %&gt;% # these two lines are where the magic is at spread_draws(r_CompanionNumber[CompanionNumber,]) %&gt;% compare_levels(r_CompanionNumber, by = CompanionNumber) %&gt;% ggplot(aes(x = r_CompanionNumber, y = CompanionNumber)) + geom_vline(xintercept = 0, color = pp[12]) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + labs(x = &quot;Contrast&quot;, y = NULL) + coord_cartesian(ylim = c(1.5, 10.5)) + theme(axis.text.y = element_text(hjust = 0)) But back to that omnibus test notion. If you really wanted to, I suppose one rough analogue would be to use information criteria to compare the hierarchical model to one that includes a single intercept with no group-level deflections. Here’s what the simpler model would look like. fit19.2 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 1, prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, stanvars = stanvars, file = &quot;fits/fit19.02&quot;) Check the model summary. print(fit19.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 1 ## Data: my_data (Number of observations: 125) ## Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup draws = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 57.46 1.58 54.38 60.51 1.00 9769 8125 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 17.66 1.15 15.60 20.14 1.00 10864 7660 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here are their LOO values and their difference score. fit19.1 &lt;- add_criterion(fit19.1, criterion = &quot;loo&quot;) fit19.2 &lt;- add_criterion(fit19.2, criterion = &quot;loo&quot;) loo_compare(fit19.1, fit19.2) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit19.1 0.0 0.0 -517.7 7.0 5.5 0.6 1035.4 13.9 ## fit19.2 -19.3 5.3 -537.0 7.1 1.8 0.3 1074.1 14.2 The hierarchical model has a better LOO. Here are the stacking-based model weights. (mw &lt;- model_weights(fit19.1, fit19.2)) ## fit19.1 fit19.2 ## 9.999984e-01 1.583495e-06 If you don’t like scientific notation, just round(). mw %&gt;% round(digits = 3) ## fit19.1 fit19.2 ## 1 0 Yep, in complimenting the LOO difference, virtually all the stacking weight went to the hierarchical model. You might think of this another way. The conceptual question we’re asking is: Does it make sense to say that the \\(\\sigma_\\beta\\) parameter is zero? Is zero a credible value? We’ll, I suppose we could just look at the posterior to assess for that. draws %&gt;% ggplot(aes(x = sd_CompanionNumber__Intercept, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 50)) + labs(title = expression(&quot;Behold the fit19.1 posterior for &quot;*sigma[beta]*&quot;.&quot;), subtitle = &quot;This parameter&#39;s many things, but zero isn&#39;t one of them.&quot;, x = NULL) Yeah, zero and other values close to zero don’t look credible for that parameter. 95% of the mass is between 5 and 30, with the bulk hovering around 10. We don’t need an \\(F\\)-test or even a LOO model comparison to see the writing on wall. 19.3.4 Multiple comparisons and shrinkage. The previous section suggested that an analyst should investigate all contrasts of interest. This recommendation can be thought to conflict with traditional advice in the context on null hypothesis significance testing, which instead recommends that a minimal number of comparisons should be conducted in order to maximize the power of each test while keeping the overall false alarm rate capped at 5% (or whatever maximum is desired)…. Instead, a Bayesian analysis can mitigate false alarms by incorporating prior knowledge into the model. In particular, hierarchical structure (which is an expression of prior knowledge) produces shrinkage of estimates, and shrinkage can help rein in estimates of spurious outlying data. For example, in the posterior distribution from the fruit fly data, the modal values of the posterior group means have a range of \\(23.2\\). The sample means of the groups have a range of \\(26.1\\). Thus, there is some shrinkage in the estimated means. The amount of shrinkage is dictated only by the data and by the prior structure, not by the intended tests. (p. 568) We may as well compute those ranges by hand. Here’s the range of the observed data. my_data %&gt;% group_by(CompanionNumber) %&gt;% summarise(mean = mean(Longevity)) %&gt;% summarise(range = max(mean) - min(mean)) ## # A tibble: 1 × 1 ## range ## &lt;dbl&gt; ## 1 26.1 For our hierarchical model fit19.1, the posterior means are rank ordered in the same way as the empirical data. coef(fit19.1)$CompanionNumber[, , &quot;Intercept&quot;] %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;companion_number&quot;) %&gt;% arrange(Estimate) %&gt;% mutate_if(is.double, round, digits = 1) ## companion_number Estimate Est.Error Q2.5 Q97.5 ## 1 Virgin8 39.9 3.0 33.8 45.7 ## 2 Virgin1 56.8 2.9 51.2 62.5 ## 3 Pregnant8 63.0 2.9 57.3 68.8 ## 4 None0 63.2 2.9 57.4 69.0 ## 5 Pregnant1 64.3 3.0 58.5 70.2 If we compute the range by a difference of the point estimates of the highest and lowest posterior means, we can get a quick number. coef(fit19.1)$CompanionNumber[, , &quot;Intercept&quot;] %&gt;% as_tibble() %&gt;% summarise(range = max(Estimate) - min(Estimate)) ## # A tibble: 1 × 1 ## range ## &lt;dbl&gt; ## 1 24.5 Note that wasn’t fully Bayesian of us. Those means and their difference carry uncertainty and that uncertainty can be fully expressed if we use all the posterior draws (i.e., use summary = F and wrangle). coef(fit19.1, summary = F)$CompanionNumber[, , &quot;Intercept&quot;] %&gt;% as_tibble() %&gt;% transmute(range = Pregnant1 - Virgin8) %&gt;% mode_hdi(range) ## # A tibble: 1 × 6 ## range .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 23.5 16.0 32.8 0.95 mode hdi Happily, the central tendency of the range is near equivalent with both methods, but now we have 95% intervals, too. Do note how wide they are. This is why we work with the full set of posterior draws. 19.3.5 The two-group case. A special case of our current scenario is when there are only two groups. The model of the present section could, in principle, be applied to the two-group case, but the hierarchical structure would do little good because there is virtually no shrinkage when there are so few groups (and the top-level prior on \\(\\sigma_\\beta\\) is broad as assumed here). (p. 568) For kicks and giggles, let’s practice. Since Pregnant1 and Virgin8 had the highest and lowest empirical means—making them the groups best suited to define our range, we’ll use them to fit the 2-group hierarchical model. To fit it with haste, just use update(). fit19.3 &lt;- update(fit19.1, newdata = my_data %&gt;% filter(CompanionNumber %in% c(&quot;Pregnant1&quot;, &quot;Virgin8&quot;)), seed = 19, file = &quot;fits/fit19.03&quot;) ## The desired updates require recompiling the model Even with just two groups, there were no gross issues with fitting the model. print(fit19.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 1 + (1 | CompanionNumber) ## Data: my_data %&gt;% filter(CompanionNumber %in% c(&quot;Pregnan (Number of observations: 50) ## Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup draws = 12000 ## ## Group-Level Effects: ## ~CompanionNumber (Number of levels: 2) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 32.27 22.75 8.34 92.69 1.00 2934 4189 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 52.43 23.66 3.31 104.56 1.00 2939 3057 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 14.23 1.47 11.70 17.50 1.00 6194 5825 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you compare the posteriors for \\(\\sigma_\\beta\\) across the two models, you’ll see how the one for fit19.3 is substantially larger. posterior_summary(fit19.1)[&quot;sd_CompanionNumber__Intercept&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 14.915311 7.568772 6.268457 35.819113 posterior_summary(fit19.3)[&quot;sd_CompanionNumber__Intercept&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 32.268681 22.754157 8.342271 92.686976 Here that is in a coefficient plot using tidybayes::stat_interval(). bind_rows(as_draws_df(fit19.1) %&gt;% select(sd_CompanionNumber__Intercept), as_draws_df(fit19.3) %&gt;% select(sd_CompanionNumber__Intercept)) %&gt;% mutate(fit = rep(c(&quot;fit19.1&quot;, &quot;fit19.3&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = sd_CompanionNumber__Intercept, y = fit)) + stat_interval(point_interval = mode_hdi, .width = c(.5, .8, .95)) + scale_color_manual(values = pp[c(11, 5, 7)], labels = c(&quot;95%&quot;, &quot;80%&quot;, &quot;50%&quot;)) + scale_x_continuous(expression(sigma[beta]), limits = c(0, NA), expand = expansion(mult = c(0, 0.05))) + ylab(NULL) + theme(legend.key.size = unit(0.45, &quot;cm&quot;)) This all implies less shrinkage and a larger range. coef(fit19.3, summary = F)$CompanionNumber[, , &quot;Intercept&quot;] %&gt;% as_tibble() %&gt;% transmute(range = Pregnant1 - Virgin8) %&gt;% mode_hdi(range) ## # A tibble: 1 × 6 ## range .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 25.5 17.5 33.3 0.95 mode hdi And indeed, the range between the two groups is larger. Now the posterior mode for their difference has almost converged to that of the raw data. Kruschke then went on to recommend using a single-level model in such situations, instead. That is why the two-group model in Section 16.3 did not use hierarchical structure, as illustrated in Figure 16.11 (p. 468). That model also used a \\(t\\) distribution to accommodate outliers in the data, and that model allowed for heterogeneous variances across groups. Thus, for two groups, it is more appropriate to use the model of Section 16.3. The hierarchical multi-group model is generalized to accommodate outliers and heterogeneous variances in Section 19.5. (p. 568) As a refresher, here’s what the brms code for that Chapter 16 model looked like. fit16.3 &lt;- brm(data = my_data, family = student, bf(Score ~ 0 + Group, sigma ~ 0 + Group), prior = c(prior(normal(mean_y, sd_y * 100), class = b), prior(normal(0, log(sd_y)), class = b, dpar = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvars, seed = 16, file = &quot;fits/fit16.03&quot;) Let’s adjust it for our data. Since we have a reduced data set, we’ll need to re-compute our stanvars values, which were based on the raw data. # it&#39;s easier to just make a reduced data set my_small_data &lt;- my_data %&gt;% filter(CompanionNumber %in% c(&quot;Pregnant1&quot;, &quot;Virgin8&quot;)) (mean_y &lt;- mean(my_small_data$Longevity)) ## [1] 51.76 (sd_y &lt;- sd(my_small_data$Longevity)) ## [1] 19.11145 omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y (s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)) ## $shape ## [1] 1.283196 ## ## $rate ## [1] 0.02963623 Here we update stanvars. stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) Note that our priors, here, are something of a blend of those from Chapter 16 and those from our hierarchical model, fit19.1. fit19.4 &lt;- brm(data = my_small_data, family = student, bf(Longevity ~ 0 + CompanionNumber, sigma ~ 0 + CompanionNumber), prior = c(prior(normal(mean_y, sd_y * 10), class = b), prior(normal(0, log(sd_y)), class = b, dpar = sigma), prior(exponential(one_over_twentynine), class = nu)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, stanvars = stanvars, file = &quot;fits/fit19.04&quot;) Here’s the model summary. print(fit19.4) ## Family: student ## Links: mu = identity; sigma = log; nu = identity ## Formula: Longevity ~ 0 + CompanionNumber ## sigma ~ 0 + CompanionNumber ## Data: my_small_data (Number of observations: 50) ## Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup draws = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## CompanionNumberPregnant1 64.66 3.29 58.12 71.07 1.00 12528 8120 ## CompanionNumberVirgin8 38.79 2.54 33.79 43.71 1.00 12367 8561 ## sigma_CompanionNumberPregnant1 2.74 0.15 2.44 3.05 1.00 13120 9473 ## sigma_CompanionNumberVirgin8 2.48 0.16 2.18 2.80 1.00 14053 9013 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## nu 39.41 30.75 5.96 120.03 1.00 12705 8742 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Man, look at those Bulk_ESS values! As it turns out, they can be greater than the number of post-warmup samples. And here’s the range in posterior means. fixef(fit19.4, summary = F) %&gt;% as_tibble() %&gt;% transmute(range = CompanionNumberPregnant1 - CompanionNumberVirgin8) %&gt;% mode_hdi(range) ## # A tibble: 1 × 6 ## range .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 25.6 17.9 34.1 0.95 mode hdi The results are pretty much the same as that of the two-group hierarchical model, maybe a touch larger. Yep, Kruschke was right. Hierarchical models with two groups and permissive priors on \\(\\sigma_\\beta\\) don’t shrink the estimates to the grand mean all that much. 19.4 Including a metric predictor “In Figure 19.3, the data within each group have a large standard deviation. For example, longevities in the Virgin8 group range from \\(20\\) to \\(60\\) days” (p. 568). Turns out Kruschke’s slightly wrong on this. Probably just a typo. my_data %&gt;% group_by(CompanionNumber) %&gt;% summarise(min = min(Longevity), max = max(Longevity), range = max(Longevity) - min(Longevity)) ## # A tibble: 5 × 4 ## CompanionNumber min max range ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 None0 37 96 59 ## 2 Pregnant1 42 97 55 ## 3 Pregnant8 35 86 51 ## 4 Virgin1 21 81 60 ## 5 Virgin8 16 60 44 But you get the point. For each group, there was quite a range. We might add predictors to the model to help account for those ranges. The additional metric predictor is sometimes called a covariate. In the experimental setting, the focus of interest is usually on the nominal predictor (i.e., the experimental treatments), and the covariate is typically thought of as an ancillary predictor to help isolate the effect of the nominal predictor. But mathematically the nominal and metric predictors have equal status in the model. Let’s denote the value of the metric covariate for subject \\(i\\) as \\(x_\\text{cov}(i)\\). Then the expected value of the predicted variable for subject \\(i\\) is \\[\\mu (i) = \\beta_0 + \\sum_j \\beta_{[j]} x_{[j]} (i) + \\beta_\\text{cov} x_\\text{cov}(i)\\] with the usual sum-to-zero constraint on the deflections of the nominal predictor stated in Equation 19.2. In words, Equation 19.5 says that the predicted value for subject \\(i\\) is a baseline plus a deflection due to the group of \\(i\\) plus a shift due to the value of \\(i\\) on the covariate. (p. 569) And the \\(j\\) subscript, recall, denotes group membership. In this context, it often makes sense to set the intercept as the mean of predicted values if the covariate is re-centered at its mean value, which is denoted \\(\\overline x_\\text{cov}\\). Therefore Equation 19.5 is algebraically reformulated to make the baseline respect those constraints…. The first equation below is simply Equation 19.5 with \\(x_\\text{cov}\\) recentered on its mean, \\(\\overline x_\\text{cov}\\). The second line below merely algebraically rearranges the terms so that the nominal deflections sum to zero and the constants are combined into the overall baseline: \\[\\begin{align*} \\mu &amp; = \\alpha_0 + \\sum_j \\alpha_{[j]} x_{[j]} + \\alpha_\\text{cov} (x_\\text{cov} - \\overline{x}_\\text{cov}) \\\\ &amp; = \\underbrace{\\alpha_0 + \\overline{\\alpha} - \\alpha_\\text{cov} \\overline{x}_\\text{cov}}_{\\beta_0} + \\sum_j \\underbrace{(\\alpha_{[j]} - \\overline{\\alpha})}_{\\beta_[j]} x_{[j]} + \\underbrace{\\alpha_\\text{cov}}_{\\beta_{\\text{cov}}} x_\\text{cov} \\\\ &amp; \\text{where } \\overline{\\alpha} = \\frac{1}{J} \\sum^J_{j = 1} \\alpha_{[j]} \\end{align*}\\] (pp. 569–570) We have a visual depiction of all this in the hierarchical model diagram of Figure 19.4. # bracket p1 &lt;- tibble(x = .99, y = .5, label = &quot;{_}&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, hjust = 1, color = pp[8], family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # plain arrow p2 &lt;- tibble(x = .71, y = 1, xend = .68, yend = .25) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # normal density p3 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # second normal density p4 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;0&quot;, &quot;sigma[beta]&quot;), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # third density p5 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;italic(M)[italic(c)]&quot;, &quot;italic(S)[italic(c)]&quot;), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # three annotated arrows p6 &lt;- tibble(x = c(.09, .49, .9), y = c(1, 1, 1), xend = c(.20, .40, .64), yend = c(0, 0, 0)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(.11, .42, .47, .74), y = .5, label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;, &quot;&#39;~&#39;&quot;), size = c(10, 10, 7, 10), color = pp[4], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # likelihood formula p7 &lt;- tibble(x = .99, y = .25, label = &quot;beta[0]+sum()[italic(j)]*beta[&#39;[&#39;*italic(j)*&#39;]&#39;]*italic(x)[&#39;[&#39;*italic(j)*&#39;]&#39;](italic(i))+beta[italic(cov)]*italic(x)[italic(cov)](italic(i))&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(hjust = 1, size = 7, color = pp[4], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # half-normal density p8 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma]&quot;, size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # annotated arrow p9 &lt;- tibble(x = .38, y = .65, label = &quot;&#39;=&#39;&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, color = pp[4], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = .25, arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # the fourth normal density p10 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;mu[italic(i)]&quot;, &quot;sigma[italic(y)]&quot;), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # another annotated arrow p11 &lt;- tibble(x = .5, y = .6, label = &quot;&#39;~&#39;&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, color = pp[4], parse = T, family = &quot;Times&quot;) + geom_segment(x = .85, xend = .27, y = 1, yend = .2, arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # the final annotated arrow p12 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = pp[4], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # some text p13 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pp[4], parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 1, l = 6, r = 7), area(t = 2, b = 3, l = 6, r = 7), area(t = 3, b = 4, l = 1, r = 3), area(t = 3, b = 4, l = 5, r = 7), area(t = 3, b = 4, l = 9, r = 11), area(t = 6, b = 7, l = 1, r = 9), area(t = 5, b = 6, l = 1, r = 11), area(t = 6, b = 7, l = 11, r = 13), area(t = 9, b = 10, l = 5, r = 7), area(t = 8, b = 9, l = 5, r = 7), area(t = 8, b = 9, l = 5, r = 13), area(t = 11, b = 11, l = 5, r = 7), area(t = 12, b = 12, l = 5, r = 7) ) # combine and plot! (p1 + p2 + p3 + p4 + p5 + p7 + p6 + p8 + p10 + p9 + p11 + p12 + p13) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) 19.4.1 Example: Sex, death, and size. Kruschke recalled fit19.1’s estimate for \\(\\sigma_y\\) had a posterior mode around 14.8. Let’s confirm with a plot. as_draws_df(fit19.1) %&gt;% ggplot(aes(x = sigma, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma[italic(y)])) + theme(panel.grid = element_blank()) Yep, that looks about right. That large of a difference in days would indeed make it difficult to detect between-group differences if those differences were typically on the scale of just a few days. Since Thorax is moderately correlated with Longevity, including Thorax in the statistical model should help shrink that \\(\\sigma_y\\) estimate, making it easier to compare group means. Following the sensibilities from the equations just above, here we’ll mean-center our covariate, first. my_data &lt;- my_data %&gt;% mutate(thorax_c = Thorax - mean(Thorax)) head(my_data) ## # A tibble: 6 × 4 ## Longevity CompanionNumber Thorax thorax_c ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 35 Pregnant8 0.64 -0.181 ## 2 37 Pregnant8 0.68 -0.141 ## 3 49 Pregnant8 0.68 -0.141 ## 4 46 Pregnant8 0.72 -0.101 ## 5 63 Pregnant8 0.72 -0.101 ## 6 39 Pregnant8 0.76 -0.0610 Our model code follows the structure of that in Kruschke’s Jags-Ymet-Xnom1met1-MnormalHom-Example.R and Jags-Ymet-Xnom1met1-MnormalHom.R files. As a preparatory step, we redefine the values necessary for stanvars. (mean_y &lt;- mean(my_data$Longevity)) ## [1] 57.44 (sd_y &lt;- sd(my_data$Longevity)) ## [1] 17.56389 (sd_thorax_c &lt;- sd(my_data$thorax_c)) ## [1] 0.07745367 omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y (s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)) ## $shape ## [1] 1.283196 ## ## $rate ## [1] 0.03224747 stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(sd_thorax_c, name = &quot;sd_thorax_c&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Now we’re ready to fit the brm() model, our hierarchical alternative to ANCOVA. fit19.5 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 1 + thorax_c + (1 | CompanionNumber), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(normal(0, 2 * sd_y / sd_thorax_c), class = b), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, control = list(adapt_delta = 0.99), stanvars = stanvars, file = &quot;fits/fit19.05&quot;) Here’s the model summary. print(fit19.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 1 + thorax_c + (1 | CompanionNumber) ## Data: my_data (Number of observations: 125) ## Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup draws = 12000 ## ## Group-Level Effects: ## ~CompanionNumber (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 14.04 7.33 6.05 33.04 1.00 2607 3631 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 57.53 7.10 43.16 71.89 1.00 2459 3023 ## thorax_c 136.36 12.68 111.54 160.94 1.00 7527 6492 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 10.60 0.69 9.36 12.05 1.00 7211 6816 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s see if that \\(\\sigma_y\\) posterior shrank. as_draws_df(fit19.5) %&gt;% ggplot(aes(x = sigma, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma[italic(y)])) Yep, sure did! Now our between-group comparisons should be more precise. Heck, if we wanted to we could even make a difference plot. tibble(sigma1 = as_draws_df(fit19.1) %&gt;% pull(sigma), sigma5 = as_draws_df(fit19.5) %&gt;% pull(sigma)) %&gt;% transmute(dif = sigma1 - sigma5) %&gt;% ggplot(aes(x = dif, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;This is a difference distribution&quot;, x = expression(sigma[italic(y)][&quot; | fit19.1&quot;]-sigma[italic(y)][&quot; | fit19.5&quot;])) If you want a quick and dirty plot of the relation between thorax_c and Longevity, you might employ brms::conditional_effects(). conditional_effects(fit19.5) %&gt;% plot(line_args = list(color = pp[5], fill = pp[11])) But to make plots like the ones at the top of Figure 19.5, we’ll have to work a little harder. First, we need some intermediary values marking off the three values along the Thorax-axis Kruschke singled out in his top panel plots. As far as I can tell, they were the min(), the max(), and their mean(). (r &lt;- range(my_data$Thorax)) ## [1] 0.64 0.94 mean(r) ## [1] 0.79 Next, we’ll make the data necessary for our side-tipped Gaussians. For kicks and giggles, we’ll choose 80 draws instead of 20. But do note how we used our r values, from above, to specify both Thorax and thorax_c values in addition to the CompanionNumber categories for the newdata argument. Otherwise, this workflow is very much the same as in previous plots. n_draws &lt;- 80 densities &lt;- my_data %&gt;% distinct(CompanionNumber) %&gt;% expand(CompanionNumber, Thorax = c(r[1], mean(r), r[2])) %&gt;% mutate(thorax_c = Thorax - mean(my_data$Thorax)) %&gt;% add_epred_draws(fit19.5, ndraws = n_draws, seed = 19, dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Longevity = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Longevity) %&gt;% mutate(density = dnorm(Longevity, mu, sigma)) glimpse(densities) ## Rows: 120,000 ## Columns: 14 ## Groups: CompanionNumber, Thorax, thorax_c, .row [15] ## $ CompanionNumber &lt;chr&gt; &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;N… ## $ Thorax &lt;dbl&gt; 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.… ## $ thorax_c &lt;dbl&gt; -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.1… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .draw &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ .epred &lt;dbl&gt; 36.23788, 36.23788, 36.23788, 36.23788, 36.23788, 36.23788, 36.23788, 36.2… ## $ mu &lt;dbl&gt; 36.23788, 36.23788, 36.23788, 36.23788, 36.23788, 36.23788, 36.23788, 36.2… ## $ sigma &lt;dbl&gt; 10.73103, 10.73103, 10.73103, 10.73103, 10.73103, 10.73103, 10.73103, 10.7… ## $ ll &lt;dbl&gt; 15.20545, 15.20545, 15.20545, 15.20545, 15.20545, 15.20545, 15.20545, 15.2… ## $ ul &lt;dbl&gt; 57.27032, 57.27032, 57.27032, 57.27032, 57.27032, 57.27032, 57.27032, 57.2… ## $ Longevity &lt;dbl&gt; 15.20545, 15.63034, 16.05524, 16.48014, 16.90504, 17.32994, 17.75483, 18.1… ## $ density &lt;dbl&gt; 0.005446361, 0.005881248, 0.006340911, 0.006825791, 0.007336239, 0.0078725… Here, we’ll use a simplified workflow to extract the fitted() values in order to make the regression lines. Since these are straight lines, all we need are two values for each draw, one at the extremes of the Thorax axis. f &lt;- my_data %&gt;% distinct(CompanionNumber) %&gt;% expand(CompanionNumber, Thorax = c(r[1], mean(r), r[2])) %&gt;% mutate(thorax_c = Thorax - mean(my_data$Thorax)) %&gt;% add_epred_draws(fit19.5, ndraws = n_draws, seed = 19, value = &quot;Longevity&quot;) glimpse(f) ## Rows: 1,200 ## Columns: 8 ## Groups: CompanionNumber, Thorax, thorax_c, .row [15] ## $ CompanionNumber &lt;chr&gt; &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;N… ## $ Thorax &lt;dbl&gt; 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.… ## $ thorax_c &lt;dbl&gt; -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.1… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .draw &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,… ## $ Longevity &lt;dbl&gt; 36.23788, 35.72981, 36.47699, 38.54599, 32.58731, 40.84551, 34.12519, 35.5… Now we’re ready to make our plots for the top row of Figure 19.3. densities %&gt;% ggplot(aes(x = Longevity, y = Thorax)) + # the Gaussians geom_ridgeline(aes(height = -density, group = interaction(Thorax, .draw)), fill = NA, size = 1/5, scale = 5/3, color = adjustcolor(pp[4], alpha.f = 1/5), min_height = NA) + # the vertical lines below the Gaussians geom_line(aes(group = interaction(Thorax, .draw)), color = pp[4], alpha = 1/5, size = 1/5) + # the regression lines geom_line(data = f, aes(group = .draw), alpha = 1/5, size = 1/5, color = pp[4]) + # the data geom_point(data = my_data, alpha = 3/4, color = pp[10]) + coord_flip(xlim = c(0, 110), ylim = c(.58, 1)) + facet_wrap(~ CompanionNumber, ncol = 5) Now we have a covariate in the model, we have to decide on which of its values we want to base our group comparisons. Unless there’s a substantive reason for another value, the mean is a good standard choice. And since the covariate thorax_c is already mean centered, that means we can effectively leave it out of the equation. Here we make and save them in the simple difference metric. draws &lt;- as_draws_df(fit19.5) differences &lt;- draws %&gt;% transmute(`Pregnant1.Pregnant8 vs None0` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`, `Pregnant1.Pregnant8.None0 vs Virgin1` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - `r_CompanionNumber[Virgin1,Intercept]`, `Virgin1 vs Virgin8` = `r_CompanionNumber[Virgin1,Intercept]` - `r_CompanionNumber[Virgin8,Intercept]`, `Pregnant1.Pregnant8.None0 vs Virgin1.Virgin8` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - (`r_CompanionNumber[Virgin1,Intercept]` + `r_CompanionNumber[Virgin8,Intercept]`) / 2) p1 &lt;- differences %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + theme(strip.text = element_text(size = 6.4)) + facet_wrap(~ name, scales = &quot;free_x&quot;, ncol = 4) Now we’ll look at the differences in the effect size metric. Since we saved our leg work above, it’s really easy to just convert the differences in bulk with mutate_all(). After the conversion, we’ll bind the two rows of subplots together with a little patchwork and display the results. p2 &lt;- differences %&gt;% mutate_all(.funs = ~. / draws$sigma) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Effect Size&quot;) + theme(strip.text = element_text(size = 6.4)) + facet_wrap(~ name, scales = &quot;free_x&quot;, ncol = 4) # combine p1 / p2 “The HDI widths of all the contrasts have gotten smaller by virtue of including the covariate in the analysis” (p. 571). 19.4.2 Analogous to traditional ANCOVA. In contrast with ANCOVA, Bayesian methods do not partition the least-squares variance to make estimates, and therefore the Bayesian method is analogous to ANCOVA but is not ANCOVA. Frequentist practitioners are urged to test (with \\(p\\) values) whether the assumptions of (a) equal slope in all groups, (b) equal standard deviation in all groups, and (c) normally distributed noise can be rejected. In a Bayesian approach, the descriptive model is generalized to address these concerns, as will be discussed in Section 19.5. (p. 572) 19.4.3 Relation to hierarchical linear regression. Here Kruschke contrasts our last model with the one from way back in Section 17.3. As a refresher, here’s what that code looked like. fit17.4 &lt;- brm(data = my_data, family = student, y_z ~ 1 + x_z + (1 + x_z || Subj), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 1), class = sigma), # the next line is new prior(normal(0, 1), class = sd), prior(exponential(one_over_twentynine) + 1, class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 17, file = &quot;fits/fit17.04&quot;) And for convenience, here’s the code from the model we just fit. fit19.5 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 1 + thorax_c + (1 | CompanionNumber), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(normal(0, 2 * sd_y / sd_thorax_c), class = b), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, control = list(adapt_delta = 0.99), stanvars = stanvars, file = &quot;fits/fit19.05&quot;) It’s easy to get lost in the differences in the priors and the technical details with the model chains and such. The main thing to notice, here, is the differences in the model formulas (i.e., the likelihoods). Both models had intercepts and slopes. But whereas the model from 17.3 set both parameters to random, only the intercept in our last model was random. The covariate thorax_c was fixed–it did not vary by group. Had we wanted it to, our formula syntax would have been something like Longevity ~ 1 + thorax_c + (1 + thorax_c || CompanionNumber). And again, as noted in Section 17.3.1, the || portion of the syntax set the random intercepts and slopes to be orthogonal (i.e., correlate exactly at zero). As we’ll see, this will often not be the case. But let’s not get ahead of ourselves. Conceptually, the main difference between the models is merely the focus of attention. In the hierarchical linear regression model, the focus was on the slope coefficient. In that case, we were trying to estimate the magnitude of the slope, simultaneously for individuals and overall. The intercepts, which describe the levels of the nominal predictor, were of ancillary interest. In the present section, on the other hand, the focus of attention is reversed. We are most interested in the intercepts and their differences between groups, with the slopes on the covariate being of ancillary interest. (p. 573) 19.5 Heterogeneous variances and robustness against outliers In Figure 19.6 on page 574, Kruschke laid out the diagram for a hierarchical Student’s-\\(t\\) model in for which both the \\(\\mu\\) and \\(\\sigma\\) parameters are random. If you recall, Bürkner (2022a) calls these distributional models and they are indeed available within the brms framework. But there’s a catch. Though we can model \\(\\sigma\\) all day long and we can even make it hierarchical, brms limits us to modeling the hierarchical \\(\\sigma\\) parameters within the typical Gaussian framework. That is, we will depart from Kruschke’s schematic in that we will be modeling the log of \\(\\sigma\\), indicating its grand mean with the sigma ~ 1 syntax, modeling the group-level deflections as Gaussian with a mean of 0 and standard deviation \\(\\sigma_\\sigma\\) estimated from the data, and choosing a sensible prior for \\(\\sigma_\\sigma\\) that is left-bound at 0 and gently slopes to the right (i.e., a folded \\(t\\) or gamma distribution). Thus, here’s our brms-centric variant of the diagram in Figure 19.6. # bracket p1 &lt;- tibble(x = .99, y = .5, label = &quot;{_}&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, hjust = 1, color = pp[8], family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # plain arrow p2 &lt;- tibble(x = .68, y = 1, xend = .68, yend = .25) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # normal density p3 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # second normal density p4 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;0&quot;, &quot;sigma[beta]&quot;), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # third normal density p5 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(0, 1.15), y = .6, label = c(&quot;italic(M)[mu[sigma]]&quot;, &quot;italic(S)[mu[sigma]]&quot;), hjust = c(.5, 0), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # half-normal density p6 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 6, color = pp[4]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma[sigma]]&quot;, size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # exponential density p7 &lt;- tibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;exp&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(K)&quot;, size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # likelihood formula p8 &lt;- tibble(x = .5, y = .25, label = &quot;beta[0]+sum()[italic(j)]*beta[&#39;[&#39;*italic(j)*&#39;]&#39;]*italic(x)[&#39;[&#39;*italic(j)*&#39;]&#39;](italic(i))&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pp[4], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # normal density p9 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(0, 1.2), y = .6, hjust = c(.5, 0), label = c(&quot;mu[sigma]&quot;, &quot;sigma[sigma]&quot;), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # four annotated arrows p10 &lt;- tibble(x = c(.06, .37, .67, .95), y = c(1, 1, 1, 1), xend = c(.15, .31, .665, .745), yend = c(0, 0, .2, .2)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(.065, .31, .36, .64, .79), y = .5, label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;, &quot;&#39;~&#39;&quot;, &quot;&#39;~&#39;&quot; ), size = c(10, 10, 7, 10, 10), color = pp[4], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # student-t density p11 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) + geom_area(fill = pp[9]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;student t&quot;, size = 7, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(-1.4, 0), y = .6, label = c(&quot;nu&quot;, &quot;mu[italic(i)]&quot;), size = 7, color = pp[4], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pp[4])) # log sigma p12 &lt;- tibble(x = .65, y = .6, label = &quot;log~sigma[italic(j)*&#39;(&#39;*italic(i)*&#39;)&#39;]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(hjust = 0, size = 7, color = pp[4], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) + ylim(0, 1) + theme_void() # two annotated arrows p13 &lt;- tibble(x = c(.15, .15), y = c(1, .47), xend = c(.15, .72), yend = c(.75, .1)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pp[4]) + annotate(geom = &quot;text&quot;, x = c(.1, .15, .28), y = c(.92, .64, .22), label = c(&quot;&#39;~&#39;&quot;, &quot;nu*minute+1&quot;, &quot;&#39;=&#39;&quot;), size = c(10, 7, 10), color = pp[4], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # one annotated arrow p14 &lt;- tibble(x = .38, y = .65, label = &quot;&#39;=&#39;&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, color = pp[4], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = .15, arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # another annotated arrow p15 &lt;- tibble(x = c(.58, .71), y = .6, label = c(&quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = pp[4], parse = T, family = &quot;Times&quot;) + geom_segment(x = .85, xend = .42, y = 1, yend = .18, arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # the final annotated arrow p16 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = pp[4], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow, color = pp[4]) + xlim(0, 1) + theme_void() # some text p17 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pp[4], parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 1, l = 8, r = 9), area(t = 2, b = 3, l = 8, r = 9), area(t = 3, b = 4, l = 3, r = 5), area(t = 3, b = 4, l = 7, r = 9), area(t = 3, b = 4, l = 11, r = 13), area(t = 3, b = 4, l = 15, r = 17), area(t = 6, b = 7, l = 1, r = 3), area(t = 6, b = 7, l = 5, r = 9), area(t = 6, b = 7, l = 11, r = 13), area(t = 5, b = 6, l = 3, r = 17), area(t = 10, b = 11, l = 6, r = 8), area(t = 10, b = 11, l = 7, r = 9), area(t = 8, b = 10, l = 1, r = 8), area(t = 8, b = 10, l = 6, r = 8), area(t = 8, b = 10, l = 6, r = 13), area(t = 12, b = 12, l = 6, r = 8), area(t = 13, b = 13, l = 6, r = 8) ) # combine and plot! (p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10 + p11 + p12 + p13 + p14 + p15 + p16 + p17) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Since we’re modeling \\(\\log \\left (\\sigma_{j(i)} \\right )\\), we might use Gaussian prior centered on sd(my_data$y) %&gt;% log() and a reasonable spread like 1. We can simulate a little to get a sense of what those distributions look like. n_draws &lt;- 1e3 set.seed(19) tibble(prior = rnorm(n_draws, mean = log(1), sd = 1)) %&gt;% mutate(prior_exp = exp(prior)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + stat_dots(slab_fill = pp[5], slab_color = pp[5]) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + facet_wrap(~ name, scales = &quot;free&quot;) Here’s what is looks like with sd = 2. set.seed(19) tibble(prior = rnorm(n_draws, mean = log(1), sd = 2)) %&gt;% mutate(prior_exp = exp(prior)) %&gt;% ggplot(aes(x = prior_exp)) + stat_dots(slab_fill = pp[5], slab_color = pp[5]) + scale_x_continuous(expand = expansion(mult = c(0, 0.05))) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + coord_cartesian(xlim = c(0, 17)) Though we’re still peaking around 1, there’s more mass in the tail, making it easier for the likelihood to pull away from the prior mode. But all this is the prior on the fixed effect, the grand mean of \\(\\log (\\sigma)\\). Keep in mind we’re also estimating group-level deflections using a hierarchical model. The good old folded \\(t\\) on the unit scale is already pretty permissive for an estimate that is itself on the log scale. To make it more conservative, set \\(\\nu\\) to infinity and go with a folded Gaussian. Or keep your regularization loose and go with a low-\\(\\nu\\) folded \\(t\\) or even a folded Cauchy. And, of course, one could even go with a gamma. Consider we have data my_data for which our primary variable of interest is y. Starting from preparing our stanvars values, here’s what the model code might look like. # get ready for `stanvars` mean_y &lt;- mean(my_data$y) sd_y &lt;- sd(my_data$y) omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma) # define `stanvars` stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) # fit the model fit &lt;- brm(data = my_data, family = student, bf(Longevity ~ 1 + (1 | CompanionNumber), sigma ~ 1 + (1 | CompanionNumber)), prior = c(# grand means prior(normal(mean_y, sd_y * 5), class = Intercept), prior(normal(log(sd_y), 1), class = Intercept, dpar = sigma), # the priors controlling the spread for our hierarchical deflections prior(gamma(alpha, beta), class = sd), prior(normal(0, 1), class = sd, dpar = sigma), # don&#39;t forget our student-t nu prior(exponential(one_over_twentynine), class = nu)), stanvars = stanvars) 19.5.1 Example: Contrast of means with different variances. Let’s load and take a look at Kruschke’s simulated group data. my_data &lt;- read_csv(&quot;data.R/NonhomogVarData.csv&quot;) head(my_data) ## # A tibble: 6 × 2 ## Group Y ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 97.8 ## 2 A 99.9 ## 3 A 92.4 ## 4 A 96.9 ## 5 A 101. ## 6 A 80.7 Here are the means and \\(\\textit{SD}\\)s for each Group. my_data %&gt;% group_by(Group) %&gt;% summarise(mean = mean(Y), sd = sd(Y)) ## # A tibble: 4 × 3 ## Group mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 97 8.00 ## 2 B 99 1.00 ## 3 C 102 1.00 ## 4 D 104. 8 First we’ll fit the model with homogeneous variances. To keep things simple, here we’ll fit a conventional model following the form of our original fit1. Here are our stanvars. (mean_y &lt;- mean(my_data$Y)) ## [1] 100.5 (sd_y &lt;- sd(my_data$Y)) ## [1] 6.228965 omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y (s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)) ## $shape ## [1] 1.283196 ## ## $rate ## [1] 0.09092861 # define `stanvars` stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Now fit the ANOVA-like homogeneous-variances model. fit19.6 &lt;- brm(data = my_data, family = gaussian, Y ~ 1 + (1 | Group), prior = c(prior(normal(mean_y, sd_y * 10), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, control = list(adapt_delta = .995), stanvars = stanvars, file = &quot;fits/fit19.06&quot;) Here’s the model summary. print(fit19.6) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Y ~ 1 + (1 | Group) ## Data: my_data (Number of observations: 96) ## Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup draws = 12000 ## ## Group-Level Effects: ## ~Group (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 4.85 3.45 1.49 13.81 1.00 2325 3508 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 100.53 2.85 94.60 106.38 1.00 2447 2774 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.76 0.43 4.99 6.66 1.00 6178 5676 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s get ready to make our version of the top of Figure 19.7. First we wrangle. # how many model-implied Gaussians would you like? n_draws &lt;- 20 densities &lt;- my_data %&gt;% distinct(Group) %&gt;% add_epred_draws(fit19.6, ndraws = n_draws, seed = 19, dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Y = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Y) %&gt;% mutate(density = dnorm(Y, mu, sigma)) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) glimpse(densities) ## Rows: 8,000 ## Columns: 12 ## Groups: .draw [20] ## $ Group &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;,… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ .draw &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .epred &lt;dbl&gt; 97.49239, 97.49239, 97.49239, 97.49239, 97.49239, 97.49239, 97.49239, 97.49239,… ## $ mu &lt;dbl&gt; 97.49239, 97.49239, 97.49239, 97.49239, 97.49239, 97.49239, 97.49239, 97.49239,… ## $ sigma &lt;dbl&gt; 6.508508, 6.508508, 6.508508, 6.508508, 6.508508, 6.508508, 6.508508, 6.508508,… ## $ ll &lt;dbl&gt; 84.73595, 84.73595, 84.73595, 84.73595, 84.73595, 84.73595, 84.73595, 84.73595,… ## $ ul &lt;dbl&gt; 110.2488, 110.2488, 110.2488, 110.2488, 110.2488, 110.2488, 110.2488, 110.2488,… ## $ Y &lt;dbl&gt; 84.73595, 84.99366, 85.25136, 85.50907, 85.76677, 86.02448, 86.28219, 86.53989,… ## $ density &lt;dbl&gt; 0.1465288, 0.1582290, 0.1705958, 0.1836410, 0.1973740, 0.2118018, 0.2269281, 0.… In our wrangling code, the main thing to notice is those last two lines. If you look closely to Kruschke’s Gaussians, you’ll notice they all have the same maximum height. Up to this point, ours haven’t. This has to do with technicalities on how densities are scaled. In brief, the wider densities have been shorter. So those last two lines scaled all the densities within the same group to the same metric. Otherwise the code was business as usual. Anyway, here’s our version of the top panel of Figure 19.7. densities %&gt;% ggplot(aes(x = Y, y = Group)) + geom_ridgeline(aes(height = density, group = interaction(Group, .draw)), fill = NA, color = adjustcolor(pp[7], alpha.f = 2/3), size = 1/3, scale = 3/4) + geom_jitter(data = my_data, height = .04, alpha = 3/4, color = pp[10]) + scale_x_continuous(breaks = seq(from = 80, to = 120, by = 10)) + labs(title = &quot;Data with Posterior Predictive Distrib.&quot;, y = NULL) + coord_cartesian(xlim = c(75, 125), ylim = c(1.25, 4.5)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) Here are the difference distributions in the middle of Figure 19.7. draws &lt;- as_draws_df(fit19.6) differences &lt;- draws %&gt;% transmute(`D vs A` = `r_Group[D,Intercept]` - `r_Group[A,Intercept]`, `C vs B` = `r_Group[C,Intercept]` - `r_Group[B,Intercept]`) differences %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;D vs A&quot;, &quot;C vs B&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;Difference&quot;) + facet_wrap(~ name, scales = &quot;free_x&quot;, ncol = 4) Now here are the effect sizes at the bottom of the figure. differences %&gt;% mutate_all(.funs = ~ . / draws$sigma) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;D vs A&quot;, &quot;C vs B&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;Effect Size&quot;) + facet_wrap(~ name, scales = &quot;free_x&quot;, ncol = 4) Oh and remember, if you’d like to get all the possible contrasts in bulk, tidybayes has got your back. fit19.6 %&gt;% spread_draws(r_Group[Group,]) %&gt;% compare_levels(r_Group, by = Group) %&gt;% # these next two lines allow us to reorder the contrasts along the y ungroup() %&gt;% mutate(Group = reorder(Group, r_Group)) %&gt;% ggplot(aes(x = r_Group, y = Group)) + geom_vline(xintercept = 0, color = pp[12]) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + labs(x = &quot;Contrast&quot;, y = NULL) + coord_cartesian(ylim = c(1.5, 6.5)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) But to get back on track, here are the stanvars for the robust hierarchical variances model. stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) Now fit that robust better-than-ANOVA model. fit19.7 &lt;- brm(data = my_data, family = student, bf(Y ~ 1 + (1 | Group), sigma ~ 1 + (1 | Group)), prior = c(# grand means prior(normal(mean_y, sd_y * 10), class = Intercept), prior(normal(log(sd_y), 1), class = Intercept, dpar = sigma), # the priors controlling the spread for our hierarchical deflections prior(gamma(alpha, beta), class = sd), prior(normal(0, 1), class = sd, dpar = sigma), # don&#39;t forget our student-t nu prior(exponential(one_over_twentynine), class = nu)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, control = list(adapt_delta = 0.99, max_treedepth = 12), stanvars = stanvars, file = &quot;fits/fit19.07&quot;) The chains look good. plot(fit19.7, widths = c(2, 3)) Here’s the parameter summary. print(fit19.7) ## Family: student ## Links: mu = identity; sigma = log; nu = identity ## Formula: Y ~ 1 + (1 | Group) ## sigma ~ 1 + (1 | Group) ## Data: my_data (Number of observations: 96) ## Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup draws = 12000 ## ## Group-Level Effects: ## ~Group (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 4.57 3.23 1.29 13.08 1.00 2709 4724 ## sd(sigma_Intercept) 1.22 0.39 0.65 2.17 1.00 4675 5633 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 100.61 2.70 95.53 106.39 1.00 4143 4138 ## sigma_Intercept 1.24 0.53 0.20 2.34 1.00 4405 5926 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## nu 32.49 28.09 4.61 108.89 1.00 10069 7665 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s get ready to make our version of the top of Figure 19.7. First we wrangle. densities &lt;- my_data %&gt;% distinct(Group) %&gt;% add_epred_draws(fit19.7, ndraws = n_draws, seed = 19, dpar = c(&quot;mu&quot;, &quot;sigma&quot;, &quot;nu&quot;)) %&gt;% mutate(ll = qt(.025, df = nu), ul = qt(.975, df = nu)) %&gt;% mutate(Y = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Y) %&gt;% mutate(density = dt(Y, nu)) %&gt;% # notice the conversion mutate(Y = mu + Y * sigma) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) glimpse(densities) ## Rows: 8,000 ## Columns: 13 ## Groups: .draw [20] ## $ Group &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;,… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ .draw &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .epred &lt;dbl&gt; 96.41185, 96.41185, 96.41185, 96.41185, 96.41185, 96.41185, 96.41185, 96.41185,… ## $ mu &lt;dbl&gt; 96.41185, 96.41185, 96.41185, 96.41185, 96.41185, 96.41185, 96.41185, 96.41185,… ## $ sigma &lt;dbl&gt; 7.029854, 7.029854, 7.029854, 7.029854, 7.029854, 7.029854, 7.029854, 7.029854,… ## $ nu &lt;dbl&gt; 17.19399, 17.19399, 17.19399, 17.19399, 17.19399, 17.19399, 17.19399, 17.19399,… ## $ ll &lt;dbl&gt; -2.108004, -2.108004, -2.108004, -2.108004, -2.108004, -2.108004, -2.108004, -2… ## $ ul &lt;dbl&gt; 2.108004, 2.108004, 2.108004, 2.108004, 2.108004, 2.108004, 2.108004, 2.108004,… ## $ Y &lt;dbl&gt; 81.59289, 81.89227, 82.19164, 82.49101, 82.79039, 83.08976, 83.38913, 83.68851,… ## $ density &lt;dbl&gt; 0.1235710, 0.1331992, 0.1434447, 0.1543309, 0.1658803, 0.1781139, 0.1910512, 0.… If you look closely at our code, above, you’ll note switching from the Gaussian to the Student \\(t\\) required changes in our flow. Most obviously, we switched from qnorm() and dnorm() to qt() and dt(), respectively. The base R Student \\(t\\) functions don’t take arguments for \\(\\mu\\) and \\(\\sigma\\). Rather, they’re presumed to be 0 and 1, respectively. That means that for our first three mutate() functions, the computations were all based on the standard Student \\(t\\), with only the \\(\\nu\\) parameter varying according to the posterior. The way we corrected for that was with the fourth mutate(). Now we’re ready to make and save our version of the top panel of Figure 19.7. p1 &lt;- densities %&gt;% ggplot(aes(x = Y, y = Group)) + geom_ridgeline(aes(height = density, group = interaction(Group, .draw)), fill = NA, color = adjustcolor(pp[7], alpha.f = 2/3), size = 1/3, scale = 3/4) + geom_jitter(data = my_data, height = .04, alpha = 3/4, color = pp[10]) + scale_x_continuous(breaks = seq(from = 80, to = 120, by = 10)) + labs(title = &quot;Data with Posterior Predictive Distrib.&quot;, y = NULL) + coord_cartesian(xlim = c(75, 125), ylim = c(1.25, 4.5)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) Here we make the difference distributions in the middle of Figure 19.8. draws &lt;- as_draws_df(fit19.7) p2 &lt;- draws %&gt;% transmute(`D vs A` = `r_Group[D,Intercept]` - `r_Group[A,Intercept]`, `C vs B` = `r_Group[C,Intercept]` - `r_Group[B,Intercept]`) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;D vs A&quot;, &quot;C vs B&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + facet_wrap(~ name, scales = &quot;free_x&quot;, ncol = 4) And here we make the plots of the corresponding effect sizes at the bottom of the Figure 19.8. # first compute and save the sigma_j&#39;s, which will come in handy later draws &lt;- draws %&gt;% mutate(sigma_A = exp(b_sigma_Intercept + `r_Group__sigma[A,Intercept]`), sigma_B = exp(b_sigma_Intercept + `r_Group__sigma[B,Intercept]`), sigma_C = exp(b_sigma_Intercept + `r_Group__sigma[C,Intercept]`), sigma_D = exp(b_sigma_Intercept + `r_Group__sigma[D,Intercept]`)) p3 &lt;- draws %&gt;% # note we&#39;re using pooled standard deviations to standardize our effect sizes, here transmute(`D vs A` = (`r_Group[D,Intercept]` - `r_Group[A,Intercept]`) / sqrt((sigma_A^2 + sigma_D^2) / 2), `C vs B` = (`r_Group[C,Intercept]` - `r_Group[B,Intercept]`) / sqrt((sigma_B^2 + sigma_C^2) / 2)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;D vs A&quot;, &quot;C vs B&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Effect Size&quot;) + facet_wrap(~ name, scales = &quot;free_x&quot;, ncol = 4) Combine them all and plot! p1 / p2 / p3 + plot_layout(heights = c(2, 1, 1)) Notice that because (a) the sigma parameters were heterogeneous and (b) they were estimated on the log scale, we had to do quite a bit more data processing before they effect size estimates were ready. “Finally, because each group has its own estimated scale (i.e., \\(\\sigma_j\\)), we can investigate differences in scales across groups” (p. 578). That’s not a bad idea. Even though Kruschke didn’t show this in the text, we may as well give it a go. # recall we computed the sigma_j&#39;s a couple blocks up; # now we put them to use draws %&gt;% transmute(`D vs A` = sigma_D - sigma_A, `C vs B` = sigma_C - sigma_B, `D vs C` = sigma_D - sigma_C, `B vs A` = sigma_B - sigma_A) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;D vs A&quot;, &quot;C vs B&quot;, &quot;D vs C&quot;, &quot;B vs A&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[5], color = pp[4], slab_size = 0, quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(Differences~&#39;in&#39;~sigma[italic(j)])) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 4) For more on models including a hierarchical structure on both the mean and scale structures, check out Donald Williams and colleagues’ work on what they call Mixed Effect Location and Scale Models [MELSM; e.g., Donald R. Williams et al. (2021); Donald R. Williams et al. (2019)]. They’re quite handy and I’ve begun using them in my applied work (e.g., here). You can also find a brief introduction to MELSM’s within the context of the multilevel growth model in Section 14.6 of my (2020) translation of McElreath’s (2020) second edition. 19.6 Exercises Walk out an effect size We computed a lot of effect sizes in this chapter. They were all standardized mean differences. Cohen (1988) discussed these kinds of effect sizes in this way: We need a “pure” number, one free of our original measurement unit, with which to index what can be alternately called the degree of departure from the null hypothesis of the alternate hypothesis, or the ES (effect size) we wish to detect. This is accomplished by standardizing the raw effect size as expressed in the measurement unit of the dependent variable by dividing it by the (common) standard deviation of the measures in their respective populations, the latter also in the original measurement unit. (p. 20) Though Cohen framed his discussion in terms of null-hypothesis significance testing, we can just as easily apply it to our Bayesian modeling framework. The main thing is we can use his definitions from above to define a particular kind of effect size–the standardized mean difference between two groups. This is commonly referred to as a Cohen’s \\(d\\), which follows the formula \\[d = \\frac{\\bar y_A - \\bar y_B}{s_p},\\] where the unstandardized means of the variable of interest \\(y\\) are compared between two groups, A and B. From the raw data, we compute their two means, \\(\\bar y_A\\) and \\(\\bar y_B\\), and divide their difference by the common (i.e., pooled) standard deviation \\(s_p\\). In practice, the empirically-derived means and standard deviations are stand-ins (i.e., estimates) of the population parameters. If we’re willing to ignore uncertainty, we can do this all by hand. Let’s walk this out with the fruit-fly data from Section 19.3.2. my_data &lt;- read_csv(&quot;data.R/FruitflyDataReduced.csv&quot;) glimpse(my_data) ## Rows: 125 ## Columns: 3 ## $ Longevity &lt;dbl&gt; 35, 37, 49, 46, 63, 39, 46, 56, 63, 65, 56, 65, 70, 63, 65, 70, 77, 81, 86… ## $ CompanionNumber &lt;chr&gt; &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant… ## $ Thorax &lt;dbl&gt; 0.64, 0.68, 0.68, 0.72, 0.72, 0.76, 0.76, 0.76, 0.76, 0.76, 0.80, 0.80, 0.… Recall we have five groups indexed by CompanionNumber, each with \\(n = 25\\). my_data %&gt;% count(CompanionNumber) ## # A tibble: 5 × 2 ## CompanionNumber n ## &lt;chr&gt; &lt;int&gt; ## 1 None0 25 ## 2 Pregnant1 25 ## 3 Pregnant8 25 ## 4 Virgin1 25 ## 5 Virgin8 25 Let’s focus on just two groups, the male fruit flies for which individual males were supplied access to one or with virgin female fruit flies per day. In the data, these are CompanionNumber == Virgin1 and CompanionNumber == Virgin8, respectively. Here’s a look at their mean Longevity values. my_data %&gt;% filter(str_detect(CompanionNumber, &quot;Virgin&quot;)) %&gt;% group_by(CompanionNumber) %&gt;% summarise(mean = mean(Longevity)) ## # A tibble: 2 × 2 ## CompanionNumber mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Virgin1 56.8 ## 2 Virgin8 38.7 If we’re willing to treat the males in the Virgin1 group as group “a” and those in the Virgin8 group as group “b”, we can save those mean values like so. y_bar_a &lt;- filter(my_data, CompanionNumber == &quot;Virgin1&quot;) %&gt;% summarise(y_bar = mean(Longevity)) %&gt;% pull() y_bar_b &lt;- filter(my_data, CompanionNumber == &quot;Virgin8&quot;) %&gt;% summarise(y_bar = mean(Longevity)) %&gt;% pull() Now we’ll compute their pooled standard deviation. my_data %&gt;% filter(str_detect(CompanionNumber, &quot;Virgin&quot;)) %&gt;% group_by(CompanionNumber) %&gt;% summarise(s = sd(Longevity)) %&gt;% summarise(s_p = sqrt(sum(s^2) / 2)) ## # A tibble: 1 × 1 ## s_p ## &lt;dbl&gt; ## 1 13.6 Save that value. s_a &lt;- filter(my_data, CompanionNumber == &quot;Virgin1&quot;) %&gt;% summarise(s = sd(Longevity)) %&gt;% pull() s_b &lt;- filter(my_data, CompanionNumber == &quot;Virgin8&quot;) %&gt;% summarise(s = sd(Longevity)) %&gt;% pull() s_p &lt;- sqrt((s_a^2 + s_b^2) / 2) If you’re confused by how we computed the pooled standard deviation, recall that when comparing two groups that may have different group-level standard deviations, the formula for the \\(s_p\\) is \\[s_p = \\sqrt{\\frac{s_A^2 + s_B^2}{2}},\\] where \\(s_A\\) and \\(s_B\\) are the group-level standard deviations. Kruschke introduced this formula back in Section 16.3 and we briefly emphasized it in our Bonus Section 16.3.0.1. Now we have the sample \\(s_p\\) in hand, computing Cohen’s \\(d\\) is just simple arithmetic. (y_bar_a - y_bar_b) / s_p ## [1] 1.327554 Though I’m not up on contemporary standards in fruit fly research, a Cohen’s \\(d\\) of that size would be considered [conspicuously] large in most areas of my field (psychology). If we’d like to compute the \\(d\\) estimates for any other combination of experimental conditions, we’d just follow the corresponding arithmetic. As I hinted at earlier, the problem with this approach is it ignores uncertainty. Frequentists use various formulas to express this in terms of 95% confidence intervals. Our approach will be to express it with the posterior distribution of a Bayesian model. We’ve already accomplished this with our fit19.1 from above. Here we’ll use three other approaches. Instead of the Bayesian hierarchical alternative to the frequentist ANOVA, we can use a single-level model where we predict a metric variable with separate intercepts for the two levels of CompanionNumber. First, we subset the data and define our stanvars. my_data &lt;- my_data %&gt;% filter(str_detect(CompanionNumber, &quot;Virgin&quot;)) mean_y &lt;- (y_bar_a + y_bar_b) / 2 stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(s_p, name = &quot;sd_y&quot;) Fit the model with brm(). fit19.8 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 0 + CompanionNumber, prior = c(prior(normal(mean_y, sd_y * 5), class = b), prior(cauchy(0, sd_y), class = sigma)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 19, stanvars = stanvars, file = &quot;fits/fit19.08&quot;) Check the summary. print(fit19.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 0 + CompanionNumber ## Data: my_data (Number of observations: 50) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## CompanionNumberVirgin1 56.73 2.79 51.28 62.08 1.00 7472 5631 ## CompanionNumberVirgin8 38.79 2.74 33.50 44.17 1.00 7514 5542 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 13.80 1.44 11.34 16.96 1.00 7950 5657 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract the posterior draws. draws &lt;- as_draws_df(fit19.8) Here we’ll plot the three dimensions of the posterior, each with the corresponding value from the Cohen’s \\(d\\) formula marked off as a vertical line in the foreground. lines &lt;- tibble(name = c(&quot;b_CompanionNumberVirgin1&quot;, &quot;b_CompanionNumberVirgin8&quot;, &quot;sigma&quot;), value = c(y_bar_a, y_bar_b, s_p)) draws %&gt;% pivot_longer(b_CompanionNumberVirgin1:sigma) %&gt;% ggplot(aes(x = value, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[2], color = pp[7], slab_size = 0, quantiles = 100) + geom_vline(data = lines, aes(xintercept = value), color = pp[13], linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;posterior&quot;) + facet_wrap(~ name, scales = &quot;free&quot;) The model did a great job capturing all three parameters. If we would like to compute our Cohen’s \\(d\\) using the posterior iterations from fit19.8, we’d execute something like this. draws %&gt;% mutate(d = (b_CompanionNumberVirgin1 - b_CompanionNumberVirgin8) / sigma) %&gt;% ggplot(aes(x = d, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[2], color = pp[7], slab_size = 0, quantiles = 100) + geom_vline(xintercept = (y_bar_a - y_bar_b) / s_p, color = pp[13], linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(&quot;Cohen&#39;s&quot;~italic(d)~&quot;expressed as a posterior&quot;)) Similar to the previous plots, this time we superimposed the posterior density with the sample estimate for \\(d\\) we computed above, (y_bar_a - y_bar_b) / s_p. Happily, the hand-calculated estimate coheres nicely with the central tendency of our posterior distribution. But now we get a full measure of uncertainty. Notice how wide those 95% HDIs are. Hopefully this isn’t a surprise given our noncommittal priors and only \\(n = 25\\) for both groups. There’s a lot of uncertainty in that posterior. A second way we might use a single-level model to compute a Cohen’s \\(d\\) effect size is using a dummy variable. We’ll convert our nominal variable CompanionNumber into a binary variable Virgin1 for which 1 corresponds to CompanionNumber == Virgin1 and 0 corresponds to CompanionNumber == Virgin8. Compute the dummy. my_data &lt;- my_data %&gt;% mutate(Virgin1 = if_else(CompanionNumber == &quot;Virgin1&quot;, 1, 0)) Now fit the dummy-predictor model with brm(). fit19.9 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 1 + Virgin1, prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(normal(0, sd_y * 5), class = b), prior(cauchy(0, sd_y), class = sigma)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 19, stanvars = stanvars, file = &quot;fits/fit19.09&quot;) print(fit19.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 1 + Virgin1 ## Data: my_data (Number of observations: 50) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 38.78 2.77 33.35 44.15 1.00 6678 5009 ## Virgin1 17.97 3.87 10.24 25.54 1.00 7088 5360 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 13.80 1.45 11.36 16.97 1.00 7021 5250 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With this parameterization, our posterior for Intercept is the same, within simulation variation, as CompanionNumberVirgin8 from fit7. The posterior for sigma is about the same for both models, too. But focus on Virgin1. This is the unstandardized mean difference, what we called \\(\\bar y_A - \\bar y_B\\) when we computed Cohen’s \\(d\\) using sample statistics. Here’s a look at its posterior distribution with its empirical estimate superimposed with a vertical line. draws &lt;- as_draws_df(fit19.9) draws %&gt;% ggplot(aes(x = b_Virgin1, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[2], color = pp[7], slab_size = 0, quantiles = 100) + geom_vline(xintercept = y_bar_a - y_bar_b, color = pp[13], linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Unstandardized mean difference&quot;) Here’s how to standardize that unstandardized effect size into a Cohen’s-\\(d\\) metric. draws %&gt;% mutate(d = b_Virgin1 / sigma) %&gt;% ggplot(aes(x = d, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[2], color = pp[7], slab_size = 0, quantiles = 100) + geom_vline(xintercept = (y_bar_a - y_bar_b) / s_p, color = pp[13], linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(&quot;Cohen&#39;s&quot;~italic(d)~&quot;expressed as a posterior&quot;)) Let’s work this one more way. By simple algebra, a standardized mean difference is the same as the difference between two standardized means. The trick, though, is you have to use the pooled standard deviation (\\(s_p\\)) as your standardizer. Thus, if we standardize the criterion Longevity before fitting the model and continue using the dummy variable approach, the Virgin1 posterior will be the same as a Cohen’s \\(d\\). Standardize the criterion with s_p. my_data &lt;- my_data %&gt;% mutate(Longevity_s = (Longevity - mean(Longevity)) / s_p) Because our criterion in a standardized metric, we no longer need our stanvars. fit19.10 &lt;- brm(data = my_data, family = gaussian, Longevity_s ~ 1 + Virgin1, prior = c(prior(normal(0, 1 * 5), class = Intercept), prior(normal(0, 1 * 5), class = b), prior(cauchy(0, 1), class = sigma)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 19, file = &quot;fits/fit19.10&quot;) Behold our out-of-the-box Bayesian Cohen’s \\(d\\). # no transformation necessary as_draws_df(fit19.10) %&gt;% ggplot(aes(x = b_Virgin1, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[2], color = pp[7], slab_size = 0, quantiles = 100) + geom_vline(xintercept = (y_bar_a - y_bar_b) / s_p, color = pp[13], linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(&quot;Cohen&#39;s&quot;~italic(d)~&quot;expressed as a posterior&quot;)) If you work them through, the three approaches we just took can be generalized to models with more than two groups. You just need to be careful how to compute the \\(s_p\\) for each comparison. It’s also the case the that standardized mean differences we computed for fit19.1, above, are not quite Cohen’s \\(d\\) effect sizes in the same way these have been. This is because the hierarchical approach we used partially pooled the estimates for each group toward the grand mean. You might say they were hierarchically-regularized Cohen’s \\(d\\)s. But then again, Cohen’s formula for his \\(d\\) statistic did not account for Bayesian priors, either. So perhaps a purist would deny that any of the standardized mean differences we’ve computed in this chapter were proper Cohen’s \\(d\\) effect sizes. To be on the safe side, tell your readers exactly how you computed your models and what formulas you used to compute your effect sizes. 19.6.1 Your sample sizes may differ. In the examples, above, the two groups had equal sample sizes, which allowed us to be lazy with how we hand computed the sample estimate of the pooled standard deviation. When working with data for which \\(n_A \\neq n_B\\), it’s a good idea to switch out the equation for the pooled standard deviation \\(s_p\\) for \\(s_p^*\\), which is robust to unequal sample sizes. We wan write the equation for the two-groups version of \\(s_p^*\\) as \\[s_p^* = \\sqrt{\\frac{(n_A - 1)s_A^2 + (n_B - 1)s_B^2}{n_A + n_B - 2}},\\] which strategically weights the sample estimate for the pooled standard deviation by sample size. We should practice with \\(s_p^*\\), a bit, to see how it works together with our Bayesian modeling paradigm. Back in Section 16.1.2, we saw an example of this with the TwoGroupIQ data. Let’s load them, again. my_data &lt;- read_csv(&quot;data.R/TwoGroupIQ.csv&quot;) glimpse(my_data) ## Rows: 120 ## Columns: 2 ## $ Score &lt;dbl&gt; 102, 107, 92, 101, 110, 68, 119, 106, 99, 103, 90, 93, 79, 89, 137, 119, 126, 110, 7… ## $ Group &lt;chr&gt; &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, … The data are IQ scores for participants in two groups. They look like this. my_data %&gt;% ggplot(aes(x = Score, Group)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[2], color = pp[7], slab_size = 0, quantiles = 100) + xlab(&quot;IQ score&quot;) + coord_cartesian(ylim = c(1.5, 2.25)) Unlike the examples from the last section, the samples sizes are different for our two levels of Group. my_data %&gt;% count(Group) ## # A tibble: 2 × 2 ## Group n ## &lt;chr&gt; &lt;int&gt; ## 1 Placebo 57 ## 2 Smart Drug 63 Here’s how we can use that information to hand compute the sample estimate for Cohen’s \\(d\\). # save the sample means for the groups y_bar_a &lt;- filter(my_data, Group == &quot;Smart Drug&quot;) %&gt;% summarise(m = mean(Score)) %&gt;% pull() y_bar_b &lt;- filter(my_data, Group == &quot;Placebo&quot;) %&gt;% summarise(m = mean(Score)) %&gt;% pull() # save the sample sizes for the groups n_a &lt;- filter(my_data, Group == &quot;Smart Drug&quot;) %&gt;% count() %&gt;% pull() n_b &lt;- filter(my_data, Group == &quot;Placebo&quot;) %&gt;% count() %&gt;% pull() # save the sample standard deviations s_a &lt;- filter(my_data, Group == &quot;Smart Drug&quot;) %&gt;% summarise(s = sd(Score)) %&gt;% pull() s_b &lt;- filter(my_data, Group == &quot;Placebo&quot;) %&gt;% summarise(s = sd(Score)) %&gt;% pull() # compute and save the sample pooled standard deviation s_p &lt;- sqrt(((n_a - 1) * s_a^2 + (n_b - 1) * s_b^2) / (n_a + n_b - 2)) # compute Cohen&#39;s d (y_bar_a - y_bar_b) / s_p ## [1] 0.3518743 Although it’s a lot of work to compute a sample-size corrected Cohen’s \\(d\\) with unequally-sized sample data, it’s straightforward to compute the effect size in a Bayesian model. We could use any of the three approaches, from above. Here we’ll practice with the Score ~ 0 + Group syntax. mean_y &lt;- (y_bar_a + y_bar_b) / 2 stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(s_p, name = &quot;sd_y&quot;) fit19.11 &lt;- brm(data = my_data, family = gaussian, Score ~ 0 + Group, prior = c(prior(normal(mean_y, sd_y * 5), class = b), prior(cauchy(0, sd_y), class = sigma)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 19, stanvars = stanvars, file = &quot;fits/fit19.11&quot;) Review the model summary. print(fit19.11) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Score ~ 0 + Group ## Data: my_data (Number of observations: 120) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## GroupPlacebo 100.03 2.97 94.24 105.74 1.00 7073 5933 ## GroupSmartDrug 107.89 2.81 102.35 113.56 1.00 6670 5303 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 22.34 1.47 19.71 25.42 1.00 8176 6135 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The \\(\\sigma\\) parameter within this model is an estimate of the population value for \\(\\sigma_p\\). Happily, it already accommodates the differences in sample sizes, which we tried to correct for, above, with \\(s_p^*\\). To give a sense, here’s a plot of the \\(\\sigma\\) posterior with our hand-computed s_p value superimposed as a dashed line. as_draws_df(fit19.11) %&gt;% ggplot(aes(x = sigma, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[2], color = pp[7], slab_size = 0, quantiles = 100) + geom_vline(xintercept = s_p, color = pp[13], linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma[italic(p)])) Nailed it! Now here’s how we might use the posterior samples to then compute the standardized mean difference. as_draws_df(fit19.11) %&gt;% mutate(d = (b_GroupSmartDrug - b_GroupPlacebo) / sigma) %&gt;% ggplot(aes(x = d, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[2], color = pp[7], slab_size = 0, quantiles = 100) + geom_vline(xintercept = (y_bar_a - y_bar_b) / s_p, color = pp[13], linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression((mu[italic(B)]-mu[italic(A)])/sigma[italic(p)]*&quot;, where &quot;*sigma[italic(p)]%~~%sqrt(((italic(n[A])-1)*italic(s)[italic(A)]^2+(italic(n[B])-1)*italic(s)[italic(B)]^2)/(italic(n[A])+italic(n[B])-2)))) + theme(axis.title.x = element_text(size = 7)) Sometimes fitting a model is easier than computing estimates, by hand. 19.6.2 Populations and samples. You may have noticed that in our equation for \\(d\\), above, we defined our standardized mean differences in terms of sample statistics, \\[d = \\frac{\\bar y_A - \\bar y_B}{s_p},\\] where \\(s_p\\) can either assume equal sample sizes, or it can be replaced with \\(s_p^*\\) when sample sizes differ. Sometimes we speak of the true population effect size \\(\\delta\\), which is correspondingly defined as \\[\\delta = \\frac{\\mu_A - \\mu_B}{\\sigma},\\] where \\(\\mu_A\\) and \\(\\mu_B\\) are the population means for the two groups under consideration and \\(\\sigma\\) is the pooled standard deviation in the population. Often times we don’t have access to these values, which is why we run experiments and fit statistical models. But sometimes we do have access to the exact population parameters. In those cases, we can just plug them into the formula rather than estimate them in our models or with our sample statistics. In the case of our IQ score data from the last section, we actually know the population mean and standard deviation for IQ are 100 and 15, respectively. We know this because the people who make IQ tests design them that way. Let’s see how well our sample statistics approximate the population parameters. my_data %&gt;% group_by(Group) %&gt;% summarise(mean = mean(Score), sd = sd(Score)) ## # A tibble: 2 × 3 ## Group mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Placebo 100. 17.9 ## 2 Smart Drug 108. 25.4 # pooled standard deviation s_p ## [1] 22.18458 Unsurprisingly, the values for the Smart Drug group are notably different from the population parameters. But notice how close the values from the Placebo group are to the population parameters. If they weren’t, we’d be concerned the Placebo condition was not a valid control. Looks like it was. However, notice that the mean and standard deviation for the Placebo group are not the exact values of 100 and 15 the way they are in the population. If we wanted to compute a standardized mean difference between our Smart Drug group and folks in the population, we could just plug those values directly into our effect size equation. Here’s what that would look like if we plug in the population mean for the control group. (y_bar_a - 100) / s_p ## [1] 0.3534559 The result is very close to the one above. But this time our equation for \\(d\\) was \\[d = \\frac{\\bar y_A - \\mu_B}{s_p},\\] where we used the population mean \\(\\mu_B\\), but the other two terms were based on values from the sample. As long as you are defining the Placebo control as a stand-in for the population, this is a more precise way to compute \\(d\\). Going further, we can also replace our sample estimate \\(s_p\\) with the true value for \\(\\sigma\\), 15. (y_bar_a - 100) / 15 ## [1] 0.5227513 Now our hand-computed estimate for \\(d\\) is quite different. Why? Recall that sample standard deviations for both groups were larger than 15, which therefore produced an \\(s_p\\) value that was larger than 15. When working with fractions, larger denominators return smaller products. Here’s what this looks like if we work with the posterior from the last model, fit19.11. as_draws_df(fit19.11) %&gt;% mutate(d = (b_GroupSmartDrug - 100) / 15) %&gt;% ggplot(aes(x = d, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, slab_fill = pp[2], color = pp[7], slab_size = 0, quantiles = 100) + geom_vline(xintercept = (y_bar_a - 100) / 15, color = pp[13], linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression((mu[italic(B)]-100)/15)) Note how we’ve now changed our effect size formula to \\[d = \\frac{\\bar y_A - \\mu_B}{\\sigma}.\\] 19.6.3 Report your effect sizes effectively. Wrapping up, we’ve been practicing computing standardized mean differences (\\(d\\)’s) by hand with sample statistics, with posterior samples from our Bayesian models, and with combinations of the two. We’ve also seen that whereas unequal sample sizes can matter a lot for hand computing your \\(d\\) estimates, the procedure is more straightforward when using the Bayesian posterior approach. Finally, we played around a bit with how we defined our formula for \\(d\\), depending on whether we knew the true population values of any of the parameters. Confusingly, you might see all these variants, and more, referred to as Cohen’s \\(d\\) within the literature. As with all the other decisions you make with experimental design and data analysis, use careful reasoning to decide on how you’d like to compute your effect sizes. To stave off confusion, report the formulas for your effect sizes transparently in your work. When possible, use equations, prose, and authoritative citations. Though we’ve followed Kruschke’s lead and focused on the Cohen’s \\(d\\) approach to effect sizes, there are many other ways to express effect sizes. Furthermore, \\(d\\)-type effect sizes aren’t appropriate for some model types or for some research questions. To expand your effect size repertoire, you might brush up on Cohen’s (1988) authoritative text or Cummings newer (2012) text. For nice conceptual overview on effect sizes, I recommend Kelley and Preacher’s (2012) paper, On effect size. Also see Pek and Flora’s (2018) handy paper, Reporting effect sizes in original psychological research: A discussion and tutorial. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_3.0.2 bayesplot_1.9.0 brms_2.18.0 Rcpp_1.0.9 patchwork_1.1.2 ## [6] ggridges_0.5.3 palettetown_0.1.1 forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 ## [11] purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 ## [16] tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 igraph_1.3.4 ## [5] svUnit_1.0.6 splines_4.2.0 crosstalk_1.2.0 TH.data_1.1-1 ## [9] rstantools_2.2.0 inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 googlesheets4_1.0.1 ## [17] tzdb_0.3.0 modelr_0.1.8 RcppParallel_5.1.5 matrixStats_0.62.0 ## [21] vroom_1.5.7 xts_0.12.1 sandwich_3.0-2 prettyunits_1.1.1 ## [25] colorspace_2.0-3 rvest_1.0.2 ggdist_3.2.0 haven_2.5.1 ## [29] xfun_0.35 callr_3.7.3 crayon_1.5.2 jsonlite_1.8.3 ## [33] lme4_1.1-31 survival_3.4-0 zoo_1.8-10 glue_1.6.2 ## [37] gtable_0.3.1 gargle_1.2.0 emmeans_1.8.0 distributional_0.3.1 ## [41] pkgbuild_1.3.1 rstan_2.21.7 abind_1.4-5 scales_1.2.1 ## [45] mvtnorm_1.1-3 DBI_1.1.3 miniUI_0.1.1.1 xtable_1.8-4 ## [49] HDInterval_0.2.2 bit_4.0.4 stats4_4.2.0 StanHeaders_2.21.0-7 ## [53] DT_0.24 htmlwidgets_1.5.4 httr_1.4.4 threejs_0.3.3 ## [57] arrayhelpers_1.1-0 posterior_1.3.1 ellipsis_0.3.2 pkgconfig_2.0.3 ## [61] loo_2.5.1 farver_2.1.1 sass_0.4.2 dbplyr_2.2.1 ## [65] utf8_1.2.2 tidyselect_1.1.2 labeling_0.4.2 rlang_1.0.6 ## [69] reshape2_1.4.4 later_1.3.0 munsell_0.5.0 cellranger_1.1.0 ## [73] tools_4.2.0 cachem_1.0.6 cli_3.5.0 generics_0.1.3 ## [77] broom_1.0.1 evaluate_0.18 fastmap_1.1.0 processx_3.8.0 ## [81] knitr_1.40 bit64_4.0.5 fs_1.5.2 nlme_3.1-159 ## [85] projpred_2.2.1 mime_0.12 xml2_1.3.3 compiler_4.2.0 ## [89] shinythemes_1.2.0 rstudioapi_0.13 gamm4_0.2-6 reprex_2.0.2 ## [93] bslib_0.4.0 stringi_1.7.8 highr_0.9 ps_1.7.2 ## [97] Brobdingnag_1.2-8 lattice_0.20-45 Matrix_1.4-1 nloptr_2.0.3 ## [101] markdown_1.1 shinyjs_2.1.0 tensorA_0.36.2 vctrs_0.5.1 ## [105] pillar_1.8.1 lifecycle_1.0.3 jquerylib_0.1.4 bridgesampling_1.1-2 ## [109] estimability_1.4.1 httpuv_1.6.5 R6_2.5.1 bookdown_0.28 ## [113] promises_1.2.0.1 gridExtra_2.3 codetools_0.2-18 boot_1.3-28 ## [117] MASS_7.3-58.1 colourpicker_1.1.1 gtools_3.9.3 assertthat_0.2.1 ## [121] withr_2.5.0 shinystan_2.6.0 multcomp_1.4-20 mgcv_1.8-40 ## [125] parallel_4.2.0 hms_1.1.1 grid_4.2.0 minqa_1.2.5 ## [129] coda_0.19-4 rmarkdown_2.16 googledrive_2.0.0 shiny_1.7.2 ## [133] lubridate_1.8.0 base64enc_0.1-3 dygraphs_1.1.1.6 References Bürkner, P.-C. (2022a). Estimating distributional models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html Chung, Y., Rabe-Hesketh, S., Dorie, V., Gelman, A., &amp; Liu, J. (2013). A nondegenerate penalized likelihood estimator for variance parameters in multilevel models. Psychometrika, 78(4), 685–709. https://doi.org/10.1007/s11336-013-9328-2 Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd Edition). Routledge. https://doi.org/10.4324/9780203771587 Cumming, G. (2012). Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis. Routledge. https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682 Efron, B., &amp; Morris, C. (1977). Stein’s paradox in statistics. Scientific American, 236(5), 119–127. https://doi.org/10.1038/scientificamerican0577-119 Fernandes, M., Walls, L., Munson, S., Hullman, J., &amp; Kay, M. (2018). Uncertainty displays using quantile dotplots or CDFs improve transit decision-making. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (pp. 1–12). Association for Computing Machinery. https://doi.org/10.1145/3173574.3173718 Fisher, R. A. (1925). Statistical methods for research workers, 11th ed. rev. Edinburgh. https://psycnet.apa.org/record/1925-15003-000 Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper). Bayesian Analysis, 1(3), 515–534. https://doi.org/10.1214/06-BA117A Hanley, J., A, &amp; Shapiro, S., H. (1994). Sexual activity and the lifespan of male fruitflies: A dataset that gets attention. Journal of Statistics Education, 2(1), null. https://doi.org/10.1080/10691898.1994.11910467 Kay, M. (2021). Extracting and visualizing tidy draws from brms models. https://mjskay.github.io/tidybayes/articles/tidy-brms.html Kelley, K., &amp; Preacher, K. J. (2012). On effect size. Psychological Methods, 17(2), 137. https://doi.org/10.1037/a0028086 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Kurz, A. S. (2020). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 Lucas, T. (2016). palettetown: Use Pokemon inspired colour palettes [Manual]. https://CRAN.R-project.org/package=palettetown McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/ Pek, J., &amp; Flora, D. B. (2018). Reporting effect sizes in original psychological research: A discussion and tutorial. Psychological Methods, 23(2), 208. https://doi.org/https://doi.apa.org/fulltext/2017-10871-001.html Williams, Donald R., Martin, S. R., Liu, S., &amp; Rast, P. (2021). Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity. European Journal of Psychological Assessment. https://doi.org/10.1027/1015-5759/a000624 Williams, Donald R., Zimprich, D. R., &amp; Rast, P. (2019). A Bayesian nonlinear mixed-effects location scale model for learning. Behavior Research Methods, 51(5), 1968–1986. https://doi.org/10.3758/s13428-019-01255-9 "],["metric-predicted-variable-with-multiple-nominal-predictors.html", "20 Metric Predicted Variable with Multiple Nominal Predictors 20.1 Describing groups of metric data with multiple nominal predictors 20.2 Hierarchical Bayesian approach 20.3 Rescaling can change interactions, homogeneity, and normality 20.4 Heterogeneous variances and robustness against outliers 20.5 Within-subject designs Session info Footnote", " 20 Metric Predicted Variable with Multiple Nominal Predictors This chapter considers data structures that consist of a metric predicted variable and two (or more) nominal predictors. This chapter extends ideas introduced in the previous chapter, so please read the previous chapter if you have not already… The traditional treatment of this sort of data structure is called multifactor analysis of variance (ANOVA). Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups. (Kruschke, 2015, pp. 583–584) 20.1 Describing groups of metric data with multiple nominal predictors Quick reprise: Suppose we have two nominal predictors, denoted \\(\\overrightarrow x_1\\) and \\(\\overrightarrow x_2\\). A datum from the \\(j\\)th level of \\(\\overrightarrow x_1\\) is denoted \\(x_{1[j]}\\), and analogously for the second factor. The predicted value is a baseline plus a deflection due to the level of factor \\(1\\) plus a deflection due to the level of factor \\(2\\) plus a residual deflection due to the interaction of factors: \\[\\begin{align*} \\mu &amp; = \\beta_0 + \\overrightarrow \\beta_1 \\cdot \\overrightarrow x_1 + \\overrightarrow \\beta_2 \\cdot \\overrightarrow x_2 + \\overrightarrow \\beta_{1 \\times 2} \\cdot \\overrightarrow x_{1 \\times 2} \\\\ &amp; = \\beta_0 + \\sum_j \\beta_{1[j]} x_{1[j]} + \\sum_k \\beta_{2[k]} x_{2[k]} + \\sum_{j, k} \\beta_{1 \\times 2[j, k]} x_{1 \\times 2[j, k]} \\end{align*}\\] The deflections within factors and within the interaction are constrained to sum to zero: \\[\\begin{align*} \\sum_j \\beta_{1[j]} = 0 &amp;&amp;&amp; \\text{and} &amp;&amp; \\sum_k \\beta_{2[k]} = 0 \\;\\;\\; \\text{and} \\\\ \\sum_j \\beta_{1 \\times 2[j, k]} = 0 \\text{ for all } k &amp;&amp;&amp; \\text{and} &amp;&amp; \\sum_k \\beta_{1 \\times 2[j, k]} = 0 \\text{ for all } j \\end{align*}\\] ([these equations] are repetitions of Equations 15.9 and 15.10, p. 434). The actual data are assumed to be randomly distributed around the predicted value. (pp. 584–585) 20.1.1 Interaction. An important concept of models with multiple predictors is interaction. Interaction means that the effect of a predictor depends on the level of another predictor. A little more technically, interaction is what is left over after the main effects of the factors are added: interaction is the nonadditive influence of the factors. (p. 585) Here are the data necessary for our version of Figure 20.1, which displays an interaction of two 2-level factors. library(tidyverse) grand_mean &lt;- 5 deflection_1 &lt;- 1.8 deflection_2 &lt;- 0.2 nonadditive_component &lt;- -1 ( d &lt;- tibble(x1 = rep(c(-1, 1), each = 2), x2 = rep(c(1, -1), times = 2)) %&gt;% mutate(mu_additive = grand_mean + (x1 * deflection_1) + (x2 * deflection_2)) %&gt;% mutate(mu_multiplicative = mu_additive + (x1 * x2 * nonadditive_component), # we&#39;ll need this to accommodate `position = &quot;dodge&quot;` within `geom_col()` x1_offset = x1 + x2 * -.45, # we&#39;ll need this for the fill x2_c = factor(x2, levels = c(1, -1))) ) ## # A tibble: 4 × 6 ## x1 x2 mu_additive mu_multiplicative x1_offset x2_c ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -1 1 3.4 4.4 -1.45 1 ## 2 -1 -1 3 2 -0.55 -1 ## 3 1 1 7 6 0.55 1 ## 4 1 -1 6.6 7.6 1.45 -1 There’s enough going on with the lines, arrows, and titles across the three panels that to my mind it seems easier to make three distinct plots and them join them at the end with syntax from the patchwork package. But enough of the settings are common among the panels that it also makes sense to keep from repeating that part of the code. So we’ll take a three-step solution. For the first step, we’ll make the baseline or foundational plot, which we’ll call p. Before we make p, let’s talk color and theme. For this chapter, we’ll carry forward our practice from Chapter 19 and take our color palette from the palettetown package. Our color palette will be #15, which is based on Beedrill. library(palettetown) scales::show_col(pokepal(pokemon = 15)) bd &lt;- pokepal(pokemon = 15) bd ## [1] &quot;#181818&quot; &quot;#D8C8F0&quot; &quot;#F8F8F8&quot; &quot;#B8A8C0&quot; &quot;#606060&quot; &quot;#E8A030&quot; &quot;#A090A8&quot; &quot;#F8C848&quot; &quot;#885000&quot; ## [10] &quot;#980008&quot; &quot;#E8E0F8&quot; &quot;#F8F0A0&quot; &quot;#F89068&quot; &quot;#D01830&quot; Also like in the last chapter, our overall plot theme will be based on the default theme_grey() with a good number of adjustments. This time, it will have more of a theme_black() vibe. theme_set( theme_grey() + theme(text = element_text(color = bd[3]), axis.text = element_text(color = bd[3]), axis.ticks = element_line(color = bd[3]), legend.background = element_blank(), legend.box.background = element_blank(), legend.key = element_rect(fill = bd[1]), panel.background = element_rect(fill = bd[1], color = bd[3]), panel.grid = element_blank(), plot.background = element_rect(fill = bd[1], color = bd[1]), strip.background = element_rect(fill = alpha(bd[5], 1/3), color = alpha(bd[5], 1/3)), strip.text = element_text(color = bd[3])) ) Okay, it’s time to make p, the baseline or foundational plot for our Figure 20.1. p &lt;- d %&gt;% ggplot(aes(x = x1, y = mu_multiplicative)) + geom_col(aes(fill = x2_c), position = &quot;dodge&quot;) + scale_fill_manual(NULL, values = bd[c(11, 6)], labels = c(&quot;x2[1]&quot;, &quot;x2[2]&quot;)) + scale_x_continuous(breaks = c(-1, 1), labels = c(&quot;x1[1]&quot;, &quot;x1[2]&quot;)) + scale_y_continuous(expression(mu), breaks = seq(from = 0, to = 10, by = 2), expand = expansion(mult = c(0, 0.05))) + coord_cartesian(ylim = c(0, 10)) + theme(axis.ticks.x = element_blank(), legend.position = c(.17, .875)) p Now we have p, we’ll add panel-specific elements to it, which we’ll save as individual objects, p1, p2, and p3. That’s step 2. Then for step 3, we’ll bring them all together with patchwork. # deflection from additive p1 &lt;- p + geom_segment(aes(x = x1_offset, xend = x1_offset, y = mu_additive, yend = mu_multiplicative), size = 1.25, color = bd[5], arrow = arrow(length = unit(.275, &quot;cm&quot;))) + geom_line(aes(x = x1_offset, y = mu_additive, group = x2), linetype = 2, color = bd[5]) + geom_line(aes(x = x1_offset, y = mu_additive, group = x1), linetype = 2, color = bd[5]) + coord_cartesian(ylim = c(0, 10)) + ggtitle(&quot;Deflection from additive&quot;) # effect of x1 depends on x2 p2 &lt;- p + geom_segment(aes(x = x1_offset, xend = x1_offset, y = mu_additive, yend = mu_multiplicative), size = .5, color = bd[5], arrow = arrow(length = unit(.15, &quot;cm&quot;))) + geom_line(aes(x = x1_offset, y = mu_additive, group = x2), linetype = 2, color = bd[5]) + geom_line(aes(x = x1_offset, y = mu_multiplicative, group = x2), size = 1.25, color = bd[5]) + ggtitle(&quot;Effect of x1 depends on x2&quot;) # effect of x2 depends on x1 p3 &lt;- p + geom_segment(aes(x = x1_offset, xend = x1_offset, y = mu_additive, yend = mu_multiplicative), size = .5, color = bd[5], arrow = arrow(length = unit(.15, &quot;cm&quot;))) + geom_line(aes(x = x1_offset, y = mu_multiplicative, group = x1), size = 1.25, color = bd[5]) + geom_line(aes(x = x1_offset, y = mu_additive, group = x1), linetype = 2, color = bd[5]) + ggtitle(&quot;Effect of x2 depends on x1&quot;) library(patchwork) p1 + p2 + p3 And in case it’s not clear, “the average deflection from baseline due to a predictor… is called the main effect of the predictor. The main effects of the predictors correspond to the dashed lines in the left panel of Figure 20.1” (p. 587). And further The left panel of Figure 20.1 highlights the interaction as the nonadditive component, emphasized by the heavy vertical arrows that mark the departure from additivity. The middle panel of Figure 20.1 highlights the interaction by emphasizing that the effect of \\(x_1\\) depends on the level of \\(x_2\\). The heavy lines mark the effect of \\(x_1\\), that is, the changes from level \\(1\\) of \\(x_1\\) to level \\(2\\) of \\(x_1\\). Notice that the heavy lines have different slopes: The heavy line for level \\(1\\) of \\(x_2\\) has a shallower slope than the heavy line for level \\(2\\) of \\(x_2\\). The right panel of Figure 20.1 highlights the interaction by emphasizing that the effect of \\(x_2\\) depends on the level of \\(x_1\\). (p. 587) 20.1.2 Traditional ANOVA. As was explained in Section 19.2 (p. 556), the terminology, “analysis of variance,” comes from a decomposition of overall data variance into within-group variance and between-group variance… The Bayesian approach is not ANOVA, but is analogous to ANOVA. Traditional ANOVA makes decisions about equality of groups (i.e., null hypotheses) on the basis of \\(p\\) values using a null hypothesis that assumes (i) the data are normally distributed within groups, and (ii) the standard deviation of the data within each group is the same for all groups. The second assumption is sometimes called “homogeneity of variance.” The entrenched precedent of ANOVA is why basic models of grouped data make those assumptions, and why the basic models presented in this chapter will also make those assumptions. Later in the chapter, those constraints will be relaxed. (pp. 587–588) 20.2 Hierarchical Bayesian approach “Our goal is to estimate the main and interaction deflections, and other parameters, based on the observed data” (p. 588). Figure 20.2 will provides a generic model diagram of how this can work. # bracket p1 &lt;- tibble(x = .99, y = .5, label = &quot;{_}&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, hjust = 1, color = bd[2], family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() ## plain arrow # save our custom arrow settings my_arrow &lt;- arrow(angle = 20, length = unit(0.35, &quot;cm&quot;), type = &quot;closed&quot;) p2 &lt;- tibble(x = .68, y = 1, xend = .68, yend = .25) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bd[3]) + xlim(0, 1) + theme_void() # normal density p3 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. # normal density p4 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(0, 1.15), y = .6, label = c(&quot;0&quot;, &quot;sigma[beta][1]&quot;), hjust = c(.5, 0), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # normal density p5 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(0, 1.15), y = .6, label = c(&quot;0&quot;, &quot;sigma[beta][2]&quot;), hjust = c(.5, 0), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # normal density p6 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(0, 0.67), y = .6, hjust = c(.5, 0), label = c(&quot;0&quot;, &quot;sigma[beta][1%*%2]&quot;), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # four annotated arrows p7 &lt;- tibble(x = c(.05, .34, .64, .945), y = c(1, 1, 1, 1), xend = c(.05, .18, .45, .74), yend = c(0, 0, 0, 0)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(.025, .23, .30, .52, .585, .81, .91), y = .5, label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(k)&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(jk)&quot;), size = c(10, 10, 7, 10, 7, 10, 7), color = bd[3], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # likelihood formula p8 &lt;- tibble(x = .5, y = .25, label = &quot;beta[0]+sum()[italic(j)]*beta[1][&#39;[&#39;*italic(j)*&#39;]&#39;]*italic(x)[1][&#39;[&#39;*italic(j)*&#39;]&#39;](italic(i))+sum()[italic(k)]*beta[2][&#39;[&#39;*italic(k)*&#39;]&#39;]*italic(x)[2][&#39;[&#39;*italic(k)*&#39;]&#39;](italic(i))+sum()[italic(jk)]*beta[1%*%2][&#39;[&#39;*italic(jk)*&#39;]&#39;]*italic(x)[1%*%2][&#39;[&#39;*italic(jk)*&#39;]&#39;](italic(i))&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(hjust = .5, size = 7, color = bd[3], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # half-normal density p9 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma]&quot;, size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # the final normal density p10 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(0, 1.15), y = .6, label = c(&quot;mu[italic(i)]&quot;, &quot;sigma[italic(y)]&quot;), hjust = c(.5, 0), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # an annotated arrow p11 &lt;- tibble(x = .4, y = .5, label = &quot;&#39;=&#39;&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, color = bd[3], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = .1, arrow = my_arrow, color = bd[3]) + xlim(0, 1) + theme_void() # another annotated arrow p12 &lt;- tibble(x = .49, y = .55, label = &quot;&#39;~&#39;&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, color = bd[3], parse = T, family = &quot;Times&quot;) + geom_segment(x = .79, xend = .4, y = 1, yend = .2, arrow = my_arrow, color = bd[3]) + xlim(0, 1) + theme_void() # the final annotated arrow p13 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = bd[3], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow, color = bd[3]) + xlim(0, 1) + theme_void() # some text p14 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = bd[3], parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 1, l = 6, r = 7), area(t = 1, b = 1, l = 10, r = 11), area(t = 1, b = 1, l = 14, r = 15), area(t = 3, b = 4, l = 1, r = 3), area(t = 3, b = 4, l = 5, r = 7), area(t = 3, b = 4, l = 9, r = 11), area(t = 3, b = 4, l = 13, r = 15), area(t = 2, b = 3, l = 6, r = 7), area(t = 2, b = 3, l = 10, r = 11), area(t = 2, b = 3, l = 14, r = 15), area(t = 6, b = 7, l = 1, r = 15), area(t = 5, b = 6, l = 1, r = 15), area(t = 9, b = 10, l = 10, r = 12), area(t = 12, b = 13, l = 7, r = 9), area(t = 8, b = 12, l = 7, r = 9), area(t = 11, b = 12, l = 7, r = 12), area(t = 14, b = 14, l = 7, r = 9), area(t = 15, b = 15, l = 7, r = 9) ) # combine and plot! (p1 + p1 + p1 + p3 + p4 + p5 + p6 + p2 + p2 + p2 + p8 + p7 + p9 + p10 + p11 + p12 + p13 + p14) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Wow that plot has a lot of working parts! 😮 20.2.1 Implementation in JAGS brms. Below is how to implement the model based on the code from Kruschke’s Jags-Ymet-Xnom2fac-MnormalHom.R and Jags-Ymet-Xnom2fac-MnormalHom-Example.R scripts. With brms, we’ll need to specify the stanvars. mean_y &lt;- mean(my_data$y) sd_y &lt;- sd(my_data$y) omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) And before that, of course, make sure you’ve defined the gamma_a_b_from_omega_sigma() function. E.g., gamma_a_b_from_omega_sigma &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } With the preparatory work done, now all we’d need to do is run the brm() code. fit &lt;- brm(data = my_data, family = gaussian, y ~ 1 + (1 | factor_1) + (1 | factor_2) + (1 | factor_1:factor_2), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), stanvars = stanvars) If you have reason to use different priors for the random effects, you can always specify multiple lines of class = sd, each with the appropriate coef argument. The big new element is multiple (|) parts in the formula. In this simple model type, we’re only working random intercepts, in this case with two factors and their interaction. The formula above presumes the interaction is not itself coded within the data. But consider the case you have data including a term for the interaction of the two lower-level factors, called interaction. In that case, you’d have that last part of the formula read (1 | interaction), instead. 20.2.2 Example: It’s only money. Load the salary data7. my_data &lt;- read_csv(&quot;data.R/Salary.csv&quot;) glimpse(my_data) ## Rows: 1,080 ## Columns: 6 ## $ Org &lt;chr&gt; &quot;PL&quot;, &quot;MUTH&quot;, &quot;ENG&quot;, &quot;CMLT&quot;, &quot;LGED&quot;, &quot;MGMT&quot;, &quot;INFO&quot;, &quot;CRIN&quot;, &quot;CRIN&quot;, &quot;PSY&quot;, &quot;SOC&quot;,… ## $ OrgName &lt;chr&gt; &quot;Philosophy&quot;, &quot;Music Theory&quot;, &quot;English&quot;, &quot;Comparative Literature&quot;, &quot;Language Educa… ## $ Cla &lt;chr&gt; &quot;PC&quot;, &quot;PC&quot;, &quot;PC&quot;, &quot;PC&quot;, &quot;PT&quot;, &quot;PR&quot;, &quot;PT&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PC&quot;… ## $ Pos &lt;chr&gt; &quot;FT2&quot;, &quot;FT2&quot;, &quot;FT2&quot;, &quot;FT2&quot;, &quot;FT3&quot;, &quot;NDW&quot;, &quot;FT3&quot;, &quot;FT1&quot;, &quot;NDW&quot;, &quot;FT1&quot;, &quot;FT1&quot;, &quot;FT1&quot;… ## $ ClaPos &lt;chr&gt; &quot;PC.FT2&quot;, &quot;PC.FT2&quot;, &quot;PC.FT2&quot;, &quot;PC.FT2&quot;, &quot;PT.FT3&quot;, &quot;PR.NDW&quot;, &quot;PT.FT3&quot;, &quot;PR.FT1&quot;, &quot;P… ## $ Salary &lt;dbl&gt; 72395, 61017, 82370, 68805, 63796, 219600, 98814, 107745, 114275, 173302, 117240, … We’ll follow Kruschke’s example on page 593 and modify the Pos variable a bit. my_data &lt;- my_data %&gt;% mutate(Pos = factor(Pos, levels = c(&quot;FT3&quot;, &quot;FT2&quot;, &quot;FT1&quot;, &quot;NDW&quot;, &quot;DST&quot;) , ordered = T, labels = c(&quot;Assis&quot;, &quot;Assoc&quot;, &quot;Full&quot;, &quot;Endow&quot;, &quot;Disting&quot;))) With 1,080 cases, two factors, and a criterion, these data are a little too unwieldy to look at the individual case level. But if we’re tricky on how we aggregate, we can get a good sense of their structure with a geom_tile() plot. Here our strategy is to aggregate by our two factors, Pos and Org. Since our criterion is Salary, we’ll compute the mean value of the cases within each unique paring, encoded as m_salary. Also, we’ll get a sense of how many cases there are within each factor pairing with n. my_data %&gt;% group_by(Pos, Org) %&gt;% summarise(m_salary = mean(Salary), n = n()) %&gt;% ungroup() %&gt;% mutate(Org = fct_reorder(Org, m_salary), Pos = fct_reorder(Pos, m_salary)) %&gt;% ggplot(aes(x = Org, y = Pos, fill = m_salary, label = n)) + geom_tile() + geom_text(size = 2.75) + # everything below this is really just aesthetic flourish scale_fill_gradient(low = bd[9], high = bd[12], breaks = c(55e3, 15e4, 26e4), labels = c(&quot;$55K&quot;, &quot;$150K&quot;, &quot;$260K&quot;)) + scale_x_discrete(&quot;Org&quot;, expand = c(0, 0)) + scale_y_discrete(&quot;Pos&quot;, expand = c(0, 0)) + theme(axis.text.x = element_text(angle = 90, hjust = 0), axis.text.y = element_text(hjust = 0), axis.ticks = element_blank(), legend.position = &quot;top&quot;) Hopefully it’s clear that each cell is a unique pairing of Org and Pos. The cells are color coded by the mean Salary. The numbers in the cells give the \\(n\\) cases they represent. When there’s no data for a unique combination of Org and Pos, the cells are left blank charcoal. Load brms. library(brms) Define our stanvars. mean_y &lt;- mean(my_data$Salary) sd_y &lt;- sd(my_data$Salary) omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Now fit the model. fit20.1 &lt;- brm(data = my_data, family = gaussian, Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 20, control = list(adapt_delta = 0.999, max_treedepth = 13), stanvars = stanvars, file = &quot;fits/fit20.01&quot;) The chains look fine. bayesplot::color_scheme_set(scheme = bd[c(13, 7:9, 11:12)]) plot(fit20.1, widths = c(2, 3)) Here’s the model summary. print(fit20.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org) ## Data: my_data (Number of observations: 1080) ## Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup draws = 8000 ## ## Group-Level Effects: ## ~Org (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 30652.56 3076.54 25214.54 37332.82 1.00 1626 2793 ## ## ~Pos (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 56421.66 26569.30 26666.69 125632.37 1.00 2545 3645 ## ## ~Pos:Org (Number of levels: 216) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 9688.54 1197.77 7421.22 12108.03 1.00 2426 4739 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 126114.36 27044.36 70278.60 180836.82 1.00 2425 3133 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 17981.56 435.71 17138.37 18837.35 1.00 6201 5865 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This was a difficult model to fit with brms. Stan does well when the criteria are on or close to a standardized metric and these Salary data are a far cry from that. Tuning adapt_delta and max_treedepth went a long way to help the model out. Okay, let’s get ready for our version of Figure 20.3. First, we’ll use tidybayes::add_epred_draws() to help organize the necessary posterior draws. library(tidybayes) # how many draws would you like? n_draw &lt;- 20 # wrangle f &lt;- my_data %&gt;% distinct(Pos) %&gt;% expand(Pos, Org = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;)) %&gt;% add_epred_draws(fit20.1, ndraws = n_draw, seed = 20, allow_new_levels = T, dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Salary = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Salary) %&gt;% mutate(density = dnorm(Salary, mu, sigma)) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) %&gt;% mutate(Org = factor(Org, levels = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;))) glimpse(f) ## Rows: 40,000 ## Columns: 13 ## Groups: .draw [20] ## $ Pos &lt;ord&gt; Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, As… ## $ Org &lt;fct&gt; BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, B… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ .draw &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .epred &lt;dbl&gt; 191748.1, 191748.1, 191748.1, 191748.1, 191748.1, 191748.1, 191748.1, 191748.1,… ## $ mu &lt;dbl&gt; 191748.1, 191748.1, 191748.1, 191748.1, 191748.1, 191748.1, 191748.1, 191748.1,… ## $ sigma &lt;dbl&gt; 18067.46, 18067.46, 18067.46, 18067.46, 18067.46, 18067.46, 18067.46, 18067.46,… ## $ ll &lt;dbl&gt; 156336.5, 156336.5, 156336.5, 156336.5, 156336.5, 156336.5, 156336.5, 156336.5,… ## $ ul &lt;dbl&gt; 227159.7, 227159.7, 227159.7, 227159.7, 227159.7, 227159.7, 227159.7, 227159.7,… ## $ Salary &lt;dbl&gt; 156336.5, 157051.9, 157767.3, 158482.7, 159198.1, 159913.5, 160628.9, 161344.2,… ## $ density &lt;dbl&gt; 0.1465288, 0.1582290, 0.1705958, 0.1836410, 0.1973740, 0.2118018, 0.2269281, 0.… We’re ready to plot. library(ggridges) f %&gt;% ggplot(aes(x = Salary, y = Pos)) + geom_vline(xintercept = fixef(fit20.1)[, 1], color = bd[5]) + geom_ridgeline(aes(height = density, group = interaction(Pos, .draw), color = Pos), fill = NA, show.legend = F, size = 1/4, scale = 3/4) + geom_jitter(data = my_data %&gt;% filter(Org %in% c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;)) %&gt;% mutate(Org = factor(Org, levels = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;))), height = .025, alpha = 1/2, size = 2/3, color = bd[11]) + scale_color_manual(values = bd[c(14, 13, 8, 12, 3)]) + scale_x_continuous(breaks = seq(from = 0, to = 300000, length.out = 4), labels = c(&quot;$0&quot;, &quot;$100K&quot;, &quot;200K&quot;, &quot;$300K&quot;)) + coord_cartesian(xlim = c(0, 35e4), ylim = c(1.25, 5.5)) + labs(title = &quot;Data with Posterior Predictive Distributions&quot;, subtitle = &quot;The white vertical line is the model-implied grand mean.&quot;, y = &quot;Pos&quot;) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) + facet_wrap(~ Org, ncol = 2) The brms package doesn’t have a convenience function that returns output quite like what Kruschke displayed in his Table 20.2. But we can get close. The posterior_summary() will return posterior means, \\(\\textit{SD}\\)s, and percentile-based 95% intervals for all model parameters. Due to space concerns, I’ll just show the first ten lines. posterior_summary(fit20.1)[1:10,] ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 126114.356 27044.3618 70278.603 180836.818 ## sd_Org__Intercept 30652.564 3076.5357 25214.535 37332.825 ## sd_Pos__Intercept 56421.662 26569.3026 26666.692 125632.368 ## sd_Pos:Org__Intercept 9688.537 1197.7705 7421.223 12108.029 ## sigma 17981.559 435.7087 17138.368 18837.352 ## r_Org[ACTG,Intercept] 80561.579 7593.5101 65644.819 95444.294 ## r_Org[AFRO,Intercept] -14967.759 9681.3929 -34019.522 3944.769 ## r_Org[AMST,Intercept] -16263.028 10147.5361 -36024.080 3939.405 ## r_Org[ANTH,Intercept] -18562.858 7784.4292 -33771.714 -3096.282 ## r_Org[APHS,Intercept] 3207.660 7957.3468 -12288.543 18766.078 The summarise_draws() function from the posterior package (Bürkner et al., 2021), however, will take us a long ways towards making Table 20.2. library(posterior) as_draws_df(fit20.1) %&gt;% select(b_Intercept, starts_with(&quot;r_Pos[&quot;), `r_Org[ENG,Intercept]`, `r_Org[PSY,Intercept]`, `r_Org[CHEM,Intercept]`, `r_Org[BFIN,Intercept]`, `r_Pos:Org[Assis_PSY,Intercept]`, `r_Pos:Org[Full_PSY,Intercept]`, `r_Pos:Org[Assis_CHEM,Intercept]`, `r_Pos:Org[Full_CHEM,Intercept]`, sigma) %&gt;% summarise_draws(mean, median, mode = Mode, ess_bulk, ess_tail, ~ quantile(.x, probs = c(.025, .975))) %&gt;% mutate_if(is.double, round, digits = 0) ## # A tibble: 15 × 8 ## variable mean median mode ess_bulk ess_tail `2.5%` `97.5%` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 126114 126147 123804 2400 3092 70279 180837 ## 2 r_Pos[Assis,Intercept] -45535 -45278 -43009 2497 3056 -99854 8979 ## 3 r_Pos[Assoc,Intercept] -32184 -32191 -31484 2491 3074 -86752 23303 ## 4 r_Pos[Full,Intercept] -2298 -2199 -838 2500 3092 -56764 53355 ## 5 r_Pos[Endow,Intercept] 27809 27705 26852 2494 3106 -26850 83306 ## 6 r_Pos[Disting,Intercept] 56395 56228 56547 2556 3207 1664 113164 ## 7 r_Org[ENG,Intercept] -18712 -18774 -19711 1439 3113 -32563 -4655 ## 8 r_Org[PSY,Intercept] 6618 6617 6947 1216 2574 -6078 19895 ## 9 r_Org[CHEM,Intercept] 18937 18884 17139 1371 2529 5266 32180 ## 10 r_Org[BFIN,Intercept] 107306 107297 106996 1724 4125 92515 122262 ## 11 r_Pos:Org[Assis_PSY,Intercept] -3190 -3183 -3284 7648 6415 -17128 10004 ## 12 r_Pos:Org[Full_PSY,Intercept] -14945 -14797 -14825 5972 6236 -27430 -3039 ## 13 r_Pos:Org[Assis_CHEM,Intercept] -12482 -12375 -12044 6820 6783 -25560 101 ## 14 r_Pos:Org[Full_CHEM,Intercept] 13267 13211 12786 6850 5668 441 26096 ## 15 sigma 17982 17973 17996 6091 5725 17138 18837 Note how we’ve used by kinds of effective-sample size estimates available for brms and note that we’ve used percentile-based intervals. As Kruschke then pointed out, “individual salaries vary tremendously around the predicted cell mean” (p. 594), which you can quantify using \\(\\sigma_y\\). Here it is using posterior_summary(). posterior_summary(fit20.1)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 17981.5593 435.7087 17138.3682 18837.3524 And we can get a better sense of the distribution with a dot plot. # extract the posterior draws draws &lt;- as_draws_df(fit20.1) # plot draws %&gt;% ggplot(aes(x = sigma, y = 0)) + stat_dotsinterval(point_interval = mode_hdi, .width = .95, justification = -0.04, shape = 23, stroke = 1/4, point_size = 3, slab_size = 1/4, color = bd[2], point_color = bd[1], slab_color = bd[1], point_fill = bd[2], slab_fill = bd[6], quantiles = 100) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma[y])) ## Warning: Using the `size` aesthietic with geom_segment was deprecated in ggplot2 3.4.0. ## ℹ Please use the `linewidth` aesthetic instead. As Kruschke pointed out, this parameter is held constant across all subgroups. That is, the subgroups are homogeneous with respect to their variances. We’ll relax this constraint later on. Before we move on to the next section, look above at how many arguments we fiddled with to configure stat_dotsinterval(). Given how many more dot plots we have looming in our not-too-distant future, we might go ahead and save these settings as a new function. We’ll call it stat_beedrill(). stat_beedrill &lt;- function(point_size = 3, slab_color = bd[1], quantiles = 100, ...) { stat_dotsinterval(point_interval = mode_hdi, .width = .95, shape = 23, stroke = 1/4, point_size = point_size, slab_size = 1/4, color = bd[2], point_color = bd[1], point_fill = bd[2], slab_color = slab_color, slab_fill = bd[6], quantiles = quantiles, # learn more about this at https://github.com/mjskay/ggdist/issues/93 justification = -0.04, ...) } Note how we hard coded the settings for some of the parameters within the function (e.g., point_interval) but allows others to be adjustable with new default settings (e.g., point_size). 20.2.3 Main effect contrasts. In applications with multiple levels of the factors, it is virtually always the case that we are interested in comparing particular levels with each other…. These sorts of comparisons, which involve levels of a single factor and collapse across the other factor(s), are called main effect comparisons or contrasts.(p. 595) The fitted() function provides a versatile framework for contrasts among the main effects. In order to follow Kruschke’s aim to compare “levels of a single factor and collapse across the other factor(s)” when those factors are modeled in a hierarchical structure, one will have to make use of the re_formula argument within fitted(). Here’s how to do that for the first contrast. # define the new data nd &lt;- tibble(Pos = c(&quot;Assis&quot;, &quot;Assoc&quot;)) # feed the new data into `fitted()` f &lt;- fitted(fit20.1, newdata = nd, # this part is crucial re_formula = ~ (1 | Pos), summary = F) %&gt;% as_tibble() %&gt;% set_names(&quot;Assis&quot;, &quot;Assoc&quot;) %&gt;% mutate(`Assoc vs Assis` = Assoc - Assis) # plot f %&gt;% ggplot(aes(x = `Assoc vs Assis`, y = 0)) + stat_beedrill() + scale_y_continuous(NULL, breaks = NULL) + xlim(0, NA) + labs(title = &quot;Assoc vs Assis&quot;, x = &quot;Difference&quot;) In case you were curious, here are the summary statistics. f %&gt;% mode_hdi(`Assoc vs Assis`) %&gt;% select(`Assoc vs Assis`:.upper) %&gt;% mutate_if(is.double, round, digits = 0) ## # A tibble: 1 × 3 ## `Assoc vs Assis` .lower .upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 13541 8424 17975 Now make the next two contrasts. nd &lt;- tibble(Org = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;ENG&quot;, &quot;PSY&quot;)) f &lt;- fitted(fit20.1, newdata = nd, # note the change from above re_formula = ~ (1 | Org), summary = F) %&gt;% as_tibble() %&gt;% set_names(pull(nd, Org)) %&gt;% transmute(`CHEM vs PSY` = CHEM - PSY, `BFIN vs other 3` = BFIN - ((CHEM + ENG + PSY) / 3)) # plot f %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_beedrill() + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + expand_limits(x = 0) + facet_wrap(~ name, scales = &quot;free&quot;) And here are their numeric summaries. f %&gt;% pivot_longer(everything(), names_to = &quot;contrast&quot;, values_to = &quot;mode&quot;) %&gt;% group_by(contrast) %&gt;% mode_hdi(mode) %&gt;% select(contrast:.upper) %&gt;% mutate_if(is.double, round, digits = 0) ## # A tibble: 2 × 4 ## contrast mode .lower .upper ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BFIN vs other 3 103597 90783 118831 ## 2 CHEM vs PSY 12808 -2335 26865 For more on marginal contrasts in brms, see the discussion in issue #552 in the brms GitHub repo and this discussion thread on the Stan forums. 20.2.4 Interaction contrasts and simple effects. If we’d like to make the simple effects and interaction contrasts like Kruschke displayed in Figure 20.5 within our tidyverse/brms paradigm, it’ll be simplest to just redefine our nd data and use fitted(), again. This time, however, we won’t be using the re_formula argument. # define our new data nd &lt;- crossing(Pos = c(&quot;Assis&quot;, &quot;Full&quot;), Org = c(&quot;CHEM&quot;, &quot;PSY&quot;)) %&gt;% # we&#39;ll need to update our column names mutate(col_names = str_c(Pos, &quot;_&quot;, Org)) # get the draws with `fitted()` f1 &lt;- fitted(fit20.1, newdata = nd, summary = F) %&gt;% # wrangle as_tibble() %&gt;% set_names(nd %&gt;% pull(col_names)) %&gt;% mutate(`Full - Assis @ PSY` = Full_PSY - Assis_PSY, `Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM) %&gt;% mutate(`Full.v.Assis\\n(x)\\nCHEM.v.PSY` = `Full - Assis @ CHEM` - `Full - Assis @ PSY`) # what have we done? head(f1) ## # A tibble: 6 × 7 ## Assis_CHEM Assis_PSY Full_CHEM Full_PSY `Full - Assis @ PSY` `Full - Assis @ CHEM` Full.v.Assis\\…¹ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 87493. 88399. 155861. 121368. 32969. 68368. 35399. ## 2 83595. 87960. 156152. 116363. 28403. 72558. 44155. ## 3 87712. 81965. 150321. 117377. 35412. 62609. 27197. ## 4 86297. 90932. 162834. 111897. 20966. 76538. 55572. ## 5 84765. 78174. 151399. 119107. 40933. 66634. 25701. ## 6 86740. 74535. 151253. 109501. 34966. 64513. 29548. ## # … with abbreviated variable name ¹​`Full.v.Assis\\n(x)\\nCHEM.v.PSY` It’ll take just a tiny bit more wrangling before we’re ready to plot. # save the levels levels &lt;- c(&quot;Full - Assis @ PSY&quot;, &quot;Full - Assis @ CHEM&quot;, &quot;Full.v.Assis\\n(x)\\nCHEM.v.PSY&quot;) # rope annotation text &lt;- tibble(name = &quot;Full - Assis @ PSY&quot;, value = 15500, y = .95, label = &quot;ROPE&quot;) %&gt;% mutate(name = factor(name, levels = levels)) # wrangle f1 %&gt;% pivot_longer(-contains(&quot;_&quot;)) %&gt;% mutate(name = factor(name, levels = levels)) %&gt;% # plot! ggplot(aes(x = value, y = 0)) + # for kicks and giggles we&#39;ll throw in the ROPE geom_rect(xmin = -1e3, xmax = 1e3, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = bd[7]) + stat_beedrill() + geom_text(data = text, aes(y = y, label = label), color = bd[7], size = 5) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + expand_limits(x = 0) + facet_wrap(~ name, scales = &quot;free&quot;) If it was really important that the labels in the \\(x\\)-axes were different, like they are in Kruschke’s Figure 20.5, you could always make the three plots separately and then bind them together with patchwork syntax. Though he didn’t show the results, on page 598 Kruschke mentioned a few other contrasts we might consider. The example entailed comparing the differences within BFIN to the average of the other three. Let’s walk that out. # define our new data nd &lt;- crossing(Pos = c(&quot;Assis&quot;, &quot;Full&quot;), Org = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;ENG&quot;, &quot;PSY&quot;)) %&gt;% # we&#39;ll need to update our column names mutate(col_names = str_c(Pos, &quot;_&quot;, Org)) # get the draws with `fitted()` f2 &lt;- fitted(fit20.1, newdata = nd, summary = F) %&gt;% # wrangle as_tibble() %&gt;% set_names(nd %&gt;% pull(col_names)) %&gt;% mutate(`Full - Assis @ BFIN` = Full_BFIN - Assis_BFIN, `Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM, `Full - Assis @ ENG` = Full_ENG - Assis_ENG, `Full - Assis @ PSY` = Full_PSY - Assis_PSY) %&gt;% mutate(`Full.v.Assis\\n(x)\\nBFIN.v.the rest` = `Full - Assis @ BFIN` - (`Full - Assis @ CHEM` + `Full - Assis @ ENG` + `Full - Assis @ PSY`) / 3) # what have we done? glimpse(f2) ## Rows: 8,000 ## Columns: 13 ## $ Assis_BFIN &lt;dbl&gt; 200469.4, 191873.4, 198335.4, 191613.8, 195909.4, 206… ## $ Assis_CHEM &lt;dbl&gt; 87493.04, 83594.72, 87712.43, 86296.59, 84764.91, 867… ## $ Assis_ENG &lt;dbl&gt; 57860.04, 57409.70, 59502.40, 73364.35, 49609.69, 591… ## $ Assis_PSY &lt;dbl&gt; 88399.37, 87959.67, 81965.11, 90931.63, 78174.34, 745… ## $ Full_BFIN &lt;dbl&gt; 233435.4, 228240.2, 229553.3, 237348.6, 226177.0, 240… ## $ Full_CHEM &lt;dbl&gt; 155861.1, 156152.5, 150321.2, 162834.4, 151398.6, 151… ## $ Full_ENG &lt;dbl&gt; 104843.2, 103208.0, 107112.6, 109459.4, 105296.2, 116… ## $ Full_PSY &lt;dbl&gt; 121368.0, 116362.9, 117377.3, 111897.5, 119107.5, 109… ## $ `Full - Assis @ BFIN` &lt;dbl&gt; 32966.06, 36366.78, 31217.95, 45734.81, 30267.68, 344… ## $ `Full - Assis @ CHEM` &lt;dbl&gt; 68368.06, 72557.77, 62608.79, 76537.77, 66633.65, 645… ## $ `Full - Assis @ ENG` &lt;dbl&gt; 46983.13, 45798.29, 47610.24, 36095.04, 55686.55, 576… ## $ `Full - Assis @ PSY` &lt;dbl&gt; 32968.64, 28403.20, 35412.15, 20965.84, 40933.12, 349… ## $ `Full.v.Assis\\n(x)\\nBFIN.v.the rest` &lt;dbl&gt; -16473.8876, -12552.9692, -17325.7805, 1201.9276, -24… Now plot. f2 %&gt;% pivot_longer(-contains(&quot;_&quot;)) %&gt;% ggplot(aes(x = value, y = 0)) + geom_rect(xmin = -1e3, xmax = 1e3, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = bd[7]) + stat_beedrill() + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + expand_limits(x = 0) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) So while the overall pay averages for those in BFIN were larger than those in the other three departments, the differences between full and associate professors within BFIN wasn’t substantially different from the differences within the other three departments. To be sure, the interquartile range of that last difference distribution fell below both zero and the ROPE, but there’s still a lot of spread in the rest of the distribution. 20.2.4.1 Interaction effects: High uncertainty and shrinkage. “It is important to realize that the estimates of interaction contrasts are typically much more uncertain than the estimates of simple effects or main effects” (p. 598). If we start with our fitted() object f1, we can wrangle a bit, compute the HDIs with tidybayes::mode_hdi() and then use simple subtraction to compute the interval range for each difference. f1 %&gt;% pivot_longer(-contains(&quot;_&quot;)) %&gt;% mutate(name = factor(name, levels = c(&quot;Full - Assis @ PSY&quot;, &quot;Full - Assis @ CHEM&quot;, &quot;Full.v.Assis\\n(x)\\nCHEM.v.PSY&quot;))) %&gt;% group_by(name) %&gt;% mode_hdi(value) %&gt;% select(name:.upper) %&gt;% mutate(`interval range` = .upper - .lower) ## # A tibble: 3 × 5 ## name value .lower .upper `interval range` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;Full - Assis @ PSY&quot; 29981. 17898. 45915. 28018. ## 2 &quot;Full - Assis @ CHEM&quot; 68436. 54681. 82383. 27702. ## 3 &quot;Full.v.Assis\\n(x)\\nCHEM.v.PSY&quot; 35847. 17241. 56792. 39551. Just like Kruschke pointed out in the text, the interval for the interaction estimate was quite larger than the intervals for the simple contrasts. This large uncertainty of an interaction contrast is caused by the fact that it involves at least four sources of uncertainty (i.e., at least four groups of data), unlike its component simple effects which each involve only half of those sources of uncertainty. In general, interaction contrasts require a lot of data to estimate accurately. (p. 598) Gelman has blogged on this, a bit (e.g., You need 16 times the sample size to estimate an interaction than to estimate a main effect). There is also shrinkage. The interaction contrasts also can experience notable shrinkage from the hierarchical model. In the present application, for example, there are \\(300\\) interaction deflections (\\(5\\) levels of seniority times \\(60\\) departments) that are assumed to come from a higher-level distribution that has an estimated standard deviation, denoted \\(\\sigma_{\\beta 1 \\times 2}\\) in Figure 20.2. Chances are that most of the \\(300\\) interaction deflections will be small, and therefore the estimated standard deviation of the interaction deflections will be small, and therefore the estimated deflections themselves will be shrunken toward zero. This shrinkage is inherently neither good nor bad; it is simply the correct consequence of the model assumptions. The shrinkage can be good insofar as it mitigates false alarms about interactions, but the shrinkage can be bad if it inappropriately obscures meaningful interactions. (p. 598) Here’s that \\(\\sigma_{\\beta_{1 \\times 2}}\\). draws %&gt;% ggplot(aes(x = `sd_Pos:Org__Intercept`, y = 0)) + stat_beedrill() + xlab(expression(sigma[beta[1%*%2]])) + scale_y_continuous(NULL, breaks = NULL) 20.3 Rescaling can change interactions, homogeneity, and normality When interpreting interactions, it can be important to consider the scale on which the data are measured. This is because an interaction means non-additive effects when measured on the current scale. If the data are nonlinearly transformed to a different scale, then the non-additivity can also change. (p. 599) Here is Kruschke’s initial example of a possible interaction effect of sex and political party with respect to wages. d &lt;- tibble(monetary_units = c(10, 12, 15, 18), politics = rep(c(&quot;democrat&quot;, &quot;republican&quot;), each = 2), sex = rep(c(&quot;women&quot;, &quot;men&quot;), times = 2)) %&gt;% mutate(sex_number = if_else(sex == &quot;women&quot;, 1, 2), politics = factor(politics, levels = c(&quot;republican&quot;, &quot;democrat&quot;))) d %&gt;% ggplot(aes(x = sex_number, y = monetary_units, color = politics)) + geom_line(size = 3) + scale_color_manual(NULL, values = bd[8:7]) + scale_x_continuous(&quot;sex&quot;, breaks = 1:2, labels = c(&quot;women&quot;, &quot;men&quot;)) + coord_cartesian(ylim = c(0, 20)) + theme(legend.position = c(.2, .15)) Because the pay discrepancy between men and women is not equal between Democrats and Republicans, in this example, it can be tempting to claim there is a subtle interaction. Not necessarily so. tibble(politics = c(&quot;democrat&quot;, &quot;republican&quot;), female_salary = c(10, 15)) %&gt;% mutate(male_salary = 1.2 * female_salary) ## # A tibble: 2 × 3 ## politics female_salary male_salary ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 democrat 10 12 ## 2 republican 15 18 If we take female salary as the baseline and then add 20% to it for the men, the salary difference between Republican men and women will be larger than that between Democratic men and women. Even though the rate increase from women to men was the same, the increase in absolute value was greater within Republicans because Republican women made more than Democratic women. Look what happens to our original plot when we transform monetary_units with log10(). d %&gt;% ggplot(aes(x = sex_number, y = log10(monetary_units), color = politics)) + geom_line(size = 3) + scale_color_manual(NULL, values = bd[8:7]) + scale_x_continuous(&quot;sex&quot;, breaks = 1:2, labels = c(&quot;women&quot;, &quot;men&quot;)) + theme(legend.position = c(.2, .4)) “Equal ratios are transformed to equal distances by a logarithmic transformation” (p. 599). We can get a better sense of this with our version of Figure 20.6. Probably the easiest way to inset the offset labels will be with help from the geom_dl() function from the directlabels package (Hocking, 2021). library(directlabels) # define the data d &lt;- crossing(int = c(&quot;Non−crossover Interaction&quot;, &quot;Crossover Interaction&quot;), x1 = factor(1:2), x2 = factor(1:2)) %&gt;% mutate(y = c(6, 2, 15, 48, 2, 6, 15, 48)) %&gt;% mutate(ly = log(y)) # save the subplots p1 &lt;- d %&gt;% filter(int == &quot;Non−crossover Interaction&quot;) %&gt;% ggplot(aes(x = x1, y = y, group = x2, label = x2)) + geom_line(aes(color = x2), size = 1) + geom_dl(method = list(dl.combine(&quot;first.points&quot;, &quot;last.points&quot;)), color = bd[3]) + scale_color_manual(values = bd[8:7]) + scale_x_discrete(NULL, breaks = NULL, expand = expansion(mult = 0.1)) + labs(subtitle = &quot;Non−crossover Interaction&quot;) + theme(legend.position = c(.15, .8), legend.key.size = unit(1/3, &quot;cm&quot;)) p2 &lt;- d %&gt;% filter(int == &quot;Crossover Interaction&quot;) %&gt;% ggplot(aes(x = x1, y = y, group = x2, label = x2)) + geom_line(aes(color = x2), size = 1) + geom_dl(method = list(dl.combine(&quot;first.points&quot;, &quot;last.points&quot;)), color = bd[3]) + scale_color_manual(values = bd[8:7], breaks = NULL) + scale_x_discrete(NULL, breaks = NULL, expand = expansion(mult = 0.1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Crossover Interaction&quot;) p3 &lt;- d %&gt;% filter(int == &quot;Crossover Interaction&quot;) %&gt;% ggplot(aes(x = x2, y = y, group = x1, label = x1)) + geom_line(aes(color = x1), size = 2) + geom_dl(method = list(dl.combine(&quot;first.points&quot;, &quot;last.points&quot;)), color = bd[3]) + scale_color_manual(values = bd[12:11]) + scale_x_discrete(NULL, breaks = NULL, expand = expansion(mult = 0.1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Crossover Interaction&quot;) + theme(legend.position = c(.15, .8), legend.key.size = unit(1/3, &quot;cm&quot;)) p4 &lt;- d %&gt;% filter(int == &quot;Non−crossover Interaction&quot;) %&gt;% ggplot(aes(x = x1, y = ly, group = x2, label = x2)) + geom_line(aes(color = x2), size = 1) + geom_dl(method = list(dl.combine(&quot;first.points&quot;, &quot;last.points&quot;)), color = bd[3]) + scale_color_manual(values = bd[8:7], breaks = NULL) + scale_x_discrete(expand = expansion(mult = 0.1)) + ylab(expression(log(y))) p5 &lt;- d %&gt;% filter(int == &quot;Crossover Interaction&quot;) %&gt;% ggplot(aes(x = x1, y = ly, group = x2, label = x2)) + geom_line(aes(color = x2), size = 1) + geom_dl(method = list(dl.combine(&quot;first.points&quot;, &quot;last.points&quot;)), color = bd[3]) + scale_color_manual(values = bd[8:7], breaks = NULL) + scale_x_discrete(expand = expansion(mult = 0.1)) + scale_y_continuous(NULL, breaks = NULL) + ylab(expression(log(y))) p6 &lt;- d %&gt;% filter(int == &quot;Crossover Interaction&quot;) %&gt;% ggplot(aes(x = x2, y = ly, group = x1, label = x1)) + geom_line(aes(color = x1), size = 2) + geom_dl(method = list(dl.combine(&quot;first.points&quot;, &quot;last.points&quot;)), color = bd[3]) + scale_color_manual(values = bd[12:11], breaks = NULL) + scale_x_discrete(expand = expansion(mult = 0.1)) + scale_y_continuous(NULL, breaks = NULL) + ylab(expression(log(y))) # combine p1 + p2 + p3 + p4 + p5 + p6 “The transformability from interaction to non-interaction is only possible for non-crossover interactions. This terminology, ‘noncrossover,’ is merely a description of the graph: The lines do not cross over each other and they have the same sign slope” (p. 601). The plots in the leftmost column are examples of non-crossover interactions. The plots in the center column are examples of crossover interactions. Kruschke then pointed out that transforming data can have unexpected consequences for summary values, such as variances: Suppose one condition has data values of \\(100\\), \\(110\\), and \\(120\\), while a second condition has data values of \\(1100\\), \\(1110\\), and \\(1120\\). For both conditions, the variance is \\(66.7\\), so there is homogeneity of variance. When the data are logarithmically transformed, the variance of the first group becomes \\(1.05e−3\\), but the variance of the second group becomes two orders of magnitude smaller, namely \\(1.02e−5\\). In the transformed data there is not homogeneity of variance. (p. 601) See for yourself. tibble(x = c(100, 110, 120, 1100, 1110, 1120), y = rep(letters[1:2], each = 3)) %&gt;% group_by(y) %&gt;% summarise(variance_of_x = var(x), variance_of_log_x = var(log(x))) ## # A tibble: 2 × 3 ## y variance_of_x variance_of_log_x ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 100 0.00832 ## 2 b 100 0.0000812 Though it looks like the numbers Kruschke reported in the text are off, his overall point stands. While we had homogeneity of variance for x, the variance is heterogeneous when working with log(x). 20.4 Heterogeneous variances and robustness against outliers As we will see in just a moment, our approach to the variant of this model with heterogeneous variances and robustness against outliers will differ slightly from the one Kruschke presented in the text. The two are the same in spirit, but ours differs in how we model \\(\\sigma_{[jk](i)}\\). We’ll get a sense of that difference in our version of the hierarchical model diagram of Figure 20.7. # bracket p1 &lt;- tibble(x = .99, y = .5, label = &quot;{_}&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, hjust = 1, color = bd[2], family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # plain arrow p2 &lt;- tibble(x = .73, y = 1, xend = .68, yend = .25) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bd[3]) + xlim(0, 1) + theme_void() # normal density p3 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # normal density p4 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(0, 1.15), y = .6, label = c(&quot;0&quot;, &quot;sigma[beta][1]&quot;), hjust = c(.5, 0), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # normal density p5 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(0, 1.15), y = .6, label = c(&quot;0&quot;, &quot;sigma[beta][2]&quot;), hjust = c(.5, 0), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # normal density p6 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(0, 0.75), y = .6, hjust = c(.5, 0), label = c(&quot;0&quot;, &quot;sigma[beta][1%*%2]&quot;), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # four annotated arrows p7 &lt;- tibble(x = c(.05, .34, .64, .945), y = c(1, 1, 1, 1), xend = c(.05, .18, .45, .75), yend = c(0, 0, 0, 0)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(.03, .23, .30, .52, .585, .82, .90), y = .5, label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(k)&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(jk)&quot;), size = c(10, 10, 7, 10, 7, 10, 7), color = bd[3], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # likelihood formula p8 &lt;- tibble(x = .5, y = .25, label = &quot;beta[0]+sum()[italic(j)]*beta[1][&#39;[&#39;*italic(j)*&#39;]&#39;]*italic(x)[1][&#39;[&#39;*italic(j)*&#39;]&#39;](italic(i))+sum()[italic(k)]*beta[2][&#39;[&#39;*italic(k)*&#39;]&#39;]*italic(x)[2][&#39;[&#39;*italic(k)*&#39;]&#39;](italic(i))+sum()[italic(jk)]*beta[1%*%2][&#39;[&#39;*italic(jk)*&#39;]&#39;]*italic(x)[1%*%2][&#39;[&#39;*italic(jk)*&#39;]&#39;](italic(i))&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(hjust = .5, size = 7, color = bd[3], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # normal density p9 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(0, 1.15), y = .6, label = c(&quot;italic(M)[mu[sigma]]&quot;, &quot;italic(S)[mu[sigma]]&quot;), hjust = c(.5, 0), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # half-normal density p10 &lt;- tibble(x = seq(from = 0, to = 3, by = .01)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;half-normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = 1.5, y = .6, label = &quot;0*&#39;,&#39;*~italic(S)[sigma[sigma]]&quot;, size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # exponential density p11 &lt;- tibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;exp&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = .5, y = .6, label = &quot;italic(K)&quot;, size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # normal density p12 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(0, 1.2), y = .6, hjust = c(.5, 0), label = c(&quot;mu[sigma]&quot;, &quot;sigma[sigma]&quot;), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # student-t density p13 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) + geom_area(fill = bd[6]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;student t&quot;, size = 7, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(-1.4, 0), y = .6, label = c(&quot;nu&quot;, &quot;mu[italic(i)]&quot;), size = 7, color = bd[3], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = bd[3])) # two annotated arrows p14 &lt;- tibble(x = c(.18, .82), y = c(1, 1), xend = c(.46, .66), yend = c(.28, .28)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(.24, .69), y = .62, label = &quot;&#39;~&#39;&quot;, size = 10, color = bd[3], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # log sigma p15 &lt;- tibble(x = .65, y = .6, label = &quot;log(sigma[&#39;[&#39;*italic(jk)*&#39;](&#39;*italic(i)*&#39;)&#39;])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(hjust = 0, size = 7, color = bd[3], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) + ylim(0, 1) + theme_void() # two annotated arrows p16 &lt;- tibble(x = c(.18, .18), # .2142857 y = c(1, .45), xend = c(.18, .67), yend = c(.75, .14)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(.13, .18, .26), y = c(.92, .64, .22), label = c(&quot;&#39;~&#39;&quot;, &quot;nu*minute+1&quot;, &quot;&#39;=&#39;&quot;), size = c(10, 7, 10), color = bd[3], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # one annotated arrow p17 &lt;- tibble(x = .5, y = 1, xend = .5, yend = .03) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bd[3]) + annotate(geom = &quot;text&quot;, x = .4, y = .5, label = &quot;&#39;=&#39;&quot;, size = 10, color = bd[3], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # another annotated arrow p18 &lt;- tibble(x = .87, y = 1, xend = .43, yend = .2) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = bd[3]) + annotate(geom = &quot;text&quot;, x = c(.56, .7), y = .5, label = c(&quot;&#39;~&#39;&quot;, &quot;italic(jk)&quot;), size = c(10, 7), color = bd[3], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # the final annotated arrow p19 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = bd[3], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow, color = bd[3]) + xlim(0, 1) + theme_void() # some text p20 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = bd[3], parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 1, l = 6, r = 7), area(t = 1, b = 1, l = 10, r = 11), area(t = 1, b = 1, l = 14, r = 15), area(t = 3, b = 4, l = 1, r = 3), area(t = 3, b = 4, l = 5, r = 7), area(t = 3, b = 4, l = 9, r = 11), area(t = 3, b = 4, l = 13, r = 15), area(t = 2, b = 3, l = 6, r = 7), area(t = 2, b = 3, l = 10, r = 11), area(t = 2, b = 3, l = 14, r = 15), area(t = 6, b = 7, l = 1, r = 15), area(t = 5, b = 6, l = 1, r = 15), area(t = 9, b = 10, l = 9, r = 11), area(t = 9, b = 10, l = 13, r = 15), area(t = 12, b = 13, l = 1, r = 3), area(t = 12, b = 13, l = 11, r = 13), area(t = 16, b = 17, l = 5, r = 7), area(t = 11, b = 12, l = 9, r = 15), area(t = 16, b = 17, l = 5, r = 10), area(t = 14, b = 16, l = 1, r = 7), area(t = 8, b = 16, l = 5, r = 7), area(t = 14, b = 16, l = 5, r = 13), area(t = 18, b = 18, l = 5, r = 7), area(t = 19, b = 19, l = 5, r = 7) ) # combine and plot! (p1 + p1 + p1 + p3 + p4 + p5 + p6 + p2 + p2 + p2 + p8 + p7 + p9 + p10 + p11 + p12 + p13 + p14 + p15 + p16 + p17 + p18 + p19 + p20) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Wow that’s a lot! If you compare our diagram with the one in the text, you’ll see the \\(\\nu\\) and \\(\\mu\\) structures are the same. If you look down toward the bottom of the diagram, the first big difference is that we’re modeling the log of \\(\\sigma_{[jk](i)}\\), which is the typical brms strategy for avoiding negative values when modeling a \\(\\sigma\\) parameter. Then when you look up and to the right, you’ll see that we’re modeling \\(\\log(\\sigma_{[jk](i)})\\) with a conventional hierarchical Gaussian structure. Keep this in mind when we get into our brms code, below. Before we fit the robust hierarchical variances model, we need to define our stanvars. stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) Recall that to fit a robust hierarchical variances model, we need to wrap our two formulas within the bf() function. ⚠️ Warning: This one took a few hours fit. ⚠️ fit20.2 &lt;- brm(data = my_data, family = student, bf(Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org), sigma ~ 1 + (1 | Pos:Org)), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(normal(log(sd_y), 1), class = Intercept, dpar = sigma), prior(gamma(alpha, beta), class = sd), prior(normal(0, 1), class = sd, dpar = sigma), prior(exponential(one_over_twentynine), class = nu)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 20, control = list(adapt_delta = 0.9995, max_treedepth = 15), stanvars = stanvars, file = &quot;fits/fit20.02&quot;) Behold the summary. print(fit20.2) ## Family: student ## Links: mu = identity; sigma = log; nu = identity ## Formula: Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org) ## sigma ~ 1 + (1 | Pos:Org) ## Data: my_data (Number of observations: 1080) ## Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup draws = 12000 ## ## Group-Level Effects: ## ~Org (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 31619.75 3093.75 26189.88 38368.83 1.00 1894 3299 ## ## ~Pos (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 51025.03 23822.65 23575.14 112757.06 1.00 5040 7211 ## ## ~Pos:Org (Number of levels: 216) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 5305.30 870.86 3720.00 7146.09 1.00 1976 3567 ## sd(sigma_Intercept) 0.97 0.07 0.84 1.11 1.00 2517 4486 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 122475.76 25189.06 71864.05 172894.55 1.00 3718 6007 ## sigma_Intercept 9.11 0.09 8.93 9.29 1.00 2706 4775 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## nu 6.86 3.64 3.69 14.91 1.00 4761 4901 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This time we’ll just feed the results of the wrangling code right into the plotting code for our version of the top panels of Figure 20.8. # how many draws would you like? n_draw &lt;- 20 # wrangle my_data %&gt;% distinct(Pos) %&gt;% expand(Pos, Org = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;)) %&gt;% add_epred_draws(fit20.2, ndraws = n_draw, seed = 20, allow_new_levels = T, dpar = c(&quot;mu&quot;, &quot;sigma&quot;, &quot;nu&quot;)) %&gt;% mutate(ll = qt(.025, df = nu), ul = qt(.975, df = nu)) %&gt;% mutate(Salary = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Salary) %&gt;% mutate(density = dt(Salary, nu)) %&gt;% # notice the conversion mutate(Salary = mu + Salary * sigma) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) %&gt;% mutate(Org = factor(Org, levels = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;))) %&gt;% # plot ggplot(aes(x = Salary, y = Pos)) + geom_vline(xintercept = fixef(fit20.1)[, 1], color = bd[5]) + geom_ridgeline(aes(height = density, group = interaction(Pos, .draw), color = Pos), fill = NA, show.legend = F, size = 1/4, scale = 3/4) + geom_jitter(data = my_data %&gt;% filter(Org %in% c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;)) %&gt;% mutate(Org = factor(Org, levels = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;))), height = .025, alpha = 1/2, size = 2/3, color = bd[11]) + scale_color_manual(values = bd[c(14, 13, 8, 12, 3)]) + scale_x_continuous(breaks = seq(from = 0, to = 300000, length.out = 4), labels = c(&quot;$0&quot;, &quot;$100K&quot;, &quot;200K&quot;, &quot;$300K&quot;)) + coord_cartesian(xlim = c(0, 35e4), ylim = c(1.25, 5.5)) + labs(title = &quot;Data with Posterior Predictive Distributions&quot;, subtitle = &quot;The white vertical line is the model-implied grand mean.&quot;, y = &quot;Pos&quot;) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) + facet_wrap(~ Org, ncol = 2) Our results for the bottom panel of Figure 20.8 will differ substantially from Kruschke’s. Recall that Kruschke modeled \\(\\sigma_{[j, k](i)}\\) with a hierarchical gamma distribution and using the \\(\\omega + \\sigma\\) parameterization. We, however, modeled our hierarchical \\(\\log (\\sigma)\\) with the typical normal distribution. As such, we have posteriors for \\(\\sigma_\\mu\\) and \\(\\sigma_\\sigma\\). # wrangle as_draws_df(fit20.2) %&gt;% transmute(Normality = log10(nu), `Mean of Cell Sigma&#39;s` = exp(b_sigma_Intercept), `SD of Cell Sigma&#39;s` = exp(`sd_Pos:Org__sigma_Intercept`)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;Normality&quot;, &quot;Mean of Cell Sigma&#39;s&quot;, &quot;SD of Cell Sigma&#39;s&quot;))) %&gt;% # plot ggplot(aes(x = value, y = 0)) + stat_beedrill() + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;param. value&quot;) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) Though our \\(\\sigma_\\mu\\) is on a similar metric to Kruschke’s \\(\\sigma_\\omega\\), our \\(\\sigma_\\sigma\\) is just fundamentally different from his. So it goes. If you think I’m in error, here, share your insights. Even though our hierarchical \\(\\sigma\\) parameters look different from Kruschke’s, it turns the contrast distributions are quite similar. Here’s the necessary wrangling to make our version for Figure 20.9. # define our new data nd &lt;- crossing(Pos = c(&quot;Assis&quot;, &quot;Full&quot;), Org = c(&quot;CHEM&quot;, &quot;PSY&quot;)) %&gt;% # we&#39;ll need to update our column names mutate(col_names = str_c(Pos, &quot;_&quot;, Org)) # get the draws with `fitted()` f &lt;- fitted(fit20.2, newdata = nd, summary = F) %&gt;% # wrangle as_tibble() %&gt;% set_names(nd %&gt;% pull(col_names)) %&gt;% transmute(`Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM, `Full - Assis @ PSY` = Full_PSY - Assis_PSY) %&gt;% mutate(`Full.v.Assis\\n(x)\\nCHEM.v.PSY` = `Full - Assis @ CHEM` - `Full - Assis @ PSY`) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;Full - Assis @ PSY&quot;, &quot;Full - Assis @ CHEM&quot;, &quot;Full.v.Assis\\n(x)\\nCHEM.v.PSY&quot;))) # what have we done? head(f) ## # A tibble: 6 × 2 ## name value ## &lt;fct&gt; &lt;dbl&gt; ## 1 &quot;Full - Assis @ CHEM&quot; 54990. ## 2 &quot;Full - Assis @ PSY&quot; 34041. ## 3 &quot;Full.v.Assis\\n(x)\\nCHEM.v.PSY&quot; 20949. ## 4 &quot;Full - Assis @ CHEM&quot; 73406. ## 5 &quot;Full - Assis @ PSY&quot; 30839. ## 6 &quot;Full.v.Assis\\n(x)\\nCHEM.v.PSY&quot; 42567. Now plot. f %&gt;% ggplot(aes(x = value, y = 0)) + geom_rect(xmin = -1e3, xmax = 1e3, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = bd[7]) + stat_beedrill() + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + expand_limits(x = 0) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) See? Our contrast distributions are really close those in the text. Here are the numeric estimates. f %&gt;% group_by(name) %&gt;% mode_hdi(value) %&gt;% select(name:.upper) %&gt;% mutate_if(is.double, round, digits = 0) ## # A tibble: 3 × 4 ## name value .lower .upper ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;Full - Assis @ PSY&quot; 33666 22720 43666 ## 2 &quot;Full - Assis @ CHEM&quot; 53569 40288 67472 ## 3 &quot;Full.v.Assis\\n(x)\\nCHEM.v.PSY&quot; 20329 3735 38571 In the second half of the middle paragraph on page 605, Kruschke contrasted the \\(\\sigma_{\\beta_{1 \\times 2}}\\) parameter in the two models (i.e., our fit20.1 and fit20.2). Recall that in the brms output, these are termed sd_Pos:Org__Intercept. Here are the comparisons from our brms models. posterior_summary(fit20.1)[&quot;sd_Pos:Org__Intercept&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 9688.537 1197.771 7421.223 12108.029 posterior_summary(fit20.2)[&quot;sd_Pos:Org__Intercept&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 5305.3025 870.8649 3719.9959 7146.0918 They are similar to the values in the text. And recall, of course, the brms::posterior_summary() function returns posterior means. If you really wanted the posterior modes, like Kruschke reported in the text, you’ll have to work a little harder. bind_rows(as_draws_df(fit20.1) %&gt;% select(`sd_Pos:Org__Intercept`), as_draws_df(fit20.2) %&gt;% select(`sd_Pos:Org__Intercept`)) %&gt;% mutate(fit = rep(c(&quot;fit20.1&quot;, &quot;fit20.2&quot;), times = c(8000, 12000))) %&gt;% group_by(fit) %&gt;% mode_hdi(`sd_Pos:Org__Intercept`) %&gt;% select(fit:.upper) %&gt;% mutate_if(is.double, round, digits = 0) ## # A tibble: 2 × 4 ## fit `sd_Pos:Org__Intercept` .lower .upper ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 fit20.1 9725 7492 12170 ## 2 fit20.2 4965 3667 7064 The curious might even look at those in a plot. bind_rows(as_draws_df(fit20.1) %&gt;% select(`sd_Pos:Org__Intercept`), as_draws_df(fit20.2) %&gt;% select(`sd_Pos:Org__Intercept`)) %&gt;% mutate(fit = rep(c(&quot;fit20.1&quot;, &quot;fit20.2&quot;), times = c(8000, 12000))) %&gt;% ggplot(aes(x = `sd_Pos:Org__Intercept`, y = fit)) + stat_beedrill(size = 1/2, point_size = 2) + labs(x = expression(sigma[beta[1%*%2]]), y = NULL) + coord_cartesian(ylim = c(1.5, NA)) “Which model is a better description of the data?… In principle, an intrepid programmer could do a Bayesian model comparison…” (pp. 605–606). We could also examine information criteria, like the LOO. fit20.1 &lt;- add_criterion(fit20.1, &quot;loo&quot;) fit20.2 &lt;- add_criterion(fit20.2, &quot;loo&quot;) Sigh. Both models had high pareto_k values, suggesting there were outliers relative to what was expected by their likelihoods. Just a little further in the text, Kruschke gives us hints why this might be so: Moreover, both models assume that the data within cells are distributed symmetrically above and below their central tendency, either as a normal distribution or a \\(t\\)-distribution. The data instead seem to be skewed toward larger values, especially for advanced seniorities. (p. 606) Here’s the current LOO difference. loo_compare(fit20.1, fit20.2) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit20.2 0.0 0.0 -11765.9 41.5 313.9 10.8 23531.8 82.9 ## fit20.1 -429.6 34.4 -12195.4 43.1 141.4 12.2 24390.9 86.1 But really, “we might want to create a model that describes the data within each cell as a skewed distribution such as a Weibull” (p. 606). Yes, brms can handle Weibull regression (e.g., here). 20.5 Within-subject designs When every subject contributes many measurements to every cell, then the model of the situation is a straight-forward extension of the models we have already considered. We merely add “subject” as another nominal predictor in the model, with each individual subject being a level of the predictor. If there is one predictor other than subject, the model becomes \\[ y = \\beta_0 + \\overrightarrow \\beta_1 \\overrightarrow x_1 + \\overrightarrow \\beta_S \\overrightarrow x_S + \\overrightarrow \\beta_{1 \\times S} \\overrightarrow x_{1 \\times S} \\] This is exactly the two-predictor model we have already considered, with the second predictor being subject. When there are two predictors other than subject, the model becomes \\[\\begin{align*} y = &amp; \\; \\beta_0 &amp; \\text{baseline} \\\\ &amp; + \\overrightarrow \\beta_1 \\overrightarrow x_1 + \\overrightarrow \\beta_2 \\overrightarrow x_2 + \\overrightarrow \\beta_S \\overrightarrow x_S &amp; \\text{main effects} \\\\ &amp; + \\overrightarrow \\beta_{1 \\times 2} \\overrightarrow x_{1 \\times 2} + \\overrightarrow \\beta_{1 \\times S} \\overrightarrow x_{1 \\times S} + \\overrightarrow \\beta_{2 \\times S} \\overrightarrow x_{2 \\times S} &amp; \\text{two-way interactions} \\\\ &amp; + \\overrightarrow \\beta_{1 \\times 2 \\times S} \\overrightarrow x_{1 \\times 2 \\times S} &amp; \\text{three-way interactions} \\end{align*}\\] This model includes all the two-way interactions of the factors, plus the three-way interaction. (p. 607) In situations in which subjects only contribute one observation per condition/cell, we simplify the model to \\[\\begin{align*} y = &amp; \\; \\beta_0 \\\\ &amp; + \\overrightarrow \\beta_1 \\overrightarrow x_1 + \\overrightarrow \\beta_2 \\overrightarrow x_2 + \\overrightarrow \\beta_{1 \\times 2} \\overrightarrow x_{1 \\times 2} \\\\ &amp; + \\overrightarrow \\beta_S \\overrightarrow x_S \\end{align*}\\] “In other words, we assume a main effect of subject, but no interaction of subject with other predictors. In this model, the subject effect (deflection) is constant across treatments, and the treatment effects (deflections) are constant across subjects” (p. 608). 20.5.1 Why use a within-subject design? And why not? Kruschke opined “the primary reason to use a within-subject design is that you can achieve greater precision in the estimates of the effects than in a between-subject design” (p. 608). Well, to that I counterpoint: “No one goes to the circus to see the average dog jump through the hoop significantly oftener than untrained does raised under the same circumstances” (Skinner, 1956, p. 228). And it’s unlikely you’ll make a skillful jumper of your dog without repeated trials. There’s also the related issue that between- and within-person processes aren’t necessarily the same. For an introduction to the issue, see Hamaker’s (2012) chapter, Why researchers should think “within-person”: A paradigmatic rationale, or the paper from Bolger et al. (2019), Causal processes in psychology are heterogeneous. But we digress. Here’s the 4-subject response time data. ( d &lt;- tibble(response_time = c(300, 320, 350, 370, 400, 420, 450, 470), subject = rep(1:4, each = 2), hand = rep(c(&quot;dominant&quot;, &quot;nondominant&quot;), times = 4)) ) ## # A tibble: 8 × 3 ## response_time subject hand ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 300 1 dominant ## 2 320 1 nondominant ## 3 350 2 dominant ## 4 370 2 nondominant ## 5 400 3 dominant ## 6 420 3 nondominant ## 7 450 4 dominant ## 8 470 4 nondominant “For every subject, the difference between dominant and nondominant hands is exactly 20 ms, but there are big differences across subjects in overall response times” (p. 608). Here’s what that looks like. d %&gt;% mutate(subject = factor(subject)) %&gt;% ggplot(aes(x = response_time, y = subject)) + geom_line(aes(group = subject), color = bd[3], linetype = 3) + geom_point(aes(color = hand), size = 3) + scale_color_manual(values = bd[8:9]) Here there is more variability between subjects than within them, which you’d never detect without a within-subject design including multiple subjects. 20.5.2 Split-plot design. “Split-plot experiments were invented by Fisher (1925) (p. 610).” Kruschke then wrote this to set the stage for the next subsection: Consider an agricultural experiment investigating the productivity of different soil tilling methods and different fertilizers. It is relatively easy to provide all the farmers with the several different fertilizers. But it might be relatively difficult to provide all farmers with all the machinery for several different tilling methods. Therefore, any particular farmer will use a single (randomly assigned) tilling method on his whole plot, and tilling methods will differ between whole plots. Each farmer will split his field into subplots and apply all the fertilizers to different (randomly assigned) split plots, and fertilizers will differ across split plots within whole plots. This type of experiment inspires the name, split-plot design. The generic experiment-design term for the farmer’s field is “block.” Then, the factor that varies within every field is called the within-block factor and the factor that varies between fields is called the between-block factor. Notice also that each split plot yields a single measurement (in this case the productivity measured in bushels per acre), not multiple measurements. (p. 610) 20.5.2.1 Example: Knee high by the fourth of July. Load the agronomy data. my_data &lt;- read_csv(&quot;data.R/SplitPlotAgriData.csv&quot;) glimpse(my_data) ## Rows: 99 ## Columns: 4 ## $ Field &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10,… ## $ Till &lt;chr&gt; &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chi… ## $ Fert &lt;chr&gt; &quot;Broad&quot;, &quot;Deep&quot;, &quot;Surface&quot;, &quot;Broad&quot;, &quot;Deep&quot;, &quot;Surface&quot;, &quot;Broad&quot;, &quot;Deep&quot;, &quot;Surface&quot;, … ## $ Yield &lt;dbl&gt; 119, 130, 123, 135, 148, 134, 140, 146, 142, 126, 132, 131, 128, 141, 153, 117, 130,… We might use geom_tile() to visualize the data like this. my_data %&gt;% mutate(Fert = str_c(&quot;Fert: &quot;, Fert)) %&gt;% ggplot(aes(x = Till, y = Field)) + geom_tile(aes(fill = Yield)) + scale_fill_gradient(low = bd[1], high = bd[12]) + scale_x_discrete(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + theme(panel.background = element_rect(fill = bd[7])) + facet_wrap(~ Fert) As Kruschke pointed out in the text, notice how each Field has only one level of Till, but three levels of Fert. 20.5.2.2 The descriptive model. In the classical ANOVA-style model for a split-plot design, the overall variance is conceptually decomposed into five components: the main effect of the between-subjects factor, the main effect of the within-subjects factor, the interaction of the two factors, the effect of subject within levels of the between-subject factor, and the interaction of subject with the within-subject factor. Unfortunately, because there is only a single datum per cell, the five components exactly match the data, which is to say that there are as many parameters as there are data points. (If every subject contributed multiple data points to every cell then the five-component model could be used.) Because there is no residual noise within cells, the classical approach is to treat the final component as noise, that is, treat the interaction of subject with the within-subject factor as noise. That component is not included in the model (at least, not distinct from noise). We will do the same for the descriptive model in our Bayesian analysis. (p. 612) 20.5.2.3 Implementation in JAGS brms. Define the stanvars. mean_y &lt;- mean(my_data$Yield) sd_y &lt;- sd(my_data$Yield) omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Here’s how to fit the model with brm(). fit20.3 &lt;- brm(data = my_data, family = gaussian, Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Field) + (1 | Till:Fert), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 20, control = list(adapt_delta = 0.9999, max_treedepth = 12), stanvars = stanvars, file = &quot;fits/fit20.03&quot;) 20.5.2.4 Results. Check the summary. print(fit20.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Field) + (1 | Till:Fert) ## Data: my_data (Number of observations: 99) ## Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup draws = 8000 ## ## Group-Level Effects: ## ~Fert (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 15.04 13.33 1.26 49.92 1.00 2296 2927 ## ## ~Field (Number of levels: 33) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 11.72 1.72 8.88 15.67 1.00 2177 3858 ## ## ~Till (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 15.58 13.49 1.25 52.11 1.00 2103 2801 ## ## ~Till:Fert (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 8.74 4.03 3.66 18.96 1.00 2082 4413 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 140.48 15.77 107.14 173.14 1.00 3268 3595 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.70 0.54 4.77 6.88 1.00 3601 5000 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We might compare the \\(\\sigma\\) posteriors with a plot. as_draws_df(fit20.3) %&gt;% pivot_longer(c(sigma, starts_with(&quot;sd&quot;))) %&gt;% ggplot(aes(x = value, y = name)) + stat_beedrill(slab_color = bd[6], size = 1/2, point_size = 3/2) + labs(x = NULL, y = NULL) + coord_cartesian(xlim = c(0, 50), ylim = c(1.5, NA)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) Now we have the results of fit20.3, we are ready to make our version if Figure 20.10. Note that how within add_fitted_draws(), we used the re_formula argument to average over the random effects of Field (i.e., we left (1 | Field) out of the formula). That’s our equivalent to when Kruschke wrote “The predictive normal distributions are plotted with means at \\(\\beta_0 + \\beta_B + \\beta_W + \\beta_{B \\times W}\\) (collapsed across \\(\\beta_S\\)) and with standard deviation \\(\\sigma\\)” (pp. 614–615). # wrangle my_data %&gt;% distinct(Till, Fert) %&gt;% add_epred_draws(fit20.3, ndraws = 20, re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert), dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Yield = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Yield) %&gt;% mutate(density = dnorm(Yield, mu, sigma)) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) %&gt;% # plot ggplot(aes(x = Yield, y = Fert)) + geom_path(data = my_data, aes(group = Field %&gt;% as.factor()), size = 1/4, color = bd[5]) + geom_ridgeline(aes(height = -density, group = interaction(Fert, .draw), color = Fert), fill = NA, size = 1/3, scale = 3/4, min_height = NA) + geom_jitter(data = my_data, height = .025, alpha = 1/2, color = bd[5]) + scale_color_manual(values = bd[c(9, 6, 12)]) + scale_x_continuous(breaks = seq(from = 100, to = 180, by = 20)) + coord_flip(xlim = c(90, 190), ylim = c(0.5, 2.75)) + ggtitle(&quot;Data with Posterior Predictive Distribution&quot;) + theme(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;) + facet_wrap(~ Till, labeller = label_both) Now let’s make our Figure 20.11 contrasts. nd &lt;- my_data %&gt;% distinct(Till, Fert) %&gt;% # we&#39;ll need to update our column names mutate(col_names = str_c(Till, &quot;_&quot;, Fert)) fitted(fit20.3, newdata = nd, summary = F, re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert)) %&gt;% as_tibble() %&gt;% set_names(nd %&gt;% pull(col_names)) %&gt;% transmute( `Moldbrd\\nvs\\nRidge` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 3) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3), `Moldbrd.Ridge\\nvs\\nChisel` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface + Ridge_Broad + Ridge_Deep + Ridge_Surface) / 6) - ((Chisel_Broad + Chisel_Deep + Chisel_Surface) / 3), `Deep.Surface\\nvs\\nBroad` = ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6) - ((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3), `Chisel.Moldbrd.v.Ridge\\n(x)\\nBroad.v.Deep.Surface` = (((Chisel_Broad + Chisel_Deep + Chisel_Surface + Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 6) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3)) - (((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3) - ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6)) ) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;Moldbrd\\nvs\\nRidge&quot;, &quot;Moldbrd.Ridge\\nvs\\nChisel&quot;, &quot;Deep.Surface\\nvs\\nBroad&quot;, &quot;Chisel.Moldbrd.v.Ridge\\n(x)\\nBroad.v.Deep.Surface&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + geom_rect(xmin = -5, xmax = 5, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = bd[7]) + stat_beedrill() + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + expand_limits(x = -5) + theme(strip.text = element_text(size = 6)) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 4) As far as I can tell, it appears that our contrasts indicate our variance parameter for Till ended up larger than Kruschke’s. Kruschke then posed a model “with field/subject coding suppressed, hence no lines connecting data from the same field/subject” (p. 616). Here’s how to fit that model. fit20.4 &lt;- brm(data = my_data, family = gaussian, Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 20, control = list(adapt_delta = 0.999, max_treedepth = 12), stanvars = stanvars, file = &quot;fits/fit20.04&quot;) Behold the summary. print(fit20.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert) ## Data: my_data (Number of observations: 99) ## Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup draws = 8000 ## ## Group-Level Effects: ## ~Fert (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 15.24 13.45 1.63 51.89 1.00 2717 3524 ## ## ~Till (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 16.53 13.94 1.65 54.71 1.00 2829 2727 ## ## ~Till:Fert (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 7.71 4.59 1.39 19.08 1.00 2407 3364 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 140.26 16.36 105.61 174.11 1.00 3784 3478 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 12.69 0.96 11.01 14.72 1.00 8328 5053 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Look at how much larger the posterior is for \\(\\sigma_y\\) in this model compared to fit20.3. posterior_summary(fit20.3)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 5.6954319 0.5374253 4.7675016 6.8752696 posterior_summary(fit20.4)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 12.6915001 0.9599149 11.0083386 14.7192597 Here’s the top portion of Figure 20.12. # wrangle my_data %&gt;% distinct(Till, Fert) %&gt;% add_epred_draws(fit20.4, ndraws = 20, re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert), dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Yield = map2(ll, ul, seq, length.out = 200)) %&gt;% unnest(Yield) %&gt;% mutate(density = dnorm(Yield, mu, sigma)) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) %&gt;% # plot ggplot(aes(x = Yield, y = Fert)) + geom_ridgeline(aes(height = -density, group = interaction(Fert, .draw), color = Fert), fill = NA, size = 1/3, scale = 3/4, min_height = NA) + geom_jitter(data = my_data, height = .025, alpha = 1/2, color = bd[5]) + scale_color_manual(values = bd[c(9, 6, 12)]) + scale_x_continuous(breaks = seq(from = 100, to = 180, by = 20)) + coord_flip(xlim = c(90, 190), ylim = c(0.5, 2.75)) + ggtitle(&quot;Data with Posterior Predictive Distribution&quot;) + theme(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;) + facet_wrap(~ Till, labeller = label_both) Now make the plots for the contrast distributions. fitted(fit20.4, newdata = nd, summary = F, re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert)) %&gt;% as_tibble() %&gt;% set_names(nd %&gt;% pull(col_names)) %&gt;% transmute( `Moldbrd\\nvs\\nRidge` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 3) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3), `Moldbrd.Ridge\\nvs\\nChisel` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface + Ridge_Broad + Ridge_Deep + Ridge_Surface) / 6) - ((Chisel_Broad + Chisel_Deep + Chisel_Surface) / 3), `Deep.Surface\\nvs\\nBroad` = ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6) - ((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3), `Chisel.Moldbrd.v.Ridge\\n(x)\\nBroad.v.Deep.Surface` = (((Chisel_Broad + Chisel_Deep + Chisel_Surface + Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 6) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3)) - (((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3) - ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6)) ) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;Moldbrd\\nvs\\nRidge&quot;, &quot;Moldbrd.Ridge\\nvs\\nChisel&quot;, &quot;Deep.Surface\\nvs\\nBroad&quot;, &quot;Chisel.Moldbrd.v.Ridge\\n(x)\\nBroad.v.Deep.Surface&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + geom_rect(xmin = -5, xmax = 5, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = bd[7]) + stat_beedrill() + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + expand_limits(x = -5) + theme(strip.text = element_text(size = 6)) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 4) Though not identical, these were closer to those Kruschke displayed in the text. 20.5.2.5 Model comparison approach. Like we covered in Chapter 10, I’m not aware that Stan/brms will allow for \\(\\delta\\) factor-inclusion parameters the way JAGS allows. However, if you’d like to compare models with different parameters, you can always use information criteria. fit20.3 &lt;- add_criterion(fit20.3, &quot;loo&quot;) fit20.4 &lt;- add_criterion(fit20.4, &quot;loo&quot;) Executing that yielded the following warning message: Found 4 observations with a pareto_k &gt; 0.7 in model ‘fit20.3’. It is recommended to set ‘moment_match = TRUE’ in order to perform moment matching for problematic observations. To use the moment_match approach, the model in question must have been fit with the setting save_pars = save_pars(all = TRUE) within the brm() call. The default is save_pars = NULL and we used the default settings, above. If you go through the trouble of refitting the model with those updated settings, you’ll find that the moment_match approach didn’t help in this case. For the sake of reducing clutter, I’m not going to show the code for refitting the model. However, the next warning message I got after executing fit20.3 &lt;- add_criterion(fit20.3, \"loo\", moment_match = T) was: Warning: Found 1 observations with a pareto_k &gt; 0.7 in model ‘fit20.3’. It is recommended to set ‘reloo = TRUE’ in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model 1 times to compute the ELPDs for the problematic observations directly. So it’s time to bring in the big guns. Let’s use reloo. Be warned that, because this requires refitting the model multiple times, this will take a few minutes. fit20.3 &lt;- add_criterion(fit20.3, criterion = &quot;loo&quot;, reloo = TRUE) Okay, we’re finally ready for the LOO comparison. loo_compare(fit20.3, fit20.4) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit20.3 0.0 0.0 -334.5 8.0 34.0 4.4 668.9 16.1 ## fit20.4 -61.8 7.9 -396.3 5.7 8.6 1.0 792.6 11.4 All good. Based on the LOO values, we should prefer the fuller model. This, of course, should be no surprise. The posterior for \\(\\sigma_\\text{Field}\\) was a far cry from zero. posterior_summary(fit20.3)[&quot;sd_Field__Intercept&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 11.720783 1.724052 8.876385 15.671784 Kruschke ended this chapter by mentioning Bayes’ factors: Bayes’ factor approaches to hypothesis tests in ANOVA were presented by Rouder et al. (2012) and Wetzels, Grasman, and Wagenmakers (2012). Morey and Rouder’s BayesFactor package for R is available at the Web site http://bayesfactorpcl.r-forge.r-project.org/. (p. 618) If you wanna go that route, you’re on your own. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] directlabels_2021.1.13 posterior_1.3.1 ggridges_0.5.3 tidybayes_3.0.2 ## [5] brms_2.18.0 Rcpp_1.0.9 patchwork_1.1.2 palettetown_0.1.1 ## [9] forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 ## [13] readr_2.1.2 tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 ## [17] tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 igraph_1.3.4 ## [5] svUnit_1.0.6 splines_4.2.0 crosstalk_1.2.0 TH.data_1.1-1 ## [9] rstantools_2.2.0 inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 googlesheets4_1.0.1 ## [17] tzdb_0.3.0 modelr_0.1.8 RcppParallel_5.1.5 matrixStats_0.62.0 ## [21] vroom_1.5.7 xts_0.12.1 sandwich_3.0-2 prettyunits_1.1.1 ## [25] colorspace_2.0-3 rvest_1.0.2 ggdist_3.2.0 haven_2.5.1 ## [29] xfun_0.35 callr_3.7.3 crayon_1.5.2 jsonlite_1.8.3 ## [33] lme4_1.1-31 survival_3.4-0 zoo_1.8-10 glue_1.6.2 ## [37] gtable_0.3.1 gargle_1.2.0 emmeans_1.8.0 distributional_0.3.1 ## [41] pkgbuild_1.3.1 rstan_2.21.7 abind_1.4-5 scales_1.2.1 ## [45] mvtnorm_1.1-3 emo_0.0.0.9000 DBI_1.1.3 miniUI_0.1.1.1 ## [49] xtable_1.8-4 HDInterval_0.2.2 bit_4.0.4 stats4_4.2.0 ## [53] StanHeaders_2.21.0-7 DT_0.24 htmlwidgets_1.5.4 httr_1.4.4 ## [57] threejs_0.3.3 arrayhelpers_1.1-0 ellipsis_0.3.2 pkgconfig_2.0.3 ## [61] loo_2.5.1 farver_2.1.1 sass_0.4.2 dbplyr_2.2.1 ## [65] utf8_1.2.2 tidyselect_1.1.2 labeling_0.4.2 rlang_1.0.6 ## [69] reshape2_1.4.4 later_1.3.0 munsell_0.5.0 cellranger_1.1.0 ## [73] tools_4.2.0 cachem_1.0.6 cli_3.5.0 generics_0.1.3 ## [77] broom_1.0.1 evaluate_0.18 fastmap_1.1.0 processx_3.8.0 ## [81] knitr_1.40 bit64_4.0.5 fs_1.5.2 nlme_3.1-159 ## [85] projpred_2.2.1 mime_0.12 xml2_1.3.3 compiler_4.2.0 ## [89] bayesplot_1.9.0 shinythemes_1.2.0 rstudioapi_0.13 gamm4_0.2-6 ## [93] reprex_2.0.2 bslib_0.4.0 stringi_1.7.8 highr_0.9 ## [97] ps_1.7.2 Brobdingnag_1.2-8 lattice_0.20-45 Matrix_1.4-1 ## [101] nloptr_2.0.3 markdown_1.1 shinyjs_2.1.0 tensorA_0.36.2 ## [105] vctrs_0.5.1 pillar_1.8.1 lifecycle_1.0.3 jquerylib_0.1.4 ## [109] bridgesampling_1.1-2 estimability_1.4.1 httpuv_1.6.5 R6_2.5.1 ## [113] bookdown_0.28 promises_1.2.0.1 gridExtra_2.3 codetools_0.2-18 ## [117] boot_1.3-28 MASS_7.3-58.1 colourpicker_1.1.1 gtools_3.9.3 ## [121] assertthat_0.2.1 withr_2.5.0 shinystan_2.6.0 multcomp_1.4-20 ## [125] mgcv_1.8-40 parallel_4.2.0 hms_1.1.1 quadprog_1.5-8 ## [129] grid_4.2.0 minqa_1.2.5 coda_0.19-4 rmarkdown_2.16 ## [133] googledrive_2.0.0 shiny_1.7.2 lubridate_1.8.0 base64enc_0.1-3 ## [137] dygraphs_1.1.1.6 ## Warning in rm(grand_mean, deflection_1, deflection_2, nonadditive_component, : object &#39;col_names&#39; ## not found Footnote References Bolger, N., Zee, K. S., Rossignac-Milon, M., &amp; Hassin, R. R. (2019). Causal processes in psychology are heterogeneous. Journal of Experimental Psychology: General, 148(4), 601–618. https://doi.org/10.1037/xge0000558 Bürkner, P.-C., Gabry, J., Kay, M., &amp; Vehtari, A. (2021). posterior: Tools for working with posterior distributions [Manual]. Fisher, R. A. (1925). Statistical methods for research workers, 11th ed. rev. Edinburgh. https://psycnet.apa.org/record/1925-15003-000 Hamaker, E. L. (2012). Why researchers should think \"within-person\": A paradigmatic rationale. In Handbook of research methods for studying daily life (pp. 43–61). The Guilford Press. https://www.guilford.com/books/Handbook-of-Research-Methods-for-Studying-Daily-Life/Mehl-Conner/9781462513055 Hocking, T. D. (2021). Directlabels: Direct labels for multicolor plots [Manual]. https://CRAN.R-project.org/package=directlabels Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Rouder, J. N., Morey, R. D., Speckman, P. L., &amp; Province, J. M. (2012). Default Bayes factors for ANOVA designs. Journal of Mathematical Psychology, 56(5), 356–374. https://doi.org/10.1016/j.jmp.2012.08.001 Skinner, B. F. (1956). A case history in scientific method. American Psychologist, 11(5), 221–233. https://doi.org/10.1037/h0047662 Wetzels, R., Grasman, R. P. P. P., &amp; Wagenmakers, E.-J. (2012). A default Bayesian hypothesis test for ANOVA designs. The American Statistician, 66(2), 104–111. https://doi.org/10.1080/00031305.2012.695956 It’s not quite clear where these data come from. At the top of page 591, Kruschke wrote “in this section, we will be looking at some real-world salaries.” Further down on the same page, Kruschke added: “The data are annual salaries of 1,080 tenure-track professors at a large-enrollment, small-city, Midwestern-American, research-oriented, state university. (Salaries at big- city and private universities tend to be higher, while salaries at liberal-arts colleges and teaching-oriented state universities tend to be lower.) The data span 60 academic departments that had at least seven members. The data also include the professor’s seniority.” He did not provide a reference.↩︎ "],["dichotomous-predicted-variable.html", "21 Dichotomous Predicted Variable 21.1 Multiple metric predictors 21.2 Interpreting the regression coefficients 21.3 Robust logistic regression 21.4 Nominal predictors Session info", " 21 Dichotomous Predicted Variable This chapter considers data structures that consist of a dichotomous predicted variable. The early chapters of the book were focused on this type of data, but now we reframe the analyses in terms of the generalized linear model… The traditional treatment of these sorts of data structure is called “logistic regression.” In Bayesian software it is easy to generalize the traditional models so they are robust to outliers, allow different variances within levels of a nominal predictor, and have hierarchical structure to share information across levels or factors as appropriate. (Kruschke, 2015, pp. 621–622) 21.1 Multiple metric predictors “We begin by considering a situation with multiple metric predictors, because this case makes it easiest to visualize the concepts of logistic regression” (p. 623). The 3D wireframe plot in Figure 21.1 is technically beyond the scope of our current ggplot2 paradigm. But we will discuss an alternative in the end of Section 21.1.2. 21.1.1 The model and implementation in JAGS brms. Our statistical model will follow the form \\[\\begin{align*} \\mu &amp; = \\operatorname{logistic}(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2) \\\\ y &amp; \\sim \\operatorname{Bernoulli}(\\mu) \\end{align*}\\] where \\[\\operatorname{logistic}(x) = \\frac{1}{[1 + \\exp (-x)]}.\\] The generic brms code for logistic regression using the Bernoulli likelihood looks like so. fit &lt;- brm(data = my_data, family = bernoulli, y ~ 1 + x1 + x2, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b))) Note that this syntax presumes the predictor variables have already been standardized. We’d be remiss not to point out that you can also specify the model using the binomial distribution. That code would look like this. fit &lt;- brm(data = my_data, family = binomial, y | trials(1) ~ 1 + x1 + x2, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b))) As long as the data are not aggregated, the results of these two should be the same within simulation variance. In brms, the default link for both family = bernoulli and family = binomial models is logit, which is exactly what we want, here. Also, note the additional | trials(1) syntax on the left side of the model formula. You could get away with omitting this in older versions of brms. But newer versions prompt users to specify how many of trials each row in the data represents. This is because, as with the baseball data we’ll use later in the chapter, the binomial distribution includes an \\(n\\) parameter. When working with un-aggregated data like what we’re about to do, below, it’s presumed that \\(n = 1\\). We won’t be making our version of Figure 21.1 until a little later in the chapter. However, we can go ahead and make our version of the model diagram in Kruschke’s Figure 21.2. Before we do, let’s discuss the issues of plot colors and theme. For this chapter, we’ll take our color palette from the PNWColors package (Lawlor, 2020), which provides color palettes inspired by the beauty of my birthplace,the US Pacific Northwest. Our color palette will be \"Mushroom\". library(PNWColors) pm &lt;- pnw_palette(name = &quot;Mushroom&quot;, n = 8) pm Our overall plot theme will be a \"Mushroom\" infused extension of theme_linedraw(). library(tidyverse) theme_set( theme_linedraw() + theme(text = element_text(color = pm[1]), axis.text = element_text(color = pm[1]), axis.ticks = element_line(color = pm[1]), legend.background = element_blank(), legend.box.background = element_blank(), legend.key = element_rect(fill = pm[8]), panel.background = element_rect(fill = pm[8], color = pm[8]), panel.border = element_rect(colour = pm[1]), panel.grid = element_blank(), strip.background = element_rect(fill = pm[1], color = pm[1]), strip.text = element_text(color = pm[8])) ) Now make Figure 21.2. library(patchwork) # normal density p1 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pm[5]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pm[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = pm[1], hjust = 0, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5), plot.background = element_rect(fill = pm[8], color = &quot;white&quot;, size = 1)) # second normal density p2 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pm[5]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pm[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M[j])&quot;, &quot;italic(S[j])&quot;), size = 7, color = pm[1], hjust = 0, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5), plot.background = element_rect(fill = pm[8], color = &quot;white&quot;, size = 1)) ## an annotated arrow # save our custom arrow settings my_arrow &lt;- arrow(angle = 20, length = unit(0.35, &quot;cm&quot;), type = &quot;closed&quot;) p3 &lt;- tibble(x = .5, y = 1, xend = .85, yend = 0) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pm[1]) + annotate(geom = &quot;text&quot;, x = .55, y = .4, label = &quot;&#39;~&#39;&quot;, size = 10, color = pm[1], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() ## another annotated arrow p4 &lt;- tibble(x = .5, y = 1, xend = 1/3, yend = 0) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pm[1]) + annotate(geom = &quot;text&quot;, x = c(.3, .48), y = .4, label = c(&quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;), size = c(10, 7), color = pm[1], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # likelihood formula p5 &lt;- tibble(x = .5, y = .5, label = &quot;logistic(beta[0]+sum()[italic(j)]~beta[italic(j)]~italic(x)[italic(ji)])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pm[1], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # a third annotated arrow p6 &lt;- tibble(x = c(.375, .6), y = c(1/2, 1/2), label = c(&quot;&#39;=&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = pm[1], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) + theme_void() # bar plot of Bernoulli data p7 &lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = pm[5], width = .4) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;Bernoulli&quot;, size = 7, color = pm[1]) + annotate(geom = &quot;text&quot;, x = .5, y = .94, label = &quot;mu[italic(i)]&quot;, size = 7, color = pm[1], family = &quot;Times&quot;, parse = T) + xlim(-.75, 1.75) + theme_void() + theme(axis.line.x = element_line(size = 0.5), plot.background = element_rect(fill = pm[8], color = pm[8])) # the final annotated arrow p8 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = pm[1], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, color = pm[1], arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p9 &lt;- tibble(x = 1, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pm[1], parse = T, family = &quot;Times&quot;) + xlim(0, 2) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 1, r = 2), area(t = 1, b = 2, l = 3, r = 4), area(t = 3, b = 3, l = 1, r = 2), area(t = 3, b = 3, l = 3, r = 4), area(t = 4, b = 4, l = 1, r = 4), area(t = 5, b = 5, l = 2, r = 3), area(t = 6, b = 7, l = 2, r = 3), area(t = 8, b = 8, l = 2, r = 3), area(t = 9, b = 9, l = 2, r = 3) ) # combine and plot! (p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) 21.1.2 Example: Height, weight, and gender. Load the height/weight data. library(tidyverse) my_data &lt;- read_csv(&quot;data.R/HtWtData110.csv&quot;) glimpse(my_data) ## Rows: 110 ## Columns: 3 ## $ male &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,… ## $ height &lt;dbl&gt; 63.2, 68.7, 64.8, 67.9, 68.9, 67.8, 68.2, 64.8, 64.3, 64.7, 66.9, 66.9, 67.1, 70.2,… ## $ weight &lt;dbl&gt; 168.7, 169.8, 176.6, 246.8, 151.6, 158.0, 168.6, 137.2, 177.0, 128.0, 168.4, 136.2,… Let’s standardize our predictors. my_data &lt;- my_data %&gt;% mutate(height_z = (height - mean(height)) / sd(height), weight_z = (weight - mean(weight)) / sd(weight)) Before we fit a model, we might take a quick look at the data to explore the relations among the continuous variables weight and height and the dummy variable male. The ggMarginal() function from the ggExtra package (Attali &amp; Baker, 2022) will help us get a sense of the multivariate distribution by allowing us to add marginal densities to a scatter plot. library(ggExtra) p &lt;- my_data %&gt;% ggplot(aes(x = weight, y = height, fill = male == 1)) + geom_point(aes(color = male == 1), alpha = 3/4) + scale_color_manual(values = pm[c(3, 6)]) + scale_fill_manual(values = pm[c(3, 6)]) + theme(legend.position = &quot;none&quot;) p %&gt;% ggMarginal(data = my_data, colour = pm[1], groupFill = T, alpha = .8, type = &quot;density&quot;) Looks like the data for which male == 1 are concentrated in the upper right and those for which male == 0 are more so in the lower left. What we’d like is a model that would tell us the optimal dividing line(s) between our male categories with respect to those predictor variables. Open brms. library(brms) Our first logistic model with family = bernoulli uses only weight_z as a predictor. fit21.1 &lt;- brm(data = my_data, family = bernoulli, male ~ 1 + weight_z, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.01&quot;) Here’s the model summary. print(fit21.1) ## Family: bernoulli ## Links: mu = logit ## Formula: male ~ 1 + weight_z ## Data: my_data (Number of observations: 110) ## Draws: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.08 0.22 -0.51 0.34 1.00 6863 5400 ## weight_z 1.19 0.28 0.65 1.75 1.00 5612 4505 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now let’s get ready to make our version of Figure 21.3. First, we extract the posterior draws. draws &lt;- as_draws_df(fit21.1) Now we wrangle a bit to make the top panel of Figure 21.3. length &lt;- 200 n_draw &lt;- 20 draws %&gt;% tibble() %&gt;% # take 20 random samples of the posterior draws slice_sample(n = n_draw) %&gt;% # add in a sequence of weight_z expand(nesting(.draw, b_Intercept, b_weight_z), weight_z = seq(from = -2, to = 3.5, length.out = length)) %&gt;% # compute the estimates of interest mutate(male = inv_logit_scaled(b_Intercept + b_weight_z * weight_z), weight = weight_z * sd(my_data$weight) + mean(my_data$weight), thresh = -b_Intercept / b_weight_z * sd(my_data$weight) + mean(my_data$weight)) %&gt;% # plot! ggplot(aes(x = weight)) + geom_hline(yintercept = .5, color = pm[7], size = 1/2) + geom_vline(aes(xintercept = thresh, group = .draw), color = pm[6], size = 2/5, linetype = 2) + geom_line(aes(y = male, group = .draw), color = pm[1], size = 1/3, alpha = 2/3) + geom_point(data = my_data, aes(y = male), alpha = 1/3, color = pm[1]) + labs(title = &quot;Data with Post. Pred.&quot;, y = &quot;male&quot;) + coord_cartesian(xlim = range(my_data$weight)) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. We should discuss those thresholds (i.e., the vertical lines) a bit. Kruschke: The spread of the logistic curves indicates the uncertainty of the estimate; the steepness of the logistic curves indicates the magnitude of the regression coefficient. The \\(50\\%\\) probability threshold is marked by arrows that drop down from the logistic curve to the \\(x\\)-axis, near a weight of approximately \\(160\\) pounds. The threshold is the \\(x\\) value at which \\(\\mu = 0.5\\), which is \\(x = -\\beta_0 / \\beta_1\\). (p. 626) It’s important to realize that when you compute the thresholds with \\(-\\beta_0 / \\beta_1\\), this returns the values on the scale of the predictor. In our case, the predictor was weight_z. But since we wanted to plot the data on the scale of the unstandardized variable, weight, we have to convert the thresholds to that metric by multiplying their values by \\(s_\\text{weight}\\) and then add the product to \\(\\overline{\\text{weight}}\\). If you study it closely, you’ll see that’s what we did when computing the thresh values, above. Now here we show the marginal distributions in our versions of the lower panels of Figure 21.3. library(tidybayes) draws &lt;- draws %&gt;% # convert the parameter draws to their natural metric following Equation 21.1 (pp. 624--625) transmute(Intercept = b_Intercept - (b_weight_z * mean(my_data$weight) / sd(my_data$weight)), weight = b_weight_z / sd(my_data$weight)) %&gt;% pivot_longer(everything()) # plot draws %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, shape = 15, point_size = 2.5, point_color = pm[4], slab_color = pm[1], fill = pm[7], color = pm[1], slab_size = 1/2, size = 2, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) And here are those exact posterior mode and 95% HDI values. draws %&gt;% group_by(name) %&gt;% mode_hdi() %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 2 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Intercept -5.38 -7.74 -2.91 0.95 mode hdi ## 2 weight 0.033 0.018 0.048 0.95 mode hdi If you look back at our code for the lower marginal plots for Figure 21.3, you’ll notice we did a whole lot of argument tweaking within tidybayes::stat_halfeye(). We have a lot of marginal densities ahead of us in this chapter, so we might streamline our code with those settings saved in a custom function. As the vibe I’m going for in those settings is based on some of the plots in Chapter 16 of Wilke’s (2019), Fundamentals of data visualization, we’ll call our function stat_wilke(). stat_wilke &lt;- function(.width = .95, shape = 15, point_size = 2.5, point_color = pm[4], slab_color = pm[1], fill = pm[7], color = pm[1], slab_size = 1/2, size = 2, ...) { stat_halfeye(point_interval = mode_hdi, .width = .width, shape = shape, point_size = point_size, point_color = point_color, slab_color = slab_color, fill = fill, color = color, slab_size = slab_size, size = size, normalize = &quot;panels&quot;, ...) } Now fit the two-predictor model using both weight_z and height_z. fit21.2 &lt;- brm(data = my_data, family = bernoulli, male ~ 1 + weight_z + height_z, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.02&quot;) Here’s the model summary. print(fit21.2) ## Family: bernoulli ## Links: mu = logit ## Formula: male ~ 1 + weight_z + height_z ## Data: my_data (Number of observations: 110) ## Draws: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.35 0.30 -0.94 0.22 1.00 5726 5626 ## weight_z 0.67 0.35 0.01 1.39 1.00 6719 5765 ## height_z 2.62 0.51 1.68 3.71 1.00 5932 4777 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Before we make our plots for Figure 21.4, we’ll need to extract the posterior samples and transform a little. draws &lt;- as_draws_df(fit21.2) %&gt;% mutate(b_weight = b_weight_z / sd(my_data$weight), b_height = b_height_z / sd(my_data$height), Intercept = b_Intercept - ((b_weight_z * mean(my_data$weight) / sd(my_data$weight)) + (b_height_z * mean(my_data$height) / sd(my_data$height)))) %&gt;% select(.draw, b_weight:Intercept) head(draws) ## # A tibble: 6 × 4 ## .draw b_weight b_height Intercept ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.0406 0.677 -51.7 ## 2 2 0.0371 0.847 -62.3 ## 3 3 0.0345 0.852 -62.2 ## 4 4 0.0433 0.425 -35.4 ## 5 5 0.0250 0.636 -46.7 ## 6 6 0.0174 0.748 -53.3 Here’s our version of Figure 21.4.a. set.seed(21) # we need this for the `slice_sample()` function draws %&gt;% slice_sample(n = 20) %&gt;% expand(nesting(.draw, Intercept, b_weight, b_height), weight = c(80, 280)) %&gt;% # this follows the Equation near the top of p. 629 mutate(height = (-Intercept / b_height) + (-b_weight / b_height) * weight) %&gt;% # now plot ggplot(aes(x = weight, y = height)) + geom_line(aes(group = .draw), color = pm[7], size = 2/5, alpha = 2/3) + geom_text(data = my_data, aes(label = male, color = male == 1)) + scale_color_manual(values = pm[c(4, 1)]) + ggtitle(&quot;Data with Post. Pred.&quot;) + coord_cartesian(xlim = range(my_data$weight), ylim = range(my_data$height)) + theme(legend.position = &quot;none&quot;) With just a tiny bit more wrangling, we’ll be ready to make the bottom panels of Figure 21.4. draws %&gt;% pivot_longer(-.draw) %&gt;% mutate(name = factor(str_remove(name, &quot;b_&quot;), levels = c(&quot;Intercept&quot;, &quot;weight&quot;, &quot;height&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_wilke() + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) Did you notice our use of stat_wilke()? By now, you know how to use mode_hdi() to return those exact summary values if you’d like them. Now remember how we backed away from Figure 21.1? Well, when you have a logistic regression with two predictors, there is a reasonable way to express those three dimensions on a two-dimensional grid. Now we have the results from fit21.2, let’s try it out. First, we need a grid of values for our two predictors, weight_z and height_z. length &lt;- 100 nd &lt;- crossing(weight_z = seq(from = -3.5, to = 3.5, length.out = length), height_z = seq(from = -3.5, to = 3.5, length.out = length)) Second, we plug those values into fitted() and wrangle. f &lt;- fitted(fit21.2, newdata = nd, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% # note we&#39;re only working with the posterior mean, here transmute(prob = Estimate %&gt;% inv_logit_scaled()) %&gt;% bind_cols(nd) %&gt;% mutate(weight = (weight_z * sd(my_data$weight) + mean(my_data$weight)), height = (height_z * sd(my_data$height) + mean(my_data$height))) glimpse(f) ## Rows: 10,000 ## Columns: 5 ## $ prob &lt;dbl&gt; 7.042908e-06, 8.476457e-06, 1.020180e-05, 1.227831e-05, 1.477749e-05, 1.778534e-0… ## $ weight_z &lt;dbl&gt; -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.… ## $ height_z &lt;dbl&gt; -3.500000, -3.429293, -3.358586, -3.287879, -3.217172, -3.146465, -3.075758, -3.0… ## $ weight &lt;dbl&gt; 33.45626, 33.45626, 33.45626, 33.45626, 33.45626, 33.45626, 33.45626, 33.45626, 3… ## $ height &lt;dbl&gt; 53.24736, 53.51234, 53.77731, 54.04229, 54.30727, 54.57224, 54.83722, 55.10219, 5… Third, we’re ready to plot. Here we’ll express the third dimension, probability, on a color spectrum. f %&gt;% ggplot(aes(x = weight, y = height)) + geom_raster(aes(fill = prob), interpolate = T) + geom_text(data = my_data, aes(label = male, color = male == 1), show.legend = F) + scale_color_manual(values = pm[c(8, 1)]) + scale_fill_gradientn(colours = pnw_palette(name = &quot;Mushroom&quot;, n = 101), limits = c(0, 1)) + scale_y_continuous(position = &quot;right&quot;) + coord_cartesian(xlim = range(my_data$weight), ylim = range(my_data$height)) + theme(legend.position = &quot;left&quot;) If you look way back to Figure 21.1 (p. 623), you’ll see the following formula at the top: \\[y \\sim \\operatorname{dbern}(m), m = \\operatorname{logistic}(0.018 x_1 + 0.7 x_2 - 50).\\] Now while you keep your finger on that equation, take another look at the last line in Kruschke’s Equation 21.1, \\[ \\operatorname{logit}(\\mu) = \\underbrace{\\zeta_0 - \\sum_j \\frac{\\zeta_j}{s_{x_j}} \\overline x_j}_{\\beta_0} + \\sum_j \\underbrace{\\frac{\\zeta_j}{s_{x_j}} \\overline x_j}_{\\beta_j}, \\] where the \\(\\zeta\\)’s are the parameters from the model based on standardized predictors. Our fit21.2 was based on standardized weight and height values (i.e., weight_z and height_z), yielding model coefficients in the \\(\\zeta\\) metric. Here we use the formula above to convert our fit21.2 estimates to their unstandardized \\(\\beta\\) metric. For simplicity, we’ll just take their means. as_draws_df(fit21.2) %&gt;% transmute(beta_0 = b_Intercept - ((b_weight_z * mean(my_data$weight) / sd(my_data$weight)) + ((b_height_z * mean(my_data$height) / sd(my_data$height)))), beta_1 = b_weight_z / sd(my_data$weight), beta_2 = b_height_z / sd(my_data$height)) %&gt;% summarise_all(~ mean(.) %&gt;% round(., digits = 3)) ## # A tibble: 1 × 3 ## beta_0 beta_1 beta_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -49.7 0.019 0.699 Within rounding error, those values are the same ones in the formula at the top of Kruschke’s Figure 21.1! That is, our last plot was a version of Figure 21.1. Hopefully this helps make sense of what the thresholds in Figure 21.4.a represented. But do note a major limitation of this visualization approach. By expressing the threshold with multiple lines drawn from the posterior in Figure 21.4.a, we expressed the uncertainty inherent in the posterior distribution. However, for this probability plane approach, we’ve taken a single value from the posterior, the mean (i.e., the Estimate), to compute the probabilities. Though beautiful, our probability-plane plot does a poor job expressing the uncertainty in the model. If you’re curious how one might include uncertainty into a plot like this, check out the intriguing blog post by Adam Pearce, Communicating model uncertainty over space. 21.2 Interpreting the regression coefficients In this section, I’ll discuss how to interpret the parameters in logistic regression. The first subsection explains how to interpret the numerical magnitude of the slope coefficients in terms of “log odds.” The next subsection shows how data with relatively few \\(1\\)’s or \\(0\\)’s can yield ambiguity in the parameter estimates. Then an example with strongly correlated predictors reveals tradeoffs in slope coefficients. Finally, I briefly describe the meaning of multiplicative interaction for logistic regression. (p. 629) 21.2.1 Log odds. When the logistic regression formula is written using the logit function, we have \\(\\operatorname{logit}(\\mu) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). The formula implies that whenever \\(x_1\\) goes up by \\(1\\) unit (on the \\(x_1\\) scale), then \\(\\operatorname{logit}(\\mu)\\) goes up by an amount \\(\\beta_1\\). And whenever \\(x_2\\) goes up by \\(1\\) unit (on the \\(x_2\\) scale), then \\(\\operatorname{logit}(\\mu)\\) goes up by an amount \\(\\beta_2\\). Thus, the regression coefficients are telling us about increases in \\(\\operatorname{logit}(\\mu)\\). To understand the regression coefficients, we need to understand \\(\\operatorname{logit}(\\mu)\\). (pp. 629–630) Given the logit function is the inverse of the logistic, which itself is \\[\\operatorname{logistic}(x) = \\frac{1}{1 + \\exp (−x)},\\] and given the formula \\[\\operatorname{logit}(\\mu) = \\log \\left (\\frac{\\mu}{1 - \\mu} \\right),\\] where \\[0 &lt; \\mu &lt; 1,\\] it may or may not be clear that the results of our logistic regression models have a nonlinear relation with the actual parameter of interest, \\(\\mu\\), which, recall, is the probability our criterion variable is 1 (e.g., male == 1). To get a sense of that nonlinear relation, we might make a plot. tibble(mu = seq(from = 0, to = 1, length.out = 300)) %&gt;% mutate(logit_mu = log(mu / (1 - mu))) %&gt;% ggplot(aes(x = mu, y = logit_mu)) + geom_line(color = pm[3], size = 1.5) + labs(x = expression(mu~&quot;(i.e., the probability space)&quot;), y = expression(logit~mu~&quot;(i.e., the parameter space)&quot;)) + theme(legend.position = &quot;none&quot;) So whereas our probability space is bound between 0 and 1, the parameter space shoots off into negative and positive infinity. Also, \\[\\operatorname{logit}(\\mu) = \\log \\left (\\frac{p(y = 1)}{p(y = 0)} \\right ).\\] Thus, “the ratio, \\(p(y = 1) / p(y = 0)\\), is called the odds of outcome 1 to outcome 0, and therefore \\(\\operatorname{logit}(\\mu)\\) is the log odds of outcome 1 to outcome 0” (p. 630). Here’s a table layout of the height/weight examples in the middle of page 630. tibble(b0 = -50, b1 = .02, b2 = .7, weight = 160, inches = c(63:64, 67:68)) %&gt;% mutate(logit_mu = b0 + b1 * weight + b2 * inches) %&gt;% mutate(log_odds = logit_mu) %&gt;% mutate(p_male = 1 / (1 + exp(-log_odds))) %&gt;% knitr::kable() b0 b1 b2 weight inches logit_mu log_odds p_male -50 0.02 0.7 160 63 -2.7 -2.7 0.0629734 -50 0.02 0.7 160 64 -2.0 -2.0 0.1192029 -50 0.02 0.7 160 67 0.1 0.1 0.5249792 -50 0.02 0.7 160 68 0.8 0.8 0.6899745 Thus, a regression coefficient in logistic regression indicates how much a \\(1\\) unit change of the predictor increases the log odds of outcome \\(1\\). A regression coefficient of \\(0.5\\) corresponds to a rate of probability change of about \\(12.5\\) percentage points per \\(x\\)-unit at the threshold \\(x\\) value. A regression coefficient of \\(1.0\\) corresponds to a rate of probability change of about \\(24.4\\) percentage points per \\(x\\)-unit at the threshold \\(x\\) value. When \\(x\\) is much larger or smaller than the threshold \\(x\\) value, the rate of change in probability is smaller, even though the rate of change in log odds is constant. (pp. 630–631) 21.2.2 When there are few 1’s or 0’s in the data. In logistic regression, you can think of the parameters as describing the boundary between the \\(0\\)’s and the \\(1\\)’s. If there are many \\(0\\)’s and \\(1\\)’s, then the estimate of the boundary parameters can be fairly accurate. But if there are few \\(0\\)’s or few \\(1\\)’s, the boundary can be difficult to identify very accurately, even if there are many data points overall. (p. 631) As far as I can tell, Kruschke must have used \\(n = 500\\) to simulate the data he displayed in Figure 21.5. Using the coefficient values he displayed in the middle of page 631, here’s an attempt at replicating them. b0 &lt;- -3 b1 &lt;- 1 n &lt;- 500 set.seed(21) d_rare &lt;- tibble(x = rnorm(n, mean = 0, sd = 1)) %&gt;% mutate(mu = b0 + b1 * x) %&gt;% mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu)))) glimpse(d_rare) ## Rows: 500 ## Columns: 3 ## $ x &lt;dbl&gt; 0.793013171, 0.522251264, 1.746222241, -1.271336123, 2.197389533, 0.433130777, -1.57019… ## $ mu &lt;dbl&gt; -2.2069868, -2.4777487, -1.2537778, -4.2713361, -0.8026105, -2.5668692, -4.5701996, -3.… ## $ y &lt;int&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, … We’re ready to fit the model. So far, we’ve been following along with Kruschke by using the Bernoulli distribution (i.e., family = bernoulli) in our brms models. Let’s get frisky and use the \\(n = 1\\) binomial distribution, here. You’ll see it yields the same results. fit21.3 &lt;- brm(data = d_rare, family = binomial, y | trials(1) ~ 1 + x, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.03&quot;) Recall that when you use the binomial distribution in newer versions of brms, you need to use the trials() syntax to tell brm() how many trials each row in the data corresponds to. Anyway, behold the summary. print(fit21.3) ## Family: binomial ## Links: mu = logit ## Formula: y | trials(1) ~ 1 + x ## Data: d_rare (Number of observations: 500) ## Draws: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -3.01 0.24 -3.49 -2.56 1.00 2564 3145 ## x 1.03 0.20 0.65 1.43 1.00 2532 3897 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Looks like the model did a good job recapturing those data-generating b0 and b1 values. Now make the top left panel of Figure 21.5. draws &lt;- as_draws_df(fit21.3) # unclear if Kruschke still used 20 draws or not # perhaps play with the `n_draw` values n_draw &lt;- 20 length &lt;- 100 set.seed(21) draws %&gt;% # take 20 random samples of the posterior draws slice_sample(n = n_draw) %&gt;% # add in a sequence of x expand(nesting(.draw, b_Intercept, b_x), x = seq(from = -3.5, to = 3.5, length.out = length)) %&gt;% # compute the estimates of interest mutate(y = inv_logit_scaled(b_Intercept + b_x * x), thresh = -b_Intercept / b_x) %&gt;% # plot! ggplot(aes(x = x)) + geom_hline(yintercept = .5, color = pm[7], size = 1/2) + geom_vline(aes(xintercept = thresh, group = .draw), color = pm[6], size = 2/5, linetype = 2) + geom_line(aes(y = y, group = .draw), color = pm[1], size = 1/3, alpha = 2/3) + geom_point(data = d_rare, aes(y = y), alpha = 1/3, color = pm[1]) + ggtitle(&quot;Data with Post. Pred.&quot;) + coord_cartesian(xlim = range(d_rare$x)) Here are the two subplots at the bottom, left. draws %&gt;% mutate(Intercept = b_Intercept, x = b_x) %&gt;% pivot_longer(Intercept:x) %&gt;% ggplot(aes(x = value, y = 0)) + stat_wilke() + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) Since our data were simulated without the benefit of knowing how Kruschke set his seed and such, our results will only approximate those in the text. Okay, now we need to simulate the complimentary data, those for which \\(y = 1\\) is a less-rare event. b0 &lt;- 0 b1 &lt;- 1 n &lt;- 500 set.seed(21) d_not_rare &lt;- tibble(x = rnorm(n, mean = 0, sd = 1)) %&gt;% mutate(mu = b0 + b1 * x) %&gt;% mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu)))) glimpse(d_not_rare) ## Rows: 500 ## Columns: 3 ## $ x &lt;dbl&gt; 0.793013171, 0.522251264, 1.746222241, -1.271336123, 2.197389533, 0.433130777, -1.57019… ## $ mu &lt;dbl&gt; 0.793013171, 0.522251264, 1.746222241, -1.271336123, 2.197389533, 0.433130777, -1.57019… ## $ y &lt;int&gt; 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, … Fitting this model is just like before. fit21.4 &lt;- update(fit21.3, newdata = d_not_rare, iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.04&quot;) ## The desired updates require recompiling the model Behold the summary. print(fit21.4) ## Family: binomial ## Links: mu = logit ## Formula: y | trials(1) ~ 1 + x ## Data: d_not_rare (Number of observations: 500) ## Draws: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.08 0.10 -0.11 0.28 1.00 8227 5871 ## x 0.91 0.11 0.68 1.13 1.00 5992 5514 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the code for the main plot in Figure 21.5.b. draws &lt;- as_draws_df(fit21.4) set.seed(21) p1 &lt;- draws %&gt;% slice_sample(n = n_draw) %&gt;% expand(nesting(.draw, b_Intercept, b_x), x = seq(from = -3.5, to = 3.5, length.out = length)) %&gt;% mutate(y = inv_logit_scaled(b_Intercept + b_x * x), thresh = -b_Intercept / b_x) %&gt;% ggplot(aes(x = x, y = y)) + geom_hline(yintercept = .5, color = pm[7], size = 1/2) + geom_vline(aes(xintercept = thresh, group = .draw), color = pm[6], size = 2/5, linetype = 2) + geom_line(aes(group = .draw), color = pm[1], size = 1/3, alpha = 2/3) + geom_point(data = d_rare, alpha = 1/3, color = pm[1]) + ggtitle(&quot;Data with Post. Pred.&quot;) + coord_cartesian(xlim = c(-3, 3)) Now make the two subplots at the bottom. p2 &lt;- draws %&gt;% mutate(Intercept = b_Intercept, x = b_x) %&gt;% pivot_longer(Intercept:x) %&gt;% ggplot(aes(x = value, y = 0)) + stat_wilke() + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) This time we’ll combine them with patchwork. p3 &lt;- plot_spacer() p1 / (p2 + p3 + plot_layout(widths = c(2, 1))) + plot_layout(height = c(4, 1)) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. You can see in Figure 21.5 that the estimate of the slope (and of the intercept) is more certain in the right panel than in the left panel. The \\(95\\%\\) HDI on the slope, \\(\\beta_1\\), is much wider in the left panel than in the right panel, and you can see that the logistic curves in the left panel have greater variation in steepness than the logistic curves in the right panel. The analogous statements hold true for the intercept parameter. Thus, if you are doing an experimental study and you can manipulate the \\(x\\) values, you will want to select \\(x\\) values that yield about equal numbers of \\(0\\)’s and \\(1\\)’s for the \\(y\\) values overall. If you are doing an observational study, such that you cannot control any independent variables, then you should be aware that the parameter estimates may be surprisingly ambiguous if your data have only a small proportion of \\(0\\)’s or \\(1\\)’s. (pp. 631–632) 21.2.3 Correlated predictors. “Another important cause of parameter uncertainty is correlated predictors. This issue was previously discussed at length, but the context of logistic regression provides novel illustration in terms of level contours” (p. 632). As far as I can tell, Kruschke chose about \\(n = 200\\) for the data in this example. After messing around with correlations for a bit, it seems \\(\\rho_{x_1, x_2} = .975\\) looks about right. To my knowledge, the best way to simulate multivariate Gaussian data with a particular correlation is with the MASS::mvrnorm() function. Since we’ll be using standardized \\(x\\)-variables, we’ll need to specify our \\(n\\), the desired correlation matrix, and a vector of means. Then we’ll be ready to do the actual simulation with mvrnorm(). n &lt;- 200 # correlation matrix s &lt;- matrix(c(1, .975, .975, 1), nrow = 2, ncol = 2) # mean vector m &lt;- c(0, 0) # simulate set.seed(21) d &lt;- MASS::mvrnorm(n = n, mu = m, Sigma = s) %&gt;% data.frame() %&gt;% set_names(str_c(&quot;x&quot;, 1:2)) Let’s confirm the correlation coefficient. cor(d) ## x1 x2 ## x1 1.0000000 0.9730091 ## x2 0.9730091 1.0000000 Solid. Now we’ll use the \\(\\beta\\) values from page 633 to simulate the data set by including our dichotomous criterion variable, y. b0 &lt;- 0 b1 &lt;- 1 b2 &lt;- 1 set.seed(21) d &lt;- d %&gt;% mutate(mu = b0 + b1 * x1 + b2 * x2) %&gt;% mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu)))) Fit the model with the highly-correlated predictors. fit21.5 &lt;- brm(data = d, family = binomial, y | trials(1) ~ 1 + x1 + x2, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.05&quot;) Behold the summary. print(fit21.5) ## Family: binomial ## Links: mu = logit ## Formula: y | trials(1) ~ 1 + x1 + x2 ## Data: d (Number of observations: 200) ## Draws: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.03 0.18 -0.39 0.31 1.00 3948 3894 ## x1 -0.06 0.70 -1.42 1.32 1.00 3085 3747 ## x2 2.08 0.76 0.59 3.59 1.00 3123 3733 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We did a good job recapturing Kruschke’s \\(\\beta\\)s in terms of our posterior means, but notice how large those posterior \\(\\textit{SD}\\)s are for \\(\\beta_1\\) and \\(\\beta_2\\). To get a better sense, let’s look at them in a coefficient plot before continuing on with the text. as_draws_df(fit21.5) %&gt;% pivot_longer(b_Intercept:b_x2) %&gt;% ggplot(aes(x = value, y = name)) + stat_gradientinterval(point_interval = mode_hdi, .width = c(.5, .95), color = pm[1], fill = pm[4]) + labs(x = NULL, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) Them are some sloppy estimates! But we digress. Here’s our version of Figure 21.6.a. set.seed(21) as_draws_df(fit21.5) %&gt;% slice_sample(n = 20) %&gt;% expand(nesting(.draw, b_Intercept, b_x1, b_x2), x1 = c(-4, 4)) %&gt;% # this follows the equation near the top of p. 629 mutate(x2 = (-b_Intercept / b_x2) + (-b_x1 / b_x2) * x1) %&gt;% # now plot ggplot(aes(x = x1, y = x2)) + geom_line(aes(group = .draw), color = pm[7], size = 2/5, alpha = 2/3) + geom_text(data = d, aes(label = y, color = y == 1), size = 2.5) + scale_color_manual(values = pm[c(4, 1)]) + ggtitle(&quot;Data with Post. Pred.&quot;) + coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) + theme(legend.position = &quot;none&quot;) It can be easy to under-appreciate how sensitive this plot is to the seed you set for sample_n(). To give a better sense of the uncertainty in the posterior for the threshold, here we show the plot for several different seeds. # make a custom function different_seed &lt;- function(i) { set.seed(i) as_draws_df(fit21.5) %&gt;% slice_sample(n = 20) %&gt;% expand(nesting(.draw, b_Intercept, b_x1, b_x2), x1 = c(-4, 4)) %&gt;% mutate(x2 = (-b_Intercept / b_x2) + (-b_x1 / b_x2) * x1) } # specify your seeds tibble(seed = 1:9) %&gt;% # pump those seeds into the `different_seed()` function mutate(sim = map(seed, different_seed)) %&gt;% unnest(sim) %&gt;% mutate(seed = str_c(&quot;seed: &quot;, seed)) %&gt;% # plot ggplot(aes(x = x1, y = x2)) + geom_line(aes(group = .draw), color = pm[7], size = 2/5, alpha = 2/3) + geom_text(data = d, aes(label = y, color = y == 1), size = 1.5) + scale_color_manual(values = pm[c(4, 1)]) + ggtitle(&quot;Data with Post. Pred.&quot;) + coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ seed) To make our version of the pairs plots in Figure 21.6.b, we’ll bring back some of our old tricks with GGally. First, we define our custom settings for the upper triangle, the diagonal, and lower triangle. library(GGally) my_upper &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_point(size = 1/2, shape = 1, alpha = 1/4, color = pm[2]) # geom_point(size = 1/4, alpha = 1/4, color = pm[2]) } my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + stat_wilke() + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(ylim = c(-0.01, NA)) } my_lower &lt;- function(data, mapping, ...) { # get the x and y data to use the other code x &lt;- eval_data_col(data, mapping$x) y &lt;- eval_data_col(data, mapping$y) # compute the correlations corr &lt;- cor(x, y, method = &quot;p&quot;, use = &quot;pairwise&quot;) abs_corr &lt;- abs(corr) # plot the cor value ggally_text( label = formatC(corr, digits = 2, format = &quot;f&quot;) %&gt;% str_replace(., &quot;0.&quot;, &quot;.&quot;), mapping = aes(), color = pm[1], size = 4) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.background = element_rect(fill = pm[8])) } Now we wrangle and plot. as_draws_df(fit21.5) %&gt;% transmute(`Intercept~(beta[0])` = b_Intercept, `x1~(beta[1])` = b_x1, `x2~(beta[2])` = b_x2) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = label_parsed) 21.2.4 Interaction of metric predictors. “There may be applications in which it is meaningful to consider a multiplicative interaction of metric predictors” (p. 633). Kruschke didn’t walk through an analysis in this section, but it’s worth the practice. Let’s simulate data based on the formula he gave in Figure 21.7, top right. n &lt;- 500 b0 &lt;- 0 b1 &lt;- 0 b2 &lt;- 0 b3 &lt;- 4 set.seed(21) d &lt;- tibble(x1 = rnorm(n, mean = 0, sd = 1), x2 = rnorm(n, mean = 0, sd = 1)) %&gt;% mutate(mu = b1 * x1 + b2 * x2 + b3 * x1 * x2 - b0) %&gt;% mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu)))) To fit the interaction model, let’s go back to the Bernoulli likelihood. fit21.6 &lt;- brm(data = d, family = bernoulli, y ~ 1 + x1 + x2 + x1:x2, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.06&quot;) Looks like the model did a nice job recapturing the data-generating parameters. print(fit21.6) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + x1 + x2 + x1:x2 ## Data: d (Number of observations: 500) ## Draws: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.17 0.12 -0.42 0.07 1.00 7746 6301 ## x1 -0.21 0.17 -0.55 0.12 1.00 8480 6110 ## x2 -0.02 0.16 -0.34 0.30 1.00 6981 5825 ## x1:x2 4.27 0.43 3.46 5.15 1.00 6987 5446 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I’m not quite sure how to expand Kruschke’s equation from the top of page 629 to our interaction model. But no worries. We can take a slightly different approach to show the consequences of our interaction model on the probability \\(y = 1\\). First, we define our newdata and then get the Estimates from fitted(). Then we wrangle as usual. length &lt;- 100 nd &lt;- crossing(x1 = seq(from = -3.5, to = 3.5, length.out = length), x2 = seq(from = -3.5, to = 3.5, length.out = length)) f &lt;- fitted(fit21.6, newdata = nd, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% transmute(prob = Estimate %&gt;% inv_logit_scaled()) Now all we have to do is integrate our f results with the nd and original d data and then we can plot. f %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = x1, y = x2)) + geom_raster(aes(fill = prob), interpolate = T) + geom_text(data = d, aes(label = y, color = y == 1), size = 2.75, show.legend = F) + scale_color_manual(values = pm[c(8, 1)]) + scale_fill_gradientn(&quot;m&quot;, colours = pnw_palette(name = &quot;Mushroom&quot;, n = 101), limits = c(0, 1)) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + coord_cartesian(xlim = range(nd$x1), ylim = range(nd$x2)) Instead of a simple threshold line, we get to visualize our interaction as a checkerboard-like probability plane. If you look back to Figure 21.7, you’ll see this is our 2D version of Kruschke’s wireframe plot on the top right. 21.3 Robust logistic regression With the robust logistic regression approach, we will describe the data as being a mixture of two different sources. One source is the logistic function of the predictor(s). The other source is sheer randomness or “guessing,” whereby the \\(y\\) value comes from the flip of a fair coin: \\(y \\sim \\operatorname{Bernoulli}(\\mu = 1/2)\\). We suppose that every data point has a small chance, \\(\\alpha\\), of being generated by the guessing process, but usually, with probability \\(1 - \\alpha\\), the \\(y\\) value comes from the logistic function of the predictor. With the two sources combined, the predicted probability that \\(y = 1\\) is \\[\\mu = \\alpha \\cdot \\frac{1}{2} + (1 - \\alpha) \\cdot \\operatorname{logistic} \\bigg ( \\beta_0 + \\sum_j \\beta_j x_j \\bigg )\\] Notice that when the guessing coefficient is zero, then the conventional logistic model is completely recovered. When the guessing coefficient is one, then the y values are completely random. (p. 635) Here’s what Kruschke’s \\(\\operatorname{Beta}(1, 9)\\) prior for \\(\\alpha\\) looks like. tibble(x = seq(from = 0, to = 1, length.out = 200)) %&gt;% ggplot(aes(x = x, y = dbeta(x, 1, 9))) + geom_area(fill = pm[4]) + scale_x_continuous(NULL, breaks = 0:5 / 5, expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + ggtitle(expression(&quot;Beta&quot;*(1*&quot;, &quot;*9))) To fit the brms analogue to Kruschke’s robust logistic regression model, we’ll need to adopt what Bürkner calls the non-linear syntax, which you can learn about in detail with his (2022) vignette, Estimating non-linear models with brms. fit21.7 &lt;- brm(data = my_data, family = bernoulli(link = identity), bf(male ~ a * .5 + (1 - a) * 1 / (1 + exp(-1 * (b0 + b1 * weight_z))), a + b0 + b1 ~ 1, nl = TRUE), prior = c(prior(normal(0, 2), nlpar = &quot;b0&quot;), prior(normal(0, 2), nlpar = &quot;b1&quot;), prior(beta(1, 9), nlpar = &quot;a&quot;, lb = 0, ub = 1)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.07&quot;) And just for clarity, you can do the same thing with family = binomial(link = identity), too. Just don’t forget to specify the number of trials with trials(). But to explain what’s going on with our syntax, above, I think it’s best to quote Bürkner at length: When looking at the above code, the first thing that becomes obvious is that we changed the formula syntax to display the non-linear formula including predictors (i.e., [weight_z]) and parameters (i.e., [a, b0, and b1]) wrapped in a call to [the bf() function]. This stands in contrast to classical R formulas, where only predictors are given and parameters are implicit. The argument [a + b0 + b1 ~ 1] serves two purposes. First, it provides information, which variables in formula are parameters, and second, it specifies the linear predictor terms for each parameter. In fact, we should think of non-linear parameters as placeholders for linear predictor terms rather than as parameters themselves (see also the following examples). In the present case, we have no further variables to predict [a, b0, and b1] and thus we just fit intercepts that represent our estimates of [\\(\\alpha\\), \\(\\beta_0\\), and \\(\\beta_1\\)]. The formula [a + b0 + b1 ~ 1] is a short form of [a ~ 1, b0 ~ 1, b1 ~ 1] that can be used if multiple non-linear parameters share the same formula. Setting nl = TRUE tells brms that the formula should be treated as non-linear. In contrast to generalized linear models, priors on population-level parameters (i.e., ‘fixed effects’) are often mandatory to identify a non-linear model. Thus, brms requires the user to explicitely specify these priors. In the present example, we used a [beta(1, 9) prior on (the population-level intercept of) a, while we used a normal(0, 4) prior on both (population-level intercepts of) b0 and b1]. Setting priors is a non-trivial task in all kinds of models, especially in non-linear models, so you should always invest some time to think of appropriate priors. Quite often, you may be forced to change your priors after fitting a non-linear model for the first time, when you observe different MCMC chains converging to different posterior regions. This is a clear sign of an idenfication problem and one solution is to set stronger (i.e., more narrow) priors. (emphasis in the original) Now, behold the summary. print(fit21.7) ## Family: bernoulli ## Links: mu = identity ## Formula: male ~ a * 0.5 + (1 - a) * 1/(1 + exp(-1 * (b0 + b1 * weight_z))) ## a ~ 1 ## b0 ~ 1 ## b1 ~ 1 ## Data: my_data (Number of observations: 110) ## Draws: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_Intercept 0.20 0.09 0.04 0.39 1.00 3184 2459 ## b0_Intercept 0.34 0.44 -0.40 1.29 1.00 3676 3642 ## b1_Intercept 2.61 0.84 1.18 4.52 1.00 2916 3869 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s a quick and dirty look at the conditional effects for weight_z. conditional_effects(fit21.7) %&gt;% plot(points = T, line_args = list(color = pm[2], fill = pm[6]), point_args = list(color = pm[1], alpha = 3/4)) The way we prep for our version of Figure 21.8 is a minor extension what we did for Figure 21.3, above. Here we make the top panel. The biggest change, from before, is our adjusted formula for computing male. draws &lt;- as_draws_df(fit21.7) p1 &lt;- draws %&gt;% slice_sample(n = n_draw) %&gt;% expand(nesting(.draw, b_a_Intercept, b_b0_Intercept, b_b1_Intercept), weight_z = seq(from = -2, to = 3.5, length.out = length)) %&gt;% # compare this to Kruschke&#39;s mu[i] code at the top of page 636 mutate(male = 0.5 * b_a_Intercept + (1 - b_a_Intercept) * inv_logit_scaled(b_b0_Intercept + b_b1_Intercept * weight_z), weight = weight_z * sd(my_data$weight) + mean(my_data$weight), thresh = -b_b0_Intercept / b_b1_Intercept * sd(my_data$weight) + mean(my_data$weight)) %&gt;% ggplot(aes(x = weight, y = male)) + geom_hline(yintercept = .5, color = pm[7], size = 1/2) + geom_vline(aes(xintercept = thresh, group = .draw), color = pm[6], size = 2/5, linetype = 2) + geom_line(aes(group = .draw), color = pm[1], size = 1/3, alpha = 2/3) + geom_point(data = my_data, alpha = 1/3, color = pm[1]) + labs(title = &quot;Data with Post. Pred.&quot;, y = &quot;male&quot;) + coord_cartesian(xlim = range(my_data$weight)) Here we make the marginal-distribution plots for our versions of the lower panels of Figure 21.8. p2 &lt;- draws %&gt;% mutate(Intercept = b_b0_Intercept - (b_b1_Intercept * mean(my_data$weight) / sd(my_data$weight)), weight = b_b1_Intercept / sd(my_data$weight), guessing = b_a_Intercept) %&gt;% pivot_longer(Intercept:guessing) %&gt;% mutate(name = factor(name, levels = c(&quot;Intercept&quot;, &quot;weight&quot;, &quot;guessing&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_wilke() + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 3) Now combine them with patchwork and behold the glory. p1 / p2 + plot_layout(height = c(4, 1)) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Here are the ggpairs() plots. as_draws_df(fit21.7) %&gt;% transmute(`Intercept~(beta[0])` = b_b0_Intercept, `weight~(beta[1])` = b_b1_Intercept, guessing = b_a_Intercept) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = label_parsed) Look at how that sweet Stan-based HMC reduced the correlation between \\(\\beta_0\\) and \\(\\beta_1\\). For more on this approach to robust logistic regression in brms and an alternative suggested by Andrew Gelman, check out a thread from the Stan Forums, Translating robust logistic regression from rstan to brms. Yet do note that subsequent work suggests Gelman’s alternative approach is not as robust as originally thought (see here). 21.4 Nominal predictors “We now turn our attention from metric predictors to nominal predictors” (p. 636). 21.4.1 Single group. If we have just a single group and no other predictors, that’s just an intercept-only model. Back in the earlier chapters we thought of such a model as \\[\\begin{align*} y &amp; \\sim \\operatorname{Bernoulli}(\\theta) \\\\ \\theta &amp; \\sim \\operatorname{Beta}(a, b). \\end{align*}\\] Now we’re expressing the model as \\[\\begin{align*} y &amp; \\sim \\operatorname{Bernoulli}(\\mu) \\\\ \\mu &amp; \\sim \\operatorname{logistic}(\\beta_0). \\end{align*}\\] For that \\(\\beta_0\\), we typically use a Gaussian prior of the form \\[\\beta_0 \\sim \\operatorname{Normal}(M_0, S_0).\\] To further explore what this means, we might make the model diagram in Figure 21.10. p1 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pm[5]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pm[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = pm[1], hjust = 0, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5), plot.background = element_rect(fill = pm[8], color = pm[8])) # an annotated arrow p2 &lt;- tibble(x = .5, y = 1, xend = .5, yend = 0) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pm[1]) + annotate(geom = &quot;text&quot;, x = .4, y = .4, label = &quot;&#39;~&#39;&quot;, size = 10, color = pm[1], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # likelihood formula p3 &lt;- tibble(x = .5, y = .5, label = &quot;logistic(beta[0])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pm[1], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # a second annotated arrow p4 &lt;- tibble(x = .375, y = 1/2, label = &quot;&#39;=&#39;&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, color = pm[1], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow, color = pm[1]) + xlim(0, 1) + theme_void() # bar plot of Bernoulli data p5 &lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = pm[5], width = .4) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;Bernoulli&quot;, size = 7, color = pm[1]) + annotate(geom = &quot;text&quot;, x = .5, y = .94, label = &quot;mu&quot;, size = 7, color = pm[1], family = &quot;Times&quot;, parse = T) + xlim(-.75, 1.75) + theme_void() + theme(axis.line.x = element_line(size = 0.5), plot.background = element_rect(fill = pm[8], color = pm[8])) # another annotated arrow p6 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = pm[1], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow, color = pm[1]) + xlim(0, 1) + theme_void() # some text p7 &lt;- tibble(x = 1, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pm[1], parse = T, family = &quot;Times&quot;) + xlim(0, 2) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 2, r = 7), area(t = 3, b = 3, l = 2, r = 7), area(t = 4, b = 4, l = 1, r = 6), area(t = 5, b = 5, l = 1, r = 6), area(t = 6, b = 7, l = 1, r = 6), area(t = 8, b = 8, l = 1, r = 6), area(t = 9, b = 9, l = 1, r = 6) ) # combine and plot! (p1 + p2 + p3 + p4 + p5 + p6 + p7) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) In a situation where we don’t have strong prior substantive knowledge, we often set \\(M_0 = 0\\), which puts the probability mass around \\(\\theta = .5\\), a reasonable default hypothesis. Often times \\(S_0\\) is some modest single-digit integer like 2 or 4. To get a sense of how different Gaussians translate to the beta distribution, we’ll recreate Figure 21.11. # this will help streamline the conversion logistic &lt;- function(x) { 1 / (1 + exp(-x)) } # wrangle crossing(m_0 = 0:1, s_0 = c(.5, 1, 2)) %&gt;% mutate(key = str_c(&quot;mu == logistic(beta %~%&quot;, &quot; N(&quot;, m_0, &quot;, &quot;, s_0, &quot;))&quot;), sim = pmap(list(2e6, m_0, s_0), rnorm)) %&gt;% unnest(sim) %&gt;% mutate(sim = logistic(sim)) %&gt;% # plot ggplot(aes(x = sim, y = ..density..)) + geom_histogram(color = pm[8], fill = pm[7], size = 1/3, bins = 20, boundary = 0) + geom_line(stat = &quot;density&quot;, size = 1, color = pm[3]) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(mu)) + facet_wrap(~ key, scales = &quot;free_y&quot;, labeller = label_parsed) ## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. So the prior distribution doesn’t even flatten out until you’re somewhere between \\(S_0 = 1\\) and \\(S_0 = 2\\). Just for kicks, here we break that down a little further. # wrangle tibble(m_0 = 0, s_0 = c(1.25, 1.5, 1.75)) %&gt;% mutate(key = str_c(&quot;mu == logistic(beta %~%&quot;, &quot; N(&quot;, m_0, &quot;, &quot;, s_0, &quot;))&quot;), sim = pmap(list(1e7, m_0, s_0), rnorm)) %&gt;% unnest(sim) %&gt;% mutate(sim = logistic(sim)) %&gt;% # plot ggplot(aes(x = sim, y = ..density..)) + geom_histogram(color = pm[8], fill = pm[7], size = 1/3, bins = 20, boundary = 0) + geom_line(stat = &quot;density&quot;, size = 1, color = pm[3]) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(mu)) + facet_wrap(~ key, scales = &quot;free_y&quot;, labeller = label_parsed, ncol = 3) Here’s the basic brms analogue to Kruschke’s JAGS code from the bottom of page 639. fit &lt;- brm(data = my_data, family = bernoulli(link = identity), y ~ 1, prior(beta(1, 1), class = Intercept)) Here’s the brms analogue to Kruschke’s JAGS code at the top of page 641. fit &lt;- brm(data = my_data, family = bernoulli, y ~ 1, prior(normal(0, 2), class = &quot;Intercept&quot;)) 21.4.2 Multiple groups. If there’s only one group, we don’t need a grouping variable. But that’s a special case. Now we show the more general approach with multiple groups. 21.4.2.1 Example: Baseball again. Load the baseball data. my_data &lt;- read_csv(&quot;data.R/BattingAverage.csv&quot;) glimpse(my_data) ## Rows: 948 ## Columns: 6 ## $ Player &lt;chr&gt; &quot;Fernando Abad&quot;, &quot;Bobby Abreu&quot;, &quot;Tony Abreu&quot;, &quot;Dustin Ackley&quot;, &quot;Matt Adams&quot;, … ## $ PriPos &lt;chr&gt; &quot;Pitcher&quot;, &quot;Left Field&quot;, &quot;2nd Base&quot;, &quot;2nd Base&quot;, &quot;1st Base&quot;, &quot;Pitcher&quot;, &quot;Pitc… ## $ Hits &lt;dbl&gt; 1, 53, 18, 137, 21, 0, 0, 2, 150, 167, 0, 128, 66, 3, 1, 81, 180, 36, 150, 0,… ## $ AtBats &lt;dbl&gt; 7, 219, 70, 607, 86, 1, 1, 20, 549, 576, 1, 525, 275, 12, 8, 384, 629, 158, 5… ## $ PlayerNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22… ## $ PriPosNumber &lt;dbl&gt; 1, 7, 4, 4, 3, 1, 1, 3, 3, 4, 1, 5, 4, 2, 7, 4, 6, 8, 9, 1, 2, 5, 1, 1, 7, 2,… 21.4.2.2 The model. I’m not aware that Kruschke’s modeling approach for this example will work well within the brms paradigm. Accordingly, our version of the hierarchical model diagram in Figure 21.12 will differ in important ways from the one in the text. # bracket p1 &lt;- tibble(x = .4, y = .25, label = &quot;{_}&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, color = pm[3], family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # normal density p2 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pm[5]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pm[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = pm[1], hjust = 0, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5), plot.background = element_rect(fill = pm[8], color = &quot;white&quot;, size = 1)) # second normal density p3 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pm[5]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pm[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;0&quot;, &quot;sigma[beta]&quot;), size = 7, color = pm[1], hjust = 0, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5), plot.background = element_rect(fill = pm[8], color = &quot;white&quot;, size = 1)) # plain arrow p4 &lt;- tibble(x = .4, y = 1, xend = .4, yend = .25) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pm[1]) + xlim(0, 1) + theme_void() # likelihood formula p5 &lt;- tibble(x = .5, y = .3, label = &quot;logistic(beta[0]+sum()[italic(j)]*beta[&#39;[&#39;*italic(j)*&#39;]&#39;]*italic(x)[&#39;[&#39;*italic(j)*&#39;]&#39;](italic(i)))&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pm[1], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # two annotated arrows p6 &lt;- tibble(x = c(.2, .8), y = 1, xend = c(.37, .63), yend = .1) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pm[1]) + annotate(geom = &quot;text&quot;, x = c(.22, .66, .77), y = .55, label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;), size = c(10, 10, 7), color = pm[1], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # binomial density p7 &lt;- tibble(x = 0:7) %&gt;% ggplot(aes(x = x, y = (dbinom(x, size = 7, prob = .625)) / max(dbinom(x, size = 7, prob = .625)))) + geom_col(fill = pm[5], width = .4) + annotate(geom = &quot;text&quot;, x = 3.5, y = .2, label = &quot;binomial&quot;, size = 7) + annotate(geom = &quot;text&quot;, x = 7, y = .85, label = &quot;italic(N)[italic(i)*&#39;|&#39;*italic(j)]&quot;, size = 7, family = &quot;Times&quot;, parse = TRUE) + coord_cartesian(xlim = c(-1, 8), ylim = c(0, 1.2)) + theme_void() + theme(axis.line.x = element_line(size = 0.5), plot.background = element_rect(fill = pm[8], color = pm[8])) # one annotated arrow p8 &lt;- tibble(x = c(.375, .5), y = c(.75, .3), label = c(&quot;&#39;=&#39;&quot;, &quot;mu[italic(i)*&#39;|&#39;*italic(j)]&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = pm[1], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = .425, arrow = my_arrow, color = pm[1]) + xlim(0, 1) + theme_void() # another annotated arrow p9 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)*&#39;|&#39;*italic(j)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = pm[1], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow, color = pm[1]) + xlim(0, 1) + theme_void() # some text p10 &lt;- tibble(x = 1, y = .5, label = &quot;italic(y[i])[&#39;|&#39;][italic(j)]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pm[1], parse = T, family = &quot;Times&quot;) + xlim(0, 2) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 6, r = 8), area(t = 4, b = 5, l = 1, r = 3), area(t = 4, b = 5, l = 5, r = 7), area(t = 3, b = 4, l = 6, r = 8), area(t = 7, b = 8, l = 1, r = 7), area(t = 6, b = 7, l = 1, r = 7), area(t = 11, b = 12, l = 3, r = 5), area(t = 9, b = 11, l = 3, r = 5), area(t = 13, b = 14, l = 3, r = 5), area(t = 15, b = 15, l = 3, r = 5) ) # combine and plot! (p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) I recommend we fit a hierarchical aggregated-binomial regression model in place of the one Kruschke used. With this approach, we might express the statistical model as \\[\\begin{align*} \\text{Hits}_i &amp; \\sim \\operatorname{Binomial}(\\text{AtBats}_i, p_i) \\\\ \\operatorname{logit}(p_i) &amp; = \\beta_0 + \\beta_{\\text{PriPos}_i} + \\beta_{\\text{PriPos:Player}_i} \\\\ \\beta_0 &amp; \\sim \\operatorname{Normal}(0, 2) \\\\ \\beta_\\text{PriPos} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\text{PriPos}) \\\\ \\beta_\\text{PriPos:Player} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\text{PriPos:Player}) \\\\ \\sigma_\\text{PriPos} &amp; \\sim \\operatorname{HalfCauchy}(0, 1) \\\\ \\sigma_\\text{PriPos:Player} &amp; \\sim \\operatorname{HalfCauchy}(0, 1). \\end{align*}\\] Here’s how to fit the model with brms. Notice we’re finally making good use of the trials() syntax. This is because we’re fitting an aggregated binomial model. Instead of our criterion variable Hits being a vector of 0’s and 1’s, it’s the number of successful trials given the total number of trials, which is listed in the AtBats vector. Aggregated binomial. fit21.8 &lt;- brm(data = my_data, family = binomial(link = &quot;logit&quot;), Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player), prior = c(prior(normal(0, 2), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, control = list(adapt_delta = .99), file = &quot;fits/fit21.08&quot;) 21.4.2.3 Results. Before we start plotting, review the model summary. print(fit21.8) ## Family: binomial ## Links: mu = logit ## Formula: Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player) ## Data: my_data (Number of observations: 948) ## Draws: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup draws = 8000 ## ## Group-Level Effects: ## ~PriPos (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.32 0.10 0.19 0.57 1.00 2889 4640 ## ## ~PriPos:Player (Number of levels: 948) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.14 0.01 0.12 0.15 1.00 3535 5208 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.17 0.11 -1.40 -0.94 1.00 1387 2427 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you look closely at our model versus the one in the text, you’ll see ours has fewer parameters. As a down-the-line consequence, our model doesn’t support a direct analogue to the plot at the top of Figure 21.13. However, we can come close. Rather than modeling the position-based probabilities as multiple draws of beta distributions, we can simply summarize our probabilities by their posterior distributions. library(ggridges) # define our new data, `nd` nd &lt;- my_data %&gt;% group_by(PriPos) %&gt;% summarise(AtBats = mean(AtBats) %&gt;% round(0)) %&gt;% # to help join with the draws mutate(name = str_c(&quot;V&quot;, 1:n())) # push the model through `fitted()` and wrangle fitted(fit21.8, newdata = nd, re_formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos), scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% pivot_longer(everything()) %&gt;% mutate(probability = inv_logit_scaled(value)) %&gt;% left_join(nd, by = &quot;name&quot;) %&gt;% # plot ggplot(aes(x = probability, y = PriPos)) + geom_vline(xintercept = fixef(fit21.8)[1] %&gt;% inv_logit_scaled(), color = pm[6]) + geom_density_ridges(color = pm[1], fill = pm[7], size = 1/2, rel_min_height = 0.005, scale = .9) + geom_jitter(data = my_data, aes(x = Hits / AtBats), height = .025, alpha = 1/6, size = 1/6, color = pm[1]) + scale_x_continuous(&quot;Hits / AtBats&quot;, breaks = 0:5 / 5, expand = c(0, 0)) + coord_cartesian(xlim = c(0, 1), ylim = c(1, 9.5)) + ggtitle(&quot;Data with Posterior Predictive Distrib.&quot;) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) For kicks and giggles, we depicted the grand mean probability as the vertical line in the background with the geom_vline() line. However, we can make our plot more directly analogous to Kruschke’s if we’re willing to stretch a little. Recall that Kruschke used the beta distribution with the \\(\\omega-\\kappa\\) parameterization in both his statistical model and his plot code–both of which you can find detailed in his Jags-Ybinom-Xnom1fac-Mlogistic.R. file. We didn’t use the beta distribution in our brm() model and the parameters from that model didn’t have as direct correspondences to the beta distribution the way those from Kruschke’s JAGS model did. However, recall that we can re-parameterize the beta distribution in terms of its mean \\(\\mu\\) and sample size \\(n\\), following the form \\[\\begin{align*} \\alpha &amp; = \\mu n \\\\ \\beta &amp; = (1 - \\mu) n . \\end{align*}\\] When we take the inverse logit of our intercepts, we do get vales in a probability metric. We might consider inserting those probabilities into the \\(\\mu\\) parameter. Furthermore, we can take our AtBats sample sizes and insert them directly into \\(n\\). As before, we’ll use the average sample size per position. # wrangle like a boss nd %&gt;% add_epred_draws(fit21.8, ndraws = 20, re_formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos), dpar = &quot;mu&quot;) %&gt;% # use the equations from above mutate(alpha = mu * AtBats, beta = (1 - mu) * AtBats) %&gt;% mutate(ll = qbeta(.025, shape1 = alpha, shape2 = beta), ul = qbeta(.975, shape1 = alpha, shape2 = beta)) %&gt;% mutate(theta = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(theta) %&gt;% mutate(density = dbeta(theta, alpha, beta)) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) %&gt;% # plot! ggplot(aes(y = PriPos)) + geom_ridgeline(aes(x = theta, height = -density, group = interaction(PriPos, .draw)), fill = NA, color = adjustcolor(pm[1], alpha.f = 1/3), size = 1/3, alpha = 2/3, min_height = NA) + geom_jitter(data = my_data, aes(x = Hits / AtBats, size = AtBats), height = .05, alpha = 1/6, shape = 1, color = pm[1]) + scale_size_continuous(range = c(1/4, 4)) + scale_x_continuous(&quot;Hits / AtBats&quot;, expand = c(0, 0)) + labs(title = &quot;Data with Posterior Predictive Distrib.&quot;, y = NULL) + coord_flip(xlim = c(0, 1), ylim = c(0.67, 8.67)) + theme(axis.text.x = element_text(angle = 90, hjust = 1), axis.ticks.x = element_blank(), legend.position = c(.956, .8)) ⚠️ Since we didn’t actually presume the beta distribution anywhere in our brm() statistical model, I would not attempt to present this workflow in a scientific outlet. Go with the previous plot. This attempt seems dishonest. But it is kinda fun to see how far we can push our results. ⚠️ Happily, our contrasts will be less contentious. Here’s the initial wrangling. # define our subset of positions positions &lt;- c(&quot;1st Base&quot;, &quot;Catcher&quot;, &quot;Pitcher&quot;) # redefine `nd` nd &lt;- my_data %&gt;% filter(PriPos %in% positions) %&gt;% group_by(PriPos) %&gt;% summarise(AtBats = mean(AtBats) %&gt;% round(0)) # push the model through `fitted()` and wrangle f &lt;- fitted(fit21.8, newdata = nd, re_formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos), scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% set_names(positions) # what did we do? head(f) ## # A tibble: 6 × 3 ## `1st Base` Catcher Pitcher ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.11 -1.15 -1.85 ## 2 -1.08 -1.16 -1.84 ## 3 -1.11 -1.10 -1.88 ## 4 -1.07 -1.15 -1.95 ## 5 -1.05 -1.10 -1.85 ## 6 -1.10 -1.13 -1.89 Here we make are our versions of the middle two panels of Figure 21.13. p1 &lt;- f %&gt;% # compute the differences and put the data in the long format mutate(`Pitcher vs. Catcher` = Pitcher - Catcher, `Catcher vs. 1st Base` = Catcher - `1st Base`) %&gt;% pivot_longer(contains(&quot;vs.&quot;)) %&gt;% mutate(name = factor(name, levels = c(&quot;Pitcher vs. Catcher&quot;, &quot;Catcher vs. 1st Base&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + geom_vline(xintercept = 0, color = pm[6]) + stat_wilke() + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference (in b)&quot;) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) Now make our versions of the bottom two panels of Figure 21.13. p2 &lt;- f %&gt;% # do the transformation before computing the differences mutate_all(inv_logit_scaled) %&gt;% mutate(`Pitcher vs. Catcher` = Pitcher - Catcher, `Catcher vs. 1st Base` = Catcher - `1st Base`) %&gt;% pivot_longer(contains(&quot;vs.&quot;)) %&gt;% mutate(name = factor(name, levels = c(&quot;Pitcher vs. Catcher&quot;, &quot;Catcher vs. 1st Base&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + geom_vline(xintercept = 0, color = pm[6]) + stat_wilke() + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference (in probability)&quot;) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) Combine and plot. p1 / p2 ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Note how our distributions are described as differences in probability, rather than in \\(\\omega\\). Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggridges_0.5.3 GGally_2.1.2 tidybayes_3.0.2 brms_2.18.0 Rcpp_1.0.9 ggExtra_0.10.0 ## [7] patchwork_1.1.2 forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 ## [13] tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 PNWColors_0.1.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 igraph_1.3.4 ## [5] svUnit_1.0.6 splines_4.2.0 crosstalk_1.2.0 TH.data_1.1-1 ## [9] rstantools_2.2.0 inline_0.3.19 digest_0.6.30 htmltools_0.5.3 ## [13] fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 googlesheets4_1.0.1 ## [17] tzdb_0.3.0 modelr_0.1.8 RcppParallel_5.1.5 matrixStats_0.62.0 ## [21] vroom_1.5.7 xts_0.12.1 sandwich_3.0-2 prettyunits_1.1.1 ## [25] colorspace_2.0-3 rvest_1.0.2 ggdist_3.2.0 haven_2.5.1 ## [29] xfun_0.35 callr_3.7.3 crayon_1.5.2 jsonlite_1.8.3 ## [33] lme4_1.1-31 survival_3.4-0 zoo_1.8-10 glue_1.6.2 ## [37] gtable_0.3.1 gargle_1.2.0 emmeans_1.8.0 distributional_0.3.1 ## [41] pkgbuild_1.3.1 rstan_2.21.7 abind_1.4-5 scales_1.2.1 ## [45] mvtnorm_1.1-3 emo_0.0.0.9000 DBI_1.1.3 miniUI_0.1.1.1 ## [49] xtable_1.8-4 HDInterval_0.2.2 bit_4.0.4 stats4_4.2.0 ## [53] StanHeaders_2.21.0-7 DT_0.24 htmlwidgets_1.5.4 httr_1.4.4 ## [57] threejs_0.3.3 RColorBrewer_1.1-3 arrayhelpers_1.1-0 posterior_1.3.1 ## [61] ellipsis_0.3.2 reshape_0.8.9 pkgconfig_2.0.3 loo_2.5.1 ## [65] farver_2.1.1 sass_0.4.2 dbplyr_2.2.1 utf8_1.2.2 ## [69] tidyselect_1.1.2 labeling_0.4.2 rlang_1.0.6 reshape2_1.4.4 ## [73] later_1.3.0 munsell_0.5.0 cellranger_1.1.0 tools_4.2.0 ## [77] cachem_1.0.6 cli_3.5.0 generics_0.1.3 broom_1.0.1 ## [81] evaluate_0.18 fastmap_1.1.0 processx_3.8.0 knitr_1.40 ## [85] bit64_4.0.5 fs_1.5.2 nlme_3.1-159 projpred_2.2.1 ## [89] mime_0.12 xml2_1.3.3 compiler_4.2.0 bayesplot_1.9.0 ## [93] shinythemes_1.2.0 rstudioapi_0.13 gamm4_0.2-6 reprex_2.0.2 ## [97] bslib_0.4.0 stringi_1.7.8 highr_0.9 ps_1.7.2 ## [101] Brobdingnag_1.2-8 lattice_0.20-45 Matrix_1.4-1 nloptr_2.0.3 ## [105] markdown_1.1 shinyjs_2.1.0 tensorA_0.36.2 vctrs_0.5.1 ## [109] pillar_1.8.1 lifecycle_1.0.3 jquerylib_0.1.4 bridgesampling_1.1-2 ## [113] estimability_1.4.1 httpuv_1.6.5 R6_2.5.1 bookdown_0.28 ## [117] promises_1.2.0.1 gridExtra_2.3 codetools_0.2-18 boot_1.3-28 ## [121] MASS_7.3-58.1 colourpicker_1.1.1 gtools_3.9.3 assertthat_0.2.1 ## [125] withr_2.5.0 shinystan_2.6.0 multcomp_1.4-20 mgcv_1.8-40 ## [129] parallel_4.2.0 hms_1.1.1 grid_4.2.0 minqa_1.2.5 ## [133] coda_0.19-4 rmarkdown_2.16 googledrive_2.0.0 shiny_1.7.2 ## [137] lubridate_1.8.0 base64enc_0.1-3 dygraphs_1.1.1.6 References Attali, D., &amp; Baker, C. (2022). ggExtra: Add marginal histograms to ’ggplot2’, and more ’ggplot2’ enhancements. https://CRAN.R-project.org/package=ggExtra Bürkner, P.-C. (2022). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Lawlor, J. (2020). PNWColors: Color palettes inspired by nature in the US Pacific Northwest [Manual]. https://CRAN.R-project.org/package=PNWColors Wilke, C. O. (2019). Fundamentals of data visualization. https://clauswilke.com/dataviz/ "],["nominal-predicted-variable.html", "22 Nominal Predicted Variable 22.1 Softmax regression 22.2 Conditional logistic regression 22.3 Implementation in JAGS brms 22.4 Generalizations and variations of the models Session info", " 22 Nominal Predicted Variable This chapter considers data structures that have a nominal predicted variable. When the nominal predicted variable has only two possible values, this reduces to the case of the dichotomous predicted variable considered in the previous chapter. In the present chapter, we generalize to cases in which the predicted variable has three or more categorical values… The traditional treatment of this sort of data structure is called multinomial logistic regression or conditional logistic regression. We will consider Bayesian approaches to these methods. As usual, in Bayesian software it is easy to generalize the traditional models so they are robust to outliers, allow different variances within levels of a nominal predictor, and have hierarchical structure to share information across levels or factors as appropriate. (Kruschke, 2015, p. 649) 22.1 Softmax regression “The key descriptor of the [models in this chapter is their] inverse-link function, which is the softmax function (which will be defined below). Therefore, [Kruschke] refer[ed] to the method as softmax regression instead of multinomial logistic regression” (p. 650) Say we have a metric predictor \\(x\\) and a multinomial criterion \\(y\\) with \\(k\\) categories. We can express the basic linear model as \\[\\lambda_k = \\beta_{0, k} + \\beta_{1, k} x,\\] for which the subscripts \\(k\\) indicate there’s a linear model for each of the \\(k\\) categories. We call the possible set of \\(k\\) outcomes \\(S\\). Taking the case where \\(k = 3\\), we’d have \\[\\begin{align*} \\lambda_{[1]} &amp; = \\beta_{0, [1]} + \\beta_{1, [1]} x, \\\\ \\lambda_{[2]} &amp; = \\beta_{0, [2]} + \\beta_{1, [2]} x, \\text{and} \\\\ \\lambda_{[3]} &amp; = \\beta_{0, [3]} + \\beta_{1, [3]} x. \\end{align*}\\] In this scenario, what we want to know is the probability of \\(\\lambda_{[1]}\\), \\(\\lambda_{[2]}\\), and \\(\\lambda_{[3]}\\). The probability of a given outcome \\(k\\) follows the formula \\[\\phi_k = \\operatorname{softmax}_S (\\{\\lambda_k\\}) = \\frac{\\exp (\\lambda_k)}{\\sum_{c \\in S} \\exp (\\lambda_c)}.\\] In words, [the equation] says that the probability of outcome \\(k\\) is the exponentiated linear propensity of outcome \\(k\\) relative to the sum of exponentiated linear propensities across all outcomes in the set \\(S\\). You may be wondering, Why exponentiate? Intuitively, we have to go from propensities that can have negative values to probabilities that can only have non-negative values, and we have to preserve order. The exponential function satisfies that need. (p. 650) You may be wondering what happened to \\(y\\) and where all those \\(\\lambda\\)’s came from. Here we’re using \\(\\lambda\\) to describe the propensity of outcome \\(k\\), as indexed within our criterion \\(y\\). So, the output of these models, \\(\\phi_k\\), is the relative probability we’ll see each of our \\(k\\) categories within our criterion \\(y\\). What we want is \\(\\phi_k\\). The way we parameterize that with the softmax function is with \\(\\lambda_k\\). There are are indeterminacies in the system of equations Kruschke covered in this section, the upshot of which is we’ll end up making one of the \\(k\\) categories the reference category, which we term \\(r\\). Continuing on with our univariable model, we choose convenient constants for our parameters for \\(r\\): \\(\\beta_{0, r} = 0\\) and \\(\\beta_{1, r} = 0\\). As such, the regression coefficients for the remaining categories are relative to those for \\(r\\). Kruschke saved the data for Figure 22.1 in the SoftmaxRegData1.csv and SoftmaxRegData2.csv files. library(tidyverse) d1 &lt;- read_csv(&quot;data.R/SoftmaxRegData1.csv&quot;) d2 &lt;- read_csv(&quot;data.R/SoftmaxRegData2.csv&quot;) glimpse(d1) ## Rows: 475 ## Columns: 3 ## $ X1 &lt;dbl&gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.53341559, -0.18932650… ## $ X2 &lt;dbl&gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.77465062, -0.53727640, … ## $ Y &lt;dbl&gt; 2, 1, 3, 3, 3, 3, 1, 4, 2, 2, 3, 2, 4, 4, 4, 1, 2, 3, 3, 3, 3, 2, 1, 1, 3, 2, 3, 2, 4, … glimpse(d2) ## Rows: 475 ## Columns: 3 ## $ X1 &lt;dbl&gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.53341559, -0.18932650… ## $ X2 &lt;dbl&gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.77465062, -0.53727640, … ## $ Y &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 2, 3, 2, 1, 3, 2, 4, 3, 3, 2, 2, 1, 1, 3, 3, 4, 2, 2, 3, 2, 3, 4, 4, … Before we explore these data in a plot, let’s talk color and theme. For this chapter, we’ll carry forward our practice from Chapter 21 and take our color palette from the PNWColors package. This time, our color palette will be \"Lake\". library(PNWColors) pl &lt;- pnw_palette(name = &quot;Lake&quot;) pl We’ll base our overall plot theme on cowplot::theme_minimal_grid(), with many color adjustments from PNWColors::pnw_palette(name = \"Lake\"). library(cowplot) theme_set( theme_minimal_grid() + theme(text = element_text(color = pl[1]), axis.text = element_text(color = pl[1]), axis.ticks = element_line(color = pl[1]), legend.background = element_blank(), legend.box.background = element_blank(), legend.key = element_rect(fill = pl[8]), panel.background = element_rect(fill = pl[8], color = pl[8]), panel.grid = element_blank(), strip.background = element_rect(fill = pl[7], color = pl[7]), strip.text = element_text(color = pl[1])) ) Now bind the two data frames together and plot our version of Figure 22.1. bind_rows(d1, d2) %&gt;% mutate(data = rep(str_c(&quot;d&quot;, 1:2), each = n() / 2), Y = factor(Y)) %&gt;% ggplot(aes(x = X1, y = X2, label = Y, color = Y)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_text(size = 3) + scale_color_manual(values = pl[2:5]) + labs(x = expression(x[1]), y = expression(x[2])) + coord_equal() + theme(legend.position = &quot;none&quot;) + facet_wrap(~ data, ncol = 2) 22.1.1 Softmax reduces to logistic for two outcomes. “When there are only two outcomes, the softmax formulation reduces to the logistic regression of Chapter 21” (p. 653). 22.1.2 Independence from irrelevant attributes. An important property of the softmax function of Equation 22.2 is known as independence from irrelevant attributes (Luce, 2012, 2008). The model implies that the ratio of probabilities of two outcomes is the same regardless of what other possible outcomes are included in the set. Let \\(S\\) denote the set of possible outcomes. Then, from the definition of the softmax function, the ratio of outcomes \\(j\\) and \\(k\\) is \\[\\frac{\\phi_j}{\\phi_k} = \\frac{\\exp (\\lambda_j) / \\sum_{c \\in S} \\exp (\\lambda_c)}{\\exp (\\lambda_k) / \\sum_{c \\in S} \\exp (\\lambda_c)}\\] The summation in the denominators cancels and has no effect on the ratio of probabilities. Obviously if we changed the set of outcomes \\(S\\) to any other set \\(S^*\\) that still contains outcomes \\(j\\) and \\(k\\), the summation \\(\\sum_{c \\in S^*}\\) would still cancel and have no effect on the ratio of probabilities. (p. 654) Just to walk out that denominators-canceling business a little further, \\[\\begin{align*} \\frac{\\phi_j}{\\phi_k} &amp; = \\frac{\\exp (\\lambda_j) / \\sum_{c \\in S} \\exp (\\lambda_c)}{\\exp (\\lambda_k) / \\sum_{c \\in S} \\exp (\\lambda_c)} \\\\ &amp; = \\frac{\\exp (\\lambda_j)}{\\exp (\\lambda_k)}. \\end{align*}\\] Thus even in the case of a very different set of possible outcomes \\(S^\\text{very different}\\), it remains that \\(\\frac{\\phi_j}{\\phi_k} = \\frac{\\exp (\\lambda_j)}{\\exp (\\lambda_k)}\\). Getting more applied, here’s a tibble presentation of Kruschke’s commute example with three modes of transportation. tibble(mode = c(&quot;walking&quot;, &quot;bicycling&quot;, &quot;bussing&quot;), preference = 3:1) %&gt;% mutate(`chance %` = (100 * preference / sum(preference)) %&gt;% round(digits = 1)) ## # A tibble: 3 × 3 ## mode preference `chance %` ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 walking 3 50 ## 2 bicycling 2 33.3 ## 3 bussing 1 16.7 Sticking with the example, if we take bicycling out of the picture, the preference values remain, but the chance % values change. tibble(mode = c(&quot;walking&quot;, &quot;bussing&quot;), preference = c(3, 1)) %&gt;% mutate(`chance %` = 100 * preference / sum(preference)) ## # A tibble: 2 × 3 ## mode preference `chance %` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 walking 3 75 ## 2 bussing 1 25 Though we retain the same walking/bussing ratio, we end up with a different model of relative probabilities. 22.2 Conditional logistic regression Softmax regression conceives of each outcome as an independent change in log odds from the reference outcome, and a special case of that is dichotomous logistic regression. But we can generalize logistic regression another way, which may better capture some patterns of data. The idea of this generalization is that we divide the set of outcomes into a hierarchy of two-set divisions, and use a logistic to describe the probability of each branch of the two-set divisions. (p. 655) The model follows the generic equation \\[\\begin{align*} \\phi_{S^* | S} = \\operatorname{logistic}(\\lambda_{S^* | S}) \\\\ \\lambda_{S^* | S} = \\beta_{0, S^* | S} + \\beta_{1, {S^* | S}} x, \\end{align*}\\] where the conditional response probability (i.e., the goal of the analysis) is \\(\\phi_{S^* | S}\\). \\(S^*\\) and \\(S\\) denote the subset of outcomes and larger set of outcomes, respectively, and \\(\\lambda_{S^* | S}\\) is the propensity based on some linear model. The overall point is these “regression coefficients refer to the conditional probability of outcomes for the designated subsets, not necessarily to a single outcome among the full set of outcomes” (p. 655). In Figure 22.2 (p. 656), Kruschke depicted the two hierarchies of binary divisions of the models he fit to the data in his CondLogistRegData1.csv and CondLogistRegData2.csv files. Here we load those data, save them as d3 and d4, and take a look at their structures. d3 &lt;- read_csv(&quot;data.R/CondLogistRegData1.csv&quot;) d4 &lt;- read_csv(&quot;data.R/CondLogistRegData2.csv&quot;) glimpse(d3) ## Rows: 475 ## Columns: 3 ## $ X1 &lt;dbl&gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.53341559, -0.18932650… ## $ X2 &lt;dbl&gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.77465062, -0.53727640, … ## $ Y &lt;dbl&gt; 2, 1, 3, 1, 3, 3, 2, 3, 2, 4, 1, 2, 2, 3, 4, 2, 2, 4, 2, 3, 4, 2, 1, 1, 1, 2, 1, 2, 3, … glimpse(d4) ## Rows: 475 ## Columns: 3 ## $ X1 &lt;dbl&gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.53341559, -0.18932650… ## $ X2 &lt;dbl&gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.77465062, -0.53727640, … ## $ Y &lt;dbl&gt; 4, 4, 3, 4, 2, 3, 4, 3, 4, 4, 2, 4, 4, 3, 3, 4, 4, 4, 4, 3, 4, 4, 1, 1, 2, 4, 3, 4, 3, … In both data sets, the nominal criterion is Y and the two predictors are X1 and X2. Though the data seem simple, the conditional logistic models are complex enough that it seems like we’ll be better served by focusing on them one at a time, which means I’m going to break up Figure 22.2. Here’s how to make the diagram in the left panel. # the big numbers numbers &lt;- tibble( x = c(3, 5, 2, 4, 1, 3, 2), y = c(0, 0, 1, 1, 2, 2, 3), label = c(&quot;3&quot;, &quot;4&quot;, &quot;2&quot;, &quot;3,4&quot;, &quot;1&quot;, &quot;2,3,4&quot;, &quot;1,2,3,4&quot;) ) # the smaller Greek numbers greek &lt;- tibble( x = c(3.4, 4.6, 2.4, 3.6, 1.4, 2.6), y = c(0.5, 0.5, 1.5, 1.5, 2.5, 2.5), hjust = c(1, 0, 1, 0, 1, 0), label = c(&quot;phi[&#39;{3}|{3,4}&#39;]&quot;, &quot;1-phi[&#39;{3}|{3,4}&#39;]&quot;, &quot;phi[&#39;{2}|{2,3,4}&#39;]&quot;, &quot;1-phi[&#39;{2}|{2,3,4}&#39;]&quot;, &quot;phi[&#39;{1}|{1,2,3,4}&#39;]&quot;, &quot;1-phi[&#39;{1}|{1,2,3,4}&#39;]&quot;) ) # arrows tibble( x = c(4, 4, 3, 3, 2, 2), y = c(0.85, 0.85, 1.85, 1.85, 2.85, 2.85), xend = c(3, 5, 2, 4, 1, 3), yend = c(0.15, 0.15, 1.15, 1.15, 2.15, 2.15) ) %&gt;% # plot! ggplot(aes(x = x, y = y)) + geom_segment(aes(xend = xend, yend = yend), size = 1/4, arrow = arrow(length = unit(0.08, &quot;in&quot;), type = &quot;closed&quot;)) + geom_text(data = numbers, aes(label = label), size = 5, family = &quot;Times&quot;)+ geom_text(data = greek, aes(label = label, hjust = hjust), size = 4.25, family = &quot;Times&quot;, parse = T) + xlim(-1, 7) + theme_void() ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. The large numbers are the four levels in the criterion Y and the smaller numbers in the curly braces are various sets of those numbers. The diagram shows three levels of outcome-set divisions: 1 versus 2, 3, or 4; 2 versus 3 or 4; and 3 versus 4. The divisions in each of these levels can be expressed as linear models which we’ll denote \\(\\lambda\\). Given our data with two predictors X1 and X2, we can express the three linear models as \\[ \\begin{align*} \\lambda_{\\{ 1 \\} | \\{ 1,2,3,4 \\}} &amp; = \\beta_{0,\\{ 1 \\} | \\{ 1,2,3,4 \\}} + \\beta_{1,\\{ 1 \\} | \\{ 1,2,3,4 \\}} \\text{X1} + \\beta_{2,\\{ 1 \\} | \\{ 1,2,3,4 \\}} \\text{X2} \\\\ \\lambda_{\\{ 2 \\} | \\{ 2,3,4 \\}} &amp; = \\beta_{0,\\{ 2 \\} | \\{ 2,3,4 \\}} + \\beta_{1,\\{ 2 \\} | \\{ 2,3,4 \\}} \\text{X1} + \\beta_{2,\\{ 2 \\} | \\{ 2,3,4 \\}} \\text{X2} \\\\ \\lambda_{\\{ 3 \\} | \\{ 3,4 \\}} &amp; = \\beta_{0,\\{ 3 \\} | \\{ 3,4 \\}} + \\beta_{1,\\{ 3 \\} | \\{ 3,4 \\}} \\text{X1} + \\beta_{2,\\{ 3 \\} | \\{ 3,4 \\}} \\text{X2}, \\end{align*} \\] where, for convenience, we’re omitting the typical \\(i\\) subscripts. As these linear models are all defined within the context of the logit link, we can express the conditional probabilities of the outcome sets as \\[ \\begin{align*} \\phi_{\\{ 1 \\} | \\{ 1,2,3,4 \\}} &amp; = \\operatorname{logistic} \\left (\\lambda_{\\{ 1 \\} | \\{ 1,2,3,4 \\}} \\right) \\\\ \\phi_{\\{ 2 \\} | \\{ 2,3,4 \\}} &amp; = \\operatorname{logistic} \\left (\\lambda_{\\{ 2 \\} | \\{ 2,3,4 \\}} \\right) \\\\ \\phi_{\\{ 3 \\} | \\{ 3,4 \\}} &amp; = \\operatorname{logistic} \\left (\\lambda_{\\{ 3 \\} | \\{ 3,4 \\}} \\right), \\end{align*} \\] where \\(\\phi_{\\{ 1 \\} | \\{ 1,2,3,4 \\}}\\) through \\(\\phi_{\\{ 3 \\} | \\{ 3,4 \\}}\\) are the conditional probabilities for the outcome sets. If, however, we want the conditional probabilities for the actual levels of the criterion Y, we define those with a series of (in this case) four equations: \\[ \\begin{align*} \\phi_1 &amp; = \\phi_{\\{ 1 \\} | \\{ 1,2,3,4 \\}} \\\\ \\phi_2 &amp; = \\phi_{\\{ 2 \\} | \\{ 2,3,4 \\}} \\cdot \\left ( 1 - \\phi_{\\{ 1 \\} | \\{ 1,2,3,4 \\}} \\right) \\\\ \\phi_3 &amp; = \\phi_{\\{ 3 \\} | \\{ 3,4 \\}} \\cdot \\left ( 1 - \\phi_{\\{ 2 \\} | \\{ 2,3,4 \\}} \\right) \\cdot \\left ( 1 - \\phi_{\\{ 1 \\} | \\{ 1,2,3,4 \\}} \\right) \\\\ \\phi_4 &amp; = \\left ( 1 - \\phi_{\\{ 3 \\} | \\{ 3,4 \\}} \\right) \\cdot \\left ( 1 - \\phi_{\\{ 2 \\} | \\{ 2,3,4 \\}} \\right) \\cdot \\left ( 1 - \\phi_{\\{ 1 \\} | \\{ 1,2,3,4 \\}} \\right), \\end{align*} \\] where the sum of the probabilities \\(\\phi_1\\) through \\(\\phi_4\\) is \\(1\\). To get a sense of what this all means in practice, let’s visualize the data and the data-generating equations for our version of Figure 22.3. As with the previous figure, I’m going to break this figure up to focus on one model at a time. Thus, here’s the left panel of Figure 22.3. ## define the various population parameters # lambda 1 b01 &lt;- -4 b11 &lt;- -5 b21 &lt;- 0.01 # rounding up to avoid dividing by zero # lambda 2 b02 &lt;- -2 b12 &lt;- 1 b22 &lt;- -5 # lambda 3 b03 &lt;- -1 b13 &lt;- 3 b23 &lt;- 3 # use the parameters to define the lines lines &lt;- tibble( intercept = c(-b01 / b21, -b02 / b22, -b03 / b23), slope = c(-b11 / b21, -b12 / b22, -b13 / b23), label = c(&quot;1&quot;, &quot;2&quot;,&quot;3&quot;) ) # wrangle d3 %&gt;% mutate(Y = factor(Y)) %&gt;% # plot! ggplot() + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_text(aes(x = X1, y = X2, label = Y, color = Y), size = 3, show.legend = F) + geom_abline(data = lines, aes(intercept = intercept, slope = slope, linetype = label), color = pl[1]) + scale_color_manual(values = pl[2:5]) + scale_linetype(NULL, labels = parse(text = c( &quot;lambda[&#39;{1}|{1,2,3,4}&#39;]==-4+-5*x[1]+0*x[2]&quot;, &quot;lambda[&#39;{2}|{2,3,4}&#39;]==-2+1*x[1]+-5*x[2]&quot;, &quot;lambda[&#39;{3}|{3,4}&#39;]==-1+3*x[1]+3*x[2]&quot;)), guide = guide_legend( direction = &quot;vertical&quot;, label.hjust = 0.5, label.theme = element_text(size = 10))) + coord_equal() + labs(x = expression(x[1]), y = expression(x[2])) + theme(legend.justification = 0.5, legend.position = &quot;top&quot;) Recall back on page 629, Kruschke showed the equation for the 50% threshold of a logistic regression model given two continuous predictors was \\[x_2 = (-\\beta_0 / \\beta_2) + (-\\beta_1 / \\beta_2) x_1.\\] It was that equation that gave us the values for the intercept and slope arguments (\\(-\\beta_0 / \\beta_2\\) and \\(-\\beta_1 / \\beta_2\\), respectively) for the geom_abline() function. It still might not be clear how the various \\(\\phi_{S^* | S}\\) values connect to the data. Though not in the text, here’s an alternative way of expressing the relations in Figure 22.3. This time the plot is faceted by the three levels of \\(\\phi_{S^* | S}\\) and the background fill is based on those conditional probabilities. # define a grid of X1 and X2 values crossing(X1 = seq(from = -2, to = 2, length.out = 50), X2 = seq(from = -2, to = 2, length.out = 50)) %&gt;% # compute the lambda&#39;s mutate(`lambda[&#39;{1}|{1,2,3,4}&#39;]` = b01 + b11 * X1 + b21 * X2, `lambda[&#39;{2}|{2,3,4}&#39;]` = b02 + b12 * X1 + b22 * X2, `lambda[&#39;{3}|{3,4}&#39;]` = b03 + b13 * X1 + b23 * X2) %&gt;% # compute the phi&#39;s mutate(`phi[&#39;{1}|{1,2,3,4}&#39;]` = plogis(`lambda[&#39;{1}|{1,2,3,4}&#39;]`), `phi[&#39;{2}|{2,3,4}&#39;]` = plogis(`lambda[&#39;{2}|{2,3,4}&#39;]`), `phi[&#39;{3}|{3,4}&#39;]` = plogis(`lambda[&#39;{3}|{3,4}&#39;]`)) %&gt;% # wrangle pivot_longer(contains(&quot;phi&quot;), values_to = &quot;phi&quot;) %&gt;% # plot! ggplot(aes(x = X1, y = X2)) + geom_raster(aes(fill = phi), interpolate = T) + # note how we&#39;re subsetting the d3 data by facet geom_text(data = bind_rows( d3 %&gt;% mutate(name = &quot;phi[&#39;{1}|{1,2,3,4}&#39;]&quot;), d3 %&gt;% mutate(name = &quot;phi[&#39;{2}|{2,3,4}&#39;]&quot;) %&gt;% filter(Y &gt; 1), d3 %&gt;% mutate(name = &quot;phi[&#39;{3}|{3,4}&#39;]&quot;) %&gt;% filter(Y &gt; 2)), aes(label = Y), size = 2.5) + scale_fill_gradientn(expression(phi[italic(S)*&quot;*|&quot;*italic(S)]), colours = pnw_palette(name = &quot;Lake&quot;, n = 101), breaks = 0:2 / 2, limits = c(0, 1)) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + coord_equal() + theme(legend.position = c(0.8, 0.2)) + facet_wrap(~ name, labeller = label_parsed, ncol = 2) Notice how because each of the levels of \\(\\phi\\) is defined by a different subset of the data, each of the facets contains a different subset of the d3 data, too. For example, since \\(\\phi_{\\{ 1 \\} | \\{ 1,2,3,4 \\}}\\) is defined by the full subset of the possible values of Y, you see all the Y data displayed by geom_text() for that facet. In contrast, since \\(\\phi_{\\{ 3 \\} | \\{ 3,4 \\}}\\) is defined by a subset of the data for which Y is only 3 or 4, those are the only values you see displayed within that facet of the plot. Now we’ll consider an alternative way to set up the binary-choices hierarchy, as seen in the right panel of Figure 22.2. First, here’s that half of the figure. # the big numbers numbers &lt;- tibble( x = c(0, 2, 6, 8, 1, 7, 4), y = c(0, 0, 0, 0, 1, 1, 2), label = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;1,2&quot;, &quot;3,4&quot;, &quot;1,2,3,4&quot;) ) # the smaller Greek numbers greek &lt;- tibble( x = c(0.4, 1.6, 6.4, 7.6, 2.1, 5.8), y = c(0.5, 0.5, 0.5, 0.5, 1.5, 1.5), hjust = c(1, 0, 1, 0, 1, 0), label = c(&quot;phi[&#39;{1}|{1,2}&#39;]&quot;, &quot;1-phi[&#39;{1}|{1,2}&#39;]&quot;, &quot;phi[&#39;{3}|{3,4}&#39;]&quot;, &quot;1-phi[&#39;{3}|{3,4}&#39;]&quot;, &quot;phi[&#39;{1,2}|{1,2,3,4}&#39;]&quot;, &quot;1-phi[&#39;{1,2}|{1,2,3,4}&#39;]&quot;) ) # arrows tibble( x = c(1, 1, 7, 7, 4, 4), y = c(0.85, 0.85, 0.85, 0.85, 1.85, 1.85), xend = c(0, 2, 6, 8, 1, 7), yend = c(0.15, 0.15, 0.15, 0.15, 1.15, 1.15) ) %&gt;% # plot! ggplot(aes(x = x, y = y)) + geom_segment(aes(xend = xend, yend = yend), size = 1/4, arrow = arrow(length = unit(0.08, &quot;in&quot;), type = &quot;closed&quot;)) + geom_text(data = numbers, aes(label = label), size = 5, family = &quot;Times&quot;)+ geom_text(data = greek, aes(label = label, hjust = hjust), size = 4.25, family = &quot;Times&quot;, parse = T) + xlim(-1, 10) + theme_void() This diagram shows three levels of outcome-set divisions: 1 or 2 versus 3 or 4; 1 versus 2; and 3 versus 4. Given our data with two predictors X1 and X2, we can express the three linear models as \\[ \\begin{align*} \\lambda_{\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} &amp; = \\beta_{0,\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} + \\beta_{1,\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} \\text{X1} + \\beta_{2,\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} \\text{X2} \\\\ \\lambda_{\\{ 1 \\} | \\{ 1,2 \\}} &amp; = \\beta_{0,\\{ 1 \\} | \\{ 1,2 \\}} + \\beta_{1,\\{ 1 \\} | \\{ 1,2 \\}} \\text{X1} + \\beta_{2,\\{ 1 \\} | \\{ 1,2 \\}} \\text{X2} \\\\ \\lambda_{\\{ 3 \\} | \\{ 3,4 \\}} &amp; = \\beta_{0,\\{ 3 \\} | \\{ 3,4 \\}} + \\beta_{1,\\{ 3 \\} | \\{ 3,4 \\}} \\text{X1} + \\beta_{2,\\{ 3 \\} | \\{ 3,4 \\}} \\text{X2}. \\end{align*} \\] We can then express the conditional probabilities of the outcome sets as \\[ \\begin{align*} \\phi_{\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} &amp; = \\operatorname{logistic} \\left (\\lambda_{\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} \\right) \\\\ \\phi_{\\{ 1 \\} | \\{ 1,2 \\}} &amp; = \\operatorname{logistic} \\left (\\lambda_{\\{ 1 \\} | \\{ 1,2 \\}} \\right) \\\\ \\phi_{\\{ 3 \\} | \\{ 3,4 \\}} &amp; = \\operatorname{logistic} \\left (\\lambda_{\\{ 3 \\} | \\{ 3,4 \\}} \\right). \\end{align*} \\] For the conditional probabilities of the actual levels of the criterion Y, we define those with a series of (in this case) four equations: \\[ \\begin{align*} \\phi_1 &amp; = \\phi_{\\{ 1 \\} | \\{ 1,2 \\}} \\cdot \\phi_{\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} \\\\ \\phi_2 &amp; = \\left ( 1 - \\phi_{\\{ 1 \\} | \\{ 1,2 \\}} \\right) \\cdot \\phi_{\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} \\\\ \\phi_3 &amp; = \\phi_{\\{ 3 \\} | \\{ 3,4 \\}} \\cdot \\left ( 1 - \\phi_{\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} \\right) \\\\ \\phi_4 &amp; = \\left ( 1 - \\phi_{\\{ 3 \\} | \\{ 3,4 \\}} \\right) \\cdot \\left ( 1 - \\phi_{\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} \\right), \\end{align*} \\] where the sum of the probabilities \\(\\phi_1\\) through \\(\\phi_4\\) is \\(1\\). To get a sense of what this all means, let’s visualize the data and the data-generating equations in our version of the right panel of Figure 22.3. d4 %&gt;% mutate(Y = factor(Y)) %&gt;% ggplot() + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_text(aes(x = X1, y = X2, label = Y, color = Y), size = 3, show.legend = F) + geom_abline(data = lines, aes(intercept = intercept, slope = slope, linetype = label), color = pl[1]) + scale_color_manual(values = pl[2:5]) + scale_linetype(NULL, labels = parse(text = c( &quot;lambda[&#39;{1,2}|{1,2,3,4}&#39;]==-4+-5*x[1]+0*x[2]&quot;, &quot;lambda[&#39;{1}|{1,2}&#39;]==-2+1*x[1]+-5*x[2]&quot;, &quot;lambda[&#39;{3}|{3,4}&#39;]==-1+3*x[1]+3*x[2]&quot;)), guide = guide_legend( direction = &quot;vertical&quot;, label.hjust = 0.5, label.theme = element_text(size = 10))) + coord_equal() + labs(x = expression(x[1]), y = expression(x[2])) + theme(legend.justification = 0.5, legend.position = &quot;top&quot;) Here’s an alternative way of expression the relations in the right panel of Figure 22.3. This time the plot is faceted by the three levels of \\(\\phi_{S^* | S}\\) and the background fill is based on those conditional probabilities. # define a grid of X1 and X2 values crossing(X1 = seq(from = -2, to = 2, length.out = 50), X2 = seq(from = -2, to = 2, length.out = 50)) %&gt;% # compute the lambda&#39;s mutate(`lambda[&#39;{1,2}|{1,2,3,4}&#39;]` = b01 + b11 * X1 + b21 * X2, `lambda[&#39;{1}|{1,2}&#39;]` = b02 + b12 * X1 + b22 * X2, `lambda[&#39;{3}|{3,4}&#39;]` = b03 + b13 * X1 + b23 * X2) %&gt;% # compute the phi&#39;s mutate(`phi[&#39;{1,2}|{1,2,3,4}&#39;]` = plogis(`lambda[&#39;{1,2}|{1,2,3,4}&#39;]`), `phi[&#39;{1}|{1,2}&#39;]` = plogis(`lambda[&#39;{1}|{1,2}&#39;]`), `phi[&#39;{3}|{3,4}&#39;]` = plogis(`lambda[&#39;{3}|{3,4}&#39;]`)) %&gt;% # wrangle pivot_longer(contains(&quot;phi&quot;), values_to = &quot;phi&quot;) %&gt;% # plot! ggplot(aes(x = X1, y = X2)) + geom_raster(aes(fill = phi), interpolate = T) + # note how we&#39;re subsetting the d3 data by facet geom_text(data = bind_rows( d4 %&gt;% mutate(name = &quot;phi[&#39;{1,2}|{1,2,3,4}&#39;]&quot;), d4 %&gt;% mutate(name = &quot;phi[&#39;{1}|{1,2}&#39;]&quot;) %&gt;% filter(Y &lt; 3), d4 %&gt;% mutate(name = &quot;phi[&#39;{3}|{3,4}&#39;]&quot;) %&gt;% filter(Y &gt; 2)), aes(label = Y), size = 2.5) + scale_fill_gradientn(expression(phi[italic(S)*&quot;*|&quot;*italic(S)]), colours = pnw_palette(name = &quot;Lake&quot;, n = 101), breaks = 0:2 / 2, limits = c(0, 1)) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + coord_equal() + theme(legend.position = c(0.8, 0.2)) + facet_wrap(~ name, labeller = label_parsed, ncol = 2) It could be easy to miss due to the way we broke up our workflow, but if you look closely at the \\(\\lambda\\) equations at the top of both panels of Figure 22.3, you’ll see the right-hand side of the equations are the same. But because of the differences in the two data hierarchies, those \\(\\lambda\\) equations had different consequences for how the X1 and X2 values generated the Y data. Also, In general, conditional logistic regression requires that there is a linear division between two subsets of the outcomes, and then within each of those subsets there is a linear division of smaller subsets, and so on. This sort of linear division is not required of the softmax regression model… Real data can be extremely noisy, and there can be multiple predictors, so it can be challenging or impossible to visually ascertain which sort of model is most appropriate. The choice of model is driven primarily by theoretical meaningfulness. (p. 659) 22.3 Implementation in JAGS brms 22.3.1 Softmax model. Kruschke pointed out in his Figure 22.4 and the surrounding prose that we speak of the categorical distribution when fitting softmax models. Our brms paradigm will be much the same. To fit a softmax model with the brm() function, you specify family = categorical. The default is to use the logit link. In his (2022c) Parameterization of response distributions in brms vignette, Bürkner clarified: The categorical family is currently only implemented with the multivariate logit link function and has density \\[f(y) = \\mu_y = \\frac{\\exp (\\eta_y)}{\\sum_{k = 1}^K \\exp (\\eta_k)}\\] Note that \\(\\eta\\) does also depend on the category \\(k\\). For reasons of identifiability, \\(\\eta_1\\) is set to \\(0\\). Though there’s no explicit softmax talk in that vignette, you can find it documented in his code here, starting in line 1891. Now onto our ggplot2 + patchwork version of the model diagram in Figure 22.4. I’m not gonna lie. The requisite code is a slog. We’ll take the task in bits. First, we make and save the elements for the diagram on the left. library(patchwork) # normal density p1 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pl[6], color = pl[5]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pl[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = pl[1], hjust = 0, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pl[1]), plot.background = element_rect(fill = pl[8], color = &quot;white&quot;, size = 1)) # second normal density p2 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pl[6], color = pl[5]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pl[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M[jk])&quot;, &quot;italic(S[jk])&quot;), size = 7, color = pl[1], hjust = 0, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pl[1]), plot.background = element_rect(fill = pl[8], color = &quot;white&quot;, size = 1)) ## an annotated arrow # save our custom arrow settings my_arrow &lt;- arrow(angle = 20, length = unit(0.35, &quot;cm&quot;), type = &quot;closed&quot;) p3 &lt;- tibble(x = .5, y = 1, xend = .73, yend = 0) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pl[1]) + annotate(geom = &quot;text&quot;, x = c(.48, .72), y = .5, label = c(&quot;&#39;~&#39;&quot;, &quot;italic(k)&quot;), size = c(10, 7), color = pl[1], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() ## another annotated arrow p4 &lt;- tibble(x = .5, y = 1, xend = .4, yend = 0) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pl[1]) + annotate(geom = &quot;text&quot;, x = c(.34, .6), y = .5, label = c(&quot;&#39;~&#39;&quot;, &quot;italic(jk)&quot;), size = c(10, 7), color = pl[1], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # likelihood formula p5 &lt;- tibble(x = .5, y = .5, label = &quot;softmax(beta[0][&#39;[&#39;*italic(k)*&#39;]&#39;]+sum()[italic(j)]~beta[italic(j)][&#39;[&#39;*italic(k)*&#39;]&#39;]~italic(x)[italic(ji)])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pl[1], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # a third annotated arrow p6 &lt;- tibble(x = c(.375, .6), y = c(1/2, 1/2), label = c(&quot;&#39;=&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = pl[1], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow, color = pl[1]) + xlim(0, 1) + theme_void() # bar plot of categorical data p7 &lt;- tibble(x = 0:3, d = c(.5, .85, .5, .85)) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = pl[6], color = pl[5], width = .45) + annotate(geom = &quot;text&quot;, x = 1.5, y = .2, label = &quot;categorical&quot;, size = 7, color = pl[1]) + annotate(geom = &quot;text&quot;, x = 1.25, y = .9, hjust = 0, label = &quot;mu[italic(i)*&#39;[&#39;*italic(k)*&#39;]&#39;]&quot;, size = 7, color = pl[1], family = &quot;Times&quot;, parse = TRUE) + coord_cartesian(xlim = c(-.5, 3.5), ylim = 0:1) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pl[1]), plot.background = element_rect(fill = pl[8], color = &quot;white&quot;, size = 1)) # the final annotated arrow p8 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = pl[1], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, color = pl[1], arrow = my_arrow) + xlim(0, 1) + theme_void() # some text p9 &lt;- tibble(x = 1, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pl[1], parse = T, family = &quot;Times&quot;) + xlim(0, 2) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 2, l = 1, r = 2), area(t = 1, b = 2, l = 3, r = 4), area(t = 3, b = 3, l = 1, r = 2), area(t = 3, b = 3, l = 3, r = 4), area(t = 4, b = 4, l = 1, r = 4), area(t = 5, b = 5, l = 2, r = 3), area(t = 6, b = 7, l = 2, r = 3), area(t = 8, b = 8, l = 2, r = 3), area(t = 9, b = 9, l = 2, r = 3) ) # combine and plot! a &lt;- ( (p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) ) Now we make and save the elements for the diagram on the right. # third normal density p2 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = pl[6], color = pl[5]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = pl[1]) + annotate(geom = &quot;text&quot;, x = c(0, 1.5), y = .6, label = c(&quot;italic(M[j])&quot;, &quot;italic(S[j])&quot;), size = 7, color = pl[1], hjust = 0, family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pl[1]), plot.background = element_rect(fill = pl[8], color = &quot;white&quot;, size = 1)) ## an annotated arrow p3 &lt;- tibble(x = .5, y = 1, xend = .85, yend = 0) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pl[1]) + annotate(geom = &quot;text&quot;, x = .49, y = .5, label = &quot;&#39;~&#39;&quot;, size = 10, color = pl[1], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() ## another annotated arrow p4 &lt;- tibble(x = .5, y = 1, xend = .4, yend = 0) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = pl[1]) + annotate(geom = &quot;text&quot;, x = c(.35, .57), y = .5, label = c(&quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;), size = c(10, 7), color = pl[1], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # likelihood formula p5 &lt;- tibble(x = .5, y = .5, label = &quot;logistic(beta[0]+sum()[italic(j)]~beta[italic(j)]~italic(x)[italic(ji)])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = pl[1], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # bar plot of Bernoulli data p7 &lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = pl[6], color = pl[5], width = .4) + annotate(geom = &quot;text&quot;, x = .5, y = .2, label = &quot;Bernoulli&quot;, size = 7, color = pl[1]) + annotate(geom = &quot;text&quot;, x = .5, y = .9, label = &quot;mu[italic(i)]&quot;, size = 7, color = pl[1], family = &quot;Times&quot;, parse = T) + xlim(-.75, 1.75) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = pl[1]), plot.background = element_rect(fill = pl[8], color = &quot;white&quot;, size = 1)) # combine and plot! c &lt;- ( (p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) ) Here we combine the two model diagrams and plot! b &lt;- plot_spacer() (a | b | c) + plot_layout(widths = c(4, 1, 4)) 22.3.2 Conditional logistic model. The conditional logistic regression models are not natively supported in brms at this time. Based on issue #560 in the brms GitHub, there are ways to fit them using the nonlinear syntax. If you compare the syntax Bürkner used in that thread on January 30th to the JAGS syntax Kruschke showed on pages 661 and 662, you’ll see they appear to follow contrasting parameterizations. However, there are at least two other ways to fit conditional logistic models with brms. Based on insights from Henrik Singmann, we can define conditional logistic models using the custom family approach. In contrast, Mattan Ben-Shachar has shown we can also fit conditional logistic models using a tricky application of sequential ordinal regression. Rather than present them in the abstract, here, we will showcase both of these approaches in the sections below. 22.3.3 Results: Interpreting the regression coefficients. 22.3.3.1 Softmax model. Load brms. library(brms) Along with Kruschke, we’ll be modeling the d1 data. In case it’s not clear, the X1 and X2 variables are already in a standardized metric. d1 %&gt;% pivot_longer(-Y) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), sd = sd(value)) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 × 3 ## name mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 X1 0 1 ## 2 X2 0 1 This will make it easier to set the priors. Here we’ll just use the rather wide priors Kruschke indicated on page 662. Note our use of the dpar argument within the prior() function. fit22.1 &lt;- brm(data = d1, family = categorical(link = logit), Y ~ 0 + Intercept + X1 + X2, prior = c(prior(normal(0, 20), class = b, dpar = mu2), prior(normal(0, 20), class = b, dpar = mu3), prior(normal(0, 20), class = b, dpar = mu4)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 22, file = &quot;fits/fit22.01&quot;) Since it’s the default, we didn’t have to include the (link = logit) bit in the family argument. I’m just being explicit for the sake of pedagogy. Take a look at the parameter summary. print(fit22.1) ## Family: categorical ## Links: mu2 = logit; mu3 = logit; mu4 = logit ## Formula: Y ~ 0 + Intercept + X1 + X2 ## Data: d1 (Number of observations: 475) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## mu2_Intercept 3.39 0.62 2.29 4.67 1.01 1324 1482 ## mu2_X1 5.58 0.73 4.25 7.06 1.00 2019 2027 ## mu2_X2 0.82 0.51 -0.17 1.85 1.00 1759 1934 ## mu3_Intercept 2.07 0.68 0.78 3.44 1.00 1379 1650 ## mu3_X1 0.73 0.57 -0.38 1.86 1.00 1658 2190 ## mu3_X2 5.98 0.68 4.74 7.39 1.00 2148 2187 ## mu4_Intercept -0.41 0.89 -2.10 1.34 1.00 1917 2035 ## mu4_X1 12.38 1.17 10.25 14.77 1.00 2251 2268 ## mu4_X2 3.56 0.65 2.33 4.86 1.00 1949 2091 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As indicated in the formulas in Section 22.1, we get posteriors for each level of Y, except for Y == 1. That serves as the reference category. The values for \\(\\beta_{i, k = 1}\\) are all fixed at \\(0\\). Here’s how we might make the histograms in Figure 22.5. library(tidybayes) # extract the posterior draws draws &lt;- as_draws_df(fit22.1) # wrangle draws %&gt;% pivot_longer(starts_with(&quot;b_&quot;)) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;)) %&gt;% mutate(lambda = str_extract(name, &quot;[2-4]+&quot;) %&gt;% str_c(&quot;lambda==&quot;, .), parameter = case_when(str_detect(name, &quot;Intercept&quot;) ~ &quot;beta[0]&quot;, str_detect(name, &quot;X1&quot;) ~ &quot;beta[1]&quot;, str_detect(name, &quot;X2&quot;) ~ &quot;beta[2]&quot;)) %&gt;% # plot ggplot(aes(x = value, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = pl[4], slab_color = pl[3], color = pl[1], point_color = pl[2], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;marginal posterior&quot;) + facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &quot;free_x&quot;) Because the \\(\\beta\\) values for when \\(\\lambda = 1\\) are all fixed to 0, we left those plots out of our version of the figure. If you really wanted them, you’d have to enter the corresponding cells into the data before plotting. If you summarize each parameter by it’s posterior mean, round(), and wrangle a little, you can arrange the results in a similar way that the equations for \\(\\lambda_2\\) through \\(\\lambda_4\\) are displayed on the left side of Figure 22.5. draws %&gt;% pivot_longer(starts_with(&quot;b_&quot;)) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;)) %&gt;% mutate(lambda = str_extract(name, &quot;[2-4]+&quot;) %&gt;% str_c(&quot;lambda[&quot;, ., &quot;]&quot;), parameter = case_when(str_detect(name, &quot;Intercept&quot;) ~ &quot;beta[0]&quot;, str_detect(name, &quot;X1&quot;) ~ &quot;beta[1]&quot;, str_detect(name, &quot;X2&quot;) ~ &quot;beta[2]&quot;)) %&gt;% group_by(lambda, parameter) %&gt;% summarise(mean = mean(value) %&gt;% round(digits = 1)) %&gt;% pivot_wider(names_from = parameter, values_from = mean) ## # A tibble: 3 × 4 ## # Groups: lambda [3] ## lambda `beta[0]` `beta[1]` `beta[2]` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lambda[2] 3.4 5.6 0.8 ## 2 lambda[3] 2.1 0.7 6 ## 3 lambda[4] -0.4 12.4 3.6 As Kruschke mentioned in the text, “the estimated parameter values should be near the generating values, but not exactly the same because the data are merely a finite random sample” (pp. 662–663). Furthermore, interpreting the parameters is always contextualized relative to the model. For the softmax model, the regression coefficient for outcome \\(k\\) on predictor \\(x_j\\) indicates that rate at which the log odds of that outcome increase relative to the reference outcome for a one unit increase in \\(x_j\\), assuming that a softmax model is a reasonable description of the data. (p. 663) Unfortunately, this makes the parameters difficult to interpret directly. Kruschke didn’t show a plot like this, but it might be helpful to further understand what this model means in terms of probabilities for a given Y value. Here we’ll use the fitted() function to return the conditional probabilities for all four response options for Y based on various combinations of X1 and X2. nd &lt;- crossing(X1 = seq(from = -2, to = 2, length.out = 50), X2 = seq(from = -2, to = 2, length.out = 50)) fitted(fit22.1, newdata = nd) %&gt;% data.frame() %&gt;% select(contains(&quot;Estimate&quot;)) %&gt;% set_names(str_c(&quot;Y==&quot;, 1:4)) %&gt;% bind_cols(nd) %&gt;% pivot_longer(contains(&quot;Y&quot;), values_to = &quot;phi&quot;) %&gt;% ggplot(aes(x = X1, y = X2, fill = phi)) + geom_raster(interpolate = T) + scale_fill_gradientn(expression(phi[italic(k)*&quot;|&quot;*italic(S)]), colours = pnw_palette(name = &quot;Lake&quot;, n = 101), limits = c(0, 1)) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + facet_wrap(~ name, labeller = label_parsed) Now use that plot while you walk through the final paragraph in this subsection. It is easy to transform the estimated parameter values to a different reference category. Recall from Equation 22.3 (p. 651) that arbitrary constants can be added to all the regression coefficients without changing the model prediction. Therefore, to change the parameters estimates so they are relative to outcome \\(R\\), we simply subtract \\(\\beta_{j, R}\\) from \\(\\beta_{j, k}\\) for all predictors \\(j\\) and all outcomes \\(k\\). We do this at every step in the MCMC chain. For example, in Figure 22.5, consider the regression coefficient on \\(x_1\\) for outcome \\(2\\). Relative to reference outcome \\(1\\), this coefficient is positive, meaning that the probability of outcome \\(2\\) increases relative to outcome \\(1\\) when \\(x_1\\) increases. You can see this in the data graph, as the region of \\(2\\)’s falls to right side (positive \\(x_1\\) direction) of the region of \\(1\\)’s. But if the reference outcome is changed to outcome \\(4\\), then the coefficient on \\(x_1\\) for outcome \\(2\\) changes to a negative value. Algebraically this happens because the coefficient on \\(x_1\\) for outcome \\(4\\) is larger than for outcome \\(2\\), so when the coefficient for outcome \\(4\\) is subtracted, the result is a negative value for the coefficient on outcome \\(2\\). Visually, you can see this in the data graph, as the region of \\(2\\)’s falls to the left side (negative \\(x_1\\) direction) of the region of \\(4\\)’s. Thus, interpreting regression coefficients in a softmax model is rather different than in linear regression. In linear regression, a positive regression coefficient implies that \\(y\\) increases when the predictor increases. But not in softmax regression, where a positive regression coefficient is only positive with respect to a particular reference outcome. (p. 664, emphasis added) 22.3.3.1.1 Bonus: Consider the interceps-only softmax model. Models like fit22.1, above, are great when you want to explore predictors for your nominal variables. However, these models are also really useful when you aren’t interested in predictor variables. In these cases, the intercepts-only model will help you compute high-quality Bayesian intervals around the category percentages. Let’s walk through an example to see what I mean. Fit an intercepts-only version of the model, above. fit22.2 &lt;- brm(data = d1, family = categorical(link = logit), Y ~ 1, prior = c(prior(normal(0, 20), class = Intercept, dpar = mu2), prior(normal(0, 20), class = Intercept, dpar = mu3), prior(normal(0, 20), class = Intercept, dpar = mu4)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 22, file = &quot;fits/fit22.02&quot;) Review the model summary. print(fit22.2) ## Family: categorical ## Links: mu2 = logit; mu3 = logit; mu4 = logit ## Formula: Y ~ 1 ## Data: d1 (Number of observations: 475) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## mu2_Intercept 0.36 0.14 0.08 0.64 1.00 1752 2129 ## mu3_Intercept 0.55 0.14 0.27 0.83 1.00 1590 1663 ## mu4_Intercept 0.59 0.14 0.31 0.86 1.00 1549 1480 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Even with an intercepts-only model, the parameters in a softmax model are difficult to interpret directly. Though you might be tempted to presume mu2_Intercept through mu4_Intercept were probabilities on the log-odds scale, they’re not. This, recall, is due to their connection to the reference category. If we return to the equation from Section 22.1, \\[\\phi_k = \\operatorname{softmax}_S (\\{\\lambda_k\\}) = \\frac{\\exp (\\lambda_k)}{\\sum_{c \\in S} \\exp (\\lambda_c)},\\] we can get a sense of how to convert these parameters to relative probabilities. Here’s how to do so by hand with the posterior draws. as_draws_df(fit22.2) %&gt;% mutate(`lambda[1]` = 0, # recall this is the default `lambda[2]` = b_mu2_Intercept, `lambda[3]` = b_mu3_Intercept, `lambda[4]` = b_mu4_Intercept) %&gt;% pivot_longer(contains(&quot;lambda&quot;)) %&gt;% # the next two rows are where the magic happens group_by(.draw) %&gt;% mutate(phi = exp(value) / sum(exp(value)), Y = str_extract(name, &quot;\\\\d&quot;)) %&gt;% group_by(Y) %&gt;% mean_qi(phi) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 4 × 7 ## Y phi .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 0.17 0.14 0.2 0.95 mean qi ## 2 2 0.24 0.2 0.28 0.95 mean qi ## 3 3 0.29 0.25 0.33 0.95 mean qi ## 4 4 0.3 0.26 0.34 0.95 mean qi We can compute these relative probability values (\\(\\phi_k\\)) much easier with fitted(). f &lt;- fitted(fit22.2) f[1, , ] %&gt;% t() %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## P(Y = 1) 0.17 0.02 0.14 0.20 ## P(Y = 2) 0.24 0.02 0.20 0.28 ## P(Y = 3) 0.29 0.02 0.25 0.33 ## P(Y = 4) 0.30 0.02 0.26 0.34 Anyway, the reason you might want to go through the trouble of fitting an intercepts-only softmax model is to improve on the kinds of bar plots people often report in their manuscripts. Consider these two: # descriptive statistics p1 &lt;- d1 %&gt;% count(Y) %&gt;% mutate(p = n / sum(n)) %&gt;% ggplot(aes(x = Y, y = p)) + geom_col(fill = pl[6]) + scale_y_continuous(NULL, labels = scales::percent, breaks = 0:4 / 10, expand = expansion(mult = c(0, 0.05)), limits = c(0, .4)) + labs(subtitle = &quot;sample statistics&quot;) # population percentages p2 &lt;- f[1, , ] %&gt;% t() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;level&quot;) %&gt;% mutate(Y = str_extract(level, &quot;\\\\d&quot;)) %&gt;% ggplot(aes(x = Y, y = Estimate)) + geom_col(fill = pl[6]) + geom_linerange(aes(ymin = Q2.5, ymax = Q97.5), color = pl[2], size = 1) + scale_y_continuous(NULL, labels = scales::percent, breaks = 0:4 / 10, expand = expansion(mult = c(0, 0.05)), limits = c(0, .4)) + labs(subtitle = &quot;model-based population percentages\\n(with 95% interval bars)&quot;) p1 + p2 + plot_annotation(title = &quot;The softmax model adds information to the conventional sample-based\\nbar plot.&quot;) The plot on the left is the kind of sample data summary you’ll see in countless articles and data presentations. Though it’s a great way to quickly summarize the relative percentages of each category, it does nothing to express how (un)certain we are those sample statistics will describe the population. The intercepts-only softmax model returns the posterior distributions for the population probabilities. In the plot on the right, the bars mark off the posterior means and the vertical lines mark off the 95% intervals. This is why we want the intercepts-only softmax model. Okay, now just for kicks and giggles, I’d like to go on a plotting tangent. If data analysts broadly replaced the typical sample-based plots (on the left) for the model-based plots (on the right), it would be a great improvement. At a personal level, though, I think simple bar plots are over used. Let’s explore four alternatives. In this block, we’ll make and save the first three. # rotated bar plot p1 &lt;- f[1, , ] %&gt;% t() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;level&quot;) %&gt;% mutate(Y = str_extract(level, &quot;\\\\d&quot;)) %&gt;% ggplot(aes(x = Estimate, y = Y)) + geom_col(fill = pl[6]) + geom_linerange(aes(xmin = Q2.5, xmax = Q97.5), color = pl[2], size = 1) + scale_x_continuous(NULL, labels = scales::percent, expand = expansion(mult = c(0, 0.05))) + labs(subtitle = &quot;rotated bar plot&quot;) # coefficient plot p2 &lt;- f[1, , ] %&gt;% t() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;level&quot;) %&gt;% mutate(Y = str_extract(level, &quot;\\\\d&quot;)) %&gt;% ggplot(aes(x = Y, y = Estimate)) + geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5), color = pl[2], size = 1, fatten = 2) + scale_y_continuous(NULL, labels = scales::percent, expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + labs(subtitle = &quot;coefficient plot&quot;) # CCDF bar plots p3 &lt;- fitted(fit22.2, summary = F)[, 1, ] %&gt;% data.frame() %&gt;% set_names(1:4) %&gt;% pivot_longer(everything(), values_to = &quot;p&quot;) %&gt;% mutate(Y = factor(name)) %&gt;% ggplot(aes(x = Y, y = p)) + stat_ccdfinterval(.width = .95, fill = pl[6], color = pl[2], size = 1.5, point_size = 2) + scale_y_continuous(NULL, labels = scales::percent, breaks = 0:4 / 10, expand = expansion(mult = c(0, 0.05)), limits = c(0, .4)) + labs(subtitle = &quot;CCDF bar plot&quot;) The fourth alternative is a little weird AND it’s going to take a bit more work than the first three. # for annotation text &lt;- fitted(fit22.2)[1, , ] %&gt;% t() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;level&quot;) %&gt;% mutate(y = (Estimate / 2) + lag(cumsum(Estimate), default = 0), summary = str_c(round(100 * Estimate, 1), &quot;*&#39;% ± &#39;*&quot;, round(200 * Est.Error, 1)), label_y = str_c(&quot;Y = &quot;, 1:4)) p4 &lt;- # wrangle fitted(fit22.2, summary = F)[, 1, ] %&gt;% data.frame() %&gt;% set_names(1:4) %&gt;% mutate(row = 1:n()) %&gt;% pivot_longer(-row) %&gt;% mutate(Y = fct_rev(name)) %&gt;% # plot ggplot(aes(x = row, y = value)) + geom_col(aes(fill = Y), position = &quot;stack&quot;, size = 0, width = 1) + geom_text(data = text, aes(x = -2000, y = y, label = label_y), color = pl[1]) + geom_text(data = text, aes(x = 6000, y = y, label = summary), color = pl[1], parse = T) + scale_fill_manual(values = pl[7:4], breaks = NULL) + scale_x_continuous(NULL, breaks = NULL, limits = c(-4000, 8000)) + scale_y_continuous(NULL, labels = scales::percent, expand = c(0, 0)) + labs(subtitle = &quot;stacked bar plot with uncertain boundaries&quot;) Okay, now combine the four ggplots and behold! # combine p1 + p2 + p3 + p4 + plot_annotation(title = &quot;Alternatives to the conventional bar plot&quot;, tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) The rotated bar plot (a) is most useful when the names of the levels are longer character strings. For example, imagine that instead of 1 through 4, the four levels were countries or pharmaceutical drugs. You wouldn’t want to mess with formatting those on an \\(x\\)-axis. Use a rotated bar plot, instead. For my taste, the simple coefficient plot (b) gets the job done nicely without the unnecessary clutter of the bars. Matthew Kay’s CCDF bar plots (c) provide a fuller expression of the shape of the posterior uncertainty in each percentage. If desired, you could even omit the dot intervals from those and the visual expression of uncertainty would still remain. The final stacked bar plot with the fuzzy boundaries (d) is the result of a little back-and-forth on twitter (original tweet). It’s the oddball of the group, but what it does uniquely well is show how the percentages of the groups all depend on one another and must, by definition, sum to 100. 22.3.3.2 Conditional logistic model. Since we will be fitting the conditional logistic model with two different strategies, I’m going to deviate from how Kruschke organized this part of the text and break this section up into two subsections: First we’ll walk through the custom family approach. Second we’ll explore the sequential ordinal approach. 22.3.3.2.1 Conditional logistic models with custom likelihoods. As we briefly learned in Section 8.6.1, brms users can define their own custom likelihood functions, which Bürkner outlined in his (2022) vignette, Define custom response distributions with brms. As part of the Nominal data and Kruschke’s “conditional logistic” approach thread on the Stan forums, Henrik Singmann showed how you can use this functionality to fit conditional logistic models with brms. We will practice how to do this for the models of both the d3 and d4 data sets, which were showcased in the left and right panels of Figure 22.3 in Section 22.2. Going in order, we’ll focus first on how to model the data in d3. For the first step, we use the custom_family() function to name the new family with the name argument, name the family’s parameters with the dpars argument, name the link function(s) with the links argument, define whether the distribution is discrete or continuous with the type argument, provide the names of any variables that are part of the internal workings of the family but are not among the distributional parameters with the vars argument, and provide supporting information with the specials argument. cond_log_1 &lt;- custom_family( name = &quot;cond_log_1&quot;, dpars = c(&quot;mu&quot;, &quot;mub&quot;, &quot;muc&quot;), links = &quot;identity&quot;, type = &quot;int&quot;, vars = c(&quot;n_cat&quot;), specials = &quot;categorical&quot; ) In the second step, we use the stanvar() function to define our custom probability mass function and the corresponding function that will allow us to return predictions. stan_lpmf_1 &lt;- stanvar(block = &quot;functions&quot;, scode = &quot; real cond_log_1_lpmf(int y, real mu, real mu_b, real mu_c, int n_cat) { real p_mu = inv_logit(mu); real p_mub = inv_logit(mu_b); real p_muc = inv_logit(mu_c); vector[n_cat] prob; prob[1] = p_mu; prob[2] = p_mub * (1 - p_mu); prob[3] = p_muc * (1 - p_mub) * (1 - p_mu); prob[4] = (1 - p_mu) * (1 - p_mub) * (1 - p_muc); return(categorical_lpmf(y | prob)); } vector cond_log_1_pred(int y, real mu, real mu_b, real mu_c, int n_cat) { real p_mu = inv_logit(mu); real p_mub = inv_logit(mu_b); real p_muc = inv_logit(mu_c); vector[n_cat] prob; prob[1] = p_mu; prob[2] = p_mub * (1 - p_mu); prob[3] = p_muc * (1 - p_mub) * (1 - p_mu); prob[4] = (1 - p_mu) * (1 - p_mub) * (1 - p_muc); return(prob); } &quot;) Note how we have defined the four prob[i] values based on the four equations from above: \\[ \\begin{align*} \\phi_1 &amp; = \\phi_{\\{ 1 \\} | \\{ 1,2,3,4 \\}} \\\\ \\phi_2 &amp; = \\phi_{\\{ 2 \\} | \\{ 2,3,4 \\}} \\cdot \\left ( 1 - \\phi_{\\{ 1 \\} | \\{ 1,2,3,4 \\}} \\right) \\\\ \\phi_3 &amp; = \\phi_{\\{ 3 \\} | \\{ 3,4 \\}} \\cdot \\left ( 1 - \\phi_{\\{ 2 \\} | \\{ 2,3,4 \\}} \\right) \\cdot \\left ( 1 - \\phi_{\\{ 1 \\} | \\{ 1,2,3,4 \\}} \\right) \\\\ \\phi_4 &amp; = \\left ( 1 - \\phi_{\\{ 3 \\} | \\{ 3,4 \\}} \\right) \\cdot \\left ( 1 - \\phi_{\\{ 2 \\} | \\{ 2,3,4 \\}} \\right) \\cdot \\left ( 1 - \\phi_{\\{ 1 \\} | \\{ 1,2,3,4 \\}} \\right). \\end{align*} \\] Third, we save another stanvar() object with additional information. stanvars &lt;- stanvar(x = 4, name = &quot;n_cat&quot;, scode = &quot; int n_cat;&quot;) Now we’re ready to fit the model with brm(). Notice how our use of the family and stanvars functions. fit22.3 &lt;- brm(data = d3, family = cond_log_1, Y ~ 1 + X1 + X2, prior = c(prior(normal(0, 20), class = Intercept), prior(normal(0, 20), class = Intercept, dpar = mub), prior(normal(0, 20), class = Intercept, dpar = muc), prior(normal(0, 20), class = b), prior(normal(0, 20), class = b, dpar = mub), prior(normal(0, 20), class = b, dpar = muc)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 22, stanvars = stan_lpmf_1 + stanvars, file = &quot;fits/fit22.03&quot;) Check the model summary. print(fit22.3) ## Family: cond_log_1 ## Links: mu = identity; mub = identity; muc = identity ## Formula: Y ~ 1 + X1 + X2 ## Data: d3 (Number of observations: 475) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.02 0.46 -4.99 -3.17 1.00 2681 2389 ## mub_Intercept -2.11 0.36 -2.85 -1.45 1.00 2942 2993 ## muc_Intercept -0.96 0.32 -1.64 -0.35 1.00 3357 2857 ## X1 -4.92 0.54 -6.05 -3.92 1.00 2668 2259 ## X2 0.01 0.19 -0.37 0.38 1.00 4660 2705 ## mub_X1 0.74 0.29 0.20 1.31 1.00 3840 3365 ## mub_X2 -5.19 0.64 -6.54 -4.02 1.00 3266 2842 ## muc_X1 3.00 0.49 2.11 4.02 1.00 3047 2918 ## muc_X2 3.08 0.53 2.12 4.20 1.00 3101 3186 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As they aren’t the most intuitive, here’s how to understand our two prefixes: the lines with no prefix have to do with \\(\\lambda_{\\{ 1 \\} | \\{ 1,2,3,4 \\}}\\), mub_ has to do with \\(\\lambda_{\\{ 2 \\} | \\{ 2,3,4 \\}}\\), and muc_ has to do with \\(\\lambda_{\\{ 3 \\} | \\{ 3,4 \\}}\\). If you compare those posterior means of each of those parameters from the data-generating equations at the top of Figure 22.3, you’ll see they are spot on (within simulation variance). Here’s how we might visualize those posteriors in our version of the histograms in the top right panel(s) of Figure 22.6. # extract the posterior draws draws &lt;- as_draws_df(fit22.3) %&gt;% # our lives will be easier if we adjust the column names rename(a_Intercept = b_Intercept, b_Intercept = b_mub_Intercept, c_Intercept = b_muc_Intercept, a_X1 = b_X1, a_X2 = b_X2, b_X1 = b_mub_X1, b_X2 = b_mub_X2, c_X1 = b_muc_X1, c_X2 = b_muc_X2) # wrangle p1 &lt;- draws %&gt;% pivot_longer(a_Intercept:c_X2) %&gt;% mutate(lambda = case_when(str_detect(name, &quot;a_&quot;) ~ &quot;lambda[&#39;{1}|{1,2,3,4}&#39;]&quot;, str_detect(name, &quot;b_&quot;) ~ &quot;lambda[&#39;{2}|{2,3,4}&#39;]&quot;, str_detect(name, &quot;c_&quot;) ~ &quot;lambda[&#39;{3}|{3,4}&#39;]&quot;), parameter = case_when(str_detect(name, &quot;Intercept&quot;) ~ &quot;beta[0]&quot;, str_detect(name, &quot;X1&quot;) ~ &quot;beta[1]&quot;, str_detect(name, &quot;X2&quot;) ~ &quot;beta[2]&quot;)) %&gt;% # plot ggplot(aes(x = value, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = pl[4], slab_color = pl[3], color = pl[1], point_color = pl[2], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;marginal posterior&quot;) + facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &quot;free_x&quot;) + theme(axis.text = element_text(size = 8)) If we use the threshold formula from above, \\[x_2 = (-\\beta_0 / \\beta_2) + (-\\beta_1 / \\beta_2)x_1,\\] to the posterior draws, we can make our version of the upper left panel of Figure 22.6. set.seed(22) p2 &lt;- draws %&gt;% slice_sample(n = 30) %&gt;% pivot_longer(a_Intercept:c_X2) %&gt;% separate(name, into = c(&quot;mu&quot;, &quot;parameter&quot;), sep = &quot;_&quot;) %&gt;% pivot_wider(names_from = parameter, values_from = value) %&gt;% mutate(intercept = -Intercept / X2, slope = -X1 / X2) %&gt;% ggplot() + geom_text(data = d3, aes(x = X1, y = X2, label = Y, color = factor(Y)), size = 3, show.legend = F) + geom_abline(aes(intercept = intercept, slope = slope, group = interaction(.draw, mu), linetype = mu), size = 1/4, alpha = 1/2, color = pl[1]) + scale_color_manual(values = pl[2:5]) + scale_linetype(NULL, labels = parse(text = c( &quot;lambda[&#39;{1}|{1,2,3,4}&#39;]&quot;, &quot;lambda[&#39;{2}|{2,3,4}&#39;]&quot;, &quot;lambda[&#39;{3}|{3,4}&#39;]&quot;)), guide = guide_legend( direction = &quot;vertical&quot;, label.hjust = 0.5, label.theme = element_text(size = 10))) + labs(x = expression(x[1]), y = expression(x[2])) + theme(legend.justification = 0.5, legend.position = &quot;top&quot;) Now combine the two ggplots, add a little formatting, and show the full upper half of Figure 22.6, based on the custom_family() approach. (p2 + p1) &amp; plot_layout(widths = c(1, 2)) &amp; plot_annotation(title = &quot;Figure 22.6, upper half&quot;, subtitle = &quot;Results from the conditional logistic model fit to the d3 data via the custom-family approach&quot;) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Though it isn’t necessary to reproduce any of the plots in this section of Kruschke’s text, we’ll want to use the expose_functions() function if we wanted to use any of the brms post-processing functions for our model fit with the custom likelihood. expose_functions(fit22.3, vectorize = TRUE) Here’s what we’d need to do before computing information criteria estimates, such as with the WAIC. log_lik_cond_log_1 &lt;- function(i, prep) { mu &lt;- brms::get_dpar(prep, &quot;mu&quot;, i = i) mub &lt;- brms::get_dpar(prep, &quot;mub&quot;, i = i) muc &lt;- brms::get_dpar(prep, &quot;muc&quot;, i = i) n_cat &lt;- prep$data$n_cat y &lt;- prep$data$Y[i] cond_log_1_lpmf(y, mu, mub, muc, n_cat) } fit22.3 &lt;- add_criterion(fit22.3, criterion = &quot;waic&quot;) waic(fit22.3) ## ## Computed from 4000 by 475 log-likelihood matrix ## ## Estimate SE ## elpd_waic -230.8 16.8 ## p_waic 9.3 1.1 ## waic 461.5 33.6 ## ## 2 (0.4%) p_waic estimates greater than 0.4. We recommend trying loo instead. If we wanted to use one of the functions that relies on conditional expectations, such as conditional_effects(), we’d execute something like this. posterior_epred_cond_log_1 &lt;- function(prep) { mu &lt;- brms::get_dpar(prep, &quot;mu&quot;) mu_b &lt;- brms::get_dpar(prep, &quot;mub&quot;) mu_c &lt;- brms::get_dpar(prep, &quot;muc&quot;) n_cat &lt;- prep$data$n_cat y &lt;- prep$data$Y prob &lt;- cond_log_1_pred(y = y, mu = mu, mu_b = mu_b, mu_c = mu_c, n_cat = n_cat) dim(prob) &lt;- c(dim(prob)[1], dim(mu)) prob &lt;- aperm(prob, c(2,3,1)) dimnames(prob) &lt;- list( as.character(seq_len(dim(prob)[1])), NULL, as.character(seq_len(dim(prob)[3])) ) prob } ce &lt;- conditional_effects( fit22.3, categorical = T, effects = &quot;X1&quot;) plot(ce, plot = FALSE)[[1]] + scale_fill_manual(values = pl[2:5]) + scale_color_manual(values = pl[2:5]) If we wanted to do a posterior predictive check with the pp_check() function, we’d need to do something like this. posterior_predict_cond_log_1 &lt;- function(i, prep, ...) { mu &lt;- brms::get_dpar(prep, &quot;mu&quot;, i = i) mu_b &lt;- brms::get_dpar(prep, &quot;mub&quot;, i = i) mu_c &lt;- brms::get_dpar(prep, &quot;muc&quot;, i = i) n_cat &lt;- prep$data$n_cat y &lt;- prep$data$Y[i] prob &lt;- cond_log_1_pred(y, mu, mu_b, mu_c, n_cat) # make sure you have the extraDistr package extraDistr::rcat(length(mu), t(prob)) } bayesplot::color_scheme_set(pl[7:2]) pp_check(fit22.3, type = &quot;bars&quot;, ndraws = 100, size = 1/2, fatten = 2) So far all of this has been with the conditional logistic model based on the first hierarchy of two-set divisions, which Kruschke used to simulate the d3 data. Now we’ll switch to consider the second hierarchy of two-set divisions, with which Kruschke simulated the d4 data. That second hierarchy, recall, resulted in the following definition for the conditional probabilities for the four levels of Y: \\[ \\begin{align*} \\phi_1 &amp; = \\phi_{\\{ 1 \\} | \\{ 1,2 \\}} \\cdot \\phi_{\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} \\\\ \\phi_2 &amp; = \\left ( 1 - \\phi_{\\{ 1 \\} | \\{ 1,2 \\}} \\right) \\cdot \\phi_{\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} \\\\ \\phi_3 &amp; = \\phi_{\\{ 3 \\} | \\{ 3,4 \\}} \\cdot \\left ( 1 - \\phi_{\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} \\right) \\\\ \\phi_4 &amp; = \\left ( 1 - \\phi_{\\{ 3 \\} | \\{ 3,4 \\}} \\right) \\cdot \\left ( 1 - \\phi_{\\{ 1,2 \\} | \\{ 1,2,3,4 \\}} \\right). \\end{align*} \\] This will require us to define a new custom family, which we’ll call cond_log_2. cond_log_2 &lt;- custom_family( name = &quot;cond_log_2&quot;, dpars = c(&quot;mu&quot;, &quot;mub&quot;, &quot;muc&quot;), links = &quot;identity&quot;, type = &quot;int&quot;, vars = c(&quot;n_cat&quot;), specials = &quot;categorical&quot; ) Next, we use the stanvar() function to define our custom probability mass function and the corresponding function that will allow us to return predictions, which we’ll just save as stan_lpmf_2. Other than the names, notice that the major change is how we have defined the prob[i] parameters. stan_lpmf_2 &lt;- stanvar(block = &quot;functions&quot;, scode = &quot; real cond_log_2_lpmf(int y, real mu, real mu_b, real mu_c, int n_cat) { real p_mu = inv_logit(mu); real p_mub = inv_logit(mu_b); real p_muc = inv_logit(mu_c); vector[n_cat] prob; prob[1] = p_mub * p_mu; prob[2] = (1 - p_mub) * p_mu; prob[3] = p_muc * (1 - p_mu); prob[4] = (1 - p_muc) * (1 - p_mu); return(categorical_lpmf(y | prob)); } vector cond_log_2_pred(int y, real mu, real mu_b, real mu_c, int n_cat) { real p_mu = inv_logit(mu); real p_mub = inv_logit(mu_b); real p_muc = inv_logit(mu_c); vector[n_cat] prob; prob[1] = p_mub * p_mu; prob[2] = (1 - p_mub) * p_mu; prob[3] = p_muc * (1 - p_mu); prob[4] = (1 - p_muc) * (1 - p_mu); return(prob); } &quot;) Now we’re ready to fit the model with brm(). Again, notice how our use of the family and stanvars functions. fit22.4 &lt;- brm(data = d4, family = cond_log_2, Y ~ 1 + X1 + X2, prior = c(prior(normal(0, 20), class = Intercept), prior(normal(0, 20), class = Intercept, dpar = mub), prior(normal(0, 20), class = Intercept, dpar = muc), prior(normal(0, 20), class = b), prior(normal(0, 20), class = b, dpar = mub), prior(normal(0, 20), class = b, dpar = muc)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 22, stanvars = stan_lpmf_2 + stanvars, file = &quot;fits/fit22.04&quot;) Check the model summary. print(fit22.4) ## Family: cond_log_2 ## Links: mu = identity; mub = identity; muc = identity ## Formula: Y ~ 1 + X1 + X2 ## Data: d4 (Number of observations: 475) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.06 0.46 -5.03 -3.22 1.00 2431 2044 ## mub_Intercept -1.40 1.21 -3.86 0.93 1.00 2543 1878 ## muc_Intercept -1.03 0.24 -1.51 -0.58 1.00 2945 2658 ## X1 -4.80 0.51 -5.87 -3.87 1.00 2420 2016 ## X2 0.36 0.20 -0.03 0.74 1.00 4353 2921 ## mub_X1 1.54 0.90 -0.15 3.44 1.00 2675 2365 ## mub_X2 -5.37 1.18 -8.07 -3.36 1.00 2927 1298 ## muc_X1 3.03 0.39 2.32 3.82 1.00 2386 2232 ## muc_X2 3.13 0.36 2.47 3.88 1.00 2649 2446 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We can use the same basic workflow as before to make our version of the upper half of Figure 22.7. # extract the posterior draws draws &lt;- as_draws_df(fit22.4) %&gt;% # like before, let&#39;s adjust the column names rename(a_Intercept = b_Intercept, b_Intercept = b_mub_Intercept, c_Intercept = b_muc_Intercept, a_X1 = b_X1, a_X2 = b_X2, b_X1 = b_mub_X1, b_X2 = b_mub_X2, c_X1 = b_muc_X1, c_X2 = b_muc_X2) # 2D thresholds on the left set.seed(22) p1 &lt;- draws %&gt;% slice_sample(n = 30) %&gt;% pivot_longer(a_Intercept:c_X2) %&gt;% separate(name, into = c(&quot;mu&quot;, &quot;parameter&quot;), sep = &quot;_&quot;) %&gt;% pivot_wider(names_from = parameter, values_from = value) %&gt;% mutate(intercept = -Intercept / X2, slope = -X1 / X2) %&gt;% ggplot() + geom_text(data = d4, aes(x = X1, y = X2, label = Y, color = factor(Y)), size = 3, show.legend = F) + geom_abline(aes(intercept = intercept, slope = slope, group = interaction(.draw, mu), linetype = mu), size = 1/4, alpha = 1/2, color = pl[1]) + scale_color_manual(values = pl[2:5]) + scale_linetype(NULL, labels = parse(text = c( &quot;lambda[&#39;{1,2}|{1,2,3,4}&#39;]&quot;, &quot;lambda[&#39;{1}|{1,2}&#39;]&quot;, &quot;lambda[&#39;{3}|{3,4}&#39;]&quot;)), guide = guide_legend( direction = &quot;vertical&quot;, label.hjust = 0.5, label.theme = element_text(size = 10))) + labs(x = expression(x[1]), y = expression(x[2])) + theme(legend.justification = 0.5, legend.position = &quot;top&quot;) # marginal posteriors on the right p2 &lt;- draws %&gt;% pivot_longer(a_Intercept:c_X2) %&gt;% mutate(lambda = case_when(str_detect(name, &quot;a_&quot;) ~ &quot;lambda[&#39;{1,2}|{1,2,3,4}&#39;]&quot;, str_detect(name, &quot;b_&quot;) ~ &quot;lambda[&#39;{1}|{1,2}&#39;]&quot;, str_detect(name, &quot;c_&quot;) ~ &quot;lambda[&#39;{3}|{3,4}&#39;]&quot;), parameter = case_when(str_detect(name, &quot;Intercept&quot;) ~ &quot;beta[0]&quot;, str_detect(name, &quot;X1&quot;) ~ &quot;beta[1]&quot;, str_detect(name, &quot;X2&quot;) ~ &quot;beta[2]&quot;)) %&gt;% # plot ggplot(aes(x = value, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = pl[4], slab_color = pl[3], color = pl[1], point_color = pl[2], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;marginal posterior&quot;) + facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &quot;free_x&quot;) # combine, entitle, and display the results (p1 + p2) &amp; plot_layout(widths = c(1, 2)) &amp; plot_annotation(title = &quot;Figure 22.7, upper half&quot;, subtitle = &quot;Results from the conditional logistic model fit to the d4 data via the custom-family approach&quot;) As Kruschke pointed out in the text, notice that the estimates for \\(\\lambda_2\\) are more uncertain, with wider HDI’s, than the other coefficients. This uncertainty is also shown in the threshold lines on the data: The lines separating the \\(1\\)’s from the \\(2\\)’s have a much wider spread than the other boundaries. Inspection of the scatter plot explains why: There is only a small zone of data that informs the separation of \\(1\\)’s from \\(2\\)’s, and therefore the estimate must be relatively ambiguous. (p. 665) I’m not going to go through a full demonstration like before, but if you want to use more brms post processing functions for fit22.4 or any other model fit with our custom cond_log_2 function, you’d need to execute this block of code first. Then post process to your heart’s desire. expose_functions(fit22.4, vectorize = TRUE) # for information criteria log_lik_cond_log_2 &lt;- function(i, prep) { mu &lt;- brms::get_dpar(prep, &quot;mu&quot;, i = i) mub &lt;- brms::get_dpar(prep, &quot;mub&quot;, i = i) muc &lt;- brms::get_dpar(prep, &quot;muc&quot;, i = i) n_cat &lt;- prep$data$n_cat y &lt;- prep$data$Y[i] cond_log_2_lpmf(y, mu, mub, muc, n_cat) } # for conditional expectations posterior_epred_cond_log_2 &lt;- function(prep) { mu &lt;- brms::get_dpar(prep, &quot;mu&quot;) mu_b &lt;- brms::get_dpar(prep, &quot;mub&quot;) mu_c &lt;- brms::get_dpar(prep, &quot;muc&quot;) n_cat &lt;- prep$data$n_cat y &lt;- prep$data$Y prob &lt;- cond_log_2_pred(y = y, mu = mu, mu_b = mu_b, mu_c = mu_c, n_cat = n_cat) dim(prob) &lt;- c(dim(prob)[1], dim(mu)) prob &lt;- aperm(prob, c(2,3,1)) dimnames(prob) &lt;- list( as.character(seq_len(dim(prob)[1])), NULL, as.character(seq_len(dim(prob)[3])) ) prob } # for posterior predictions posterior_predict_cond_log_2 &lt;- function(i, prep, ...) { mu &lt;- brms::get_dpar(prep, &quot;mu&quot;, i = i) mu_b &lt;- brms::get_dpar(prep, &quot;mub&quot;, i = i) mu_c &lt;- brms::get_dpar(prep, &quot;muc&quot;, i = i) n_cat &lt;- prep$data$n_cat y &lt;- prep$data$Y[i] prob &lt;- cond_log_2_pred(y, mu, mu_b, mu_c, n_cat) # make sure you have the extraDistr package extraDistr::rcat(length(mu), t(prob)) } In this section of the text, Kruschke also showed the results of when he analyzed the two data sets with the non-data-generating likelihoods. In the lower half of Figure 22.6, he showed the results of his second version of the conditional logistic model applied to the d3 data. In the lower half of Figure 22.7, he showed the results of his first version of the conditional logistic model applied to the d4 data. Since this section is already complicated enough, we’re not going to do that. But if you’d like to see what happens, consider it a personal homework assignment. In principle, the different conditional logistic models could be put into an overarching hierarchical model comparison. If you have only a few specific candidate models to compare, this could be a feasible approach. But it is not an easily pursued approach to selecting a partition of outcomes from all possible partitions of outcomes when there are many outcomes… Therefore, it is typical to consider a single model, or small set of models, that are motivated by being meaningful in the context of the application, and interpreting the parameter estimates in that meaningful context. (p. 667) Kruschke finished this section with: Finally, when you run the models in JAGS, you may find that there is high autocorrelation in the MCMC chains (even with standardized data), which requires a very long chain for adequate ESS. This suggests that Stan might be a more efficient approach. Since we fit our models with Stan via brms, high autocorrelations and low effective sample sizes weren’t a problem. For example, here are the bulk and tail effective sample sizes for both of our two models. library(posterior) bind_rows( as_draws_df(fit22.3) %&gt;% summarise_draws(), as_draws_df(fit22.4) %&gt;% summarise_draws() ) %&gt;% mutate(fit = rep(c(&quot;fit22.3&quot;, &quot;fit22.4&quot;), each = n() / 2)) %&gt;% pivot_longer(starts_with(&quot;ess&quot;)) %&gt;% ggplot(aes(x = value)) + geom_dotplot(binwidth = 325, color = pl[8], fill = pl[5], stroke = 1/2) + scale_y_continuous(NULL, breaks = NULL) + xlim(0, NA) + facet_grid(fit ~ name) + theme(axis.text = element_text(size = 9)) The values look pretty good. We may as well look at the autocorrelations. To keep things simple, this time we’ll restrict our analysis to fit22.4. [The results are largely the same for fit22.3.] library(bayesplot) ac &lt;- as_draws_df(fit22.4) %&gt;% mutate(chain = .chain) %&gt;% select(b_Intercept:b_muc_X2, chain) %&gt;% mcmc_acf(lags = 5) ac$data %&gt;% filter(Lag &gt; 0) %&gt;% ggplot(aes(x = AC)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_dotplot(binwidth = 1/14, color = pl[8], fill = pl[5], stroke = 1/2) + scale_x_continuous(&quot;autocorrelation&quot;, limits = c(-1, 1), labels = c(&quot;-1&quot;, &quot;-.5&quot;, &quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL) + facet_grid(Lag ~ Chain, labeller = label_both) On the whole, the autocorrelations are reasonably low across all parameters, chains, and lags. 22.3.3.2.2 Conditional logistic models by sequential ordinal regression. In their (2019) paper, Ordinal regression models in psychology: A tutorial, Bürkner and Vourre outlined a framework for fitting a variety of orginal models with brms. We’ll learn more about ordinal models in Chapter 23. In this section, we’ll use Mattan Ben-Shachar’s strategy and purpose one of the ordinal models to fit a conditional logistic model to our nominal data. As outlined in Bürkner &amp; Vuorre (2019), and as we will learn in greater detain in the next chapter, many ordinal regression models presume an underlying continuous process. However, you can use a sequential model in cases where one level of the criterion is only possible after the lower levels of the criterion have been achieved. Although this is not technically correct for the nominal variable Y in the d3 data set, the simple hierarchical sequence Kruschke used to model those data does follow that same pattern. Ben-Shachar’s insight was that if we treat our nominal variable Y as ordinal, the sequential model will mimic the sequential-ness of Kruschke’s binary-choices hierarchy. To get this to work, we first have to save an ordinal version of Y, which we’ll call Y_ord. d3 &lt;- d3 %&gt;% mutate(Y_ord = ordered(Y)) # what are the new attributes? attributes(d3$Y_ord) ## $levels ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## ## $class ## [1] &quot;ordered&quot; &quot;factor&quot; Within brm() we fit sequential models using family = sratio, which defaults to the logit link. If you want to use predictors in a model of this kind and you would like those coefficients to vary across the different levels of the criterion, you need to insert the predictor terms within the cs() function. Here’s how to fit the model with brm(). fit22.5 &lt;- brm(data = d3, family = sratio, Y_ord ~ 1 + cs(X1) + cs(X2), prior = c(prior(normal(0, 20), class = Intercept), prior(normal(0, 20), class = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 22, file = &quot;fits/fit22.05&quot;) Check the model summary. print(fit22.5) ## Family: sratio ## Links: mu = logit; disc = identity ## Formula: Y_ord ~ 1 + cs(X1) + cs(X2) ## Data: d3 (Number of observations: 475) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -4.01 0.45 -4.96 -3.22 1.00 3175 2508 ## Intercept[2] -2.13 0.37 -2.91 -1.47 1.00 2787 2545 ## Intercept[3] -0.96 0.33 -1.64 -0.36 1.00 2907 2690 ## X1[1] 4.91 0.51 3.98 5.98 1.00 3171 2532 ## X1[2] -0.74 0.30 -1.34 -0.18 1.00 3524 2945 ## X1[3] -3.00 0.50 -4.08 -2.12 1.00 3315 2819 ## X2[1] -0.01 0.19 -0.40 0.37 1.00 4535 2885 ## X2[2] 5.23 0.65 4.07 6.57 1.00 2869 2615 ## X2[3] -3.10 0.53 -4.21 -2.14 1.00 2876 2378 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 NA NA NA ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). One thing that might not be apparent at first glance is that although this model is essentially equivalent to the family = cond_log_1 version of the model we fit with fit22.3, above, the parameters are a little different. The intercepts are largely the same. However, the coefficients for the X1 and X2 predictors have switched signs. This will be easier to see with a coefficient plot comparing fit22.3 and fit22.5. rbind(fixef(fit22.3)[c(1:4, 6, 8, 5, 7, 9), ], fixef(fit22.5)) %&gt;% data.frame() %&gt;% mutate(beta = rep(str_c(&quot;beta[&quot;, c(0:2, 0:2), &quot;]&quot;), each = 3), lambda = rep(str_c(&quot;lambda==&quot;, 1:3), times = 3 * 2), family = rep(c(&quot;cond_log_1&quot;, &quot;sratio&quot;), each = 9)) %&gt;% # plot! ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = family)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_pointrange(size = 1/2, fatten = 5/4) + # stat_pointinterval(.width = .95, point_size = 1.5, size = 1) + labs(x = &quot;marginal posterior&quot;, y = NULL) + facet_grid(lambda ~ beta, labeller = label_parsed, scales = &quot;free_x&quot;) Even though the \\(\\beta_1\\) and \\(\\beta_2\\) parameters switched signs, their magnitudes are about the same. Thus, if we want to use our fit22.5 to plot the thresholds as in Figure 22.6, we’ll have to update our threshold formula to \\[x_2 = (\\color{#984136}{+\\beta_0} / \\beta_2) + (\\beta_1 / \\beta_2)x_1.\\] With that adjustment in line, here’s our updated version of the left panel of Figure 22.6. # no need to rename the columns, this time as_draws_df(fit22.5) %&gt;% slice_sample(n = 30) %&gt;% pivot_longer(starts_with(&quot;b&quot;)) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;) %&gt;% str_remove(., &quot;bcs_&quot;)) %&gt;% separate(name, into = c(&quot;parameter&quot;, &quot;lambda&quot;)) %&gt;% pivot_wider(names_from = parameter, values_from = value) %&gt;% mutate(intercept = Intercept / X2, slope = -X1 / X2) %&gt;% ggplot() + geom_text(data = d3, aes(x = X1, y = X2, label = Y, color = factor(Y)), size = 3, show.legend = F) + geom_abline(aes(intercept = intercept, slope = slope, group = interaction(.draw, lambda), linetype = lambda), size = 1/4, alpha = 1/2, color = pl[1]) + scale_color_manual(values = pl[2:5]) + scale_linetype(NULL, labels = parse(text = c( &quot;lambda[&#39;{1}|{1,2,3,4}&#39;]&quot;, &quot;lambda[&#39;{2}|{2,3,4}&#39;]&quot;, &quot;lambda[&#39;{3}|{3,4}&#39;]&quot;)), guide = guide_legend( direction = &quot;vertical&quot;, label.hjust = 0.5, label.theme = element_text(size = 10))) + coord_equal() + labs(x = expression(x[1]), y = expression(x[2])) + theme(legend.justification = 0.5, legend.position = &quot;top&quot;) Though we used a different likelihood and a different formula for the thresholds, we got same basic model results. They’re just parameterized in a slightly different way. The nice thing with the family = sratio approach is all of the typical brms post processing functions will work out of the box. For example, here’s the posterior predictive check via pp_check(). pp_check(fit22.5, type = &quot;bars&quot;, ndraws = 100, size = 1/2, fatten = 2) Also note how the information criteria estimates for the two approaches are essentially the same. fit22.5 &lt;- add_criterion(fit22.5, criterion = &quot;waic&quot;) loo_compare(fit22.3, fit22.5, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit22.3 0.0 0.0 -230.8 16.8 9.3 1.1 461.5 33.6 ## fit22.5 -0.1 0.1 -230.8 16.8 9.3 1.1 461.6 33.6 A limitation of the family = sratio method for conditional logistic models is it requires a simple binary-divisions hierarchy that resembles the one we just used, the one in the left panel of Figure 22.2. It is not well suited for the more complicated hierarchy displayed in the right panel of Figure 22.2, nor will it help you make sense of data generated by that kind of mechanism. For example, consider what happens when we try to use family = sratio with the d4 data. # make an ordinal version of Y d4 &lt;- d4 %&gt;% mutate(Y_ord = ordered(Y)) # fit the model fit22.6 &lt;- brm(data = d4, family = sratio, Y_ord ~ 1 + cs(X1) + cs(X2), prior = c(prior(normal(0, 20), class = Intercept), prior(normal(0, 20), class = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 22, file = &quot;fits/fit22.06&quot;) print(fit22.6) ## Family: sratio ## Links: mu = logit; disc = identity ## Formula: Y_ord ~ 1 + cs(X1) + cs(X2) ## Data: d4 (Number of observations: 475) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -6.01 0.72 -7.53 -4.70 1.00 1968 2049 ## Intercept[2] -5.43 0.70 -6.98 -4.19 1.00 2174 1810 ## Intercept[3] -1.02 0.23 -1.48 -0.57 1.00 2985 3132 ## X1[1] 2.70 0.44 1.88 3.61 1.00 2412 2527 ## X1[2] 5.59 0.72 4.31 7.17 1.00 2389 1746 ## X1[3] -3.02 0.38 -3.81 -2.30 1.00 2558 2829 ## X2[1] 2.37 0.42 1.60 3.22 1.00 2518 2706 ## X2[2] -1.17 0.29 -1.75 -0.62 1.00 2941 2773 ## X2[3] -3.11 0.35 -3.85 -2.47 1.00 2981 2833 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 NA NA NA ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you look at the parameter summary, nothing obviously bad happened. The computer didn’t crash or anything. To get a better sense of the damage, we plot. # extract the posterior draws for fit22.6 draws &lt;- as_draws_df(fit22.6) # 2D thresholds on the left set.seed(22) p1 &lt;- draws %&gt;% slice_sample(n = 30) %&gt;% pivot_longer(starts_with(&quot;b&quot;)) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;) %&gt;% str_remove(., &quot;bcs_&quot;)) %&gt;% separate(name, into = c(&quot;parameter&quot;, &quot;lambda&quot;)) %&gt;% pivot_wider(names_from = parameter, values_from = value) %&gt;% # still using the adjusted formula for the thresholds mutate(intercept = Intercept / X2, slope = -X1 / X2) %&gt;% ggplot() + geom_text(data = d3, aes(x = X1, y = X2, label = Y, color = factor(Y)), size = 3, show.legend = F) + geom_abline(aes(intercept = intercept, slope = slope, group = interaction(.draw, lambda), linetype = lambda), size = 1/4, alpha = 1/2, color = pl[1]) + scale_color_manual(values = pl[2:5]) + scale_linetype(NULL, labels = parse(text = c( &quot;lambda[&#39;{1}|{1,2,3,4}&#39;]&quot;, &quot;lambda[&#39;{2}|{2,3,4}&#39;]&quot;, &quot;lambda[&#39;{3}|{3,4}&#39;]&quot;)), guide = guide_legend( direction = &quot;vertical&quot;, label.hjust = 0.5, label.theme = element_text(size = 10))) + labs(x = expression(x[1]), y = expression(x[2])) + theme(legend.justification = 0.5, legend.position = &quot;top&quot;) # marginal posteriors on the right p2 &lt;- draws %&gt;% pivot_longer(starts_with(&quot;b&quot;)) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;) %&gt;% str_remove(., &quot;bcs_&quot;)) %&gt;% separate(name, into = c(&quot;parameter&quot;, &quot;lambda&quot;)) %&gt;% mutate(lambda = case_when(lambda == &quot;1&quot; ~ &quot;lambda[&#39;{1}|{1,2,3,4}&#39;]&quot;, lambda == &quot;2&quot; ~ &quot;lambda[&#39;{2}|{2,3,4}&#39;]&quot;, lambda == &quot;3&quot; ~ &quot;lambda[&#39;{3}|{3,4}&#39;]&quot;), parameter = case_when(parameter == &quot;Intercept&quot; ~ &quot;beta[0]&quot;, parameter == &quot;X1&quot; ~ &quot;beta[1]&quot;, parameter == &quot;X2&quot; ~ &quot;beta[2]&quot;)) %&gt;% # plot ggplot(aes(x = value, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = pl[4], slab_color = pl[3], color = pl[1], point_color = pl[2], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;marginal posterior&quot;) + facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &quot;free_x&quot;) + theme(axis.text = element_text(size = 9)) # combine, entitle, and display the results (p1 + p2) &amp; plot_layout(widths = c(1, 2)) &amp; plot_annotation(title = &quot;Figure 22.7, lower half&quot;, subtitle = &quot;Results from the conditional logistic model fit to the d4 data via the sequential-ordinal approach&quot;) We ended up with our version of the lower half of Figure 22.7. As with the previous model, the sequential-ordinal approach reverses the signs for the \\(\\beta_1\\) and \\(\\beta_2\\) parameters, which isn’t a big deal as long as you keep that in mind. The larger issue is that the thresholds displayed in the left panel do a poor job differentiating among the various Y categories. The model underlying those thresholds is a bad match for the data. 22.3.3.2.3 Conditional logistic wrap-up. To wrap this section up, we walked through approaches for fitting conditional logistic models with brms. First we considered Singmann’s method for using the brms custom family functionality to define bespoke likelihood functions. Though it requires a lot of custom coding and an above-average knowledge of the inner workings of brms and Stan, the custom-family approach is very general and will possibly work for all your conditional-logistic needs. Then we considered Ben-Shachar sequential-ordinal approach. Ben-Shachar’s insight was that if we are willing to augment the nominal data with the ordered() function, modeling them with a sequential-ordinal model via family = sratio will return near equivalent results to the conditional-logistic method. Though this method is attractive in that it uses a built-in likelihood and thus avoids a lot of custom coding, it is limited in that it will only handle nominal data which are well described by the simple binary-divisions hierarchy displayed in the left panel of Figure 22.2. In closing, I would like to thank Singmann and Ben-Shachar for their time and insights. 🍻 I could not have finished this section without them. If you would like more examples of both of their methods applied to different data sets, check out the Stan forum thread called Nominal data and Kruschke’s “conditional logistic” approach. 22.4 Generalizations and variations of the models These models can be generalized to include different kinds of predictors, variants robust to outliers, and model comparison via information criteria and so forth. You can find a few more examples with softmax regression in Chapter 10 of the first edition of McElreath’s (2015) Statistical rethinking and Chapter 11 of his second edition (McElreath, 2020). See also Kurz (2021, pp. Section 11.3) and Kurz (2020, pp. Section 10.3.1) for walk-throughs with brms. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bayesplot_1.9.0 posterior_1.3.1 tidybayes_3.0.2 brms_2.18.0 Rcpp_1.0.9 patchwork_1.1.2 ## [7] cowplot_1.1.1 PNWColors_0.1.0 forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 ## [13] readr_2.1.2 tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 RcppEigen_0.3.3.9.3 plyr_1.8.7 ## [5] igraph_1.3.4 svUnit_1.0.6 splines_4.2.0 crosstalk_1.2.0 ## [9] TH.data_1.1-1 rstantools_2.2.0 inline_0.3.19 digest_0.6.30 ## [13] htmltools_0.5.3 fansi_1.0.3 BH_1.78.0-0 magrittr_2.0.3 ## [17] checkmate_2.1.0 googlesheets4_1.0.1 tzdb_0.3.0 modelr_0.1.8 ## [21] RcppParallel_5.1.5 matrixStats_0.62.0 vroom_1.5.7 xts_0.12.1 ## [25] sandwich_3.0-2 prettyunits_1.1.1 colorspace_2.0-3 rvest_1.0.2 ## [29] ggdist_3.2.0 haven_2.5.1 xfun_0.35 callr_3.7.3 ## [33] crayon_1.5.2 jsonlite_1.8.3 lme4_1.1-31 survival_3.4-0 ## [37] zoo_1.8-10 glue_1.6.2 gtable_0.3.1 gargle_1.2.0 ## [41] emmeans_1.8.0 distributional_0.3.1 pkgbuild_1.3.1 rstan_2.21.7 ## [45] abind_1.4-5 scales_1.2.1 mvtnorm_1.1-3 emo_0.0.0.9000 ## [49] DBI_1.1.3 miniUI_0.1.1.1 xtable_1.8-4 HDInterval_0.2.2 ## [53] bit_4.0.4 stats4_4.2.0 StanHeaders_2.21.0-7 DT_0.24 ## [57] htmlwidgets_1.5.4 httr_1.4.4 threejs_0.3.3 arrayhelpers_1.1-0 ## [61] ellipsis_0.3.2 pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 ## [65] sass_0.4.2 dbplyr_2.2.1 utf8_1.2.2 tidyselect_1.1.2 ## [69] labeling_0.4.2 rlang_1.0.6 reshape2_1.4.4 later_1.3.0 ## [73] munsell_0.5.0 cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [77] cli_3.5.0 generics_0.1.3 broom_1.0.1 ggridges_0.5.3 ## [81] evaluate_0.18 fastmap_1.1.0 processx_3.8.0 knitr_1.40 ## [85] bit64_4.0.5 fs_1.5.2 nlme_3.1-159 projpred_2.2.1 ## [89] mime_0.12 xml2_1.3.3 compiler_4.2.0 shinythemes_1.2.0 ## [93] rstudioapi_0.13 gamm4_0.2-6 reprex_2.0.2 bslib_0.4.0 ## [97] stringi_1.7.8 highr_0.9 ps_1.7.2 Brobdingnag_1.2-8 ## [101] lattice_0.20-45 Matrix_1.4-1 nloptr_2.0.3 markdown_1.1 ## [105] shinyjs_2.1.0 tensorA_0.36.2 vctrs_0.5.1 pillar_1.8.1 ## [109] lifecycle_1.0.3 jquerylib_0.1.4 bridgesampling_1.1-2 estimability_1.4.1 ## [113] httpuv_1.6.5 extraDistr_1.9.1 R6_2.5.1 bookdown_0.28 ## [117] promises_1.2.0.1 gridExtra_2.3 codetools_0.2-18 boot_1.3-28 ## [121] MASS_7.3-58.1 colourpicker_1.1.1 gtools_3.9.3 assertthat_0.2.1 ## [125] withr_2.5.0 shinystan_2.6.0 multcomp_1.4-20 mgcv_1.8-40 ## [129] parallel_4.2.0 hms_1.1.1 grid_4.2.0 minqa_1.2.5 ## [133] coda_0.19-4 rmarkdown_2.16 googledrive_2.0.0 shiny_1.7.2 ## [137] lubridate_1.8.0 base64enc_0.1-3 dygraphs_1.1.1.6 References Bürkner, P.-C. (2022). Define custom response distributions with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html Bürkner, P.-C. (2022c). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html Bürkner, P.-C., &amp; Vuorre, M. (2019). Ordinal regression models in psychology: A tutorial. Advances in Methods and Practices in Psychological Science, 2(1), 77–101. https://doi.org/10.1177/2515245918823199 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Kurz, A. S. (2021). Statistical rethinking with brms, ggplot2, and the tidyverse: Second Edition (version 0.2.0). https://bookdown.org/content/4857/ Kurz, A. S. (2020). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 Luce, R. D. (2012). Individual choice behavior: A theoretical analysis. Courier Corporation. https://books.google.com?id=ERQsKkPiKkkC Luce, R. D. (2008). Luce’s choice axiom. Scholarpedia, 3(12), 8077. https://doi.org/10.4249/scholarpedia.8077 McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/ "],["ordinal-predicted-variable.html", "23 Ordinal Predicted Variable 23.1 Modeling ordinal data with an underlying metric variable 23.2 The case of a single group 23.3 The case of two groups 23.4 The Case of metric predictors 23.5 Posterior prediction 23.6 Generalizations and extensions Session info", " 23 Ordinal Predicted Variable This chapter considers data that have an ordinal predicted variable. For example, we might want to predict people’s happiness ratings on a \\(1\\)-to-\\(7\\) scale as a function of their total financial assets. Or we might want to predict ratings of movies as a function of the year they were made. One traditional treatment of this sort of data structure is called ordinal or ordered probit regression. We will consider a Bayesian approach to this model. As usual, in Bayesian software, it is easy to generalize the traditional model so it is robust to outliers, allows different variances within levels of a nominal predictor, or has hierarchical structure to share information across levels or factors as appropriate. In the context of the generalized linear model (GLM) introduced in Chapter 15, this chapter’s situation involves an inverse-link function that is a thresholded cumulative normal with a categorical distribution for describing noise in the data, as indicated in the fourth row of Table 15.2 (p. 443). For a reminder of how this chapter’s combination of predicted and predictor variables relates to other combinations, see Table 15.3 (p. 444). (Kruschke, 2015, p. 671, emphasis in the original) We might follow Kruschke’s advice and rewind a little before tackling this chapter. The cumulative normal function is denoted \\(\\Phi(x; \\mu, \\sigma)\\), where \\(x\\) is some real number and \\(\\mu\\) and \\(\\sigma\\) have their typical meaning. The function is cumulative in that it produces values ranging from zero to 1. Figure 15.8 showed an example of the normal distribution atop of the cumulative normal. Here it is, again. library(tidyverse) d &lt;- tibble(z = seq(from = -3, to = 3, by = .1)) %&gt;% # add the density values mutate(`p(z)` = dnorm(z, mean = 0, sd = 1), # add the CDF values `Phi(z)` = pnorm(z, mean = 0, sd = 1)) head(d) ## # A tibble: 6 × 3 ## z `p(z)` `Phi(z)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -3 0.00443 0.00135 ## 2 -2.9 0.00595 0.00187 ## 3 -2.8 0.00792 0.00256 ## 4 -2.7 0.0104 0.00347 ## 5 -2.6 0.0136 0.00466 ## 6 -2.5 0.0175 0.00621 It’s time to talk color and theme. For this chapter, we’ll take our color palette from the scico package (Thomas Lin Pedersen &amp; Crameri, 2021), which provides 17 perceptually-uniform and colorblind-safe palettes based on the work of Fabio Crameri. Our palette of interest will be \"lajolla\". library(scico) sl &lt;- scico(palette = &quot;lajolla&quot;, n = 9) scales::show_col(sl) Our overall plot theme will be based on ggplot2::theme_linedraw() with just a few adjustments. theme_set( theme_linedraw() + theme(panel.grid = element_blank(), strip.background = element_rect(color = sl[9], fill = sl[9]), strip.text = element_text(color = sl[1])) ) Now plot! p1 &lt;- d %&gt;% ggplot(aes(x = z, y = `p(z)`)) + geom_area(aes(fill = z &lt;= 1), show.legend = F) + geom_line(size = 1, color = sl[3]) + scale_fill_manual(values = c(&quot;transparent&quot;, sl[2])) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Normal Density&quot;, y = expression(p(italic(z)))) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. p2 &lt;- d %&gt;% ggplot(aes(x = z, y = `Phi(z)`)) + geom_area(aes(fill = z &lt;= 1), show.legend = F) + geom_line(size = 1, color = sl[3]) + scale_fill_manual(values = c(&quot;transparent&quot;, sl[2])) + scale_y_continuous(expand = expansion(mult = c(0, 0)), limits = 0:1) + labs(title = &quot;Cumulative Normal&quot;, y = expression(Phi(italic(z)))) # combine and adjust with patchwork library(patchwork) p1 / p2 &amp; scale_x_continuous(breaks = -2:2) &amp; coord_cartesian(xlim = c(-2.5, 2.5)) For both plots, z is in a standardized metric (i.e., \\(z\\)-score). With the cumulative normal function, the cumulative probability \\(\\Phi(z)\\) increases nonlinearly with the \\(z\\)-scores such that, much like with the logistic curve, the greatest change occurs around \\(z = 0\\) and tapers off in the tails. The inverse of \\(\\Phi(x)\\) is the probit function. As indicated in the above block quote, we’ll be making extensive use of the probit function in this chapter for our Bayesian models. 23.1 Modeling ordinal data with an underlying metric variable You can imagine that the distribution of ordinal values might not resemble a normal distribution, even though the underlying metric values are normally distributed. Figure 23.1 shows some examples of ordinal outcome probabilities generated from an underlying normal distribution. The horizontal axis is the underlying continuous metric value. Thresholds are plotted as vertical dashed lines, labeled \\(\\theta\\). In all examples, the ordinal scale has \\(7\\) levels, and hence, there are \\(6\\) thresholds. The lowest threshold is set at \\(\\theta_1 = 1.5\\) (to separate outcomes \\(1\\) and \\(2\\)), and the highest threshold is set at \\(\\theta_1 = 6.5\\) (to separate outcomes \\(6\\) and \\(7\\)). The normal curve in each panel shows the distribution of underlying continuous values. What differs across panels are the settings of means, standard deviations, and remaining thresholds. (p. 672) The various Figure 23.1 subplots require a lot of ins and outs. We’ll start with the top panel and build from there. Here is how we might make the values necessary for the density curve. den &lt;- # define the parameters for the underlying normal distribution tibble(mu = 4, sigma = 1.5) %&gt;% mutate(strip = str_c(&quot;mu==&quot;, mu, &quot;~~sigma==&quot;, sigma)) %&gt;% # this will allow us to rescale the density in terms of the bar plot mutate(multiplier = 26 / dnorm(mu, mu, sigma)) %&gt;% # we need values for the x-axis expand(nesting(mu, sigma, strip, multiplier), y = seq(from = -1, to = 9, by = .1)) %&gt;% # compute the density values mutate(density = dnorm(y, mu, sigma)) %&gt;% # use that multiplier value from above to rescale the density values mutate(percent = density * multiplier) head(den) ## # A tibble: 6 × 7 ## mu sigma strip multiplier y density percent ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 1.5 mu==4~~sigma==1.5 97.8 -1 0.00103 0.101 ## 2 4 1.5 mu==4~~sigma==1.5 97.8 -0.9 0.00128 0.125 ## 3 4 1.5 mu==4~~sigma==1.5 97.8 -0.8 0.00159 0.155 ## 4 4 1.5 mu==4~~sigma==1.5 97.8 -0.7 0.00196 0.192 ## 5 4 1.5 mu==4~~sigma==1.5 97.8 -0.6 0.00241 0.236 ## 6 4 1.5 mu==4~~sigma==1.5 97.8 -0.5 0.00295 0.289 Before making the data for the bar portion of the plot, we’ll need to define the \\(\\theta\\)-values they’ll be placed between. We also need to define the exact points on the \\(x\\)-axis from which we’d like those bars to originate. Those points, which we’ll call label_1, will double as names for the individual bars. (theta_1 &lt;- seq(from = 1.5, to = 6.5, by = 1)) ## [1] 1.5 2.5 3.5 4.5 5.5 6.5 (label_1 &lt;- 1:7) ## [1] 1 2 3 4 5 6 7 Now we can define the data for the bars. bar &lt;- # define the parameters for the underlying normal distribution tibble(mu = 4, sigma = 1.5) %&gt;% mutate(strip = str_c(&quot;mu==&quot;, mu, &quot;~~sigma==&quot;, sigma)) %&gt;% # take random draws from the underlying normal distribution mutate(draw = map2(mu, sigma, ~rnorm(1e4, mean = .x, sd = .y))) %&gt;% unnest(draw) %&gt;% # bin those draws into ordinal categories defined by `theta_1` # and named by `label_1` mutate(y = case_when( draw &lt; theta_1[1] ~ label_1[1], draw &lt; theta_1[2] ~ label_1[2], draw &lt; theta_1[3] ~ label_1[3], draw &lt; theta_1[4] ~ label_1[4], draw &lt; theta_1[5] ~ label_1[5], draw &lt; theta_1[6] ~ label_1[6], draw &gt;= theta_1[6] ~ label_1[7] )) %&gt;% # summarize count(y) %&gt;% mutate(percent = (100 * n / sum(n)) %&gt;% round(0)) %&gt;% mutate(percent_label = str_c(percent, &quot;%&quot;), percent_max = max(percent)) head(bar) ## # A tibble: 6 × 5 ## y n percent percent_label percent_max ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 453 5 5% 26 ## 2 2 1162 12 12% 26 ## 3 3 2077 21 21% 26 ## 4 4 2608 26 26% 26 ## 5 5 2081 21 21% 26 ## 6 6 1140 11 11% 26 Make the top subplot. bar %&gt;% ggplot(aes(x = y, y = percent)) + geom_area(data = den, fill = sl[3]) + geom_vline(xintercept = theta_1, color = &quot;white&quot;, linetype = 3) + geom_col(width = .5, alpha = .85, fill = sl[7]) + geom_text(aes(y = percent + 2, label = percent_label), size = 3.5) + annotate(geom = &quot;text&quot;, x = theta_1, y = -5.55, label = theta_1, size = 3) + scale_x_continuous(NULL, expand = c(0, 0), breaks = theta_1, labels = parse(text = str_c(&quot;theta[&quot;, 1:6, &quot;]&quot;))) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.06))) + coord_cartesian(ylim = c(0, 28.5), clip = F) + theme(plot.margin = margin(5.5, 5.5, 11, 5.5)) + facet_wrap(~ strip, labeller = label_parsed) This method works okay for plotting one or two panels. The sheer number of code lines and moving parts seem unwieldy for plotting four. It’d be convenient if we could save the density information for all four panels in one data object. Here’s one way how. den &lt;- tibble(panel = 1:4, mu = c(4, 1, 4, 4), sigma = c(1.5, 2.5, 1, 3)) %&gt;% mutate(strip = factor(panel, labels = str_c(&quot;mu==&quot;, mu, &quot;~~sigma==&quot;, sigma), ordered = T)) %&gt;% mutate(multiplier = c(26, 58, 24, 26) / dnorm(mu, mu, sigma)) %&gt;% expand(nesting(panel, mu, sigma, strip, multiplier), y = seq(from = -1, to = 9, by = .1)) %&gt;% mutate(density = dnorm(y, mu, sigma)) %&gt;% mutate(percent = density * multiplier) head(den) ## # A tibble: 6 × 8 ## panel mu sigma strip multiplier y density percent ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 1.5 mu==4~~sigma==1.5 97.8 -1 0.00103 0.101 ## 2 1 4 1.5 mu==4~~sigma==1.5 97.8 -0.9 0.00128 0.125 ## 3 1 4 1.5 mu==4~~sigma==1.5 97.8 -0.8 0.00159 0.155 ## 4 1 4 1.5 mu==4~~sigma==1.5 97.8 -0.7 0.00196 0.192 ## 5 1 4 1.5 mu==4~~sigma==1.5 97.8 -0.6 0.00241 0.236 ## 6 1 4 1.5 mu==4~~sigma==1.5 97.8 -0.5 0.00295 0.289 Notice we added a panel column for indexing the subplots. Next we’ll need to define theta_[i] and label_[i] values for the remaining plots. theta_3 &lt;- c(1.5, 3.1, 3.7, 4.3, 4.9, 6.5) theta_4 &lt;- c(1.5, 2.25, 3, 5, 5.75, 6.5) label_3 &lt;- c(1, 2.2, 3.4, 4, 4.6, 5.7, 7) label_4 &lt;- c(1, 1.875, 2.625, 4, 5.375, 6.125, 7) Since the values are the same for the top two panels, we didn’t bother defining a theta_2 or label_2. Now we have all the theta_[i] and label_[i] values, we’ll want to make a function that can use them within case_when() for any of the four panels. Here’s one way to make such a function, which we’ll call make_ordinal(). make_ordinal &lt;- function(x, panel) { if (panel &lt; 3) { case_when( x &lt; theta_1[1] ~ label_1[1], x &lt; theta_1[2] ~ label_1[2], x &lt; theta_1[3] ~ label_1[3], x &lt; theta_1[4] ~ label_1[4], x &lt; theta_1[5] ~ label_1[5], x &lt; theta_1[6] ~ label_1[6], x &gt;= theta_1[6] ~ label_1[7] ) } else if (panel == 3) { case_when( x &lt; theta_3[1] ~ label_3[1], x &lt; theta_3[2] ~ label_3[2], x &lt; theta_3[3] ~ label_3[3], x &lt; theta_3[4] ~ label_3[4], x &lt; theta_3[5] ~ label_3[5], x &lt; theta_3[6] ~ label_3[6], x &gt;= theta_3[6] ~ label_3[7] ) } else { case_when( x &lt; theta_4[1] ~ label_4[1], x &lt; theta_4[2] ~ label_4[2], x &lt; theta_4[3] ~ label_4[3], x &lt; theta_4[4] ~ label_4[4], x &lt; theta_4[5] ~ label_4[5], x &lt; theta_4[6] ~ label_4[6], x &gt;= theta_4[6] ~ label_4[7] ) } } Now put those values and our make_ordinal() function to work to make the data for the bar plots. set.seed(23) bar &lt;- tibble(panel = 1:4, mu = c(4, 1, 4, 4), sigma = c(1.5, 2.5, 1, 3)) %&gt;% mutate(strip = factor(panel, labels = str_c(&quot;mu==&quot;, mu, &quot;~~sigma==&quot;, sigma), ordered = T)) %&gt;% mutate(draw = map2(mu, sigma, ~rnorm(1e5, mean = .x, sd = .y))) %&gt;% unnest(draw) %&gt;% mutate(y = map2_dbl(draw, panel, make_ordinal)) %&gt;% group_by(panel, strip) %&gt;% count(y) %&gt;% mutate(percent = (100 * n / sum(n)) %&gt;% round(0)) %&gt;% mutate(percent_label = str_c(percent, &quot;%&quot;), percent_max = max(percent)) head(bar) ## # A tibble: 6 × 7 ## # Groups: panel, strip [1] ## panel strip y n percent percent_label percent_max ## &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 mu==4~~sigma==1.5 1 4763 5 5% 26 ## 2 1 mu==4~~sigma==1.5 2 10844 11 11% 26 ## 3 1 mu==4~~sigma==1.5 3 21174 21 21% 26 ## 4 1 mu==4~~sigma==1.5 4 26256 26 26% 26 ## 5 1 mu==4~~sigma==1.5 5 21233 21 21% 26 ## 6 1 mu==4~~sigma==1.5 6 10951 11 11% 26 Like before, we added a panel index. As our final preparatory step, we will make something of a super function with which we’ll plug the desired information into ggplot2, which will then make each subplot. Much of the plotting and data wrangling code will be the same across subplots. As far as I can tell, we only need to vary four parameters. First, we’ll want to be able to subset the data by panel index. We’ll do that with the panel_n argument. Second, we’ll want to select which of the theta_[i] values we’d like to use in geom_vline(), annotate(), and scale_x_continuous(). We’ll do that with the theta argument. We’ll make a y_second_x to pin down exactly where below the \\(x\\)-axis we’d like to put those secondary axis values defined by the theta_[i] values. Finally, we’ll want an ylim_ub parameter to set the upper limit of the \\(y\\)-axis with. The name of our four-parameter super function will be plot_bar_den(). plot_bar_den &lt;- function(panel_n, theta, y_second_x, ylim_ub) { bar %&gt;% filter(panel == panel_n) %&gt;% ggplot(aes(x = y)) + geom_area(data = den %&gt;% filter(panel == panel_n), aes(y = percent), fill = sl[3]) + geom_vline(xintercept = theta, color = &quot;white&quot;, linetype = 3) + geom_linerange(aes(ymin = 0, ymax = percent), color = sl[7], alpha = .85, size = 8) + geom_text(aes(y = percent + (percent_max / 15), label = percent_label), size = 3.5) + annotate(geom = &quot;text&quot;, x = theta, y = y_second_x, label = theta, size = 3) + scale_x_continuous(NULL, breaks = theta, labels = parse(text = str_c(&quot;theta[&quot;, 1:6, &quot;]&quot;)), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + coord_cartesian(ylim = c(0, ylim_ub), clip = F) + theme(plot.margin = margin(5.5, 5.5, 11, 5.5)) + facet_wrap(~ strip, labeller = label_parsed) } Finally, make all four subplots and combine them with patchwork syntax! p1 &lt;- plot_bar_den(panel_n = 1, theta = theta_1, # y_second_x = -6.75, y_second_x = -5.55, ylim_ub = 28) p2 &lt;- plot_bar_den(panel_n = 2, theta = theta_1, # y_second_x = -15.5 * 3/4, y_second_x = -12.37, ylim_ub = 63) p3 &lt;- plot_bar_den(panel_n = 3, theta = theta_3, # y_second_x = -6.25 * 3/4, y_second_x = -5.12, ylim_ub = 25.75) p4 &lt;- plot_bar_den(panel_n = 4, theta = theta_4, # y_second_x = -6.75 * 3/4, y_second_x = -5.55, ylim_ub = 28) p1 / p2 / p3 / p4 Oh mamma. “The crucial concept in Figure 23.1 is that the probability of a particular ordinal outcome is the area under the normal curve between the thresholds of that outcome” (p. 672, emphasis in the original). In each of the subplots, we used six thresholds to discretize the continuous data into seven categories. More generally, we need \\(K\\) thresholds to make \\(K + 1\\) ordinal categories. To make this work, the idea is that we consider the cumulative area under the normal up the high-side threshold, and subtract away the cumulative area under the normal up to the low-side threshold. Recall that the cumulative area under the standardized normal is denoted \\(\\Phi(z)\\), as was illustrated in Figure 15.8 [which we remade at the top of this chapter]. Thus, the area under the normal to the left of \\(\\theta_k\\) is \\(\\Phi((\\theta_k - \\mu) / \\sigma)\\), and the area under the normal to the left of \\(\\theta_{k - 1}\\) is \\(\\Phi((\\theta_{k - 1} - \\mu) / \\sigma)\\). Therefore, the area under the normal curve between the two thresholds, which is the probability of outcome \\(k\\), is \\[p(y = k | \\mu, \\sigma, \\{ \\theta_j \\}) = \\Phi((\\theta_k - \\mu) / \\sigma) - \\Phi((\\theta_{k - 1} - \\mu) / \\sigma)\\] [This equation] applies even to the least and greatest ordinal values if we append two “virtual” thresholds at \\(- \\infty\\) and \\(+ \\infty\\)… Thus, a normally distributed underlying metric value can yield a clearly non-normal distribution of discrete ordinal values. This result does not imply that the ordinal values can be treated as if they were themselves metric and normally distributed; in fact it implies the opposite: We might be able to model a distribution of ordinal values as consecutive intervals of a normal distribution on an underlying metric scale with appropriately positioned thresholds. (pp. 674–675) 23.2 The case of a single group Given a model with no predictors, “if there are \\(K\\) ordinal values, the model has \\(K + 1\\) parameters: \\(\\theta_1, \\dots,\\theta_{K - 1}, \\mu\\), and \\(\\sigma\\). If you think about it a moment, you’ll realize that the parameter values trade-off and are undetermined” (p. 675). The solution Kruschke took throughout this chapter was to fix the two thresholds at the ends, \\(\\theta_1\\) and \\(\\theta_{K - 1}\\), to the constants \\[\\begin{align*} \\theta_1 \\equiv 1 + 0.5 &amp;&amp; \\text{and} &amp;&amp; \\theta_{K - 1} \\equiv K - 0.5. \\end{align*}\\] For example, all four subplots from Figure 23.1 had \\(K = 7\\) categories, ranging from 1 to 7. Following Kruschke’s convention would mean setting the endmost thresholds to \\[\\begin{align*} \\theta_1 \\equiv 1.5 &amp;&amp; \\text{and} &amp;&amp; \\theta_6 \\equiv 6.5. \\end{align*}\\] As we’ll see, there are other ways to parameterize these models. 23.2.1 Implementation in JAGS brms. The syntax to fit a basic ordered probit model with brms::brm() is pretty simple. fit &lt;- brm(data = my_data, family = cumulative(probit), y ~ 1, prior(normal(0, 4), class = Intercept)) The family = cumulative(probit) tells brms you’d like to use the probit link for the ordered-categorical data. It’s important to specify probit because the brms default is to use the logit link, instead. We’ll talk more about that approach at the end of this chapter. Remember how, at the end of the last section, we said there are other ways to parameterize the ordered probit model? As it turns out, brms does not follow Kruschke’s approach for fixing the thresholds on the ends. Rather, brms freely estimates all thresholds, \\(\\theta_1,...,\\theta_{K - 1}\\), by fixing \\(\\mu = 0\\) and \\(\\sigma = 1\\). That is, instead of estimating \\(\\mu\\) and \\(\\sigma\\) from the normal cumulative density function \\(\\Phi(x)\\), brms::brm() uses the standard normal cumulative density function \\(\\Phi(z)\\). This all probably seems abstract. We’ll get a lot of practice comparing the two approaches as we go along. Each has its strengths and weaknesses. At this point, the thing to get is that when fitting a single-group ordered-probit model with the brm() function, there will be no priors for \\(\\mu\\) and \\(\\sigma\\). We only have to worry about setting the priors for all \\(K - 1\\) thresholds. And because those thresholds are conditional on \\(\\Phi(z)\\), we should think about their priors with respect to the scale of standard normal distribution. Thus, to continue on with Kruschke’s minimally-informative prior approach, something like prior(normal(0, 4), class = Intercept) might be a good starting place. Do feel free to experiment with different settings. 23.2.2 Examples: Bayesian estimation recovers true parameter values. The data for Kruschke’s first example come from his OrdinalProbitData-1grp-1.csv file. Load the data. my_data_1 &lt;- read_csv(&quot;data.R/OrdinalProbitData-1grp-1.csv&quot;) glimpse(my_data_1) ## Rows: 100 ## Columns: 1 ## $ Y &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… Plot the distribution for Y. my_data_1 %&gt;% mutate(Y = factor(Y)) %&gt;% ggplot(aes(x = Y)) + geom_bar(fill = sl[5]) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) It looks a lot like the distribution of the data from one of the panels from Figure 23.1. Load brms. library(brms) Fit the first cumulative-probit model. fit23.1 &lt;- brm(data = my_data_1, family = cumulative(probit), Y ~ 1, prior(normal(0, 4), class = Intercept), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.01&quot;) Examine the model summary. print(fit23.1) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Y ~ 1 ## Data: my_data_1 (Number of observations: 100) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] 0.18 0.12 -0.06 0.42 1.00 9787 6646 ## Intercept[2] 0.60 0.13 0.35 0.87 1.00 11397 6343 ## Intercept[3] 1.04 0.15 0.75 1.34 1.00 11713 6088 ## Intercept[4] 1.50 0.19 1.14 1.88 1.00 12416 6754 ## Intercept[5] 1.97 0.25 1.51 2.50 1.00 12059 6627 ## Intercept[6] 2.57 0.40 1.90 3.44 1.00 11463 6870 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 NA NA NA ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The brms output for these kinds of models names the thresholds \\(\\theta_{[i]}\\) as Intercept[i]. Again, whereas Kruschke identified his model by fixing \\(\\theta_1 = 1.5\\) (i.e., \\(1 + 0.5\\)) and \\(\\theta_6 = 5.5\\) (i.e., \\(6 - 0.5\\)), we freely estimated all six thresholds by using the cumulative density function for the standard normal. As a result, our thresholds are in a different metric from Kruschke’s. Let’s extract the posterior draws. draws &lt;- as_draws_df(fit23.1) glimpse(draws) ## Rows: 8,000 ## Columns: 12 ## $ `b_Intercept[1]` &lt;dbl&gt; 0.44703738, 0.22443431, 0.30708260, -0.01127637, -0.18681541, 0.37826981, 0.0733989… ## $ `b_Intercept[2]` &lt;dbl&gt; 0.8253956, 0.5598467, 0.6366477, 0.3522155, 0.3877171, 0.7765147, 0.6104502, 0.5962… ## $ `b_Intercept[3]` &lt;dbl&gt; 1.0714275, 1.2367075, 0.9268432, 0.9791826, 1.0762356, 1.1229631, 0.8663489, 1.0003… ## $ `b_Intercept[4]` &lt;dbl&gt; 1.675261, 1.479992, 1.508809, 1.611100, 1.629725, 1.572215, 1.466856, 1.368460, 1.8… ## $ `b_Intercept[5]` &lt;dbl&gt; 2.439194, 1.817869, 2.224741, 2.034097, 1.955867, 1.749030, 2.591565, 1.605434, 2.2… ## $ `b_Intercept[6]` &lt;dbl&gt; 3.195665, 2.412770, 2.777146, 2.669718, 3.094114, 2.040137, 3.561389, 2.170847, 2.5… ## $ disc &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ lprior &lt;dbl&gt; -14.48757, -14.24420, -14.34068, -14.29838, -14.37510, -14.19703, -14.54015, -14.16… ## $ lp__ &lt;dbl&gt; -154.6614, -152.9830, -151.9284, -151.9778, -155.8266, -153.3221, -155.7108, -150.8… ## $ .chain &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ .iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, … ## $ .draw &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, … Wrangle the draws a bit. draws &lt;- draws %&gt;% select(.draw, `b_Intercept[1]`:`b_Intercept[6]`) Here’s our brms version of the bottom plot of Figure 23.2. # compute the posterior means for each threshold means &lt;- draws %&gt;% summarise_at(vars(`b_Intercept[1]`:`b_Intercept[6]`), mean) %&gt;% pivot_longer(everything(), values_to = &quot;mean&quot;) # wrangle draws %&gt;% pivot_longer(-.draw, values_to = &quot;threshold&quot;) %&gt;% group_by(.draw) %&gt;% mutate(theta_bar = mean(threshold)) %&gt;% # finally we plot ggplot(aes(x = threshold, y = theta_bar, color = name)) + geom_vline(data = means, aes(xintercept = mean, color = name), linetype = 2) + geom_point(alpha = 1/10) + scale_color_scico_d(palette = &quot;lajolla&quot;, begin = .25) + ylab(&quot;mean threshold&quot;) + theme(legend.position = &quot;none&quot;) The initial means data at the top contains the \\(\\theta_i\\)-specific means, which we used to make the dashed vertical lines with geom_vline(). Did you see what we did there with those group_by() and mutate() lines? That’s how we computed the mean threshold within each step of the HMC chain, what Kruschke (p. 680) denoted as \\(\\bar \\theta (s) = \\sum_k^{K-1} \\theta_k (s) / (K - 1)\\), where \\(s\\) refers to particular steps in the HMC chain. Perhaps of greater interest, you might have noticed how different our plot is from the one in the text. We might should compare the results of our brms parameterization of \\(\\theta_{[i]}\\) with one based on the parameterization in the text in an expanded version of the bottom plot of Figure 23.2. To convert our brms output to match Kruschke’s, we’ll rescale our \\(\\theta_{[i]}\\) draws with help from the scales::rescale() function, about which you might learn more here. # primary data wrangling p &lt;- bind_rows( # brms parameterization draws %&gt;% pivot_longer(-.draw, values_to = &quot;threshold&quot;) %&gt;% group_by(.draw) %&gt;% mutate(theta_bar = mean(threshold)), # Kruschke&#39;s parameterization draws %&gt;% pivot_longer(-.draw, values_to = &quot;threshold&quot;) %&gt;% group_by(.draw) %&gt;% mutate(threshold = scales::rescale(threshold, to = c(1.5, 6.5))) %&gt;% mutate(theta_bar = mean(threshold)) ) %&gt;% # add an index mutate(model = rep(c(&quot;brms parameterization&quot;, &quot;Kruschke&#39;s parameterization&quot;), each = n() / 2)) # compute the means by model and threshold for the vertical lines means &lt;- p %&gt;% ungroup() %&gt;% group_by(model, name) %&gt;% summarise(mean = mean(threshold)) # plot! p %&gt;% ggplot(aes(x = threshold, y = theta_bar)) + geom_vline(data = means, aes(xintercept = mean, color = name), linetype = 2) + geom_point(aes(color = name), alpha = 1/10, size = 1/2) + scale_color_scico_d(palette = &quot;lajolla&quot;, begin = .25) + ylab(&quot;mean threshold&quot;) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ model, ncol = 1, scales = &quot;free&quot;) We can take our rescaling approach further to convert the posterior distributions for \\(\\mu\\) and \\(\\sigma\\) from the brms \\(\\operatorname{Normal}(0, 1)\\) constants to the metric from Kruschke’s approach. Say \\(y_1\\) and \\(y_2\\) are two draws from some Gaussian and \\(z_1\\) and \\(z_2\\) are their corresponding \\(z\\)-scores. Here’s how to solve for \\(\\sigma\\). \\[\\begin{align*} z_1 - z_2 &amp; = \\frac{(y_1 - \\mu)}{\\sigma} - \\frac{(y_2 - \\mu)}{\\sigma} \\\\ &amp; = \\frac{(y_1 - \\mu) - (y_2 - \\mu)}{\\sigma} \\\\ &amp; = \\frac{y_1 - \\mu - y_2 + \\mu}{\\sigma} \\\\ &amp; = \\frac{y_1 - y_2}{\\sigma}, \\;\\; \\text{therefore} \\\\ \\sigma &amp; = \\frac{y_1 - y_2}{z_1 – z_2}. \\end{align*}\\] If you’d like to compute \\(\\mu\\), it’s even simpler. \\[\\begin{align*} z_1 &amp; = \\frac{y_1 - \\mu}{\\sigma} \\\\ z_1 \\sigma &amp; = y_1 - \\mu \\\\ z_1 \\sigma + \\mu &amp; = y_1, \\;\\; \\text{therefore} \\\\ \\mu &amp; = y_1 - z_1 \\sigma \\end{align*}\\] Big shout out to my math-stats savvy friends academic twitter for the formulas, especially Ph.Demetri, Lukas Neugebauer, and Brenton Wiernik for walking out the formulas (see this twitter thread). For our application, Intercept[1] and Intercept[6] will be our two \\(z\\)-scores and Kruschke’s 1.5 and 6.5 will be their corresponding \\(y\\)-values. library(tidybayes) draws %&gt;% select(.draw, `b_Intercept[1]`, `b_Intercept[6]`) %&gt;% mutate(`y[1]` = 1.5, `y[6]` = 6.5) %&gt;% mutate(mu = `y[1]` - `b_Intercept[1]` * 1, sigma = (`y[1]` - `y[6]`) / (`b_Intercept[1]` - `b_Intercept[6]`)) %&gt;% mutate(`(mu-2)/sigma` = (mu - 2) / sigma) %&gt;% pivot_longer(mu:`(mu-2)/sigma`) %&gt;% mutate(name = factor(name, levels = c(&quot;mu&quot;, &quot;sigma&quot;, &quot;(mu-2)/sigma&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ name, scales = &quot;free&quot;, labeller = label_parsed) Our results are similar to Kruschke’s. Given we used a different algorithm, a different parameterization, and different priors, I’m not terribly surprised they’re a little different. If you have more insight on the matter or have spotted a flaw in this method, please share with the rest of us. It’s unclear, to me, how we’d interpret the effect size. The difficulty isn’t that Kruschke’s comparison of \\(C = 2.0\\) is arbitrary, but that we can only interpret the comparison given the model assumption of \\(\\theta_1 = 1.5\\) and \\(\\theta_6 = 6.5\\). If your theory doesn’t allow you to understand the meaning of those constants and why you’d prefer them to slightly different ones, you’d be fooling yourself if you attempted to interpret any effect sizes conditional on those values. Proceed with caution, friends. In the large paragraph on the lower part of page 679, Kruschke discussed why the thresholds tend to have nontrivial covariances. This is what he was trying to convey with the bottom subplot in Figure 23.2. Just for practice, we might also explore the correlation matrix among the thresholds with a customized GGally::ggpairs() plot. # load library(GGally) # customize my_upper &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_point(size = 1/5, alpha = 1/5, color = sl[7]) } my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_density(size = 0, fill = sl[3]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) } my_lower &lt;- function(data, mapping, ...) { # get the x and y data to use the other code x &lt;- eval_data_col(data, mapping$x) y &lt;- eval_data_col(data, mapping$y) # compute the correlations corr &lt;- cor(x, y, method = &quot;p&quot;, use = &quot;pairwise&quot;) # plot the cor value ggally_text( label = formatC(corr, digits = 2, format = &quot;f&quot;) %&gt;% str_replace(., &quot;0\\\\.&quot;, &quot;.&quot;), mapping = aes(), color = &quot;black&quot;, size = 4) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) } # wrangle as_draws_df(fit23.1) %&gt;% select(contains(&quot;Intercept&quot;)) %&gt;% set_names(str_c(&quot;theta[&quot;, 1:6, &quot;]&quot;)) %&gt;% # plot! ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = label_parsed) + ggtitle(&quot;The thresholds are highly correlated&quot;) Kruschke didn’t do this in the text, but it might be informative to plot the probability distributions for the seven categories from Y, \\(p(y = k | \\mu = 0, \\sigma = 1, \\{ \\theta_i \\})\\). library(tidybayes) draws %&gt;% select(-.draw) %&gt;% mutate_all(.funs = ~pnorm(. ,0, 1)) %&gt;% transmute(`p[Y==1]` = `b_Intercept[1]`, `p[Y==2]` = `b_Intercept[2]` - `b_Intercept[1]`, `p[Y==3]` = `b_Intercept[3]` - `b_Intercept[2]`, `p[Y==4]` = `b_Intercept[4]` - `b_Intercept[3]`, `p[Y==5]` = `b_Intercept[5]` - `b_Intercept[4]`, `p[Y==6]` = `b_Intercept[6]` - `b_Intercept[5]`, `p[Y==7]` = 1 - `b_Intercept[6]`) %&gt;% set_names(1:7) %&gt;% pivot_longer(everything(), names_to = &quot;Y&quot;) %&gt;% ggplot(aes(x = value, y = Y)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], size = 1/2, height = 2.5) + scale_x_continuous(expression(italic(p)*&quot;[&quot;*Y==italic(i)*&quot;]&quot;), breaks = 0:5 / 5, expand = c(0, 0), limits = c(0, 1)) Happily, the model produces data that look a lot like those from which it was generated. set.seed(23) draws %&gt;% mutate(z = rnorm(n(), mean = 0, sd = 1)) %&gt;% mutate(Y = case_when( z &lt; `b_Intercept[1]` ~ 1, z &lt; `b_Intercept[2]` ~ 2, z &lt; `b_Intercept[3]` ~ 3, z &lt; `b_Intercept[4]` ~ 4, z &lt; `b_Intercept[5]` ~ 5, z &lt; `b_Intercept[6]` ~ 6, z &gt;= `b_Intercept[6]` ~ 7 ) %&gt;% as.factor(.)) %&gt;% ggplot(aes(x = Y)) + geom_bar(fill = sl[5]) + scale_y_continuous(expand = expansion(mult = c(0, 0.05))) Along similar lines, we can use the pp_check() function to make a version of the upper right panel of Figure 23.2. The type = \"bars\" argument will allow us to summarize the posterior predictions as a dot (mean) and standard error bars superimposed on a bar plot of the original data. Note how this differs a little from Kruschke’s use of the posterior median and 95% HDIs. The ndraws = 1000 argument controls how many posterior predictions we wanted to summarize over. The rest is just formatting. bayesplot::color_scheme_set(sl[2:7]) set.seed(23) pp_check(fit23.1, type = &quot;bars&quot;, ndraws = 1000, fatten = 2) + scale_x_continuous(&quot;y&quot;, breaks = 1:7) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Data with posterior predictions&quot;, subtitle = &quot;N = 100&quot;) + theme(legend.background = element_blank(), legend.position = c(.9, .8)) Load the data for the next model. my_data_2 &lt;- read_csv(&quot;data.R/OrdinalProbitData-1grp-2.csv&quot;) Since we’re reusing all the specifications from the last model for this one, we can just use update(). fit23.2 &lt;- update(fit23.1, newdata = my_data_2, iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.02&quot;) print(fit23.2) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Y ~ 1 ## Data: my_data_2 (Number of observations: 70) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -1.41 0.21 -1.83 -1.01 1.00 4879 4982 ## Intercept[2] -0.17 0.15 -0.47 0.12 1.00 9111 6632 ## Intercept[3] 0.17 0.15 -0.11 0.47 1.00 9222 7109 ## Intercept[4] 0.46 0.15 0.15 0.76 1.00 8706 6590 ## Intercept[5] 0.83 0.17 0.51 1.17 1.00 9277 6513 ## Intercept[6] 2.00 0.31 1.46 2.67 1.00 8063 6649 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 NA NA NA ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract and wrangle the posterior draws. draws &lt;- as_draws_df(fit23.2) %&gt;% select(.draw, `b_Intercept[1]`:`b_Intercept[6]`) Now we might compare the brms parameterization of \\(\\theta_{[i]}\\) with Kruschke’s parameterization in an expanded version of the bottom plot of Figure 23.3. As we’ll be making a lot of these plots throughout this chapter, it might be worthwhile to just make a custom function. We’ll call it compare_thresholds(). compare_thresholds &lt;- function(data, lb = 1.5, ub = 6.5) { # we have two parameters: # lb = lower bound # ub = upper bound # primary data wrangling p &lt;- bind_rows( data %&gt;% pivot_longer(-.draw, values_to = &quot;threshold&quot;) %&gt;% group_by(.draw) %&gt;% mutate(theta_bar = mean(threshold)), data %&gt;% pivot_longer(-.draw, values_to = &quot;threshold&quot;) %&gt;% group_by(.draw) %&gt;% mutate(threshold = scales::rescale(threshold, to = c(lb, ub))) %&gt;% mutate(theta_bar = mean(threshold)) ) %&gt;% mutate(model = rep(c(&quot;brms parameterization&quot;, &quot;Kruschke&#39;s parameterization&quot;), each = n() / 2)) # compute the means by model and threshold for the vertical lines means &lt;- p %&gt;% ungroup() %&gt;% group_by(model, name) %&gt;% summarise(mean = mean(threshold)) # plot! p %&gt;% ggplot(aes(x = threshold, y = theta_bar)) + geom_vline(data = means, aes(xintercept = mean, color = name), linetype = 2) + geom_point(aes(color = name), alpha = 1/10, size = 1/2) + scale_color_scico_d(palette = &quot;lajolla&quot;, begin = .25) + ylab(&quot;mean threshold&quot;) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ model, ncol = 1, scales = &quot;free&quot;) } Take that puppy for a spin. draws %&gt;% compare_thresholds(lb = 1.5, ub = 6.5) Oh man, that works sweet. Now let’s use the same parameter-transformation approach from before to get our un-standardized posteriors for \\(\\mu\\), \\(\\sigma\\), and the effect size. draws %&gt;% select(.draw, `b_Intercept[1]`, `b_Intercept[6]`) %&gt;% mutate(`y[1]` = 1.5, `y[6]` = 6.5) %&gt;% mutate(mu = `y[1]` - `b_Intercept[1]` * 1, sigma = (`y[1]` - `y[6]`) / (`b_Intercept[1]` - `b_Intercept[6]`)) %&gt;% mutate(`(mu-4)/sigma` = (mu - 4) / sigma) %&gt;% pivot_longer(mu:`(mu-4)/sigma`) %&gt;% mutate(name = factor(name, levels = c(&quot;mu&quot;, &quot;sigma&quot;, &quot;(mu-4)/sigma&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ name, scales = &quot;free&quot;, labeller = label_parsed) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Use pp_check() to make our version of the upper-right panel of Figure 23.3. set.seed(23) pp_check(fit23.2, type = &quot;bars&quot;, ndraws = 1000, fatten = 2) + scale_x_continuous(&quot;y&quot;, breaks = 1:7) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Data with posterior predictions&quot;, subtitle = &quot;N = 70&quot;) + theme(legend.background = element_blank(), legend.position = c(.9, .8)) Just as in the text, “the posterior predictive distribution in the top-right subpanel accurately describes the bimodal distribution of the outcomes” (p. 680). Here are the probability distributions for each of the 7 categories of Y. draws %&gt;% select(-.draw) %&gt;% mutate_all(.funs = ~pnorm(. ,0, 1)) %&gt;% transmute(`p[Y==1]` = `b_Intercept[1]`, `p[Y==2]` = `b_Intercept[2]` - `b_Intercept[1]`, `p[Y==3]` = `b_Intercept[3]` - `b_Intercept[2]`, `p[Y==4]` = `b_Intercept[4]` - `b_Intercept[3]`, `p[Y==5]` = `b_Intercept[5]` - `b_Intercept[4]`, `p[Y==6]` = `b_Intercept[6]` - `b_Intercept[5]`, `p[Y==7]` = 1 - `b_Intercept[6]`) %&gt;% set_names(1:7) %&gt;% pivot_longer(everything(), names_to = &quot;Y&quot;) %&gt;% ggplot(aes(x = value, y = Y)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], size = 1/2) + scale_x_continuous(expression(italic(p)*&quot;[&quot;*Y==italic(i)*&quot;]&quot;), breaks = 0:5 / 5, expand = c(0, 0), limits = c(0, 1)) Before we move on, it might be helpful to nail down what the thresholds mean within the context of our brms parameterization. To keep things simple, we’ll focus on their posterior means. tibble(x = seq(from = -3.5, to = 3.5, by = .01)) %&gt;% mutate(d = dnorm(x)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = sl[3]) + geom_vline(xintercept = fixef(fit23.2)[, 1], color = &quot;white&quot;, linetype = 3) + scale_x_continuous(NULL, breaks = fixef(fit23.2)[, 1], labels = parse(text = str_c(&quot;theta[&quot;, 1:6, &quot;]&quot;))) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Standard normal distribution underlying the ordinal Y data:&quot;, subtitle = &quot;The dashed vertical lines mark the posterior means for the thresholds.&quot;) + coord_cartesian(xlim = c(-3, 3)) Compare that to Figure 23.1. 23.2.2.1 Not the same results as pretending the data are metric. “In some conventional approaches to ordinal data, the data are treated as if they were metric and normally distributed” (p. 681). Here’s what that brms::brm() model might look like using methods from back in Chapter 16. First, we’ll define our stanvars. mean_y &lt;- mean(my_data_1$Y) sd_y &lt;- sd(my_data_1$Y) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) Fit the model. fit23.3 &lt;- brm(data = my_data_1, family = gaussian, Y ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.03&quot;) Check the results. print(fit23.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Y ~ 1 ## Data: my_data_1 (Number of observations: 100) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.95 0.14 1.68 2.23 1.00 3454 2723 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.41 0.10 1.23 1.62 1.00 3535 2700 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As Kruschke indicated in the text, it yielded a distributional mean of about 1.95 and a standard deviation of about 1.41. Here we’ll use a posterior predictive check to compare histograms of data generated from this model to that of the original data. pp_check(fit23.3, type = &quot;hist&quot;, ndraws = 10, binwidth = 1) + scale_x_continuous(breaks = seq(from = -3, to = 7, by = 2)) + theme(legend.position = c(.9, .15)) Yeah, that’s not a good fit. We won’t be conducting a \\(t\\)-test like Kruschke did on page 681. But we might compromise and take a look at the marginal distribution of the intercept (i.e., for \\(\\mu\\)) and its difference from 2, the reference value. as_draws_df(fit23.3) %&gt;% mutate(`2 - b_Intercept` = 2 - b_Intercept, `effect size` = (2 - b_Intercept) / sigma) %&gt;% pivot_longer(`2 - b_Intercept`:`effect size`) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ name, scales = &quot;free&quot;) Yes indeed, 2 is a credible value for the intercept. And as reported in the text, we got a very small \\(d\\) effect size. Now we repeat the process for the second data set. mean_y &lt;- mean(my_data_2$Y) sd_y &lt;- sd(my_data_2$Y) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) fit23.4 &lt;- update(fit23.3, newdata = my_data_2, chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.04&quot;) ## The desired updates require recompiling the model Let’s just jump to the plot. This time we’re comparing the b_Intercept to the value of 4.0. as_draws_df(fit23.4) %&gt;% mutate(`2 - b_Intercept` = 4 - b_Intercept, `effect size` = (4 - b_Intercept) / sigma) %&gt;% pivot_longer(`2 - b_Intercept`:`effect size`) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ name, scales = &quot;free&quot;) As in the text, our \\(d\\) is centered around 0.3. Let’s use a posterior predictive check to see how well fit23.4 summarized these data. pp_check(fit23.4, type = &quot;hist&quot;, ndraws = 10, binwidth = 1) + scale_x_continuous(breaks = seq(from = -3, to = 7, by = 2)) + theme(legend.position = c(.9, .15)) The histograms aren’t as awful as the ones from the previous model. But they’re still not great. We might further inspect the model misspecification with a cumulative distribution function overlay, this time comparing fit23.2 directly to fit23.4. p1 &lt;- pp_check(fit23.2, type = &quot;ecdf_overlay&quot;, ndraws = 50) + ggtitle(&quot;Cumulative-normal (fit23.2)&quot;) p2 &lt;- pp_check(fit23.4, type = &quot;ecdf_overlay&quot;, ndraws = 50) + ggtitle(&quot;Conventional-normal (fit23.4)&quot;) (p1 + p2 &amp; scale_x_continuous(breaks = 0:7, limits = c(0, 7)) &amp; scale_y_continuous(expand = c(0, 0)) &amp; theme(title = element_text(size = 10.5))) + plot_layout(guides = &#39;collect&#39;) “Which of the analyses yields the more trustworthy conclusion? The one that describes the data better. In these cases, there is no doubt that the cumulative-normal model is the better description of the data” than the conventional Gaussian model (p. 682). 23.2.2.2 Ordinal outcomes versus Likert scales. Just for fun, rate how much you agree with the statement, “Bayesian estimation is more informative than null-hypothesis significance testing,” by selecting one option from the following: \\(1\\) = strongly disagree; \\(2\\) = disagree; \\(3\\) = undecided; \\(4\\) = agree; \\(5\\) = strongly agree. This sort of ordinal response interface is often called a Likert-type response (Likert, 1932), pronounced LICK-ert not LIKE-ert). Sometimes, it is called a Likert “scale” but the term “scale” in this context is more properly reserved for referring to an underlying metric variable that is indicated by the arithmetic mean of several meaningfully related Likert-type responses (Carifio &amp; Perla, 2008; e.g., Carifio &amp; Perla, 2007; Norman, 2010). (p. 681) Kruschke then briefly introduced how one might combine several such meaningfully-related Likert-type responses with latent variable methods. He then clarified this text will not explore that approach, further. The current version of brms (i.e., 2.12.0) has very limited latent variable capacities. However, they are in the works. Interested modelers can follow Bürkner’s progress in GitHub issue #304. He also has a (2020) paper on how one might use brms to fit item response theory models, which can be viewed as a special family of latent variable models. One can also fit Bayesian latent variable models with the blavaan package. 23.3 The case of two groups In both examples in the preceding text, the two groups of outcomes were on the same ordinal scale. In the first example, both questionnaire statements were answered on the same disagree–agree scale. In the second example, both groups responded on the same very unhappy–very happy scale. Therefore, we assume that both groups have the same underlying metric variable with the same thresholds. (p. 682) 23.3.1 Implementation in JAGS brms. The brm() syntax for adding a single categorical predictor to an ordered-probit model is much like that for any other likelihood. We just add the variable name to the right side of the ~ in the formula argument. If you’re like me and like to use the verbose 1 syntax for your model intercepts–thresholds in these models–just use the + operator between them. For example, this is what it’d look like for an ordered-categorical criterion y and a single categorical predictor x. fit &lt;- brm(data = my_data, family = cumulative(probit), y ~ 1 + x, prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b))) Also of note, we’ve expanded the prior section to include a line for class = b. As with the thresholds, interpret this prior through the context of the underlying standard normal cumulative distribution, \\(\\Phi(z)\\). Note the interpretation, though. By brms defaults, the underlying Gaussian for the reference category of x will be \\(\\operatorname{Normal}(0, 1)\\). Thus whatever parameter value you get for the other categories in x, those will be standardized mean differences, making them a kind of effect size. Note, the above all presumes you’re only interested in comparing means between groups. Things get more complicated if you want groups to vary by \\(\\sigma\\), too. Hold on tight! First, look back at the output from print(fit1) or print(fit2). The second line for both reads: Links: mu = probit; disc = identity. Hopefully the mu = probit part is no surprise. Probit regression is the primary focus of this chapter. But check out the disc = identity part and notice that nowhere in there is there any mention of sigma = identity like we get when treating the criterion as metric as in conventional Gaussian models (i.e., execute print(fit3) or print(fit4)). Yes, there is a relationship between disc and sigma. disc is shorthand for discrimination. The term comes from the item response theory (IRT) literature and discrimination is the inverse of \\(\\sigma\\) (see Bürkner’s Bayesian item response modelling in R with brms and Stan). In IRT, discrimination is often denoted \\(a\\) or \\(\\alpha\\). Here I’ll adopt the latter, making \\(\\sigma = 1 / \\alpha\\). But focusing back on brms summary output, notice how both disc and sigma are modeled using the identity link. If you recall from earlier chapters, we switched to the log link to constrain the values to zero and above when we allowed \\(\\sigma\\) to vary across groups. It’s the same thing for our discrimination parameter, \\(\\alpha\\). Because \\(\\alpha\\) should always be zero or above, brms defaults to the log link when modeling it with predictors. As with \\(\\sigma\\) in conventional Gaussian models, we’ll be using some version of the bf() syntax when modeling the discrimination parameter in brms. For a general introduction to what Bürkner calls distributional modeling, see his (2022a) vignette, Estimating distributional models with brms. In the case of the discrimination parameter for the cumulative model, we’ll want more focused instructions. Happily, Bürkner &amp; Vuorre (2019) have our backs. We read: Conceptually, unequal variances are incorporated in the model by specifying an additional regression formula for the variance component of the latent variable \\(\\tilde Y\\). In brms, the parameter related to latent variances is called disc (short for “discrimination”), following conventions in item response theory. Note that disc is not the variance itself, but the inverse of the standard deviation, \\(s.\\) That is, \\(s = 1/ \\text{disc}\\). Further, because disc must be strictly positive, it is by default modeled on the log scale. Predicting auxiliary parameters (parameters of the distribution other than the mean/location) in brms is accomplished by passing multiple regression formulas to the brm() function. Each formula must first be wrapped in another function, bf() or lf() (for “linear formula”)–depending on whether it is a main or an auxiliary formula, respectively. The formulas are then combined and passed to the formula argument of brm(). Because the standard deviation of the latent variable is fixed to 1 for the baseline [group, disc cannot be estimated for the baseline group]. We must therefore ensure that disc is estimated only for [non-baseline groups]. To do so, we omit the intercept from the model of disc by writing 0 + ... on the right-hand side of the regression formula. By default, R applies cell-mean coding to factors in formulas without an intercept. That would lead to disc being estimated for [all groups], so we must deactivate it via the cmc argument of lf(). (pp. 11–12) Here’s what that might look like. fit &lt;- brm(data = my_data, family = cumulative(probit), bf(y ~ 1 + x) + lf(disc ~ 0 + x, cmc = F), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(normal(0, 4), class = b, dpar = disc))) Note how when using the disc ~ 0 + ... syntax, the disc parameters are of class = b within the prior() function. If you’d like to assign them priors differing from the other b parameters, you’ll need to specify dpar = disc. Again, though the mean structure for this model is on the probit scale, the discrimination structure is on the log scale. Recalling that \\(\\sigma = 1/\\alpha\\), which means \\(\\alpha = 1/\\sigma\\), and also that we’re modeling \\(\\log (\\alpha)\\), the priors for the standard deviations of the non-reference category groups are on the scale of \\(\\log (1 / \\sigma)\\). To get a better sense of how one might set a prior on such a scale, we might compare \\(\\sigma\\), \\(\\alpha\\), and \\(\\log (\\alpha)\\). Here are the density and cumulative density functions for \\(\\operatorname{Normal}(0, 0.5)\\), \\(\\operatorname{Normal}(0, 1)\\), and \\(\\operatorname{Normal}(0, 2)\\). tibble(mu = 0, sigma = c(0.5, 1, 2)) %&gt;% expand(nesting(mu, sigma), y = seq(from = -5, to = 5, by = 0.1)) %&gt;% mutate(`p(y)` = dnorm(y, mu, sigma), `Phi(y)` = pnorm(y, mu, sigma)) %&gt;% mutate(alpha = 1 / sigma, loga = log(1 / sigma)) %&gt;% mutate(label = str_c(&quot;list(sigma==&quot;, sigma, &quot;,alpha==&quot;, alpha, &quot;,log(alpha)==&quot;, round(loga, 2), &quot;)&quot;)) %&gt;% pivot_longer(`p(y)`:`Phi(y)`) %&gt;% ggplot(aes(x = y, y = value)) + geom_line(size = 1.5, color = sl[7]) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(xlim = c(-4, 4)) + facet_grid(name ~ label, labeller = label_parsed, switch = &quot;y&quot;) Put another way, here’s how \\(\\alpha\\) and \\(\\log (\\alpha)\\) scale on values of \\(\\sigma\\) ranging from 0.0001 to 10. tibble(sigma = seq(from = 0.0001, to = 10, by = 0.01)) %&gt;% mutate(alpha = 1 / sigma, `log(alpha)` = log(1 / sigma)) %&gt;% pivot_longer(-sigma, names_to = &quot;labels&quot;) %&gt;% ggplot(aes(x = sigma, y = value)) + geom_hline(yintercept = 0, color = sl[3], linetype = 2) + geom_vline(xintercept = 0, color = sl[3], linetype = 2) + geom_line(size = 1.5, color = sl[7]) + coord_cartesian(ylim = c(-2, 10)) + facet_grid(~ labels, labeller = label_parsed) When \\(\\sigma\\) goes below 1, both explode upward. As \\(\\sigma\\) increases, \\(\\alpha\\) asymptotes at zero and \\(\\log (\\alpha)\\) slowly descends below zero. Put another way, here is how \\(\\sigma\\) scales as a function of \\(\\log (\\alpha)\\). tibble(`log(alpha)` = seq(from = -3, to = 3, by = 0.01)) %&gt;% mutate(sigma = 1 / exp(`log(alpha)`)) %&gt;% ggplot(aes(x = `log(alpha)`, y = sigma)) + geom_hline(yintercept = 0, color = sl[3], linetype = 2) + geom_line(size = 1.5, color = sl[7]) + labs(x = expression(log(alpha)), y = expression(sigma)) + coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 10)) In the context where the underlying distribution for the reference category will be the standard normal, it seems like a \\(\\operatorname{Normal}(0, 1)\\) prior would be fairly permissive for \\(\\log (\\alpha)\\). This is what I will use going forward. Choose your priors with care. 23.3.2 Examples: Not funny. Load the data for the next model. my_data &lt;- read_csv(&quot;data.R/OrdinalProbitData1.csv&quot;) glimpse(my_data) ## Rows: 88 ## Columns: 2 ## $ X &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;… ## $ Y &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,… Fit the first ordinal probit model with group-specific \\(\\mu\\) and \\(\\sigma\\) values for the underlying normal distributions for the ordinal variable Y. fit23.5 &lt;- brm(data = my_data, family = cumulative(probit), bf(Y ~ 1 + X) + lf(disc ~ 0 + X, cmc = FALSE), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(normal(0, 1), class = b, dpar = disc)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.05&quot;) Look over the summary. print(fit23.5) ## Family: cumulative ## Links: mu = probit; disc = log ## Formula: Y ~ 1 + X ## disc ~ 0 + X ## Data: my_data (Number of observations: 88) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] 0.49 0.20 0.11 0.88 1.00 9760 6028 ## Intercept[2] 1.30 0.24 0.85 1.78 1.00 11289 5956 ## Intercept[3] 2.20 0.38 1.52 3.00 1.00 5687 5511 ## Intercept[4] 3.43 0.82 2.15 5.38 1.00 4453 5631 ## XB 0.43 0.34 -0.31 1.04 1.00 4682 3784 ## disc_XB -0.32 0.28 -0.86 0.23 1.00 3221 4543 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Because our fancy new parameter disc_XB is on the \\(\\log (\\alpha)\\) scale, we can convert it to the \\(\\sigma\\) scale with \\(\\frac{1}{\\exp (\\log \\alpha)}\\). For a quick and dirty example, here it is with the posterior mean. 1 / (exp(fixef(fit23.5)[&quot;disc_XB&quot;, 1])) ## [1] 1.372627 Before we follow along with Kruschke, let’s hammer the meaning of these model parameters home. Here is a density plot of the two underlying latent distributions for Y, given X. We’ll throw in the thresholds for good measure. To keep things simple, we’ll just express the distributions in terms of the posterior means of each parameter. tibble(X = LETTERS[1:2], mu = c(0, fixef(fit23.5)[&quot;XB&quot;, 1]), sigma = c(1, 1 / (exp(fixef(fit23.5)[&quot;disc_XB&quot;, 1])))) %&gt;% expand(nesting(X, mu, sigma), y = seq(from = -5, to = 5, by = 0.1)) %&gt;% mutate(d = dnorm(y, mu, sigma)) %&gt;% ggplot(aes(x = y, y = d, fill = X)) + geom_area(alpha = 2/3, position = &quot;identity&quot;) + geom_vline(xintercept = fixef(fit23.5)[1:4, 1], linetype = 3, color = sl[9]) + scale_fill_scico_d(palette = &quot;lajolla&quot;, begin = .33, end = .67) + scale_x_continuous(sec.axis = dup_axis(breaks = fixef(fit23.5)[1:4, 1] %&gt;% as.double(), labels = parse(text = str_c(&quot;theta[&quot;, 1:4, &quot;]&quot;)))) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Underlying latent scale for Y, given X&quot;, x = NULL) + theme(axis.ticks.x.top = element_blank()) Returning to our previous workflow, extract the posterior draws and wrangle. draws &lt;- as_draws_df(fit23.5) glimpse(draws) ## Rows: 8,000 ## Columns: 11 ## $ `b_Intercept[1]` &lt;dbl&gt; 0.4952251, 0.7549364, 0.7251187, 0.5801829, 0.3644839, 0.6963412, 0.2042224, 0.4628… ## $ `b_Intercept[2]` &lt;dbl&gt; 1.0987189, 1.2573930, 1.3004260, 1.2258207, 0.9281913, 1.2340367, 1.3980522, 1.1853… ## $ `b_Intercept[3]` &lt;dbl&gt; 1.701087, 1.806990, 1.651997, 2.159145, 1.776843, 1.795096, 2.845807, 2.064523, 1.9… ## $ `b_Intercept[4]` &lt;dbl&gt; 2.377239, 2.446180, 2.469663, 2.678498, 2.348979, 2.197770, 6.031723, 3.376425, 4.1… ## $ b_XB &lt;dbl&gt; 0.48385430, 0.85921784, 0.67312708, 0.38551309, 0.09678844, 0.45100318, 0.29035289,… ## $ b_disc_XB &lt;dbl&gt; 0.230385189, 0.288865047, 0.302123290, 0.220952201, 0.033662260, -0.001259104, -1.0… ## $ lprior &lt;dbl&gt; -12.71293, -12.72098, -12.73492, -12.82612, -12.73203, -12.68872, -14.37737, -12.90… ## $ lp__ &lt;dbl&gt; -108.1915, -109.1719, -110.3546, -112.2361, -109.8350, -110.3082, -112.2452, -107.9… ## $ .chain &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ .iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, … ## $ .draw &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, … Now, let’s use our handy compare_thresholds() function to make an expanded version of the lower-left plot of Figure 23.4. draws %&gt;% select(`b_Intercept[1]`:`b_Intercept[4]`, .draw) %&gt;% compare_thresholds(lb = 1.5, ub = 4.5) It will no longer be straightforward to use the formulas from 23.2.2 to convert the output from our brms parameterization to match the way Kruschke parameterized his conditional means and standard deviations. I will leave the conversion up to the interested reader. Going forward, we will focus on the output from our brms parameterization. draws %&gt;% # simple parameters mutate(`mu[A]` = 0, `mu[B]` = b_XB, `sigma[A]` = 1, `sigma[B]` = 1 / exp(b_disc_XB)) %&gt;% # simple differences mutate(`mu[B]-mu[A]` = `mu[B]` - `mu[A]`, `sigma[B]-sigma[A]` = `sigma[B]` - `sigma[A]`) %&gt;% # effect size mutate(`(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)` = (`mu[B]-mu[A]`) / sqrt((`sigma[A]`^2 + `sigma[B]`^2) / 2)) %&gt;% # wrangle pivot_longer(`mu[A]`:`(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)`) %&gt;% mutate(name = factor(name, levels = c(&quot;mu[A]&quot;, &quot;mu[B]&quot;, &quot;mu[B]-mu[A]&quot;, &quot;sigma[A]&quot;, &quot;sigma[B]&quot;, &quot;sigma[B]-sigma[A]&quot;, &quot;(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)&quot;))) %&gt;% # plot ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, labeller = label_parsed) \\(\\mu_A\\) and \\(\\sigma_A\\) are both constants, which doesn’t show up well with our stat_halfeye() approach with the scales freed across facets. If these plots really mattered for a scientific presentation or something for industry, you could experiment using either a common scale across all facets, or making the plots individually and then combining them with patchwork syntax. Returning to interpretation, because \\(\\mu_A = 0\\), it turns out that \\(\\mu_B - \\mu_A = \\mu_B\\), which is on display on the top row. Because \\(\\sigma_A = 1\\), it turns out that \\(\\sigma_B - \\sigma_A\\) is just \\(\\sigma_B\\) moved over one unit to the left, which is hopefully clear in the panels of the second row. Very happily, the effect size formula worked with our brms parameters the same way it did for Kruschke’s. Both yield an effect size of about 0.5, with 95% intervals extending about \\(\\mp 0.5\\). Here we make good use of the type = \"bars_grouped\" and group = \"X\" arguments to make the posterior predictive plots at the top right of Figure 23.4 with the brms::pp_check() function. set.seed(23) pp_check(fit23.5, type = &quot;bars_grouped&quot;, ndraws = 100, group = &quot;X&quot;, fatten = 2) + scale_x_continuous(&quot;y&quot;, breaks = 1:7) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Data with posterior predictions&quot;, subtitle = expression(list(italic(N[A])==44, italic(N[B])==44))) + theme(legend.background = element_blank(), legend.position = c(.9, .75)) Using more tricks from back in Chapter 16, here’s the corresponding conventional Gaussian model for metric data. mean_y &lt;- mean(my_data$Y) sd_y &lt;- sd(my_data$Y) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) fit23.6 &lt;- brm(data = my_data, family = gaussian, bf(Y ~ 0 + X, sigma ~ 0 + X), prior = c(prior(normal(mean_y, sd_y * 100), class = b), prior(normal(0, 1), class = b, dpar = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.06&quot;) Check the summary. print(fit23.6) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: Y ~ 0 + X ## sigma ~ 0 + X ## Data: my_data (Number of observations: 88) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## XA 1.43 0.12 1.20 1.67 1.00 4176 2850 ## XB 1.86 0.16 1.54 2.18 1.00 4514 2958 ## sigma_XA -0.26 0.11 -0.46 -0.03 1.00 3919 2563 ## sigma_XB 0.08 0.11 -0.12 0.30 1.00 4275 3139 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here are the marginal posteriors, including the effect size (i.e., a standardized mean difference using the pooled standard deviation formula presuming equal sample sizes, \\((\\mu_2 - \\mu_1) / \\sqrt{(\\sigma_1^2 + \\sigma_2^2) / 2}\\). as_draws_df(fit23.6) %&gt;% mutate(`A mean` = b_XA, `B mean` = b_XB, `A Std. Dev.` = exp(b_sigma_XA), `B Std. Dev.` = exp(b_sigma_XB)) %&gt;% mutate(`Difference of Means` = `B mean` - `A mean`, `Difference of Std. Devs` = `B Std. Dev.` - `A Std. Dev.`) %&gt;% mutate(`Effect Size` = `Difference of Means` / sqrt((`A Std. Dev.`^2 + `B Std. Dev.`^2) / 2)) %&gt;% pivot_longer(`A mean`:`Effect Size`) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;These are based on the conventional Gaussian model, NOT the cumulative-normal\\nmodel Kruschke displayed in Figure 23.4&quot;, x = &quot;Marginal posterior&quot;) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) Compare those results to those Kruschke reported from an NHST analysis in the note below Figure 23.4: \\(M_1 = 1.43, M_2 = 1.86, t = 2.18, p = 0.032\\), with effect size \\(d = 0.466\\) with \\(95\\%\\) CI of \\(0.036-0.895\\). An \\(F\\) test of the variances concludes that the standard deviations are significantly different: \\(S_1 = 0.76, S_2 = 1.07, p = 0.027\\). Notice in this case that treating the values as metric greatly underestimates their variances, as well as erroneously concludes the variances are different. (p. 684) As to the data in the analyses Kruschke reported in Figure 23.5 and the prose in the second paragraph on page 685, I’m not aware that Kruschke provided them. From his footnote #2, we read: “Data in Figure 23.5 are from an as-yet unpublished study I conducted with the collaboration of Allison Vollmer as part of her undergraduate honors project.” In place of the real data, I eyeballed the values based on the upper-right panels in Figure 23.5. Here they are. d &lt;- tibble(x = rep(str_c(&quot;joke &quot;, c(1, 6)), each = 177), y = c(rep(1:7, times = c(95, 19, 18, 10, 17, 10, 8)), rep(1:7, times = c(53, 33, 31, 22, 23, 14, 1)))) glimpse(d) ## Rows: 354 ## Columns: 2 ## $ x &lt;chr&gt; &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;… ## $ y &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… My approximation to Kruschke’s data looks like this. d %&gt;% ggplot(aes(x = y)) + geom_bar(fill = sl[5]) + scale_x_continuous(breaks = 1:7) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + facet_wrap(~ x, ncol = 1) Here we fit the cumulative-normal model based on our version of the data, continuing to allow both \\(\\mu\\) and \\(\\sigma\\) (i.e., \\(1 / \\exp(\\log \\alpha)\\)) to differ across groups. fit23.7 &lt;- brm(data = d, family = cumulative(probit), bf(y ~ 1 + x) + lf(disc ~ 0 + x, cmc = FALSE), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(normal(0, 1), class = b, dpar = disc)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.07&quot;) Check the model summary. print(fit23.7) ## Family: cumulative ## Links: mu = probit; disc = log ## Formula: y ~ 1 + x ## disc ~ 0 + x ## Data: d (Number of observations: 354) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] 0.08 0.09 -0.11 0.25 1.00 5294 5858 ## Intercept[2] 0.37 0.09 0.19 0.54 1.00 7251 6247 ## Intercept[3] 0.65 0.09 0.47 0.83 1.00 8814 6164 ## Intercept[4] 0.87 0.10 0.68 1.07 1.00 8039 5851 ## Intercept[5] 1.26 0.12 1.03 1.49 1.00 6667 5937 ## Intercept[6] 1.81 0.16 1.51 2.13 1.00 7075 5922 ## xjoke6 0.39 0.10 0.20 0.59 1.00 7813 6439 ## disc_xjoke6 0.49 0.12 0.27 0.72 1.00 4440 5751 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Save and wrangle the posterior draws, then use our compare_thresholds() function to compare the brms parameterization of \\(\\theta_{[i]}\\) with the parameterization in the text in an expanded version of the lower-left plot of Figure 23.5. draws &lt;- as_draws_df(fit23.7) draws %&gt;% select(`b_Intercept[1]`:`b_Intercept[6]`, .draw) %&gt;% compare_thresholds(lb = 1.5, ub = 6.5) Given our data are only approximations of Kruschke’s, I think we did pretty good. Here are the histograms for our brms versions of the \\(\\mu\\)- and \\(\\sigma\\)-related parameters. draws %&gt;% transmute(`mu[Joke~1]` = 0, `mu[Joke~6]` = b_xjoke6, `sigma[Joke~1]` = 1, `sigma[Joke~6]` = 1 / exp(b_disc_xjoke6)) %&gt;% mutate(`mu[Joke~6]-mu[Joke~1]` = `mu[Joke~6]` - `mu[Joke~1]`, `sigma[Joke~6]-sigma[Joke~1]` = `sigma[Joke~6]` - `sigma[Joke~1]`) %&gt;% mutate(`(mu[Joke~6]-mu[Joke~1])/sqrt((sigma[Joke~1]^2+sigma[Joke~6]^2)/2)` = (`mu[Joke~6]-mu[Joke~1]`) / sqrt((`sigma[Joke~1]`^2 + `sigma[Joke~6]`^2) / 2)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;mu[Joke~1]&quot;, &quot;mu[Joke~6]&quot;, &quot;mu[Joke~6]-mu[Joke~1]&quot;, &quot;sigma[Joke~1]&quot;, &quot;sigma[Joke~6]&quot;, &quot;sigma[Joke~6]-sigma[Joke~1]&quot;, &quot;(mu[Joke~6]-mu[Joke~1])/sqrt((sigma[Joke~1]^2+sigma[Joke~6]^2)/2)&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~ name, scales = &quot;free&quot;, labeller = label_parsed) Here are our versions of the two panels in the upper right of Figure 23.5. set.seed(23) pp_check(fit23.7, type = &quot;bars_grouped&quot;, ndraws = 100, group = &quot;x&quot;, fatten = 2) + scale_x_continuous(&quot;y&quot;, breaks = 1:7) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + ggtitle(&quot;Data with posterior predictions&quot;, subtitle = expression(list(italic(N[&quot;joke &quot;*1])==177, italic(N[&quot;joke &quot;*6])==177))) + theme(legend.position = &quot;none&quot;) Now here’s the corresponding model where we treat the y data as metric. mean_y &lt;- mean(d$y) sd_y &lt;- sd(d$y) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) fit23.8 &lt;- brm(data = d, family = gaussian, bf(y ~ 0 + x, sigma ~ 0 + x), prior = c(prior(normal(mean_y, sd_y * 100), class = b), prior(normal(0, exp(sd_y)), class = b, dpar = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.08&quot;) Check the summary. print(fit23.8) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: y ~ 0 + x ## sigma ~ 0 + x ## Data: d (Number of observations: 354) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## xjoke1 2.42 0.14 2.15 2.69 1.00 4201 3145 ## xjoke6 2.86 0.13 2.60 3.10 1.00 4581 3030 ## sigma_xjoke1 0.64 0.05 0.54 0.75 1.00 5039 3027 ## sigma_xjoke6 0.52 0.05 0.42 0.63 1.00 4602 3110 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Make the marginal posteriors, including the effect size. as_draws_df(fit23.8) %&gt;% mutate(`Joke 1 Mean` = b_xjoke1, `Joke 6 Mean` = b_xjoke6, `Joke 1 Std. Dev.` = exp(b_sigma_xjoke1), `Joke 6 Std. Dev.` = exp(b_sigma_xjoke6)) %&gt;% mutate(`Difference of Means` = `Joke 6 Mean` - `Joke 1 Mean`, `Difference of Std. Devs` = `Joke 6 Std. Dev.` - `Joke 1 Std. Dev.`) %&gt;% mutate(`Effect Size` = `Difference of Means` / sqrt((`Joke 1 Std. Dev.`^2 + `Joke 6 Std. Dev.`^2) / 2)) %&gt;% pivot_longer(`Joke 1 Mean`:`Effect Size`) %&gt;% mutate(name = factor(name, levels = c(&quot;Joke 1 Mean&quot;, &quot;Joke 1 Std. Dev.&quot;, &quot;Joke 6 Mean&quot;, &quot;Joke 6 Std. Dev.&quot;, &quot;Difference of Means&quot;, &quot;Difference of Std. Devs&quot;, &quot;Effect Size&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;These are based on the conventional Gaussian model, NOT the cumulative-normal\\nmodel Kruschke displayed in Figure 23.5&quot;, x = &quot;Marginal posterior&quot;) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 2) If you think you have a better approximation of Kruschke’s data, please share. 23.4 The Case of metric predictors “This type of model is often referred to as ordinal probit regression or ordered probit regression because the probit function is the link function corresponding to the cumulative-normal inverse-link function” (p. 688, emphasis in the original). 23.4.1 Implementation in JAGS brms. This model is easy to specify in brms. Just make sure to think clearly about your priors. 23.4.2 Example: Happiness and money. Load the data for the next model. my_data &lt;- read_csv(&quot;data.R/OrdinalProbitData-LinReg-2.csv&quot;) glimpse(my_data) ## Rows: 200 ## Columns: 2 ## $ X &lt;dbl&gt; 1.386389, 1.223879, 1.454505, 1.112068, 1.222715, 1.545099, 1.360256, 1.533071, 1.501657, 1.426755… ## $ Y &lt;dbl&gt; 1, 1, 5, 5, 1, 4, 6, 2, 5, 4, 1, 4, 4, 4, 4, 6, 1, 1, 6, 2, 1, 7, 1, 3, 1, 1, 7, 5, 7, 1, 4, 6, 7,… Take a quick look at the data. my_data %&gt;% ggplot(aes(x = X, y = Y)) + geom_point(alpha = 1/3, color = sl[9]) + scale_y_continuous(breaks = 1:7) Kruschke standardized his predictor within his model code. Here we’ll standardize X before fitting the model. my_data &lt;- my_data %&gt;% mutate(X_s = (X - mean(X)) / sd(X)) Fit the model. fit23.9 &lt;- brm(data = my_data, family = cumulative(probit), Y ~ 1 + X_s, prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.09&quot;) Check the summary. print(fit23.9) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Y ~ 1 + X_s ## Data: my_data (Number of observations: 200) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -1.18 0.12 -1.42 -0.94 1.00 5504 5515 ## Intercept[2] -0.76 0.11 -0.99 -0.54 1.00 7562 6456 ## Intercept[3] -0.29 0.11 -0.50 -0.08 1.00 8824 6672 ## Intercept[4] 0.24 0.11 0.04 0.45 1.00 9672 6707 ## Intercept[5] 0.73 0.11 0.51 0.96 1.00 9080 7114 ## Intercept[6] 1.26 0.13 1.02 1.52 1.00 8399 7198 ## X_s 1.16 0.10 0.97 1.35 1.00 6487 6073 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 NA NA NA ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract the posterior draws and compare the brms parameterization of \\(\\theta_{[i]}\\) with the parameterization in the text in an expanded version of the bottom panel of Figure 23.7. draws &lt;- as_draws_df(fit23.9) draws %&gt;% select(`b_Intercept[1]`:`b_Intercept[6]`, .draw) %&gt;% compare_thresholds(lb = 1.5, ub = 6.5) Here’s the marginal distribution of b_X_s, our effect size for the number of jokes. draws %&gt;% ggplot(aes(x = b_X_s, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8]) + scale_y_continuous(NULL, breaks = NULL) This differs from Kruschks’s \\(\\beta_1\\), which is in an unstandardized metric based on the parameters in his version of the model. But unlike the effect sizes from previous models, this one is not in a Cohen’s-\\(d\\) metric. Rather, this is a fully-standardized regression coefficient. As to the large subplot at the top of Figure 23.7, we can make something like it by nesting conditional_effects() within plot(). conditional_effects(fit23.9) %&gt;% plot(line_args = list(color = sl[7], fill = sl[3])) Here’s a more elaborated version of the same plot, this time depicting the model with 100 fitted lines randomly drawn from the posterior. set.seed(23) conditional_effects(fit23.9, spaghetti = TRUE, ndraws = 100) %&gt;% plot(points = T, line_args = list(size = 0), point_args = list(alpha = 1/3, color = sl[9]), spaghetti_args = list(colour = alpha(sl[6], .2))) ## Warning: Predictions are treated as continuous variables in &#39;conditional_effects&#39; by default which is likely ## invalid for ordinal families. Please set &#39;categorical&#39; to TRUE. Note the warning message. There was a similar one in the first plot, which I suppressed for simplicity sake. The message suggests treating the fitted lines as “continuous variables” might lead to a deceptive plot. Here’s what happens if we follow the suggestion. set.seed(23) conditional_effects(fit23.9, categorical = T) Recall that our thresholds, \\(\\theta_1,...,\\theta_{K-1}\\), in conjunction with the standard normal density, give us the probability of a given Y value, given X_s. That is, \\(p(y = k | \\mu, \\sigma, \\{ \\theta_j \\})\\), where \\(\\mu\\) is conditional on \\(x\\). This plot returned the fitted lines of those conditional probabilities, each depicted by the posterior mean and percentile-based 95% intervals. At lower values of X_s, lower values of Y are more probable. At higher values of X_s, higher values of Y are more probable. It might be useful to get more practice in with this model. Here we’ll use fitted() to make a similar plot, depicting the model with may fitted lines instead of summary statistics. # how many fitted lines do you want? n_draws &lt;- 50 # define the `X_s` values you want to condition on # because the lines are nonlinear, you&#39;ll need many of them nd &lt;- tibble(X_s = seq(from = -2, to = 2, by = 0.05)) f &lt;- fitted(fit23.9, newdata = nd, summary = F, ndraws = n_draws) # inspect the output f %&gt;% str() ## num [1:50, 1:81, 1:7] 0.933 0.854 0.848 0.888 0.91 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## ..$ : NULL ## ..$ : chr [1:7] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... Our output came in three dimensions. We have 50 rows, corresponding to n_draws &lt;- 50 (i.e., 50 posterior draws). There are 81 columns, based on how we defined the X_s values within our nd data (i.e., seq(from = -2, to = 2, by = 0.05)). The third dimension has seven levels, one corresponding to each of the seven levels of our criterion variable Y. Here’s a way to rearrange that output into a useful format for plotting. # rearrange the output rbind( f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5], f[, , 6], f[, , 7] ) %&gt;% # wrangle data.frame() %&gt;% set_names(nd %&gt;% pull(X_s)) %&gt;% mutate(draw = rep(1:n_draws, times = 7), rating = rep(1:7, each = n_draws)) %&gt;% pivot_longer(-c(draw, rating), names_to = &quot;X_s&quot;, values_to = &quot;probability&quot;) %&gt;% mutate(rating = str_c(&quot;Y: &quot;, rating), X_s = X_s %&gt;% as.double()) %&gt;% # plot ggplot(aes(x = X_s, y = probability, group = interaction(draw, rating), color = rating)) + geom_line(size = 1/4, alpha = 1/2) + scale_color_scico_d(palette = &quot;lajolla&quot;, begin = .25) + scale_y_continuous(breaks = c(0, .5, 1), limits = c(0, 1), expand = c(0, 0)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ rating, ncol = 7) So far, we’ve been plotting our model with the context of the default scale = \"response\" setting within fitted(). Within the context of the response variable Y, our model returns response probabilities. We can also look at the output within the context of scale = \"linear\". We’ll plot these fitted lines across our nd values two different ways. For the first, p1, we’ll use summary statistics. For the second, p2, we’ll set summary = T. # adjust the nd nd &lt;- tibble(X_s = seq(from = -2.5, to = 2.5, by = 0.1)) # use summary statistics p1 &lt;- fitted(fit23.9, scale = &quot;linear&quot;, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = X_s, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_smooth(stat = &quot;identity&quot;, alpha = 1/2, color = sl[7], fill = sl[3]) + labs(title = &quot;summary statistics&quot;, y = &quot;underlying standard normal&quot;) # set `summary = F` set.seed(23) p2 &lt;- fitted(fit23.9, scale = &quot;linear&quot;, newdata = nd, summary = F, ndraws = n_draws) %&gt;% data.frame() %&gt;% set_names(nd %&gt;% pull(X_s)) %&gt;% mutate(draw = 1:n_draws) %&gt;% pivot_longer(-draw, names_to = &quot;X_s&quot;) %&gt;% mutate(X_s = X_s %&gt;% as.double()) %&gt;% ggplot(aes(x = X_s, y = value, group = draw)) + geom_line(alpha = 1/2, color = sl[7]) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;50 posterior draws&quot;) # combine and plot! p1 + p2 &amp; coord_cartesian(ylim = c(-2, 2)) Both methods returned the fitted lines in the metric of the underlying standard normal distribution. The fitted lines are nonlinear in the metric of the raw data Y, but they’re linear in the metric of the presumed underlying distribution. If it helps, we’ll make a marginal plot of the standard normal distribution and tack it onto the right. # make Phi p3 &lt;- tibble(x = seq(from = -3, to = 3, by = .01)) %&gt;% mutate(d = dnorm(x)) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = sl[5]) + # add the thresholds! geom_vline(xintercept = posterior_summary(fit23.9)[1:6, 1], color = sl[9], linetype = 3) + # mark the thresholds with the axis breaks scale_x_reverse(NULL, breaks = fixef(fit23.9)[1:6, 1], position = &quot;top&quot;, labels = parse(text = str_c(&quot;theta[&quot;, 1:6, &quot;]&quot;))) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(italic(N)(0,1))) + coord_flip(xlim = c(-2, 2)) # combine, format a bit, and plot ( ((p1 | p2 ) &amp; geom_hline(yintercept = posterior_summary(fit23.9)[1:6, 1], color = sl[9], linetype = 3) &amp; coord_cartesian(ylim = c(-2, 2)) | p3) ) + plot_layout(widths = c(4, 4, 1)) The lines intersecting the plots are the posterior means for thresholds, \\(\\theta_1, \\dots ,\\theta_6\\). But these still aren’t faithful depictions of the top panel of Figure 23.7, you say. Okay, fine. One of the distinctive elements of that panel is the left-tilted bar-and-error plots. If you look closely at the vertical lines at their bases, you’ll see that the leftmost subplot starts at the minimum value of X and the rightmost subplot starts at the maximum value of X. Since our plots, so far, have been based on X_s, we’ll use the minimum and maximum values from that. Here are those values. (r &lt;- range(my_data$X_s)) ## [1] -1.774444 1.750168 To my eye, it appears that the three middle subplots are equally distributed between those at the ends. If we proceed under that assumption, here’s how we might use fitted() to get us rolling on computing the relevant values. nd &lt;- tibble(X_s = seq(from = r[1], to = r[2], length.out = 5)) f &lt;- fitted(fit23.9, newdata = nd) # inspect the output f %&gt;% str() ## num [1:5, 1:4, 1:7] 0.8078 0.4446 0.124 0.0155 0.0009 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : NULL ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## ..$ : chr [1:7] &quot;P(Y = 1)&quot; &quot;P(Y = 2)&quot; &quot;P(Y = 3)&quot; &quot;P(Y = 4)&quot; ... Here we’ll rearrange the output to make it useful for plotting. # rearrange the output f &lt;- rbind( f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5], f[, , 6], f[, , 7] ) %&gt;% # wrangle data.frame() %&gt;% bind_cols(expand(nd, Y = 1:7, X_s)) head(f) ## Estimate Est.Error Q2.5 Q97.5 Y X_s ## 1 0.8078061176 0.0473570337 0.706100621 0.890845927 1 -1.77444380 ## 2 0.4445858032 0.0489066995 0.349544950 0.541241792 1 -0.89329094 ## 3 0.1239922037 0.0251036999 0.079693316 0.177207790 1 -0.01213809 ## 4 0.0155430683 0.0067798504 0.005795549 0.031534672 1 0.86901477 ## 5 0.0009001091 0.0007991004 0.000116457 0.003028196 1 1.75016762 ## 6 0.0920486261 0.0253704586 0.049029670 0.146783451 2 -1.77444380 Now we can make those bar-and-error plots. f %&gt;% mutate(X_s = round(X_s, digits = 3)) %&gt;% ggplot(aes(x = Y, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_col(fill = sl[4]) + geom_pointrange(fatten = 1.5, size = 1, color = sl[7]) + scale_x_continuous(breaks = 1:7) + scale_y_reverse(NULL, position = &quot;right&quot;, limits = c(1, 0), expand = c(0, 0), breaks = c(1, .5, 0), labels = c(&quot;1&quot;, &quot;.5&quot;, &quot;0&quot;)) + coord_flip() + facet_wrap(~ X_s, ncol = 7, strip.position = &quot;bottom&quot;) The X_s values are depicted in the panel strips on the bottom. The response probabilities are scaled based on the axis on the top. The points and leftmost sides of the bars are the posterior means. The thin, dark horizontal lines are the percentile-based 95% intervals. Here we reformat f a little more to put those bar-and-error plots in a format more similar to that of Figure 23.7. f %&gt;% select(-Est.Error) %&gt;% # rescale the probability summaries mutate_at(vars(Estimate:Q97.5), ~. / 2) %&gt;% # plot! ggplot() + geom_vline(xintercept = seq(from = r[1], to = r[2], length.out = 5), color = sl[2]) + # bar marking the Estimate geom_segment(aes(x = X_s, xend = X_s - Estimate, y = Y + 0.1, yend = Y + 0.1), size = 8, color = sl[4]) + # bar marking the 95% interval geom_segment(aes(x = X_s - Q2.5, xend = X_s - Q97.5, y = Y + 0.2, yend = Y + 0.2), size = 1, color = sl[7]) + # data geom_point(data = my_data, aes(x = X_s, y = Y), shape = 1, size = 2, color = sl[9]) + scale_y_continuous(&quot;Y&quot;, breaks = 1:7, limits = c(0.5, 7.5)) + coord_cartesian(xlim = c(-2.4, 2.4)) I’m not going to attempt superimposing fitted lines on this plot the way Kruschke did. Given that our ordered-probit model is nonlinear on the scale of the criterion, it seems misleading to present linear fitted lines atop the raw data. If you’d like to do so, you’re on your own. Now here’s the corresponding model is we treat the y data as metric with tricks from Chapter 17. sd_x &lt;- sd(my_data$X) sd_y &lt;- sd(my_data$Y) m_x &lt;- mean(my_data$X) m_y &lt;- mean(my_data$Y) beta_0_sigma &lt;- 10 * abs(m_x * sd_y / sd_x) beta_1_sigma &lt;- 10 * abs(sd_y / sd_x) stanvars &lt;- stanvar(beta_0_sigma, name = &quot;beta_0_sigma&quot;) + stanvar(beta_1_sigma, name = &quot;beta_1_sigma&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) fit23.10 &lt;- brm(data = my_data, family = gaussian, Y ~ 1 + X, prior = c(prior(normal(0, beta_0_sigma), class = Intercept), prior(normal(0, beta_1_sigma), class = b), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.10&quot;) It may not have been readily apparent from Kruschke’s prose in the note for Figure 23.7, but his OLS model was based on the fully unstandardized data (i.e., using X as the predictor), not the partially standardized data he used in his JAGS code from 23.4.1. We followed the same sensibilities for fit23.10. Speaking of which, here are the summaries for the marginal posteriors. posterior_summary(fit23.10)[1:3, ] %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -5.424 0.623 -6.645 -4.219 ## b_X 6.711 0.440 5.857 7.557 ## sigma 1.530 0.077 1.389 1.693 These values are very close to those he reported at the bottom of page 690. Here are what the fitted lines from that model look like when superimposed on the data, when presuming both variables are metric. set.seed(23) conditional_effects(fit23.10, spaghetti = TRUE, ndraws = 100) %&gt;% plot(points = T, line_args = list(size = 0), point_args = list(alpha = 1/3, color = sl[9]), spaghetti_args = list(colour = alpha(sl[6], .2))) For the next example, we’ll load the HappinessAssetsDebt.csv data from Shi (2009). my_data &lt;- read_csv(&quot;data.R/HappinessAssetsDebt.csv&quot;) glimpse(my_data) ## Rows: 6,759 ## Columns: 3 ## $ Happiness &lt;dbl&gt; 3, 3, 3, 3, 1, 3, 2, 2, 4, 2, 3, 5, 3, 3, 4, 3, 3, 2, 4, 3, 4, 3, 3, 3, 5, 4, 3, 4, 4, 4, … ## $ Assets &lt;dbl&gt; 0, 10000, 30000, 40000, 21000, 20000, 20000, 0, 0, 20000, 5000, 30000, 40000, 5500, 50000,… ## $ Debt &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32000, 0, 0, 0,… Here’s a quick scatter plot of the data. To help with the overplotting, the points have been horizontally jittered a bit. But as indicated in the text, Happiness is a discrete variable. my_data %&gt;% ggplot(aes(x = Assets, y = Happiness)) + geom_jitter(alpha = 1/4, height = .25, color = sl[9]) + scale_x_continuous(expand = expansion(0, 0.05)) Standardize our predictor. my_data &lt;- my_data %&gt;% mutate(Assets_s = (Assets - mean(Assets)) / sd(Assets)) Fit the model like before. fit23.11 &lt;- brm(data = my_data, family = cumulative(probit), Happiness ~ 1 + Assets_s, prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.11&quot;) Check the summary. print(fit23.11) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Happiness ~ 1 + Assets_s ## Data: my_data (Number of observations: 6759) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -2.03 0.03 -2.10 -1.96 1.00 5775 5193 ## Intercept[2] -1.17 0.02 -1.20 -1.13 1.00 8414 6710 ## Intercept[3] -0.15 0.02 -0.18 -0.12 1.00 8713 6906 ## Intercept[4] 1.48 0.02 1.44 1.53 1.00 8483 7241 ## Assets_s 0.15 0.01 0.12 0.17 1.00 7875 4749 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 NA NA NA ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract the posterior draws and compare the brms parameterization of \\(\\theta_{[i]}\\) with the parameterization in the text in an expanded version of the bottom panel of Figure 23.8. draws &lt;- as_draws_df(fit23.11) draws %&gt;% select(`b_Intercept[1]`:`b_Intercept[4]`, .draw) %&gt;% compare_thresholds(lb = 1.5, ub = 4.5) Behold the marginal distribution of b_Assets_s, our effect size for Assets. draws %&gt;% ggplot(aes(x = b_Assets_s, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8]) + scale_y_continuous(NULL, breaks = NULL) Here’s the fitted()-oriented preparatory work for our version of the top panel of Figure 23.8. # define the range for the predictor r &lt;- range(my_data$Assets_s) # re-define the new data nd &lt;- tibble(Assets_s = seq(from = r[1], to = r[2], length.out = 5)) # compute the fitted summaries f &lt;- fitted(fit23.11, newdata = nd) # rearrange the output f &lt;- rbind( f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5] ) %&gt;% # wrangle data.frame() %&gt;% bind_cols(expand(nd, Happiness = 1:5, Assets_s)) # examine head(f) ## Estimate Est.Error Q2.5 Q97.5 Happiness Assets_s ## 1 2.615146e-02 2.121289e-03 2.216336e-02 3.046707e-02 1 -0.5971712 ## 2 3.166490e-03 7.149115e-04 1.979798e-03 4.776908e-03 1 4.7565107 ## 3 2.339116e-04 1.311017e-04 7.097467e-05 5.596200e-04 1 10.1101925 ## 4 1.142924e-05 1.324943e-05 1.089479e-06 4.498606e-05 1 15.4638743 ## 5 4.138155e-07 1.012777e-06 7.215254e-09 2.390645e-06 1 20.8175561 ## 6 1.146798e-01 4.187883e-03 1.066564e-01 1.230192e-01 2 -0.5971712 Like with the same variant from Figure 23.7, we will not be superimposing linear fitted lines. The model is nonlinear on the scale of the data and I don’t want to confuse readers by pretending otherwise. f %&gt;% select(-Est.Error) %&gt;% # rescale the probability summaries mutate_at(vars(Estimate:Q97.5), ~. * 2.5) %&gt;% # plot! ggplot() + geom_vline(xintercept = seq(from = r[1], to = r[2], length.out = 5), color = sl[2]) + # bar marking the Estimate geom_segment(aes(x = Assets_s, xend = Assets_s - Estimate, y = Happiness + 0.1, yend = Happiness + 0.1), size = 8, color = sl[4]) + # bar marking the 95% interval geom_segment(aes(x = Assets_s - Q2.5, xend = Assets_s - Q97.5, y = Happiness + 0.2, yend = Happiness + 0.2), size = 1, color = sl[7]) + # data geom_point(data = my_data, aes(x = Assets_s, y = Happiness), shape = 1, size = 2, color = sl[9]) + scale_y_continuous(&quot;Happiness&quot;, breaks = 1:5, limits = c(0.5, 5.5)) + coord_cartesian(xlim = c(-4, 24)) Now here’s the corresponding model is we treat Happiness as metric. Unlike our method for the corresponding model from Figure 23.7, fit10, we will use the standardized version of the predictor, Assets_s. The unstandardized values for Happiness and Assets are on vastly different scales, which can be difficulty for HMC with broad priors of the type Kruschke often uses. Standardizing the predictor helps. sd_y &lt;- sd(my_data$Happiness) stanvars &lt;- stanvar(sd_y, name = &quot;sd_y&quot;) fit23.12 &lt;- brm(data = my_data, family = gaussian, Happiness ~ 1 + Assets_s, prior = c(prior(normal(3.5, 5), class = Intercept), prior(normal(0, 5), class = b), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.12&quot;) Here are the summaries for the marginal posteriors. posterior_summary(fit23.12)[1:3, ] %&gt;% round(digits = 6) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 3.484241 0.010519 3.463676 3.505171 ## b_Assets_s 0.116069 0.010209 0.095848 0.136378 ## sigma 0.847126 0.007192 0.833087 0.861646 They’re just a bit different from those produced by OLS. Here are what the fitted lines from that model look like when superimposed on the data, when presuming both variables are metric. set.seed(23) conditional_effects(fit23.12, spaghetti = TRUE, ndraws = 100) %&gt;% plot(points = T, line_args = list(size = 0), point_args = list(alpha = 1/3, color = sl[9]), spaghetti_args = list(colour = alpha(sl[6], .2))) 23.4.3 Example: Movies–They don’t make ’em like they used to. For this section, we’ll load the Moore (2006) Movies.csv data. my_data &lt;- read_csv(&quot;data.R/Movies.csv&quot;) glimpse(my_data) ## Rows: 100 ## Columns: 6 ## $ Title &lt;chr&gt; &quot;A_Ticklish_Affair&quot;, &quot;Action_in_the_North_Atlantic&quot;, &quot;And_the_Ship_Sails_On&quot;, &quot;Autumn_So… ## $ Year &lt;dbl&gt; 1963, 1943, 1984, 1978, 1931, 1930, 1950, 1989, 1940, 1947, 1970, 1940, 1976, 1985, 1945… ## $ Length &lt;dbl&gt; 89, 127, 138, 97, 77, 69, 93, 119, 70, 69, 101, 62, 97, 85, 62, 86, 112, 97, 93, 89, 90,… ## $ Cast &lt;dbl&gt; 5, 7, 7, 5, 6, 8, 5, 8, 9, 9, 9, 6, 10, 10, 9, 6, 10, 6, 12, 7, 5, 9, 6, 7, 6, 6, 12, 11… ## $ Rating &lt;dbl&gt; 2.0, 3.0, 3.0, 3.0, 2.5, 2.5, 3.0, 2.5, 2.5, 2.0, 3.0, 2.0, 2.5, 1.0, 1.5, 2.5, 3.0, 2.0… ## $ Description &lt;dbl&gt; 7, 9, 15, 11, 7, 10, 8, 15, 8, 8, 11, 10, 12, 13, 9, 7, 10, 11, 11, 8, 9, 9, 13, 9, 7, 7… In footnote #5 at the bottom of page 693, Kruschke explained that whereas the original Ratings data ranged from 1.0 to 4.0 in half-unit increments, he recoded them to range from 1 to 7. Here we recode Ratings in the same way using dplyr::recode(). While we’re at it, we’ll make standardized versions of the predictors, too. my_data &lt;- my_data %&gt;% mutate(Rating = recode(Rating, `1.0` = 1, `1.5` = 2, `2.0` = 3, `2.5` = 4, `3.0` = 5, `3.5` = 6, `4.0` = 7), Year_s = (Year - mean(Year)) / sd(Year), Length_s = (Length - mean(Length)) / sd(Length)) Here’s a scatter plot of the data, with points colored by Rating. my_data %&gt;% mutate(Rating = factor(Rating)) %&gt;% ggplot(aes(x = Year, y = Length, label = Rating)) + geom_point(aes(color = Rating), size = 3) + geom_text(size = 3) + scale_color_scico_d(palette = &quot;lajolla&quot;, begin = .15, end = .7) Fitting the multivariable ordered-probit model with brms is about as simple as fitting any other multivariable model. Just tack on predictors with the + operator. fit23.13 &lt;- brm(data = my_data, family = cumulative(probit), Rating ~ 1 + Year_s + Length_s, prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.13&quot;) Check the model summary. print(fit23.13) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Rating ~ 1 + Year_s + Length_s ## Data: my_data (Number of observations: 100) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -1.68 0.21 -2.10 -1.30 1.00 6902 6069 ## Intercept[2] -0.91 0.15 -1.22 -0.62 1.00 9102 7020 ## Intercept[3] -0.22 0.13 -0.47 0.04 1.00 8825 7107 ## Intercept[4] 0.61 0.14 0.34 0.88 1.00 8576 7290 ## Intercept[5] 1.69 0.20 1.31 2.09 1.00 8454 7078 ## Intercept[6] 2.58 0.38 1.93 3.42 1.00 9942 6206 ## Year_s -0.48 0.13 -0.73 -0.24 1.00 7125 6109 ## Length_s 0.62 0.13 0.36 0.87 1.00 7056 5731 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 NA NA NA ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract the posterior draws and compare the brms parameterization of \\(\\theta_{[i]}\\) with the parameterization in the text in an expanded version of the bottom panel of Figure 23.9. draws &lt;- as_draws_df(fit23.13) draws %&gt;% select(`b_Intercept[1]`:`b_Intercept[6]`, .draw) %&gt;% compare_thresholds(lb = 1.5, ub = 6.5) To sate any curiosity, here are the correlations among the parameters. # wrangle as_draws_df(fit23.13) %&gt;% select(contains(&quot;Intercept&quot;), b_Year_s, b_Length_s) %&gt;% set_names(str_c(&quot;theta[&quot;, 1:6, &quot;]&quot;), str_c(&quot;beta[&quot;, 1:2, &quot;]&quot;)) %&gt;% # plot! ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = label_parsed) + ggtitle(&quot;These parameters have milder correlations&quot;) Now focus on the marginal distribution of our two effect-size parameters. draws %&gt;% pivot_longer(ends_with(&quot;_s&quot;)) %&gt;% mutate(name = factor(name, levels = c(&quot;b_Year_s&quot;, &quot;b_Length_s&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;effect size&quot;) + facet_wrap(~ name, scales = &quot;free&quot;) Before we make the top panel from Figure 23.9, I’d like to wander a bit and look at something related. We’ll use fitted(). # define the new data nd &lt;- crossing(Year_s = seq(from = -3, to = 3, by = 0.25), Length_s = seq(from = -3, to = 3, by = 0.25)) # compute the `Response` probabilities f &lt;- fitted(fit23.13, newdata = nd) # rearrange the output f &lt;- rbind( f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5], f[, , 6], f[, , 7] ) %&gt;% # wrangle data.frame() %&gt;% bind_cols( nd %&gt;% expand(Rating = 1:7, nesting(Year_s, Length_s)) ) # what did we do? head(f) ## Estimate Est.Error Q2.5 Q97.5 Rating Year_s Length_s ## 1 0.11672604 0.07879391 0.018835945 0.31606704 1 -3 -3.00 ## 2 0.09038339 0.06469596 0.012824934 0.25784313 1 -3 -2.75 ## 3 0.06877638 0.05226785 0.008791097 0.20515430 1 -3 -2.50 ## 4 0.05146467 0.04162030 0.005882207 0.16194457 1 -3 -2.25 ## 5 0.03790478 0.03273187 0.003797599 0.12640909 1 -3 -2.00 ## 6 0.02751017 0.02548193 0.002330570 0.09644921 1 -3 -1.75 We just computed the response probabilities across the two-dimensional grid of the predictor values. Now plot using the posterior means. f %&gt;% mutate(strip = str_c(&quot;Rating: &quot;, Rating)) %&gt;% ggplot(aes(x = Year_s, y = Length_s)) + geom_raster(aes(fill = Estimate), interpolate = T) + geom_text(data = my_data %&gt;% mutate(strip = str_c(&quot;Rating: &quot;, Rating)), aes(label = Rating), color = &quot;white&quot;, size = 2.5) + scale_fill_scico(palette = &#39;lajolla&#39;, direction = -1, limits = c(0, 1), breaks = c(0, .5, 1), labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + scale_x_continuous(breaks = seq(from = -2, to = 2, by = 2), expand = c(0, 0)) + scale_y_continuous(breaks = seq(from = -2, to = 2, by = 2), expand = c(0, 0)) + theme(legend.position = &quot;bottom&quot;) + facet_wrap(~ strip, nrow = 1) This model didn’t do a great job capturing the Response probabilities. If you’re curious, you’ll find you can do a little bit better if you allow the two predictors to interact (i.e., add + Year_s:Length_s to the formula line). Even then, the model isn’t great. I leave that as an exercise for the interested reader. For this model, however, we will follow Kruschke and make a more faithful version of the top panel of Figure 23.9. We’ll need to wrangle our draws data a bit to get things ready. Here’s the work. draws &lt;- draws %&gt;% # we just need the data from three steps in the HMC chain slice(1:3) %&gt;% mutate(.draw = .draw %&gt;% as.factor(), b1 = b_Year_s, b2 = b_Length_s) %&gt;% expand(nesting(.draw, b1, b2, `b_Intercept[1]`, `b_Intercept[2]`, `b_Intercept[3]`, `b_Intercept[4]`, `b_Intercept[5]`, `b_Intercept[6]`), # because these are straight lines, two extreme x1-values are all we need x1 = c(-10, 10)) %&gt;% pivot_longer(contains(&quot;[&quot;), names_to = &quot;theta&quot;) %&gt;% # use Kruschke&#39;s Formula 23.5 mutate(x2 = (value / b2) + (-b1 / b2) * x1, # this just renames our x variables for easy plotting Year_s = x1, Length_s = x2) glimpse(draws) ## Rows: 36 ## Columns: 9 ## $ .draw &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3… ## $ b1 &lt;dbl&gt; -0.5207952, -0.5207952, -0.5207952, -0.5207952, -0.5207952, -0.5207952, -0.5207952, -0.5207… ## $ b2 &lt;dbl&gt; 0.6693850, 0.6693850, 0.6693850, 0.6693850, 0.6693850, 0.6693850, 0.6693850, 0.6693850, 0.6… ## $ x1 &lt;dbl&gt; -10, -10, -10, -10, -10, -10, 10, 10, 10, 10, 10, 10, -10, -10, -10, -10, -10, -10, 10, 10,… ## $ theta &lt;chr&gt; &quot;b_Intercept[1]&quot;, &quot;b_Intercept[2]&quot;, &quot;b_Intercept[3]&quot;, &quot;b_Intercept[4]&quot;, &quot;b_Intercept[5]&quot;, &quot;… ## $ value &lt;dbl&gt; -1.4117774, -0.9005644, -0.3276923, 0.6500697, 1.7471252, 2.7113437, -1.4117774, -0.9005644… ## $ x2 &lt;dbl&gt; -9.889270, -9.125564, -8.269746, -6.809059, -5.170158, -3.729704, 5.671137, 6.434843, 7.290… ## $ Year_s &lt;dbl&gt; -10, -10, -10, -10, -10, -10, 10, 10, 10, 10, 10, 10, -10, -10, -10, -10, -10, -10, 10, 10,… ## $ Length_s &lt;dbl&gt; -9.889270, -9.125564, -8.269746, -6.809059, -5.170158, -3.729704, 5.671137, 6.434843, 7.290… Now just plot. draws %&gt;% ggplot(aes(x = Year_s, y = Length_s)) + geom_line(aes(group = interaction(.draw, theta), color = theta, linetype = .draw)) + geom_text(data = my_data, aes(label = Rating)) + scale_color_scico_d(expression(theta), palette = &quot;lajolla&quot;, begin = .25, labels = 1:6) + coord_cartesian(xlim = range(my_data$Year_s), ylim = range(my_data$Length_s)) It might be easier to see Kruschke’s main point if we facet by .draw. draws %&gt;% ggplot(aes(x = Year_s, y = Length_s)) + geom_line(aes(group = interaction(.draw, theta), color = theta, linetype = .draw)) + geom_text(data = my_data, aes(label = Rating)) + scale_color_scico_d(expression(theta), palette = &quot;lajolla&quot;, begin = .25, labels = 1:6) + coord_cartesian(xlim = range(my_data$Year_s), ylim = range(my_data$Length_s)) + theme(legend.direction = &quot;horizontal&quot;, legend.position = c(.75, .25)) + facet_wrap(~ .draw, ncol = 2) Both our versions of the plot show what Kruschke pointed out in the text: Threshold lines from the same step in the chain must be parallel because the regression coefficients are constant at that step, but are different at another step. The threshold lines in Figure 23.9 are level contours on the underlying metric planar surface, and the lines reveal that the ratings increase toward the top left, that is, as \\(x_1\\) decreases and \\(x_2\\) increases. (p. 693) Before we move on to the next section, what these diagonal 2-dimensional threshold lines also hint at is that when we use two predictors to describe ordinal data as having been produces by an underlying unit Gaussian distribution, that underlying distribution is actually bivariate Gaussian. Here we’ll use fitted() one more time to depict that bivariate-Gaussian distribution with a little geom_raster(). # define the new data nd &lt;- crossing(Year_s = seq(from = -5, to = 5, by = 0.1), Length_s = seq(from = -5, to = 5, by = 0.1)) fitted(fit23.13, newdata = nd, # this will yield z-scores scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # convert the z-scores to density values mutate(density = dnorm(Estimate, 0, 1)) %&gt;% ggplot(aes(x = Year_s, y = Length_s)) + geom_raster(aes(fill = density), interpolate = T) + geom_text(data = my_data, aes(label = Rating), size = 2.5) + scale_fill_scico(&quot;density&quot;, palette = &#39;lajolla&#39;, direction = -1, limits = c(0, 0.4)) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) As with many of our previous approaches with geom_raster(), this plot is based on the posterior means in each cell and, therefore, does a poor job depicting the uncertainty in the posterior distribution. 23.4.4 Why are some thresholds outside the data? Now load Kruschke’s simulated data. my_data &lt;- read_csv(&quot;data.R/OrdinalProbitData-Movies.csv&quot;) glimpse(my_data) ## Rows: 400 ## Columns: 3 ## $ Year &lt;dbl&gt; 1959, 1946, 1964, 1938, 1946, 1971, 1957, 1970, 1968, 1962, 1929, 1962, 1978, 1972, 1964, 195… ## $ Length &lt;dbl&gt; 88, 117, 130, 85, 111, 105, 93, 119, 78, 77, 138, 107, 70, 60, 138, 122, 72, 109, 71, 61, 60,… ## $ Rating &lt;dbl&gt; 4, 5, 5, 4, 5, 4, 3, 4, 3, 3, 7, 4, 2, 2, 5, 5, 3, 4, 3, 2, 2, 3, 5, 3, 5, 5, 5, 4, 5, 4, 4, … These data imitate the movie ratings, but with two key differences. First and foremost, the artificial data have much smaller noise, with \\(\\sigma = 0.20\\) as opposed to \\(\\sigma \\approx 1.25\\) in the real data. Second, the artificial data have points that span the entire range of both predictors, unlike the real data which had points mostly in the central region. (p. 695) Like with the real movie data, we’ll inspect these data with a colored scatter plot. my_data %&gt;% mutate(Rating = factor(Rating)) %&gt;% ggplot(aes(x = Year, y = Length, label = Rating)) + geom_point(aes(color = Rating), size = 3) + geom_text(size = 3) + scale_color_scico_d(palette = &quot;lajolla&quot;, begin = .15, end = .7) Unlike in last section, there appears to be a clear trend in Kruschke’s simulated data. Kruschke’s simulated critic liked his movies old and long. Time to standardize the predictors. my_data &lt;- my_data %&gt;% mutate(Year_s = (Year - mean(Year)) / sd(Year), Length_s = (Length - mean(Length)) / sd(Length)) Fitting the multivariable ordered-probit model with brms is about as simple as fitting any other multivariable model. Just tack on predictors with the + operator. fit23.14 &lt;- update(fit23.13, newdata = my_data, iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.14&quot;) ## The desired updates require recompiling the model Check the model summary. print(fit23.14) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Rating ~ 1 + Year_s + Length_s ## Data: my_data (Number of observations: 400) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -9.75 0.66 -11.06 -8.50 1.00 1658 2444 ## Intercept[2] -5.95 0.42 -6.79 -5.16 1.00 1989 2629 ## Intercept[3] -2.24 0.21 -2.67 -1.83 1.00 3268 4870 ## Intercept[4] 2.42 0.21 2.01 2.83 1.00 2655 4703 ## Intercept[5] 7.77 0.54 6.73 8.88 1.00 1797 2757 ## Intercept[6] 10.85 0.76 9.42 12.42 1.00 1789 2496 ## Year_s -2.79 0.20 -3.19 -2.42 1.00 1826 2438 ## Length_s 4.72 0.31 4.14 5.35 1.00 1595 2515 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 NA NA NA ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract the posterior draws and use compare_thresholds() to make our expanded version of the bottom panel of Figure 23.10. draws &lt;- as_draws_df(fit23.14) draws %&gt;% select(`b_Intercept[1]`:`b_Intercept[6]`, .draw) %&gt;% compare_thresholds(lb = 1.5, ub = 6.5) Make the marginal distribution of our two effect-size parameters. draws %&gt;% pivot_longer(ends_with(&quot;_s&quot;)) %&gt;% mutate(name = factor(name, levels = c(&quot;b_Year_s&quot;, &quot;b_Length_s&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = sl[4], color = sl[8], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;effect size&quot;) + facet_wrap(~ name, scales = &quot;free&quot;) Make the top panel for Figure 23.10 just like we did for its analogue in Figure 23.9. # extract the posterior draws and wrangle as_draws_df(fit23.14) %&gt;% slice(1:3) %&gt;% mutate(.draw = .draw %&gt;% as.factor(), b1 = b_Year_s, b2 = b_Length_s) %&gt;% expand(nesting(.draw, b1, b2, `b_Intercept[1]`, `b_Intercept[2]`, `b_Intercept[3]`, `b_Intercept[4]`, `b_Intercept[5]`, `b_Intercept[6]`), x1 = c(-10, 10)) %&gt;% pivot_longer(contains(&quot;[&quot;), names_to = &quot;theta&quot;) %&gt;% mutate(x2 = (value / b2) + (-b1 / b2) * x1, Year_s = x1, Length_s = x2) %&gt;% # plot! ggplot(aes(x = Year_s, y = Length_s)) + geom_line(aes(group = interaction(.draw, theta), color = theta, linetype = .draw)) + geom_text(data = my_data, aes(label = Rating)) + scale_color_scico_d(expression(theta), palette = &quot;lajolla&quot;, begin = .25, labels = 1:6) + coord_cartesian(xlim = range(my_data$Year_s), ylim = range(my_data$Length_s)) Those are some tight thresholds. They “very clearly cleave parallel regions of distinct ordinal values. The example demonstrates that the threshold lines do make intuitive sense when there is low noise and a broad range of data” (p. 695, emphasis in the original). With our various bonus plots, we’ve been anticipating Figure 23.11 for some time, now. The thresholds from fit23.14 result in beautifully nonlinear probability curves for the Rating levels. Take a quick look with conditional_effects(). ce &lt;- conditional_effects(fit23.14, categorical = T) plot(ce, plot = FALSE)[[1]] + scale_fill_scico_d(palette = &quot;lajolla&quot;, begin = .25) + scale_color_scico_d(palette = &quot;lajolla&quot;, begin = .25) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) plot(ce, plot = FALSE)[[2]] + scale_fill_scico_d(palette = &quot;lajolla&quot;, begin = .25) + scale_color_scico_d(palette = &quot;lajolla&quot;, begin = .25) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) Because the model had two predictors, we got two plots. What brms::conditional_effects() called Probability on the \\(y\\)-axis is the same as what Kruschke called \\(p(y)\\) on his. Rather than generic predictors \\(x\\) on the \\(x\\)-axis, our plots had either Year_s or Length_s. Whereas Kruschke marked off his different outcomes by line styles, ours were marked by color. Since we don’t have the data Kruschke used to make Figure 23.11, we won’t be able to reproduce it exactly. However, you’ll note that our plot for Length_s corresponded nicely with his subplot on the top (i.e., the one for which \\(\\sigma = 0.1\\)). If we set effects = \"Length_s\", we can use conditional_effects() to make a similar plot to Kruschke’s subplot for which \\(\\sigma = 1\\). ce &lt;- conditional_effects(fit23.13, categorical = T, effects = &quot;Length_s&quot;) plot(ce, plot = FALSE)[[1]] + scale_fill_scico_d(palette = &quot;lajolla&quot;, begin = .25) + scale_color_scico_d(palette = &quot;lajolla&quot;, begin = .25) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) “You can see that each outcome has maximum probability within its corresponding interval, but there is considerable smearing of outcomes into adjacent intervals” (p. 695). Finishing off, Kruschke’s discussion in the text referred to \\(\\sigma\\) as “noise” merely for linguistic ease. Calling the outcomes “noisy” does not mean the underlying generator of the outcomes is inherently wildly random. The “noise” is merely variation in the outcome that cannot be accounted for by the particular model we have chosen with the particular predictors we have chosen. A different model and/or different predictors might account for the outcomes well with little residual noise. In this sense, the noise is in the model, not in the data. (p. 698, emphasis in the original) Through this lens, noisy-looking data are a symptom of weak theory and/or poor data-collection methods. 23.5 Posterior prediction The cumulative-normal model makes posterior predictions for the probabilities of the \\(K\\) categories in the criterion variable by computing \\(p (y | \\mu (x), \\sigma, \\{ \\theta_k \\} )\\) in each step in the HMC chain. In this equation, \\(\\mu (x) = \\beta_0 + \\sum_j \\beta_j x_j\\). Though recall that with our brms parameterization, we have \\(\\beta_0\\) fixed at 0. Kruschke framed part of his discussion in this chapter in terms of a single-predictor model, such as was entertained in Figure 23.8. Recall that corresponds to our fit11. Here’s that formula. fit23.11$formula ## Happiness ~ 1 + Assets_s With brms, you can get this information with fitted(). Let’s say we wanted to focus on response probabilities for Assets_s = -1). Here’s what we get. fitted(fit23.11, newdata = tibble(Assets_s = -1)) ## , , P(Y = 1) ## ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.03000513 0.002463134 0.02536485 0.0349928 ## ## , , P(Y = 2) ## ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.1246466 0.00481312 0.1154048 0.1341927 ## ## , , P(Y = 3) ## ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.3434908 0.006255248 0.331182 0.3557538 ## ## , , P(Y = 4) ## ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.450522 0.006947555 0.4367891 0.4641612 ## ## , , P(Y = 5) ## ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.0513355 0.002927854 0.04576673 0.05707253 As is typical of brms, those probability summaries were in terms of the posterior mean and percentile-based 95% intervals. If you’re like Kruschke and prefer posterior modes and HDIs, you’ll need to set summary = F and wrangle a bit. f &lt;- fitted(fit23.11, newdata = tibble(Assets_s = -1), summary = F) cbind( f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5] ) %&gt;% data.frame() %&gt;% set_names(str_c(&quot;p(Happiness = &quot;, 1:5, &quot; | Assets_s = -1)&quot;)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 5 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 p(Happiness = 1 | Assets_s = -1) 0.03 0.0254 0.035 0.95 mode hdi ## 2 p(Happiness = 2 | Assets_s = -1) 0.124 0.116 0.134 0.95 mode hdi ## 3 p(Happiness = 3 | Assets_s = -1) 0.343 0.331 0.356 0.95 mode hdi ## 4 p(Happiness = 4 | Assets_s = -1) 0.450 0.437 0.464 0.95 mode hdi ## 5 p(Happiness = 5 | Assets_s = -1) 0.0514 0.0456 0.0568 0.95 mode hdi 23.6 Generalizations and extensions In this section, Kruschke mentioned extensions of this class of models might include using the cumulative \\(t\\) function to handle outliers or adding a guessing parameter. Full disclosure: I have not fit models like these. Based on my knowledge of brms, I suspect they’re possible. For insights how, you might review Bürkner’s (2022) Define custom response distributions with brms and his (2022) Estimating non-linear models with brms vignettes. In addition, there are other likelihoods one might use to model ordinal data using brms. Your first stop should be Bürkner and Vourre’s (2019) Ordinal regression models in psychology: A Tutorial, where, in addition to the cumulative normal model, they cover the sequential and adjacent category models. You might also check out Chapter 11 of my (2020) ebook recoding McElreath’s Statistical rethinking, wherein I show how one might use the logit link (i.e., family = cumulative(logit)) to fit ordered-categorical models with brms. If you’d just like more practice with the cumulative probit model, you might check out my blog post called Notes on the Bayesian cumulative probit. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] GGally_2.1.2 tidybayes_3.0.2 brms_2.18.0 Rcpp_1.0.9 patchwork_1.1.2 scico_1.3.1 ## [7] forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 ## [13] tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 igraph_1.3.4 svUnit_1.0.6 ## [6] splines_4.2.0 crosstalk_1.2.0 TH.data_1.1-1 rstantools_2.2.0 inline_0.3.19 ## [11] digest_0.6.30 htmltools_0.5.3 fansi_1.0.3 magrittr_2.0.3 checkmate_2.1.0 ## [16] googlesheets4_1.0.1 tzdb_0.3.0 modelr_0.1.8 RcppParallel_5.1.5 matrixStats_0.62.0 ## [21] vroom_1.5.7 xts_0.12.1 sandwich_3.0-2 prettyunits_1.1.1 colorspace_2.0-3 ## [26] rvest_1.0.2 ggdist_3.2.0 haven_2.5.1 xfun_0.35 callr_3.7.3 ## [31] crayon_1.5.2 jsonlite_1.8.3 lme4_1.1-31 survival_3.4-0 zoo_1.8-10 ## [36] glue_1.6.2 gtable_0.3.1 gargle_1.2.0 emmeans_1.8.0 distributional_0.3.1 ## [41] pkgbuild_1.3.1 rstan_2.21.7 abind_1.4-5 scales_1.2.1 mvtnorm_1.1-3 ## [46] DBI_1.1.3 miniUI_0.1.1.1 xtable_1.8-4 HDInterval_0.2.2 bit_4.0.4 ## [51] stats4_4.2.0 StanHeaders_2.21.0-7 DT_0.24 htmlwidgets_1.5.4 httr_1.4.4 ## [56] threejs_0.3.3 RColorBrewer_1.1-3 arrayhelpers_1.1-0 posterior_1.3.1 ellipsis_0.3.2 ## [61] reshape_0.8.9 pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 sass_0.4.2 ## [66] dbplyr_2.2.1 utf8_1.2.2 tidyselect_1.1.2 labeling_0.4.2 rlang_1.0.6 ## [71] reshape2_1.4.4 later_1.3.0 munsell_0.5.0 cellranger_1.1.0 tools_4.2.0 ## [76] cachem_1.0.6 cli_3.5.0 generics_0.1.3 broom_1.0.1 ggridges_0.5.3 ## [81] evaluate_0.18 fastmap_1.1.0 processx_3.8.0 knitr_1.40 bit64_4.0.5 ## [86] fs_1.5.2 nlme_3.1-159 projpred_2.2.1 mime_0.12 xml2_1.3.3 ## [91] compiler_4.2.0 bayesplot_1.9.0 shinythemes_1.2.0 rstudioapi_0.13 gamm4_0.2-6 ## [96] reprex_2.0.2 bslib_0.4.0 stringi_1.7.8 highr_0.9 ps_1.7.2 ## [101] Brobdingnag_1.2-8 lattice_0.20-45 Matrix_1.4-1 nloptr_2.0.3 markdown_1.1 ## [106] shinyjs_2.1.0 tensorA_0.36.2 vctrs_0.5.1 pillar_1.8.1 lifecycle_1.0.3 ## [111] jquerylib_0.1.4 bridgesampling_1.1-2 estimability_1.4.1 httpuv_1.6.5 R6_2.5.1 ## [116] bookdown_0.28 promises_1.2.0.1 gridExtra_2.3 codetools_0.2-18 boot_1.3-28 ## [121] colourpicker_1.1.1 MASS_7.3-58.1 gtools_3.9.3 assertthat_0.2.1 withr_2.5.0 ## [126] shinystan_2.6.0 multcomp_1.4-20 mgcv_1.8-40 parallel_4.2.0 hms_1.1.1 ## [131] grid_4.2.0 minqa_1.2.5 coda_0.19-4 rmarkdown_2.16 googledrive_2.0.0 ## [136] shiny_1.7.2 lubridate_1.8.0 base64enc_0.1-3 dygraphs_1.1.1.6 References Bürkner, P.-C. (2020). Bayesian item response modeling in R with brms and Stan. http://arxiv.org/abs/1905.09501 Bürkner, P.-C. (2022). Define custom response distributions with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html Bürkner, P.-C. (2022a). Estimating distributional models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html Bürkner, P.-C. (2022). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html Bürkner, P.-C., &amp; Vuorre, M. (2019). Ordinal regression models in psychology: A tutorial. Advances in Methods and Practices in Psychological Science, 2(1), 77–101. https://doi.org/10.1177/2515245918823199 Carifio, J., &amp; Perla, R. (2008). Resolving the 50-year debate around using and misusing Likert scales. Medical Education, 42(12), 1150–1152. https://doi.org/10.1111/j.1365-2923.2008.03172.x Carifio, J., &amp; Perla, R. J. (2007). Ten common misunderstandings, misconceptions, persistent myths and urban legends about Likert scales and Likert response formats and their antidotes. Journal of Social Sciences, 3(3), 106–116. https://thescipub.com/pdf/10.3844/jssp.2007.106.116.pdf Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Kurz, A. S. (2020). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 Likert, R. (1932). A technique for the measurement of attitudes. Archives of Psychology, 22 140, 55–55. https://legacy.voteview.com/pdf/Likert_1932.pdf Norman, G. (2010). Likert scales, levels of measurement and the “laws” of statistics. Advances in Health Sciences Education, 15(5), 625–632. https://doi.org/10.1007/s10459-010-9222-y Pedersen, Thomas Lin, &amp; Crameri, F. (2021). scico: Colour palettes based on the scientific colour-maps [Manual]. https://CRAN.R-project.org/package=scico "],["count-predicted-variable.html", "24 Count Predicted Variable 24.1 Poisson exponential model 24.2 Example: Hair eye go again 24.3 Example: Interaction contrasts, shrinkage, and omnibus test 24.4 Log-linear models for contingency tables Bonus: Alternative parameterization Session info", " 24 Count Predicted Variable Consider a situation in which we observe two nominal values for every item measured…. Across the whole sample, the result is a table of counts for each combination of values of the nominal variables. The counts are what we are trying to predict and the nominal variables are the predictors. This is the type of situation addressed in this chapter…. In the context of the generalized linear model (GLM) introduced in Chapter 15, this chapter’s situation involves a predicted value that is a count, for which we will use an inverse-link function that is exponential along with a Poisson distribution for describing noise in the data (Kruschke, 2015, pp. 703–704) 24.1 Poisson exponential model Following Kruschke, we will “refer to the model that will be explained in this section as Poisson exponential because, as we will see, the noise distribution is a Poisson distribution and the inverse-link function is exponential” (p. 704). 24.1.1 Data structure. Kruschke has the Snee (1974) data for Table 24.1 saved as the HairEyeColor.csv file. library(tidyverse) library(janitor) my_data &lt;- read_csv(&quot;data.R/HairEyeColor.csv&quot;) glimpse(my_data) ## Rows: 16 ## Columns: 3 ## $ Hair &lt;chr&gt; &quot;Black&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Blond&quot;, &quot;Blond&quot;, &quot;Blond&quot;, &quot;… ## $ Eye &lt;chr&gt; &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green&quot;, &quot;Hazel&quot;, &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green&quot;, &quot;Ha… ## $ Count &lt;dbl&gt; 20, 68, 5, 15, 94, 7, 16, 10, 84, 119, 29, 54, 17, 26, 14, 14 In order to retain some of the ordering in Table 24.1, we’ll want to make Hair a factor and recode() Brown as Brunette. my_data &lt;- my_data %&gt;% mutate(Hair = recode(Hair, &quot;Brown&quot; = &quot;Brunette&quot;) %&gt;% factor(., levels = c(&quot;Black&quot;, &quot;Brunette&quot;, &quot;Red&quot;, &quot;Blond&quot;))) Here’s a quick way to use pivot_wider() to make most of the table. my_data %&gt;% pivot_wider(names_from = Hair, values_from = Count) ## # A tibble: 4 × 5 ## Eye Black Blond Brunette Red ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Blue 20 94 84 17 ## 2 Brown 68 7 119 26 ## 3 Green 5 16 29 14 ## 4 Hazel 15 10 54 14 However, that didn’t get us the marginal totals. For those, we’ll uncount() the cells in the data and then make the full table with janitor::tabyl() and janitor::adorn_totals(). my_data %&gt;% uncount(weights = Count, .remove = F) %&gt;% tabyl(Eye, Hair) %&gt;% adorn_totals(c(&quot;row&quot;, &quot;col&quot;)) %&gt;% knitr::kable() Eye Black Brunette Red Blond Total Blue 20 84 17 94 215 Brown 68 119 26 7 220 Green 5 29 14 16 64 Hazel 15 54 14 10 93 Total 108 286 71 127 592 That last knitr::kable() line just formatted the output a bit. You’ll note that the cell counts from the last two code blocks are a little different from those Kruschke displayed in his Table 24.1. I’m pretty sure Kruschke mislabeled his column names, which it appears he has acknowledged in his Corrigenda. 24.1.2 Exponential link function. To analyze data like those above, a natural candidate for the needed likelihood distribution is the Poisson (described later), which takes a non-negative value \\(\\lambda\\) and gives a probability for each integer from zero to infinity. But this motivation may seem a bit arbitrary, even if there’s nothing wrong with it in principle. A different motivation starts by treating the cell counts as representative of underlying cell probabilities, and then asking whether the two nominal variables contribute independent influences to the cell probabilities. (p. 705). The additive model of cell counts of a table of rows \\(r\\) and columns \\(c\\) follows the form \\[\\lambda_{r, c} = \\exp (\\beta_0 + \\beta_r + \\beta_c),\\] where \\(\\lambda_{r, c}\\) is the tendency of counts within row \\(r\\) and column \\(c\\). In the case of an interaction model, the equation expands to \\[\\lambda_{r, c} = \\exp (\\beta_0 + \\beta_r + \\beta_c + \\beta_{r, c}),\\] with the following constraints: \\[\\begin{align*} \\sum_r \\beta_r = 0, &amp;&amp; \\sum_c \\beta_c = 0, &amp;&amp; \\sum_r \\beta_{r, c} = 0 \\text{ for all } c, &amp;&amp; \\text{and} &amp;&amp; \\sum_c \\beta_{r, c} = 0 \\text{ for all } r. \\end{align*}\\] 24.1.3 Poisson noise distribution. Simon-Denis Poisson’s distribution follows the form \\[p(y | \\lambda) = \\frac{\\lambda^y \\exp (-\\lambda)}{y!},\\] where \\(y\\) is a non-negative integer and \\(\\lambda\\) is a non-negative real number. The mean of the Poisson distribution is \\(\\lambda\\). Importantly, the variance of the Poisson distribution is also \\(\\lambda\\) (i.e., the standard deviation is \\(\\sqrt \\lambda\\)). Thus, in the Poisson distribution, the variance is completely yoked to the mean. (p. 707) We can work with that expression directly in base R. Here we use \\(\\lambda = 5.5\\) and \\(y = 2\\). lambda &lt;- 5.5 y &lt;- 2 lambda^y * exp(-lambda) / factorial(y) ## [1] 0.06181242 If we’d like to simulate from the Poisson distribution, we’d use the rpois() function. It takes two arguments, n and lambda. Let’s generate 1,000 draws based on \\(\\lambda = 5\\). set.seed(24) d &lt;- tibble(y = rpois(n = 1000, lambda = 5)) Here are the mean and variance. d %&gt;% summarise(mean = mean(y), variance = var(y)) ## # A tibble: 1 × 2 ## mean variance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4.98 5.17 They’re not exactly the same because of simulation variance, but they get that way real quick with a larger sample. set.seed(24) tibble(y = rpois(n = 100000, lambda = 5)) %&gt;% summarise(mean = mean(y), variance = var(y)) ## # A tibble: 1 × 2 ## mean variance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4.99 4.98 Let’s put rpois() to work and make Figure 24.1. Before we go any further, let’s talk color and theme. For this chapter, we’ll take our color palette from the ochRe package (Allan et al., 2021), which provides several color palettes inspired by the art, landscapes, and wildlife of Australia. For the plots to follow, we will use the \"healthy_reef\" palette. library(ochRe) scales::show_col(ochre_palettes[[&quot;healthy_reef&quot;]]) hr &lt;- ochre_palettes[[&quot;healthy_reef&quot;]] Our overall plot theme will be based on cowplot::theme_minimal_grid() with a few adjustments. library(cowplot) theme_set( theme_minimal_grid() + theme(legend.key = element_rect(fill = hr[2], color = hr[2]), panel.background = element_rect(fill = hr[2], color = hr[2]), panel.border = element_rect(color = hr[2]), panel.grid = element_blank()) ) Now we have our palette and theme, we’re ready to make our version of the Poisson histograms in Figure 24.1. crossing(y = 0:50, lambda = c(1.8, 8.3, 32.1)) %&gt;% mutate(d = dpois(y, lambda = lambda)) %&gt;% ggplot(aes(x = y, y = d)) + geom_col(fill = hr[3], color = hr[3], width = 0.5) + scale_y_continuous(&quot;p(y)&quot;, expand = expansion(mult = c(0, 0.05))) + facet_wrap(~ lambda, ncol = 1, labeller = label_bquote(dpois(y*&quot;|&quot;*lambda == .(lambda)))) For more on our labeller = label_bquote syntax, check out this. But anyway, given \\(\\lambda\\), the Poisson distribution gives the probabilities of specific non-negative integers. 24.1.4 The complete model and implementation in JAGS brms. To get a sense of Kruschke’s hierarchical Bayesian alternative to the conventional ANOVA approach, let’s make our version of the model diagram in Figure 24.2. library(patchwork) # bracket p1 &lt;- tibble(x = .95, y = .5, label = &quot;{_}&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 10, hjust = 1, color = hr[4], family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() ## plain arrow # save our custom arrow settings my_arrow &lt;- arrow(angle = 20, length = unit(0.35, &quot;cm&quot;), type = &quot;closed&quot;) p2 &lt;- tibble(x = .68, y = 1, xend = .68, yend = .25) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = hr[8]) + xlim(0, 1) + theme_void() # normal density p3 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = hr[2]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = hr[8]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;italic(M)[0]&quot;, &quot;italic(S)[0]&quot;), size = 7, color = hr[8], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = hr[8])) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. # second normal density p4 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = hr[2]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = hr[8]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;0&quot;, &quot;sigma[beta][1]&quot;), size = 7, color = hr[8], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = hr[8])) # third normal density p5 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = hr[2]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = hr[8]) + annotate(geom = &quot;text&quot;, x = c(0, 1.45), y = .6, hjust = c(.5, 0), label = c(&quot;0&quot;, &quot;sigma[beta][2]&quot;), size = 7, color = hr[8], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = hr[8])) # fourth normal density p6 &lt;- tibble(x = seq(from = -3, to = 3, by = .1)) %&gt;% ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) + geom_area(fill = hr[2]) + annotate(geom = &quot;text&quot;, x = 0, y = .2, label = &quot;normal&quot;, size = 7, color = hr[8]) + annotate(geom = &quot;text&quot;, x = 0, y = .6, hjust = .5, label = &quot;0&quot;, size = 7, color = hr[8], family = &quot;Times&quot;, parse = T) + scale_x_continuous(expand = c(0, 0)) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = hr[8])) # likelihood formula p7 &lt;- tibble(x = .5, y = .25, label = &quot;exp(beta[0]+sum()[italic(j)]*beta[1][&#39;[&#39;*italic(j)*&#39;]&#39;]*italic(x)[1][&#39;[&#39;*italic(j)*&#39;]&#39;](italic(i))+sum()[italic(k)]*beta[2][&#39;[&#39;*italic(k)*&#39;]&#39;]*italic(x)[2][&#39;[&#39;*italic(k)*&#39;]&#39;](italic(i))+sum()[italic(jk)]*beta[1%*%2][&#39;[&#39;*italic(jk)*&#39;]&#39;]*italic(x)[1%*%2][&#39;[&#39;*italic(jk)*&#39;]&#39;](italic(i)))&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 6.75, color = hr[8], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) + ylim(0, 1) + theme_void() # sigma p8 &lt;- tibble(x = .13, y = .6, label = &quot;sigma[beta][1%*%2]&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(hjust = 0, size = 7, color = hr[8], parse = T, family = &quot;Times&quot;) + scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) + ylim(0, 1) + theme_void() # four annotated arrows p9 &lt;- tibble(x = c(.05, .34, .64, .945), y = c(1, 1, 1, 1), xend = c(.06, .2, .47, .77), yend = c(0, 0, 0, 0)) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = hr[8]) + annotate(geom = &quot;text&quot;, x = c(.025, .24, .30, .53, .60, .83, .91), y = .5, label = c(&quot;&#39;~&#39;&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(j)&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(k)&quot;, &quot;&#39;~&#39;&quot;, &quot;italic(jk)&quot;), size = c(10, 10, 7, 10, 7, 10, 7), color = hr[8], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # Poisson density p10 &lt;- tibble(x = 0:9) %&gt;% ggplot(aes(x = x, y = .87 * (dpois(x, lambda = 3.5)) / max(dpois(x, lambda = 3.5))) ) + geom_col(color = hr[2], fill = hr[2], width = .45) + annotate(geom = &quot;text&quot;, x = 4.5, y = .2, label = &quot;Poisson&quot;, size = 7, color = hr[8]) + annotate(geom = &quot;text&quot;, x = 4.5, y = .89, label = &quot;lambda[italic(i)]&quot;, size = 7, color = hr[8], family = &quot;Times&quot;, parse = TRUE) + theme_void() + theme(axis.line.x = element_line(size = 0.5, color = hr[8])) # one annotated arrow p11 &lt;- tibble(x = .5, y = 1, xend = .5, yend = .35) %&gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow, color = hr[8]) + annotate(geom = &quot;text&quot;, x = .4, y = .725, label = &quot;&#39;=&#39;&quot;, size = 10, color = hr[8], family = &quot;Times&quot;, parse = T) + xlim(0, 1) + theme_void() # the final annotated arrow p12 &lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(&quot;&#39;~&#39;&quot;, &quot;italic(i)&quot;)) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), color = hr[8], parse = T, family = &quot;Times&quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow, color = hr[8]) + xlim(0, 1) + theme_void() # some text p13 &lt;- tibble(x = .5, y = .5, label = &quot;italic(y[i])&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, color = hr[8], parse = T, family = &quot;Times&quot;) + xlim(0, 1) + theme_void() # define the layout layout &lt;- c( area(t = 1, b = 1, l = 6, r = 7), area(t = 1, b = 1, l = 10, r = 11), area(t = 1, b = 1, l = 14, r = 15), area(t = 2, b = 3, l = 6, r = 7), area(t = 2, b = 3, l = 10, r = 11), area(t = 2, b = 3, l = 14, r = 15), area(t = 3, b = 4, l = 1, r = 3), area(t = 3, b = 4, l = 5, r = 7), area(t = 3, b = 4, l = 9, r = 11), area(t = 3, b = 4, l = 13, r = 15), area(t = 6, b = 7, l = 1, r = 15), area(t = 3, b = 4, l = 15, r = 16), area(t = 5, b = 6, l = 1, r = 15), area(t = 10, b = 11, l = 7, r = 9), area(t = 8, b = 10, l = 7, r = 9), area(t = 12, b = 12, l = 7, r = 9), area(t = 13, b = 13, l = 7, r = 9) ) # combine and plot! (p1 + p1 + p1 + p2 + p2 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10 + p11 + p12 + p13) + plot_layout(design = layout) &amp; ylim(0, 1) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) Using Kruschke’s method, the prior is supposed to be broad on the scale of the data, but we must be careful about what scale is being modeled by the baseline and deflections. The counts are being directly described by \\(\\lambda\\), but it is \\(\\log (\\lambda)\\) being described by the baseline and deflections. Thus, the prior on the baseline and deflections should be broad on the scale of the logarithm of the data. To establish a generic baseline, consider that if the data points were distributed equally among the cells, the mean count would be the total count divided by the number of cells. The biggest possible standard deviation across cells would occur when all the counts were loaded into a single cell and all the other cells were zero. (pp. 709–710, emphasis added) Before we show how to fit the model, we need the old gamma_a_b_from_omega_sigma() function. gamma_a_b_from_omega_sigma &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } Here are a few intermediate values before we set the stanvars. n_x1_level &lt;- length(unique(my_data$x1)) n_x2_level &lt;- length(unique(my_data$x2)) n_cell &lt;- nrow(my_data) Now we’re ready to define the stanvars. y_log_mean &lt;- log(sum(my_data$y) / (n_x1_level * n_x2_level)) y_log_sd &lt;- c(rep(0, n_cell - 1), sum(my_data$y)) %&gt;% sd() %&gt;% log() s_r &lt;- gamma_a_b_from_omega_sigma(mode = y_log_sd, sd = 2 * y_log_sd) stanvars &lt;- stanvar(y_log_mean, name = &quot;y_log_mean&quot;) + stanvar(y_log_sd, name = &quot;y_log_sd&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) You’d then fit a Poisson model with two nominal predictors using Kruschke’s hierarchical-shrinkage method like this. fit &lt;- brm(data = my_data, family = poisson, y ~ 1 + (1 | x1) + (1 | x2) + (1 | x1:x2), prior = c(prior(normal(y_log_mean, y_log_sd * 2), class = Intercept), prior(gamma(alpha, beta), class = sd)), stanvars = stanvars) By brms default, family = poisson uses the log link. Thus family = poisson(link = \"log\") should return the same results. Notice the right-hand side of the model formula. We have three hierarchical variance parameters. This hierarchical-shrinkage approach to frequency-table data has its origins in Gelman (2005), Analysis of variance–why it is more important than ever. 24.2 Example: Hair eye go again We’ll be using the same data, from above. As an alternative to Table 24.1, it might be handy to take a more colorful approach to wading into the data. # wrangle my_data %&gt;% uncount(weights = Count, .remove = F) %&gt;% tabyl(Eye, Hair) %&gt;% adorn_totals(c(&quot;row&quot;, &quot;col&quot;)) %&gt;% data.frame() %&gt;% pivot_longer(-Eye, names_to = &quot;Hair&quot;) %&gt;% mutate(Eye = fct_rev(Eye)) %&gt;% # plot ggplot(aes(x = Hair, y = Eye, label = value)) + geom_raster(aes(fill = value)) + geom_text(aes(color = value &lt; 250)) + geom_hline(yintercept = 1.5, color = hr[8], size = 1/2) + geom_vline(xintercept = 4.5, color = hr[8], size = 1/2) + scale_fill_gradient(low = hr[9], high = hr[2]) + scale_color_manual(values = hr[c(9, 2)]) + scale_x_discrete(expand = c(0, 0), position = &quot;top&quot;) + scale_y_discrete(expand = c(0, 0)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks = element_blank(), legend.position = &quot;none&quot;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. Load the brms and tidybayes packages. library(brms) library(tidybayes) Now we’ll save the preparatory values necessary for the stanvars. n_x1_level &lt;- length(unique(my_data$Hair)) n_x2_level &lt;- length(unique(my_data$Eye)) n_cell &lt;- nrow(my_data) n_x1_level ## [1] 4 n_x2_level ## [1] 4 n_cell ## [1] 16 Here are the values we’ll save as stanvars. y_log_mean &lt;- log(sum(my_data$Count) / (n_x1_level * n_x2_level)) y_log_sd &lt;- c(rep(0, n_cell - 1), sum(my_data$Count)) %&gt;% sd() %&gt;% log() s_r &lt;- gamma_a_b_from_omega_sigma(mode = y_log_sd, sd = 2 * y_log_sd) y_log_mean ## [1] 3.610918 y_log_sd ## [1] 4.997212 s_r$shape ## [1] 1.640388 s_r$rate ## [1] 0.1281491 As a quick detour, it might be interesting to see what the kind of gamma distribution is entailed by those last two values. tibble(x = seq(from = 0, to = 70, length.out = 1e3)) %&gt;% mutate(density = dgamma(x, shape = s_r$shape, rate = s_r$rate)) %&gt;% ggplot(aes(x = x, y = density)) + geom_area(fill = hr[4]) + scale_x_continuous(NULL, expand = expansion(mult = c(0, -0.05))) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + ggtitle(expression(&quot;Kruschke&#39;s wide prior for &quot;*sigma[beta*x])) Save the stanvars. stanvars &lt;- stanvar(y_log_mean, name = &quot;y_log_mean&quot;) + stanvar(y_log_sd, name = &quot;y_log_sd&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Fit Kruschke’s model with brms. fit24.1 &lt;- brm(data = my_data, family = poisson, Count ~ 1 + (1 | Hair) + (1 | Eye) + (1 | Hair:Eye), prior = c(prior(normal(y_log_mean, y_log_sd * 2), class = Intercept), prior(gamma(alpha, beta), class = sd)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 24, stanvars = stanvars, file = &quot;fits/fit24.01&quot;) As it turns out, if you try to fit Kruschke’s model with brms as is, you’ll run into difficulties with divergent transitions and the like. One approach is to try tuning the adapt_delta and max_treedepth parameters. I had no luck with that approach. E.g., cranking adapt_delta up past 0.9999 still returned a divergent transition or two. Another approach is to step back and assess the model. We’re trying to fit a multilevel model with two grouping variables and their interaction with a total of 16 data points. That’s not a lot of data for fitting such a model. If you take a close look at our priors, you’ll notice they’re really quite weak. If you’re willing to tighten them up just a bit, the model can fit more smoothly. That will be our approach. For the \\(\\sigma\\) hyperparameter of the overall intercept’s Gaussian prior, Kruschke would have us multiply y_log_sd by 2. Here we’ll tighten up that \\(\\sigma\\) hyperparameter by simply setting it to y_log_sd. The gamma priors for the upper-level variance parameters were based on a mode of y_log_sd and a standard deviation of the same but multiplied by 2 (i.e., 2 * y_log_sd). We’ll tighten that up a bit by simply defining those gammas by a standard deviation of y_log_sd. When you make those adjustments, the model fits with less fuss. In case you’re curious, here is what those priors look like. # redifine our shape and rate s_r &lt;- gamma_a_b_from_omega_sigma(mode = y_log_sd, sd = y_log_sd) # wrangle bind_rows( # define beta[0] tibble(x = seq(from = y_log_mean - (4 * y_log_sd), to = y_log_mean + (4 * y_log_sd), length.out = 1e3)) %&gt;% mutate(density = dnorm(x, mean = y_log_mean, sd = y_log_sd)), # define sigma[beta[x]] tibble(x = seq(from = 0, to = 40, length.out = 1e3)) %&gt;% mutate(density = dgamma(x, shape = s_r$shape, rate = s_r$rate)) ) %&gt;% mutate(prior = rep(c(&quot;beta[0]&quot;, &quot;sigma[beta*x]&quot;), each = n() / 2)) %&gt;% # plot ggplot(aes(x = x, y = density)) + geom_area(fill = hr[6]) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Priors&quot;, x = NULL) + facet_wrap(~ prior, scales = &quot;free&quot;, labeller = label_parsed) Update the stanvars. stanvars &lt;- stanvar(y_log_mean, name = &quot;y_log_mean&quot;) + stanvar(y_log_sd, name = &quot;y_log_sd&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Now we’ve updated our stanvars, we’ll fit the modified model. We should note that even this version required some adjustment to the adapt_delta and max_treedepth parameters. But it wasn’t nearly the exercise in frustration entailed in the version, above. fit24.1 &lt;- brm(data = my_data, family = poisson, Count ~ 1 + (1 | Hair) + (1 | Eye) + (1 | Hair:Eye), prior = c(prior(normal(y_log_mean, y_log_sd), class = Intercept), prior(gamma(alpha, beta), class = sd)), iter = 3000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.999, max_treedepth = 12), stanvars = stanvars, seed = 24, file = &quot;fits/fit24.01&quot;) Take a look at the parameter summary. print(fit24.1) ## Family: poisson ## Links: mu = log ## Formula: Count ~ 1 + (1 | Hair) + (1 | Eye) + (1 | Hair:Eye) ## Data: my_data (Number of observations: 16) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Group-Level Effects: ## ~Eye (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.80 1.51 0.30 5.80 1.00 2907 5001 ## ## ~Hair (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.89 1.52 0.35 6.10 1.00 3302 5159 ## ## ~Hair:Eye (Number of levels: 16) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.95 0.30 0.53 1.67 1.00 2534 4849 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.13 1.47 -0.04 6.12 1.00 4494 4726 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). You’ll notice that even though we tightened up the priors, the parameter estimates are still quite small relative to the values they allowed for. So even our tightened priors were quite permissive. Let’s post process in preparation for Figure 24.3. nd &lt;- my_data %&gt;% arrange(Eye, Hair) %&gt;% # make the titles for the facet strips mutate(strip = str_c(&quot;E:&quot;, Eye, &quot; H:&quot;, Hair, &quot;\\nN = &quot;, Count)) f &lt;- fitted(fit24.1, newdata = nd, summary = F) %&gt;% data.frame() %&gt;% set_names(pull(nd, strip)) glimpse(f) ## Rows: 8,000 ## Columns: 16 ## $ `E:Blue H:Black\\nN = 20` &lt;dbl&gt; 20.12511, 27.86684, 21.45819, 21.74434, … ## $ `E:Blue H:Brunette\\nN = 84` &lt;dbl&gt; 80.80136, 75.66269, 89.25145, 87.52395, … ## $ `E:Blue H:Red\\nN = 17` &lt;dbl&gt; 23.382478, 13.285231, 13.497254, 13.4109… ## $ `E:Blue H:Blond\\nN = 94` &lt;dbl&gt; 99.57193, 96.05862, 91.89343, 110.94643,… ## $ `E:Brown H:Black\\nN = 68` &lt;dbl&gt; 54.90660, 75.31061, 81.18954, 73.27450, … ## $ `E:Brown H:Brunette\\nN = 119` &lt;dbl&gt; 124.36289, 136.53928, 106.47154, 128.541… ## $ `E:Brown H:Red\\nN = 26` &lt;dbl&gt; 32.19246, 22.14586, 21.40319, 23.58161, … ## $ `E:Brown H:Blond\\nN = 7` &lt;dbl&gt; 5.602555, 8.908353, 7.242116, 8.276528, … ## $ `E:Green H:Black\\nN = 5` &lt;dbl&gt; 8.161327, 3.095467, 4.026049, 6.789293, … ## $ `E:Green H:Brunette\\nN = 29` &lt;dbl&gt; 18.79455, 25.73498, 23.25655, 25.46208, … ## $ `E:Green H:Red\\nN = 14` &lt;dbl&gt; 10.798792, 15.753593, 7.899662, 15.24391… ## $ `E:Green H:Blond\\nN = 16` &lt;dbl&gt; 18.760393, 8.495171, 17.541942, 9.304437… ## $ `E:Hazel H:Black\\nN = 15` &lt;dbl&gt; 15.206625, 15.830510, 18.532664, 8.22359… ## $ `E:Hazel H:Brunette\\nN = 54` &lt;dbl&gt; 51.77012, 50.71060, 69.07192, 49.63804, … ## $ `E:Hazel H:Red\\nN = 14` &lt;dbl&gt; 16.434025, 12.275335, 14.023016, 14.9371… ## $ `E:Hazel H:Blond\\nN = 10` &lt;dbl&gt; 6.889019, 11.160856, 10.789524, 14.29025… Notice that when working with a Poisson model, fitted() defaults to returning estimates in the \\(\\lambda\\) metric. If we want proportions/probabilities, we’ll have to compute them by dividing by the total \\(N\\). In this case \\(N = 592\\), which we get with sum(my_data$Count). Here we convert the data to the long format, compute the proportions, and plot to make the top portion of Figure 24.3. f %&gt;% pivot_longer(everything(), values_to = &quot;count&quot;) %&gt;% mutate(proportion = count / 592) %&gt;% ggplot(aes(x = proportion, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = hr[9], color = hr[5], normalize = &quot;panels&quot;) + scale_x_continuous(breaks = c(0, .1, .2)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, .25)) + facet_wrap(~ name, scales = &quot;free_y&quot;) ## Warning: Unknown or uninitialised column: `linewidth`. ## Warning: Using the `size` aesthietic with geom_segment was deprecated in ggplot2 3.4.0. ## ℹ Please use the `linewidth` aesthetic instead. ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. We’ll have to work a bit to get the deflection differences. If this was a simple multilevel model with a single random grouping variable, we could just use the ranef() function to return the deflections. Like fitted(), it’ll return summaries by default. But you can get the posterior draws with the summary = F argument. But since we used two grouping variables and their interaction, it’d be a bit of a pain to work that way. Happily, we do have a handy alternative. First, if we use the scale = \"linear\" argument, fitted() will return the draws in the \\(\\lambda\\) scale rather than the original count metric. With the group-level draws in the \\(\\lambda\\) metric, all we need to do is subtract the fixed effect (i.e., the grand mean, the population estimate) from each to convert them to the deflection metric. So below, we’ll make a custom make_deflection() function to do the conversions, redefine our nd data to make our naming conventions a little more streamlined, use fitted() and its scale = \"linear\" argument to get the draws in the \\(\\lambda\\) metric, wrangle a touch, and use our handy make_deflection() function to convert the results to the deflection metric. I know; that’s a lot. If you get lost, just go step by step and examine the results along the way. # a. make a custom function make_deflection &lt;- function(x) { x - as_draws_df(fit24.1)$b_Intercept } # b. streamline `nd` nd &lt;- my_data %&gt;% arrange(Eye, Hair) %&gt;% mutate(strip = str_c(&quot;E:&quot;, Eye, &quot; H:&quot;, Hair)) # c. use `fitted()` deflections &lt;- fitted(fit24.1, newdata = nd, summary = F, scale = &quot;linear&quot;) %&gt;% # d. wrangle data.frame() %&gt;% set_names(pull(nd, strip)) %&gt;% # e. use the `make_deflection()` function mutate_all(.funs = make_deflection) # what have we done? glimpse(deflections) ## Rows: 8,000 ## Columns: 16 ## $ `E:Blue H:Black` &lt;dbl&gt; 1.228304623, 1.997064342, 1.849325297, -1.4963963… ## $ `E:Blue H:Brunette` &lt;dbl&gt; 2.61832992, 2.99591203, 3.27467637, -0.10383716, … ## $ `E:Blue H:Red` &lt;dbl&gt; 1.37832309, 1.25627979, 1.38570498, -1.97967502, … ## $ `E:Blue H:Blond` &lt;dbl&gt; 2.82721645, 3.23458549, 3.30384829, 0.13329787, 0… ## $ `E:Brown H:Black` &lt;dbl&gt; 2.2319697, 2.9912478, 3.1800052, -0.2815369, -0.3… ## $ `E:Brown H:Brunette` &lt;dbl&gt; 3.0495400, 3.5862392, 3.4510964, 0.2805024, 0.166… ## $ `E:Brown H:Red` &lt;dbl&gt; 1.69806854, 1.76727743, 1.84675887, -1.41528261, … ## $ `E:Brown H:Blond` &lt;dbl&gt; -0.05044110, 0.85661619, 0.76313215, -2.46232612,… ## $ `E:Green H:Black` &lt;dbl&gt; 0.32574299, -0.20043421, 0.17600424, -2.66040279,… ## $ `E:Green H:Brunette` &lt;dbl&gt; 1.15990320, 1.91747813, 1.92980535, -1.33855931, … ## $ `E:Green H:Red` &lt;dbl&gt; 0.60577044, 1.42669535, 0.85003872, -1.85156937, … ## $ `E:Green H:Blond` &lt;dbl&gt; 1.15808406, 0.80912474, 1.64781346, -2.34525825, … ## $ `E:Hazel H:Black` &lt;dbl&gt; 0.94806736, 1.43156593, 1.70275351, -2.46874167, … ## $ `E:Hazel H:Brunette` &lt;dbl&gt; 2.17314926, 2.59576188, 3.01836699, -0.67099221, … ## $ `E:Hazel H:Red` &lt;dbl&gt; 1.02569003, 1.17721885, 1.42391871, -1.87190092, … ## $ `E:Hazel H:Blond` &lt;dbl&gt; 0.15626491, 1.08203948, 1.16179435, -1.91617194, … Now we’re ready to define our difference columns and plot our version of the lower panels in Figure 24.3. deflections %&gt;% transmute(`Blue − Brown @ Black` = `E:Blue H:Black` - `E:Brown H:Black`, `Blue − Brown @ Blond` = `E:Blue H:Blond` - `E:Brown H:Blond`) %&gt;% mutate(`Blue.v.Brown\\n(x)\\nBlack.v.Blond` = `Blue − Brown @ Black` - `Blue − Brown @ Blond`) %&gt;% pivot_longer(everything(), values_to = &quot;difference&quot;) %&gt;% ggplot(aes(x = difference, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = hr[9], color = hr[5], normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. If you’re curious, here are the precise summary values. deflections %&gt;% mutate(`Blue − Brown @ Black` = `E:Blue H:Black` - `E:Brown H:Black`, `Blue − Brown @ Blond` = `E:Blue H:Blond` - `E:Brown H:Blond`) %&gt;% mutate(`Blue.v.Brown\\n(x)\\nBlack.v.Blond` = `Blue − Brown @ Black` - `Blue − Brown @ Blond`) %&gt;% pivot_longer(`Blue − Brown @ Black`:`Blue.v.Brown\\n(x)\\nBlack.v.Blond`) %&gt;% group_by(name) %&gt;% mode_hdi(value) ## # A tibble: 3 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;Blue − Brown @ Black&quot; -1.13 -1.68 -0.709 0.95 mode hdi ## 2 &quot;Blue − Brown @ Blond&quot; 2.44 1.71 3.13 0.95 mode hdi ## 3 &quot;Blue.v.Brown\\n(x)\\nBlack.v.Blond&quot; -3.58 -4.51 -2.74 0.95 mode hdi 24.3 Example: Interaction contrasts, shrinkage, and omnibus test “In this section, we consider some contrived data to illustrate aspects of interaction contrasts. Like the eye and hair data, the fictitious data have two attributes with four levels each” (p. 713). Let’s make the data. my_data &lt;- crossing(a = str_c(&quot;a&quot;, 1:4), b = str_c(&quot;b&quot;, 1:4)) %&gt;% mutate(count = c(rep(c(22, 11), each = 2) %&gt;% rep(., times = 2), rep(c(11, 22), each = 2) %&gt;% rep(., times = 2))) head(my_data) ## # A tibble: 6 × 3 ## a b count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 a1 b1 22 ## 2 a1 b2 22 ## 3 a1 b3 11 ## 4 a1 b4 11 ## 5 a2 b1 22 ## 6 a2 b2 22 In the last section, we covered how Kruschke’s broad priors can make fitting these kinds of models difficult when using HMC, particularly with so few cells. Our solution was to reign in the \\(\\sigma\\) hyperparameter for the level-one intercept and to compute the gamma prior for the hierarchical deflections based on a standard deviation of the log of the maximum standard deviation for the data rather than two times that value. Let’s explore more options. This data set has 16 cells. With so few cells, one might argue for a more conservative prior on the hierarchical deflections. Why not ditch the gamma altogether for a half normal centered on zero and with a \\(\\sigma\\) hyperparameter of 1? Even though this is much tighter than Kruschke’s gamma prior approach, it’s still permissive on the \\(\\log\\) scale. As for our intercept, we’ll continue with the same approach from last time. With that in mind, make the stanvars. n_x1_level &lt;- length(unique(my_data$a)) n_x2_level &lt;- length(unique(my_data$b)) n_cell &lt;- nrow(my_data) y_log_mean &lt;- log(sum(my_data$count) / (n_x1_level * n_x2_level)) y_log_sd &lt;- c(rep(0, n_cell - 1), sum(my_data$count)) %&gt;% sd() %&gt;% log() stanvars &lt;- stanvar(y_log_mean, name = &quot;y_log_mean&quot;) + stanvar(y_log_sd, name = &quot;y_log_sd&quot;) Just for kicks, let’s take a quick look at our priors. bind_rows( # define beta[0] tibble(x = seq(from = y_log_mean - (4 * y_log_sd), to = y_log_mean + (4 * y_log_sd), length.out = 1e3)) %&gt;% mutate(density = dnorm(x, y_log_mean, y_log_sd)), # define sigma[beta[x]] tibble(x = seq(from = 0, to = 5, length.out = 1e3)) %&gt;% mutate(density = dnorm(x, 0, 1)) ) %&gt;% mutate(prior = rep(c(&quot;beta[0]&quot;, &quot;sigma[beta*x]&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = x, y = density)) + geom_area(fill = hr[6]) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Priors&quot;, x = NULL) + facet_wrap(~ prior, scales = &quot;free&quot;, labeller = label_parsed) Fit the model. fit24.2 &lt;- brm(data = my_data, family = poisson, count ~ 1 + (1 | a) + (1 | b) + (1 | a:b), prior = c(prior(normal(y_log_mean, y_log_sd), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.995), stanvars = stanvars, seed = 24, file = &quot;fits/fit24.02&quot;) Review the summary. print(fit24.2) ## Family: poisson ## Links: mu = log ## Formula: count ~ 1 + (1 | a) + (1 | b) + (1 | a:b) ## Data: my_data (Number of observations: 16) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Group-Level Effects: ## ~a (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.17 0.19 0.00 0.70 1.00 2566 3710 ## ## ~a:b (Number of levels: 16) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.31 0.12 0.10 0.59 1.00 2321 2816 ## ## ~b (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.16 0.18 0.01 0.65 1.00 3107 4064 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 2.76 0.20 2.35 3.16 1.00 3915 3768 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We might plot our \\(\\sigma_\\text{&lt;group&gt;}\\) posteriors against our prior to get a sense of how strong it was. as_draws_df(fit24.2) %&gt;% dplyr::select(starts_with(&quot;sd&quot;)) %&gt;% # set_names(str_c(&quot;expression(sigma&quot;, c(&quot;*a&quot;, &quot;*ab&quot;, &quot;*b&quot;), &quot;)&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;sd&quot;) %&gt;% ggplot(aes(x = value)) + # prior geom_area(data = tibble(value = seq(from = 0, to = 3.5, by =.01)), aes(y = dnorm(value, 0, 1)), fill = hr[8]) + # posterior geom_density(aes(fill = sd), size = 0, alpha = 2/3) + scale_fill_manual(values = hr[c(4, 6, 3)], labels = c(expression(sigma[a]), expression(sigma[ab]), expression(sigma[b])), guide = guide_legend(label.hjust = 0)) + scale_x_continuous(NULL, expand = expansion(mult = c(0, 0.05))) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.position = c(.9, .8)) Our \\(\\operatorname{Normal}^+ (0, 1)\\) prior is that short dark shape in the background. The posteriors are the taller and more colorful mounds in the foreground. Here’s the top part of Figure 24.4. nd &lt;- my_data %&gt;% mutate(strip = str_c(&quot;a:&quot;, a, &quot; b:&quot;, b, &quot;\\nN = &quot;, count)) fitted(fit24.2, newdata = nd, summary = F) %&gt;% data.frame() %&gt;% set_names(pull(nd, strip)) %&gt;% pivot_longer(everything(), values_to = &quot;count&quot;) %&gt;% mutate(proportion = count / sum(my_data$count)) %&gt;% # plot! ggplot(aes(x = proportion, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = hr[7], color = hr[6], normalize = &quot;panels&quot;) + scale_x_continuous(breaks = c(.05, .1, .15)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, .15)) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;) ## Warning: Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. ## Unknown or uninitialised column: `linewidth`. Like before, we’ll have to work a bit to get the deflection differences. # streamline `nd` nd &lt;- my_data %&gt;% mutate(strip = str_c(&quot;a:&quot;, a, &quot; b:&quot;, b)) # use `fitted()` deflections &lt;- fitted(fit24.2, newdata = nd, summary = F, scale = &quot;linear&quot;) %&gt;% # wrangle data.frame() %&gt;% set_names(pull(nd, strip)) %&gt;% # use the `make_deflection()` function mutate_all(.funs = make_deflection) # what have we done? glimpse(deflections) ## Rows: 8,000 ## Columns: 16 ## $ `a:a1 b:b1` &lt;dbl&gt; 1.28116790, 1.58998141, 1.66069451, -1.40985460, -1.759961… ## $ `a:a1 b:b2` &lt;dbl&gt; 1.3831449, 1.5808409, 1.8599988, -1.5441678, -1.6570960, -… ## $ `a:a1 b:b3` &lt;dbl&gt; 1.35359455, 1.04628609, 1.41860247, -1.56687561, -2.442681… ## $ `a:a1 b:b4` &lt;dbl&gt; 0.28691986, 1.34971181, 1.01957835, -1.84918910, -2.106926… ## $ `a:a2 b:b1` &lt;dbl&gt; 1.58598793, 1.66754962, 1.97165841, -1.34660879, -1.778238… ## $ `a:a2 b:b2` &lt;dbl&gt; 1.42786402, 1.44340647, 1.55338659, -1.76641385, -1.383695… ## $ `a:a2 b:b3` &lt;dbl&gt; 0.5729748, 1.1807854, 1.3724508, -1.8588430, -1.5406350, -… ## $ `a:a2 b:b4` &lt;dbl&gt; 1.11247413, 0.99354902, 1.41618635, -1.63199849, -2.104763… ## $ `a:a3 b:b1` &lt;dbl&gt; 0.81909143, 1.30966244, 1.56502553, -1.54321243, -2.162512… ## $ `a:a3 b:b2` &lt;dbl&gt; 0.84223803, 1.22268092, 1.25382473, -2.04283116, -2.112058… ## $ `a:a3 b:b3` &lt;dbl&gt; 1.315673e+00, 1.516203e+00, 1.458626e+00, -1.580080e+00, -… ## $ `a:a3 b:b4` &lt;dbl&gt; 1.17864467, 1.70700248, 1.40920605, -1.51867694, -1.627360… ## $ `a:a4 b:b1` &lt;dbl&gt; 0.90766862, 1.12602218, 0.98853655, -2.02041663, -1.703785… ## $ `a:a4 b:b2` &lt;dbl&gt; 0.90848510, 1.14080661, 1.12875946, -2.07116877, -1.823556… ## $ `a:a4 b:b3` &lt;dbl&gt; 0.86341813, 1.37151630, 1.42754775, -1.55822735, -1.460486… ## $ `a:a4 b:b4` &lt;dbl&gt; 1.48476769, 1.41677863, 1.14920307, -1.77672177, -1.118891… Now we’re ready to define some of the difference columns and plot our version of the leftmost lower panel in Figure 24.4. deflections %&gt;% transmute(`a2 - a3 @ b2` = `a:a2 b:b2` - `a:a3 b:b2`, `a2 - a3 @ b3` = `a:a2 b:b3` - `a:a3 b:b3`) %&gt;% mutate(`a2.v.a3\\n(x)\\nb2.v.b3` = `a2 - a3 @ b2` - `a2 - a3 @ b3`) %&gt;% pivot_longer(`a2.v.a3\\n(x)\\nb2.v.b3`, values_to = &quot;difference&quot;) %&gt;% ggplot(aes(x = difference, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = hr[7], color = hr[6]) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-.5, 2.5)) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) For Figure 24.4, bottom right, we average across the four cells in each quadrant and then compute the contrast. deflections %&gt;% # in this intermediate step, we compute the quadrant averages # `tl` = top left, `br` = bottom right, and so on transmute(tl = (`a:a1 b:b1` + `a:a1 b:b2` + `a:a2 b:b1` + `a:a2 b:b2`) / 4, tr = (`a:a1 b:b3` + `a:a1 b:b4` + `a:a2 b:b3` + `a:a2 b:b4`) / 4, bl = (`a:a3 b:b1` + `a:a3 b:b2` + `a:a4 b:b1` + `a:a4 b:b2`) / 4, br = (`a:a3 b:b3` + `a:a3 b:b4` + `a:a4 b:b3` + `a:a4 b:b4`) / 4) %&gt;% # compute the contrast of interest mutate(`A1.A2.v.A3.A4\\n(x)\\nB1.B2.v.B3.B4` = (tl - bl) - (tr - br)) %&gt;% pivot_longer(`A1.A2.v.A3.A4\\n(x)\\nB1.B2.v.B3.B4`, values_to = &quot;difference&quot;) %&gt;% # plot ggplot(aes(x = difference, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = hr[7], color = hr[6]) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-.5, 2.5)) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) The model presented here has no way to conduct an “ominbus” test of interaction. However, like the ANOVA-style models presented in Chapters 19 and 20, it is easy to extend the model so it has an inclusion coefficient on the interaction deflections. The inclusion coefficient can have values of \\(0\\) or \\(1\\), and is given a Bernoulli prior. (p. 716) Like we discussed in earlier chapters, this isn’t a feasible approach for brms. However, we can compare this model with a simpler one that omits the interaction. First, fit the model. fit24.3 &lt;- brm(data = my_data, family = poisson, count ~ 1 + (1 | a) + (1 | b), prior = c(prior(normal(y_log_mean, y_log_sd), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.9999), stanvars = stanvars, seed = 24, file = &quot;fits/fit24.03&quot;) Now we can compare them by their stacking weights. model_weights(fit24.2, fit24.3) %&gt;% round(digits = 3) ## fit24.2 fit24.3 ## 1 0 Virtually all the weight went to the interaction model. Also, if we step back and ask ourselves what the purpose of an omnibus text of an interaction is for in this context, we’d conclude such a test is asking the question Is \\(\\sigma_{a \\times b}\\) the same as zero? Let’s look again at that posterior from fit24.2. as_draws_df(fit24.2) %&gt;% ggplot(aes(x = `sd_a:b__Intercept`, y = 0)) + stat_halfeye(.width = .95, fill = hr[7], color = hr[6]) + scale_x_continuous(expression(sigma[a%*%b]), expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Does this look the same as zero, to you?&quot;) Sure, there’s some uncertainty in that posterior. But that is not zero. We didn’t need an omnibus test or even model comparison via stacking weights to figure that one out. If you wanted to get fancy with it, we might even do a hierarchical variance decomposition. Here the question is what percentage of the hierarchical variance is attributed to a, b and their interaction? Recall that brms returns those variance parameters in the \\(\\sigma\\) metric. So before we can compare them in terms of percentages of the total variance, we have to first have to square them. draws &lt;- as_draws_df(fit24.2) %&gt;% transmute(`sigma[a]^2` = sd_a__Intercept^2, `sigma[b]^2` = sd_b__Intercept^2, `sigma[ab]^2` = `sd_a:b__Intercept`^2) %&gt;% mutate(`sigma[total]^2` = `sigma[a]^2` + `sigma[b]^2` + `sigma[ab]^2`) head(draws) ## # A tibble: 6 × 4 ## `sigma[a]^2` `sigma[b]^2` `sigma[ab]^2` `sigma[total]^2` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0355 0.0553 0.196 0.287 ## 2 0.0136 0.00123 0.0369 0.0517 ## 3 0.00471 0.0434 0.0545 0.103 ## 4 0.000156 0.0276 0.0234 0.0512 ## 5 0.00000372 0.00506 0.0977 0.103 ## 6 0.00000172 0.0275 0.0389 0.0665 Now we just need to divide the individual variance parameters by their total and multiply by 100 to get the percent of variance for each. We’ll look at the results in a plot. draws %&gt;% pivot_longer(-`sigma[total]^2`) %&gt;% mutate(`% hierarchical variance` = 100 * value / `sigma[total]^2`) %&gt;% ggplot(aes(x = `% hierarchical variance`, y = name)) + stat_halfeye(.width = .95, fill = hr[7], color = hr[6]) + scale_x_continuous(limits = c(0, 100), expand = c(0, 0)) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + coord_cartesian(ylim = c(1.5, 3.5)) + theme(plot.margin = margin(5.5, 10, 5.5, 5.5)) Just as each of the variance parameters was estimated with uncertainty, all that uncertainty got propagated into their transformations. Even in the midst of all this uncertainty, it’s clear that a good healthy portion of the hierarchical variance is from the interaction. Again, whatever you might think about \\(a \\times b\\), it’s definitely not zero. 24.4 Log-linear models for contingency tables Bonus: Alternative parameterization The Poisson distribution is widely used for count data. But notice how in our figures, we converted the results to the proportion metric. Once you’re talking about proportions, it’s not hard to further adjust your approach to thinking in terms of probabilities. So instead of thinking about the \\(n\\) within each cell of our contingency table, we might also think about the probability of a given condition. To approach the data this way, we could use a multilevel aggregated binomial model. McElreath covered this in Chapter 10 of his Statistical rethinking. See my (2020) translation of the text into brms code, too. Here’s how to fit that model. fit24.4 &lt;- brm(data = my_data, family = binomial, count | trials(264) ~ 1 + (1 | a) + (1 | b) + (1 | a:b), prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.999), seed = 24, file = &quot;fits/fit24.04&quot;) A few things about the syntax: The aggregated binomial model uses the logit link, just like with typical logistic regression. So when you specify family = binomial, you’re requesting the logit link. The left side of the formula argument, count | trials(264) indicates a few things. First, our criterion is count. The bar | that follows on its right indicates we’d like add additional information about the criterion. In the case of binomial regression, brms requires we specify how many trials the value in each cell of the data is referring to. When we coded trials(264), we indicated each cell was a total count of 264 trials. In case it isn’t clear, here is where the value 264 came from. my_data %&gt;% summarise(total_trials = sum(count)) ## # A tibble: 1 × 1 ## total_trials ## &lt;dbl&gt; ## 1 264 Now look over the summary. print(fit24.4) ## Family: binomial ## Links: mu = logit ## Formula: count | trials(264) ~ 1 + (1 | a) + (1 | b) + (1 | a:b) ## Data: my_data (Number of observations: 16) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Group-Level Effects: ## ~a (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.17 0.19 0.00 0.71 1.00 2850 3840 ## ## ~a:b (Number of levels: 16) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.34 0.13 0.12 0.63 1.00 2841 3375 ## ## ~b (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.17 0.20 0.00 0.72 1.00 2445 3403 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -2.72 0.22 -3.12 -2.24 1.00 3288 2518 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). See that mu = logit part in the second line of the summary? Yep, that’s our link function. Since we used a different likelihood and link function from earlier models, it shouldn’t be surprising the parameters look different. But notice how the aggregated binomial model yields virtually the same results for the top portion of Figure 24.4. nd &lt;- my_data %&gt;% mutate(strip = str_c(&quot;a:&quot;, a, &quot; b:&quot;, b, &quot;\\nN = &quot;, count)) fitted(fit24.4, newdata = nd, summary = F) %&gt;% data.frame() %&gt;% set_names(pull(nd, strip)) %&gt;% pivot_longer(everything(), values_to = &quot;count&quot;) %&gt;% mutate(proportion = count / sum(my_data$count)) %&gt;% # plot! ggplot(aes(x = proportion, y = 0)) + stat_histinterval(point_interval = mode_hdi, .width = .95, fill = hr[5], color = hr[3]) + scale_x_continuous(breaks = c(.05, .1, .15)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, .15)) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;) To further demonstrate the similarity of this approach to Kruschke’s multilevel Poisson approach, let’s compare the model-based cell estimates for each of the combinations of a and b, by both fit24.2 and fit24.4. # compute the fitted summary statistics rbind(fitted(fit24.2), fitted(fit24.4)) %&gt;% data.frame() %&gt;% # add an augmented version of the data bind_cols(expand(my_data, fit = c(&quot;fit2 (Poisson likelihood)&quot;, &quot;fit4 (binomial likelihood)&quot;), nesting(a, b, count))) %&gt;% mutate(cell = str_c(a, &quot;\\n&quot;, b)) %&gt;% # plot ggplot(aes(x = cell)) + geom_hline(yintercept = c(11, 22), color = hr[3], linetype = 2) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, color = fit), fatten = 1.5, position = position_dodge(width = 0.5)) + geom_point(aes(y = count), size = 2, color = hr[9]) + scale_color_manual(NULL, values = hr[c(7, 5)]) + scale_y_continuous(&quot;count&quot;, breaks = c(0, 11, 22, 33), limits = c(0, 33)) + theme(legend.position = &quot;top&quot;) The black points are the raw data. The colored point-ranges to the left and right of each data point are the posterior means and percentile-based 95% intervals for each of the cells. The results are virtually the same between the two models. Also note how both models partially pooled towards the grand mean. That’s one of the distinctive features of using the hierarchical approach. Wrapping up, this chapter focused on how one might use the Poisson likelihood to model contingency-table data from a multilevel modeling framework. The Poisson likelihood is also handy for count data within a single-level structure, with metric predictors, and with various combinations of metric and nominal predictors. For more practice along those lines, check out Section 10.2 in my ebook recoding McElreath’s Statistical rethinking. Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_3.0.2 brms_2.18.0 Rcpp_1.0.9 patchwork_1.1.2 ## [5] cowplot_1.1.1 ochRe_1.0.0 janitor_2.1.0 forcats_0.5.1 ## [9] stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 readr_2.1.2 ## [13] tidyr_1.2.1 tibble_3.1.8 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 ## [4] igraph_1.3.4 sp_1.5-0 svUnit_1.0.6 ## [7] splines_4.2.0 crosstalk_1.2.0 TH.data_1.1-1 ## [10] rstantools_2.2.0 inline_0.3.19 digest_0.6.30 ## [13] htmltools_0.5.3 fansi_1.0.3 magrittr_2.0.3 ## [16] checkmate_2.1.0 googlesheets4_1.0.1 tzdb_0.3.0 ## [19] modelr_0.1.8 RcppParallel_5.1.5 matrixStats_0.62.0 ## [22] vroom_1.5.7 xts_0.12.1 sandwich_3.0-2 ## [25] prettyunits_1.1.1 colorspace_2.0-3 rvest_1.0.2 ## [28] ggdist_3.2.0 haven_2.5.1 xfun_0.35 ## [31] callr_3.7.3 crayon_1.5.2 jsonlite_1.8.3 ## [34] lme4_1.1-31 survival_3.4-0 zoo_1.8-10 ## [37] glue_1.6.2 gtable_0.3.1 gargle_1.2.0 ## [40] emmeans_1.8.0 distributional_0.3.1 pkgbuild_1.3.1 ## [43] rstan_2.21.7 abind_1.4-5 scales_1.2.1 ## [46] mvtnorm_1.1-3 DBI_1.1.3 miniUI_0.1.1.1 ## [49] xtable_1.8-4 HDInterval_0.2.2 bit_4.0.4 ## [52] stats4_4.2.0 StanHeaders_2.21.0-7 DT_0.24 ## [55] htmlwidgets_1.5.4 httr_1.4.4 threejs_0.3.3 ## [58] arrayhelpers_1.1-0 posterior_1.3.1 ellipsis_0.3.2 ## [61] pkgconfig_2.0.3 loo_2.5.1 farver_2.1.1 ## [64] sass_0.4.2 dbplyr_2.2.1 utf8_1.2.2 ## [67] tidyselect_1.1.2 labeling_0.4.2 rlang_1.0.6 ## [70] reshape2_1.4.4 later_1.3.0 munsell_0.5.0 ## [73] cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [76] cli_3.5.0 generics_0.1.3 broom_1.0.1 ## [79] ggridges_0.5.3 evaluate_0.18 fastmap_1.1.0 ## [82] processx_3.8.0 knitr_1.40 bit64_4.0.5 ## [85] fs_1.5.2 nlme_3.1-159 projpred_2.2.1 ## [88] mime_0.12 xml2_1.3.3 compiler_4.2.0 ## [91] bayesplot_1.9.0 shinythemes_1.2.0 rstudioapi_0.13 ## [94] gamm4_0.2-6 reprex_2.0.2 bslib_0.4.0 ## [97] stringi_1.7.8 highr_0.9 ps_1.7.2 ## [100] Brobdingnag_1.2-8 lattice_0.20-45 Matrix_1.4-1 ## [103] nloptr_2.0.3 markdown_1.1 shinyjs_2.1.0 ## [106] tensorA_0.36.2 vctrs_0.5.1 pillar_1.8.1 ## [109] lifecycle_1.0.3 jquerylib_0.1.4 bridgesampling_1.1-2 ## [112] estimability_1.4.1 raster_3.5-15 httpuv_1.6.5 ## [115] R6_2.5.1 bookdown_0.28 promises_1.2.0.1 ## [118] gridExtra_2.3 codetools_0.2-18 boot_1.3-28 ## [121] MASS_7.3-58.1 colourpicker_1.1.1 gtools_3.9.3 ## [124] assertthat_0.2.1 withr_2.5.0 shinystan_2.6.0 ## [127] multcomp_1.4-20 mgcv_1.8-40 parallel_4.2.0 ## [130] hms_1.1.1 terra_1.5-21 grid_4.2.0 ## [133] minqa_1.2.5 coda_0.19-4 rmarkdown_2.16 ## [136] snakecase_0.11.0 googledrive_2.0.0 shiny_1.7.2 ## [139] lubridate_1.8.0 base64enc_0.1-3 dygraphs_1.1.1.6 References Allan, A., Cook, D., Gayler, R., Kirk, H., Peng, R., &amp; Saber, E. (2021). ochRe: Australia-themed colour palettes [Manual]. https://github.com/ropenscilabs/ochRe Gelman, A. (2005). Analysis of variance–Why it is more important than ever. Annals of Statistics, 33(1), 1–53. https://doi.org/10.1214/009053604000001048 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Kurz, A. S. (2020). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 Snee, R. D. (1974). Graphical display of two-way contingency tables. The American Statistician, 28(1), 9–12. https://doi.org/10.1080/00031305.1974.10479053 "],["tools-in-the-trunk.html", "25 Tools in the Trunk 25.1 Reporting a Bayesian analysis 25.2 Functions for computing highest density intervals 25.3 Reparameterization 25.4 Censored data in JAGS brms 25.5 What Next? Session info", " 25 Tools in the Trunk “This chapter includes some important topics that apply to many different models throughout the book… The sections can be read independently of each other and at any time” (Kruschke, 2015, p. 721). 25.1 Reporting a Bayesian analysis Bayesian data analyses are not yet standard procedure in many fields of research, and no conventional format for reporting them has been established. Therefore, the researcher who reports a Bayesian analysis must be sensitive to the background knowledge of his or her specific audience, and must frame the description accordingly. (p. 721) At the time of this writing (mid 2022), this is still generally the case. See Aczel et al. (2020), Discussion points for Bayesian inference, for a recent discussion from several Bayesian scholars. For a take from a familiar voice, see Kruschke’s (2021) Bayesian analysis reporting guidelines. 25.1.1 Essential points. Recall the basic steps of a Bayesian analysis from Section 2.3 (p. 25): Identify the data, define a descriptive model, specify a prior, compute the posterior distribution, interpret the posterior distribution, and, check that the model is a reasonable description of the data. Those steps are in logical order, with each step building on the previous step. That logical order should be preserved in the report of the analysis. (p. 722) Kruschke then gave recommendations for motivating Bayesian inference. His (2018) paper with Liddell, The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective, might be helpful in this regard. Many of the other points Kruschke made in this section (e.g., adequately reporting the data structure, the priors, evidence for convergence) can be handled by adopting open science practices. If your data and research questions are simple and straightforward, you might find it easy to detail these and other concerns in the primary manuscript. The harsh reality is many journals place tight constraints on word and/or page limits. If your projects are not of the simple and straightforward type, supplemental materials are your friend. Regardless of a journal’s policy on hosting supplemental materials on the official journal website, you can detail your data, priors, MCMC diagnostics, and all the other fine-grained details of your analysis in supplemental documents hosted in publicly-accessible repositories like the Open Science Framework (OSF) or GutHub. If possible, do consider making your data openly available. Regardless of the status of your data, please consider making all your R scripts available as supplementary material. To reiterate from Chapter 3, I strongly recommend checking out R Notebooks for that purpose. They are a type of R Markdown document with augmentations that make them more useful for working scientists. You can learn more about them here and here. And for a more comprehensive overview, check out Xie, Allaire, and Grolemund’s (2022) R markdown: The definitive guide. 25.1.2 Optional points. For more thoughts on robustness checks, check out a couple Gelman’s blog posts, What’s the point of a robustness check? and Robustness checks are a joke, along with the action in the comments section. In addition to posterior predictive checks, which are great (see Kruschke, 2013), consider prior predictive checks, too. For a great introduction to the topic, check out Gabry, Simpson, Vehtari, Betancourt, and Gelman’s (2019) Visualization in Bayesian workflow. 25.1.3 Helpful points. For more ideas on open data, check out Rouder’s (2016) The what, why, and how of born-open data. You might also check out Klein and colleagues’ (2018) A practical guide for transparency in psychological science and Martone, Garcia-Castro, and VandenBos’s (2018) Data sharing in psychology. As to posting your model fits, this could be done in any number of ways, including as official supplemental materials hosted by the journal, on GitHub, or on the OSF. At a base level, this means saving your fits as external files. We’ve already been modeling this with our brm() code throughout this book. With the save argument, we saved the model fits within the fits folder on GitHub. You might adopt a similar approach. But do be warned: brms fit objects contain a copy of the data used to create them. For example, here’s how we might reload fit24.1 from last chapter. fit24.1 &lt;- readRDS(&quot;fits/fit24.01.rds&quot;) By indexing the fit object with $data, you can see the data. library(tidyverse) library(brms) fit24.1$data %&gt;% glimpse() ## Rows: 16 ## Columns: 4 ## $ Count &lt;dbl&gt; 20, 68, 5, 15, 94, 7, 16, 10, 84, 119, 29, 54, 17, 26, 14, … ## $ Hair &lt;fct&gt; Black, Black, Black, Black, Blond, Blond, Blond, Blond, Bru… ## $ Eye &lt;chr&gt; &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green&quot;, &quot;Hazel&quot;, &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green&quot;… ## $ `Hair:Eye` &lt;chr&gt; &quot;Black_Blue&quot;, &quot;Black_Brown&quot;, &quot;Black_Green&quot;, &quot;Black_Hazel&quot;, … Here’s a quick way to remove the data from the fit object. fit24.1$data &lt;- NULL Confirm it worked. fit24.1$data ## NULL Happily, the rest of the information is still there for you. E.g., here’s the summary. summary(fit24.1) ## Family: poisson ## Links: mu = log ## Formula: Count ~ 1 + (1 | Hair) + (1 | Eye) + (1 | Hair:Eye) ## Data: NULL (Number of observations: ) ## Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup draws = 8000 ## ## Group-Level Effects: ## ~Eye (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.80 1.51 0.30 5.80 1.00 2907 5001 ## ## ~Hair (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.89 1.52 0.35 6.10 1.00 3302 5159 ## ## ~Hair:Eye (Number of levels: 16) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.95 0.30 0.53 1.67 1.00 2534 4849 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.13 1.47 -0.04 6.12 1.00 4494 4726 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 25.2 Functions for computing highest density intervals You can find a copy of Kruschke’s scripts, including DBDA2E-utilities.R, at https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/tree/master/data.R. 25.2.1 R code for computing HDI of a grid approximation. We can imagine the grid approximation of a distribution as a landscape of poles sticking up from each point on the parameter grid, with the height of each pole indicating the probability mass at that discrete point. We can imagine the highest density region by visualizing a rising tide: We gradually flood the landscape, monitoring the total mass of the poles that protrude above water, stopping the flood when 95% (say) of the mass remains protruding. The waterline at that moment defines the highest density region (e.g., Hyndman, 1996). (p. 725) We can use Kruschke’s HDIofGrid() function for such a task. HDIofGrid &lt;- function(probMassVec, credMass = 0.95) { # Arguments: # probMassVec is a vector of probability masses at each grid point. # credMass is the desired mass of the HDI region. # Return value: # A list with components: # indices is a vector of indices that are in the HDI # mass is the total mass of the included indices # height is the smallest component probability mass in the HDI # Example of use: For determining HDI of a beta(30,12) distribution # approximated on a grid: # &gt; probDensityVec = dbeta( seq(0,1,length=201) , 30 , 12 ) # &gt; probMassVec = probDensityVec / sum( probDensityVec ) # &gt; HDIinfo = HDIofGrid( probMassVec ) # &gt; show( HDIinfo ) sortedProbMass &lt;- sort(probMassVec, decreasing = TRUE) HDIheightIdx &lt;- min(which(cumsum(sortedProbMass) &gt;= credMass)) HDIheight &lt;- sortedProbMass[HDIheightIdx] HDImass &lt;- sum(probMassVec[probMassVec &gt;= HDIheight]) return(list(indices = which(probMassVec &gt;= HDIheight), mass = HDImass, height = HDIheight)) } I found Kruschke’s description of his HDIofGrid() a bit opaque. Happily, we can understand this function with a little help from an example posted at https://rdrr.io/github/kyusque/DBDA2E-utilities/man/HDIofGrid.html. prob_density_vec &lt;- dbeta(seq(0, 1, length = 201), 30, 12) prob_mass_vec &lt;- prob_density_vec / sum(prob_density_vec) HDI_info &lt;- HDIofGrid(prob_mass_vec) show(HDI_info) ## $indices ## [1] 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 ## [20] 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 ## [39] 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 ## ## $mass ## [1] 0.9528232 ## ## $height ## [1] 0.004448336 To walk that through a bit, prob_density_vec is a vector of density values for \\(\\operatorname{Beta} (30, 12)\\) based on 201 evenly-spaced values spanning the parameter space for \\(\\theta\\) (i.e., from 0 to 1). In the second line, we converted those density values to the probability metric by dividing each by their sum, which we then saved as prob_mass_vec. In the third line we shoved those probability values into Kruschke’s HDIofGrid() and saved the results as HDI_info. The output of the fourth line, show(HDI_info), showed us the results (i.e., the contents of HDI_info). As to those results, the values in saved as $indices are the row numbers for all cases in prob_mass_vec that were within the HDI. The value in $mass showed the actual width of the HDI. Because we’re only working with finite samples (i.e., length = 201), we won’t likely get a perfect 95% HDI. The value in $height is the density value for the waterline that defines the highest density region. A plot might make that less abstract. library(ggthemes) theme_set( theme_base() + theme(plot.background = element_rect(color = &quot;transparent&quot;)) ) # wrangle tibble(row = 1:length(prob_density_vec), theta = seq(0, 1, length = length(prob_density_vec)), density = prob_mass_vec, cred = if_else(row %in% HDI_info$indices, 1, 0)) %&gt;% # plot ggplot(aes(x = theta, y = density)) + # HDI geom_area(aes(fill = cred == 1)) + # density line geom_line(color = &quot;skyblue&quot;, size = 1) + # waterline geom_hline(yintercept = HDI_info$height, linetype = 3, size = 1/4) + # fluff annotate(geom = &quot;text&quot;, x = .2, y = 0.0046, label = &#39;&quot;waterline&quot; that defines all points\\ninside the highest density region&#39;) + annotate(geom = &quot;text&quot;, x = .715, y = 0.01, label = &quot;95.28% HDI&quot;, color = &quot;black&quot;, size = 5) + scale_fill_manual(values = c(&quot;transparent&quot;, &quot;grey67&quot;)) + xlab(expression(theta)) You might have noticed our theme_set() lines at the top of that code block. For the plots in this chapter, we’ll give a nod to the source material and make them with a similar aesthetic to Kruschke’s figures. 25.2.2 HDI of unimodal distribution is shortest interval. The algorithms [in the next sections] find the HDI by searching among candidate intervals of mass \\(M\\). The shortest one found is declared to be the HDI. It is an approximation, of course. See Chen &amp; Shao (1999) for more details, and Chen, He, Shao, and Xu (2003) for dealing with the unusual situation of multimodal distributions. (p. 727) 25.2.3 R code for computing HDI of a MCMC sample. In this section, Kruschke provided the code for his HDIofMCMC() function. We recreate it, below, with a few mild formatting changes. The ggthemes::theme_base() theme gets us most of the way there. HDIofMCMC &lt;- function(sampleVec, credMass = .95) { # Computes highest density interval from a sample of representative values, # estimated as shortest credible interval. # Arguments: # sampleVec # is a vector of representative values from a probability distribution. # credMass # is a scalar between 0 and 1, indicating the mass within the credible # interval that is to be estimated. # Value: # HDIlim is a vector containing the limits of the HDI sortedPts &lt;- sort(sampleVec) ciIdxInc &lt;- ceiling(credMass * length(sortedPts)) nCIs &lt;- length(sortedPts) - ciIdxInc ciWidth &lt;- rep(0, nCIs) for (i in 1:nCIs) { ciWidth[i] &lt;- sortedPts[i + ciIdxInc] - sortedPts[i] } HDImin &lt;- sortedPts[which.min(ciWidth)] HDImax &lt;- sortedPts[which.min(ciWidth) + ciIdxInc] HDIlim &lt;- c(HDImin, HDImax) return(HDIlim) } Let’s continue working with fit24.1 to see how Kruschke’s HDIofMCMC() works. First we need to extract the posterior draws. draws &lt;- as_draws_df(fit24.1) Here’s how you might use the function to get the HDIs for the intercept parameter. HDIofMCMC(draws$b_Intercept) ## [1] -0.06511808 6.08724849 Kruschke’s HDIofMCMC() works very much the same as the summary functions from tidybayes. For example, here’s good old tidybayes::mode_hdi(). library(tidybayes) mode_hdi(draws$b_Intercept) ## y ymin ymax .width .point .interval ## 1 3.207135 -0.06511808 6.087248 0.95 mode hdi If you’d like to use tidybayes to just pull the HDIs without the extra information, just use the hdi() function. hdi(draws$b_Intercept) ## [,1] [,2] ## [1,] -0.06511808 6.087248 Just in case you’re curious, Kruschke’s HDIofMCMC() function returns the same information as tidybayes::hdi(). Let’s confirm. HDIofMCMC(draws$b_Intercept) == hdi(draws$b_Intercept) ## [,1] [,2] ## [1,] TRUE TRUE Identical. 25.2.4 R code for computing HDI of a function. The function described in this section finds the HDI of a unimodal probability density function that is specified mathematically in R. For example, the function can find HDI’s of normal densities or of beta densities or of gamma densities, because those densities are specified as functions in R. (p. 728) If you recall, we’ve been using this function off and on since Chapter 4. Here is it, again, with mildly reformatted code and parameter names. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { # Arguments: # `name` is R&#39;s name for the inverse cumulative density function # of the distribution. # `width` is the desired mass of the HDI region. # `tol` is passed to R&#39;s optimize function. # Return value: # Highest density iterval (HDI) limits in a vector. # Example of use: For determining HDI of a beta(30, 12) distribution, type # `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)` # Notice that the parameters of the `name` must be explicitly stated; # e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work. # Adapted and corrected from Greg Snow&#39;s TeachingDemos package. incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Here’s how it works for the standard normal distribution. hdi_of_icdf(qnorm, mean = 0, sd = 1) ## [1] -1.959964 1.959964 By default, it returns 95% HDIs. Here’s how it’d work if you wanted the 80% intervals for \\(\\operatorname{Beta}(2, 2)\\). hdi_of_icdf(qbeta, shape1 = 2, shape2 = 2, width = .8) ## [1] 0.1958001 0.8041999 25.3 Reparameterization There are situations in which one parameterization is intuitive to express a distribution, but a different parameterization is required for mathematical convenience. For example, we may think intuitively of the standard deviation of a normal distribution, but have to parameterize the distribution in terms of the precision (i.e., reciprocal of the variance). (p. 729) The details in the rest of this section are beyond the scope of this project. 25.4 Censored data in JAGS brms “In many situations some data are censored, which means that their values are known only within a certain range” (p. 732) Happily, brms is capable of handling censored variables. The setup is a little different from how Kruschke described for JAGS. From the brmsformula section of the brms reference manual (Bürkner, 2022d), we read: With the exception of categorical, ordinal, and mixture families, left, right, and interval censoring can be modeled through y | cens(censored) ~ predictors. The censoring variable (named censored in this example) should contain the values 'left', 'none', 'right', and 'interval' (or equivalently -1, 0, 1, and 2) to indicate that the corresponding observation is left censored, not censored, right censored, or interval censored. For interval censored data, a second variable (let’s call it y2) has to be passed to cens. In this case, the formula has the structure y | cens(censored,y2) ~ predictors. While the lower bounds are given in y, the upper bounds are given in y2 for interval censored data. Intervals are assumed to be open on the left and closed on the right: (y,y2]. We’ll make sense of all this in just a moment. First, let’s see how Kruschke described the example in the text. To illustrate why it is important to include censored data in the analysis, consider a case in which \\(N = 500\\) values are generated randomly from a normal distribution with \\(\\mu = 100\\) and \\(\\sigma = 15\\). Suppose that values above \\(106\\) are censored, as are values in the interval between \\(94\\) and \\(100\\). For the censored values, all we know is the interval in which they occurred, but not their exact value. (p. 732) I’m not aware that we have access to Kruschke’s censored data, so we’ll just make our own based on his description. We’ll start off by simulating the idealized uncensored data, y, based on \\(\\operatorname{Normal}(100, 15)\\). n &lt;- 500 set.seed(25) d &lt;- tibble(y = rnorm(n, mean = 100, sd = 15)) To repeat, Kruschke described two kinds of censoring: “values above 106 are censored,” “as are values in the interval between 94 and 100.” ⚠️ Rather than examine both kinds of censoring at once, as in the text, I’m going to slow down and break this section up. First, we’ll explore right censoring. Second, we’ll explore interval censoring. For our grand finale, we’ll combine the two, as in the text. 25.4.1 Right censoring: When you’re uncertain beyond a single threshold. In the case where the “values above 106 are censored,” we need to save a single threshold value. We’ll call it t1. t1 &lt;- 106 Now we can use our t1 threshold value to make a right censored version of the y column called y1. We’ll also make a character value listing out a row’s censoring status in nominal terms. d &lt;- d %&gt;% mutate(y1 = if_else(y &gt; t1, t1, y), cen1 = if_else(y &gt; t1, &quot;right&quot;, &quot;none&quot;)) d ## # A tibble: 500 × 3 ## y y1 cen1 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 96.8 96.8 none ## 2 84.4 84.4 none ## 3 82.7 82.7 none ## 4 105. 105. none ## 5 77.5 77.5 none ## 6 93.3 93.3 none ## 7 126. 106 right ## 8 108. 106 right ## 9 101. 101. none ## 10 99.1 99.1 none ## # … with 490 more rows When the values in y1 are not censored, we see cen1 == \"none\". For all cases where the original y value exceeded the t1 threshold (i.e., 106), y1 == 106 and cen1 == \"right\". We are using the terms \"none\" and \"right\" because they are based on the block quote from the brms reference manual. For plotting purposes, we’ll make a new variable, y_na, that only has values for which cen1 == \"none\". d &lt;- d %&gt;% mutate(y_na = ifelse(cen1 == &quot;none&quot;, y, NA)) d ## # A tibble: 500 × 4 ## y y1 cen1 y_na ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 96.8 96.8 none 96.8 ## 2 84.4 84.4 none 84.4 ## 3 82.7 82.7 none 82.7 ## 4 105. 105. none 105. ## 5 77.5 77.5 none 77.5 ## 6 93.3 93.3 none 93.3 ## 7 126. 106 right NA ## 8 108. 106 right NA ## 9 101. 101. none 101. ## 10 99.1 99.1 none 99.1 ## # … with 490 more rows Now let’s get a better sense of the data with a few histograms. d %&gt;% pivot_longer(-cen1) %&gt;% mutate(name = factor(name, levels = c(&quot;y&quot;, &quot;y1&quot;, &quot;y_na&quot;))) %&gt;% ggplot(aes(x = value)) + geom_histogram(size = .25, binwidth = 2, boundary = 106, fill = &quot;skyblue4&quot;, color = &quot;white&quot;) + xlab(NULL) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ name, ncol = 3) The original un-censored data are in y. The censored data, for which all values above 106 were simply recoded 106, is y1. The y_na column shows what the distribution would look like if the censored values were omitted. Here’s how we might fit a model which only uses the uncensored values, those in y_na. # define the stanvars mean_y &lt;- mean(d$y_na, na.rm = T) sd_y &lt;- sd(d$y_na, na.rm = T) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) # fit the model fit25.1a &lt;- brm(data = d, family = gaussian, y_na ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, file = &quot;fits/fit25.01a&quot;) Check the summary for the naïve model. print(fit25.1a) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y_na ~ 1 ## Data: d (Number of observations: 333) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 91.43 0.51 90.40 92.41 1.00 3410 2678 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 9.39 0.36 8.70 10.13 1.00 3708 2881 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Relative to the true data-generating process for the original variable y, \\(\\operatorname{Normal}(100, 15)\\), those parameters look pretty biased. Now let’s practice fitting our first censored model. Here we use the y1 | cens(cen1) syntax in the left side of the model formula to indicate that the criterion variable, y1, has been censored according to the process as defined in the cens() function. Within cens() we inserted our nominal variable, cens1, which indicates which cases are censored (\"right\") or not (\"none\"). This model is one of the rare occasions where we’ll set our initial values for the model intercept. In my first few attempts, brm() had great difficulty initializing the chains using the default initial values. We’ll help it out by setting them at mean_y. Recall that when you set custom initial values in brms, you save them in a list with the number of lists equaling the number of HMC chains. Because we’re using the default chains = 4, well need four lists of intercept start values, mean_y. You can set them to different values, if you’d like. inits &lt;- list(Intercept = mean_y) inits_list &lt;- list(inits, inits, inits, inits) fit25.2a &lt;- brm(data = d, family = gaussian, y1 | cens(cen1) ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, # here we insert our start values for the intercept inits = inits_list, file = &quot;fits/fit25.02a&quot;) Now check the summary for the model accounting for the censoring. print(fit25.2a) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y1 | cens(cen1) ~ 1 ## Data: d (Number of observations: 500) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 99.43 0.71 98.09 100.90 1.00 2939 2197 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 14.29 0.61 13.17 15.56 1.00 2918 2440 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). To get a better sense of what these models are doing, we might do a posterior predictive check with predict(). # original p1 &lt;- d %&gt;% pivot_longer(cols = c(y, y_na)) %&gt;% ggplot(aes(x = value)) + geom_histogram(size = .25, binwidth = 2, boundary = 106, fill = &quot;skyblue4&quot;, color = &quot;white&quot;) + labs(subtitle = &quot;Data&quot;, x = NULL) + scale_x_continuous(breaks = 3:5 * 25) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ name, ncol = 1) # posterior simulations set.seed(25) p2 &lt;- predict(fit25.1a, summary = F, ndraws = 4) %&gt;% data.frame() %&gt;% mutate(sim = 1:n()) %&gt;% pivot_longer(-sim) %&gt;% ggplot(aes(x = value)) + geom_histogram(size = .25, binwidth = 2, boundary = 106, fill = &quot;skyblue&quot;, color = &quot;white&quot;) + geom_vline(xintercept = t1, linetype = 2) + labs(subtitle = &quot;Simulations&quot;, x = NULL) + scale_x_continuous(breaks = 3:5 * 25) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = range(d$y)) + facet_wrap(~ sim, ncol = 2, labeller = label_both) # combine library(patchwork) p1 + p2 + plot_layout(widths = c(1, 2)) + plot_annotation(title = &quot;PP check for the naïve model, fit25.1a&quot;) On the left column, the have the original uncensored data y and the y_na data where the censored values were removed. The four lighter histograms on the right are individual simulations based on our naïve model, fit25.1a, which only used the y_na data. In all cases, the simulated data are biased to be too small compared with the original data, y. They also have the undesirable quality that they don’t even match up with the y_na data, which drops sharply off at the 106 threshold, whereas the simulated data all contain values exceeding that point (depicted by the dashed vertical lines). Now let’s see what happens when we execute a few posterior predictive checks with our first censored model, fit25.2a. p1 &lt;- d %&gt;% pivot_longer(cols = c(y, y1)) %&gt;% ggplot(aes(x = value)) + geom_histogram(size = .25, binwidth = 2, boundary = 106, fill = &quot;skyblue4&quot;, color = &quot;white&quot;) + labs(subtitle = &quot;Data&quot;, x = NULL) + scale_x_continuous(breaks = 3:5 * 25) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ name, ncol = 1, scales = &quot;free_y&quot;) set.seed(25) p2 &lt;- predict(fit25.2a, summary = F, ndraws = 4) %&gt;% data.frame() %&gt;% mutate(sim = 1:n()) %&gt;% pivot_longer(-sim) %&gt;% ggplot(aes(x = value)) + geom_histogram(size = .25, binwidth = 2, boundary = 106, fill = &quot;skyblue&quot;, color = &quot;white&quot;) + labs(subtitle = &quot;Simulations&quot;, x = NULL) + scale_x_continuous(breaks = 3:5 * 25) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = range(d$y)) + facet_wrap(~ sim, ncol = 2, labeller = label_both) # combine p1 + p2 + plot_layout(widths = c(1, 2)) + plot_annotation(title = &quot;PP check for the censored model, fit25.2a&quot;) Now the posterior predictive draws match up nicely from the original uncensored data y in terms of central tendency, dispersion, and their overall shapes. This is the magic of a model that accounts for right (or left) censoring. It can take all those stacked-up 106 values and approximate the underlying distribution, had they been more accurately recorded. 25.4.2 Interval censoring: When your values are somewhere in the middle. Time to move onto interval censoring. Kruschke’s example in the text included values censored in the interval between 94 and 100. This will require two more thresholds, which we’ll call t2 and t3. t2 &lt;- 94 t3 &lt;- 100 We should revisit part of the block quote from the brms reference manual, above: For interval censored data, a second variable (let’s call it y2) has to be passed to cens. In this case, the formula has the structure y | cens(censored,y2) ~ predictors. While the lower bounds are given in y, the upper bounds are given in y2 for interval censored data. Intervals are assumed to be open on the left and closed on the right: (y,y2] It’s a little unclear, to me, if this is how Kruschke defined his intervals, but since we’re working with brms we’ll just use this convention. Thus, we will define “values in the interval between 94 and 100” as y &gt;= t2 &amp; y &lt; t3. Here we’ll follow the brms convention and define a new variable y2 for our lower bound and y3 for our upper bound. Additionally, the new cen2 variable will tell us whether a case is interval censored (\"interval\") or not (\"none\"). We have also redefined y_na in terms of our cen2 variable. d &lt;- d %&gt;% mutate(y2 = if_else(y &gt;= t2 &amp; y &lt; t3, t2, y), y3 = if_else(y &gt;= t2 &amp; y &lt; t3, t3, y), cen2 = if_else(y &gt;= t2 &amp; y &lt; t3, &quot;interval&quot;, &quot;none&quot;)) %&gt;% mutate(y_na = ifelse(cen2 == &quot;none&quot;, y, NA)) d ## # A tibble: 500 × 7 ## y y1 cen1 y_na y2 y3 cen2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 96.8 96.8 none NA 94 100 interval ## 2 84.4 84.4 none 84.4 84.4 84.4 none ## 3 82.7 82.7 none 82.7 82.7 82.7 none ## 4 105. 105. none 105. 105. 105. none ## 5 77.5 77.5 none 77.5 77.5 77.5 none ## 6 93.3 93.3 none 93.3 93.3 93.3 none ## 7 126. 106 right 126. 126. 126. none ## 8 108. 106 right 108. 108. 108. none ## 9 101. 101. none 101. 101. 101. none ## 10 99.1 99.1 none NA 94 100 interval ## # … with 490 more rows Now let’s get a better sense of the new data with a few histograms. d %&gt;% pivot_longer(cols = c(y, y2, y3, y_na)) %&gt;% mutate(name = factor(name, levels = c(&quot;y&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y_na&quot;))) %&gt;% ggplot(aes(x = value)) + geom_histogram(size = .25, binwidth = 2, boundary = 106, fill = &quot;skyblue4&quot;, color = &quot;white&quot;) + labs(subtitle = &quot;Our data have been updated&quot;, x = NULL) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ name, ncol = 2) Now we fit two models. The first, fit25.1b, will only used the values not interval censored, y_na. The second model, fit25.2b, will use the cens() function to fit account for the interval censored data with the y2, y3, and cen2 columns. As before, we’ll want to use inits to help the censored model run smoothly. # naïve fit25.1b &lt;- brm(data = d, family = gaussian, y_na ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, file = &quot;fits/fit25.01b&quot;) # censored fit25.2b &lt;- brm(data = d, family = gaussian, y2 | cens(cen2, y3) ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, # here we insert our start values for the intercept inits = inits_list, file = &quot;fits/fit25.02b&quot;) Review the model summaries. print(fit25.1b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y_na ~ 1 ## Data: d (Number of observations: 424) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 100.24 0.77 98.74 101.73 1.00 3302 2636 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 15.79 0.53 14.81 16.87 1.00 3801 2459 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit25.2b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y2 | cens(cen2, y3) ~ 1 ## Data: d (Number of observations: 500) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 99.75 0.65 98.49 101.07 1.00 3475 2566 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 14.61 0.46 13.74 15.56 1.00 2822 2343 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). In this case, both models did a reasonable job approximating the original data-generating parameters, \\(\\operatorname{Normal}(100, 15)\\). However, the naïve model fit25.1b did so at a considerable loss of precision, as indicated by the posterior standard deviations. To give a better sense, here are the model parameters in an interval plot. # wrangle bind_rows(as_draws_df(fit25.1b), as_draws_df(fit25.2b)) %&gt;% mutate(fit = rep(c(&quot;fit25.1b&quot;, &quot;fit25.2b&quot;), each = n() / 2)) %&gt;% pivot_longer(b_Intercept:sigma) %&gt;% mutate(parameter = if_else(name == &quot;b_Intercept&quot;, &quot;mu&quot;, &quot;sigma&quot;)) %&gt;% # plot ggplot(aes(x = value, y = fit)) + stat_interval(.width = c(.5, .95)) + scale_color_manual(&quot;ETI&quot;, values = c(&quot;skyblue2&quot;, &quot;skyblue4&quot;), labels = c(&quot;95%&quot;, &quot;50%&quot;)) + labs(x = &quot;marginal posterior&quot;, y = NULL) + theme(axis.ticks.y = element_blank()) + facet_wrap(~ parameter, scales = &quot;free_x&quot;, labeller = label_parsed) One could also argue the parameters for the censored model fit25.2b were a little less biased. However, I’d be leery of making that as a general claim based on this example, alone. 25.4.3 Complex censoring: When the going gets tough… Okay, let’s put what we’ve learned together and practice with data that are both right AND interval censored. This is going to require some tricky coding on our part. Since interval censoring is in play, we’ll need two y variables, again. When the data are indeed interval censored, their lower and upper bounds will be depicted by y4 and y5, respectively. When the data are right censored, y4 will contain the threshold 106 and y5 will contain the original value from y. All this tricky information will be indexed in our nominal variable cen3. Once again, we update our y_na variable, too. d &lt;- d %&gt;% mutate(y4 = if_else(y &gt;= t2 &amp; y &lt; t3, t2, if_else(y &gt; t1, t1, y)), y5 = if_else(y &gt;= t2 &amp; y &lt; t3, t3, y), cen3 = if_else(y &gt;= t2 &amp; y &lt; t3, &quot;interval&quot;, if_else(y &gt; t1, &quot;right&quot;, &quot;none&quot;))) %&gt;% mutate(y_na = ifelse(cen3 == &quot;none&quot;, y, NA)) d ## # A tibble: 500 × 10 ## y y1 cen1 y_na y2 y3 cen2 y4 y5 cen3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 96.8 96.8 none NA 94 100 interval 94 100 interval ## 2 84.4 84.4 none 84.4 84.4 84.4 none 84.4 84.4 none ## 3 82.7 82.7 none 82.7 82.7 82.7 none 82.7 82.7 none ## 4 105. 105. none 105. 105. 105. none 105. 105. none ## 5 77.5 77.5 none 77.5 77.5 77.5 none 77.5 77.5 none ## 6 93.3 93.3 none 93.3 93.3 93.3 none 93.3 93.3 none ## 7 126. 106 right NA 126. 126. none 106 126. right ## 8 108. 106 right NA 108. 108. none 106 108. right ## 9 101. 101. none 101. 101. 101. none 101. 101. none ## 10 99.1 99.1 none NA 94 100 interval 94 100 interval ## # … with 490 more rows Explore the new data with histograms. d %&gt;% pivot_longer(cols = c(y, y4, y5, y_na)) %&gt;% mutate(name = factor(name, levels = c(&quot;y&quot;, &quot;y4&quot;, &quot;y5&quot;, &quot;y_na&quot;))) %&gt;% ggplot(aes(x = value)) + geom_histogram(size = .25, binwidth = 2, boundary = 106, fill = &quot;skyblue4&quot;, color = &quot;white&quot;) + labs(subtitle = &quot;Our data have been updated, again&quot;, x = NULL) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ name, ncol = 2) In the text, Kruschke reported he had 255 uncensored values (p. 732). Here’s the breakdown of our data. d %&gt;% count(cen3) ## # A tibble: 3 × 2 ## cen3 n ## &lt;chr&gt; &lt;int&gt; ## 1 interval 76 ## 2 none 257 ## 3 right 167 We got really close! Now we fit our two models: fit25.1c, which ignores the censoring problem and just drops those cases, and fit25.2c, which makes slick use of the cens() function. # naïve fit25.1c &lt;- brm(data = d, family = gaussian, y_na ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, file = &quot;fits/fit25.01c&quot;) # censored fit25.2c &lt;- brm(data = d, family = gaussian, y4 | cens(cen3, y5) ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, # here we insert our start values for the intercept inits = inits_list, file = &quot;fits/fit25.02c&quot;) Compare the model summaries. print(fit25.1c) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y_na ~ 1 ## Data: d (Number of observations: 257) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 89.87 0.64 88.62 91.12 1.00 3532 2900 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 10.12 0.44 9.30 11.01 1.00 4041 2475 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit25.2c) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y4 | cens(cen3, y5) ~ 1 ## Data: d (Number of observations: 500) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 99.48 0.70 98.13 100.89 1.00 3074 2240 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 14.27 0.58 13.21 15.45 1.00 2792 2545 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The results for the naïve model are a mess and those from the censored model look great! Also note how the latter used all avaliable cases, whereas the former dropped nearly half of them. Before we can make our version of Figure 25.4, we’ll need to extract the posterior draws. We’ll start with fit25.1c. draws &lt;- as_draws_df(fit25.1c) %&gt;% mutate(mu = b_Intercept, `(mu-100)/sigma` = (b_Intercept - 100) / sigma) head(draws) ## # A draws_df: 6 iterations, 1 chains, and 6 variables ## b_Intercept sigma lprior lp__ mu (mu-100)/sigma ## 1 89 9.9 -11 -968 89 -1.10 ## 2 90 10.6 -11 -968 90 -0.98 ## 3 91 9.7 -11 -968 91 -0.98 ## 4 90 10.3 -11 -967 90 -0.99 ## 5 90 9.8 -11 -968 90 -0.97 ## 6 90 9.6 -11 -968 90 -1.08 ## # ... hidden reserved variables {&#39;.chain&#39;, &#39;.iteration&#39;, &#39;.draw&#39;} These subplots look a lot like those from back in Section 16.2. Since this is the last plot from the book, it seems like we should go all out. To reduce some of the code redundancy with the six subplots of the marginal posteriors, we’ll make a custom geom, geom_hist(). geom_hist &lt;- function(xintercept = xintercept, binwidth = binwidth, ...) { list( geom_histogram(fill = &quot;skyblue&quot;, color = &quot;white&quot;, size = .2, binwidth = binwidth, boundary = 106), geom_vline(xintercept = xintercept, color = &quot;skyblue3&quot;, size = 1/2, linetype = 3), stat_pointinterval(aes(y = 0), point_interval = mode_hdi, .width = .95), scale_y_continuous(NULL, breaks = NULL) ) } Now we have our geom_hist(), here are the first three histograms for the marginal posteriors from fit25.1c. p1 &lt;- draws %&gt;% ggplot(aes(x = mu)) + geom_hist(xintercept = 100, binwidth = 0.1) + xlab(expression(mu)) p3 &lt;- draws %&gt;% ggplot(aes(x = sigma)) + geom_hist(xintercept = 15, binwidth = 0.08) + xlab(expression(sigma)) p4 &lt;- draws %&gt;% ggplot(aes(x = `(mu-100)/sigma`)) + geom_hist(xintercept = 0, binwidth = 0.015) + xlab(expression((mu-100)/sigma)) The histogram of the censored data with the posterior predictive density curves superimposed will take a little more work. n_lines &lt;- 50 p2 &lt;- draws %&gt;% slice(1:n_lines) %&gt;% expand(nesting(.draw, mu, sigma), y_na = seq(from = 40, to = 120, by = 1)) %&gt;% mutate(density = dnorm(x = y_na, mean = mu, sd = sigma)) %&gt;% ggplot(aes(x = y_na)) + geom_histogram(data = d, aes(y = stat(density)), size = .25, binwidth = 2, boundary = 106, fill = &quot;skyblue4&quot;, color = &quot;white&quot;) + geom_line(aes(y = density, group = .draw), size = 1/4, alpha = 1/3, color = &quot;skyblue&quot;) + scale_x_continuous(&quot;y&quot;, limits = c(40, 110)) + scale_y_continuous(NULL, breaks = NULL) Now extract the posterior draws from our censored model, fit25.2, and repeat the process. draws &lt;- as_draws_df(fit25.2c) %&gt;% mutate(mu = b_Intercept, `(mu-100)/sigma` = (b_Intercept - 100) / sigma) p5 &lt;- draws %&gt;% ggplot(aes(x = mu)) + geom_hist(xintercept = 100, binwidth = 0.1) + xlab(expression(mu)) p7 &lt;- draws %&gt;% ggplot(aes(x = sigma)) + geom_hist(xintercept = 15, binwidth = 0.1) + xlab(expression(sigma)) p8 &lt;- draws %&gt;% ggplot(aes(x = `(mu-100)/sigma`)) + geom_hist(xintercept = 0, binwidth = 0.01) + xlab(expression((mu-100)/sigma)) p6 &lt;- draws %&gt;% slice(1:n_lines) %&gt;% expand(nesting(.draw, mu, sigma), y_na = seq(from = 40, to = 120, by = 1)) %&gt;% mutate(density = dnorm(x = y_na, mean = mu, sd = sigma)) %&gt;% ggplot(aes(x = y_na)) + geom_histogram(data = d, aes(y = stat(density)), size = .25, binwidth = 2, boundary = 106, fill = &quot;skyblue4&quot;, color = &quot;white&quot;) + geom_line(aes(y = density, group = .draw), size = 1/4, alpha = 1/3, color = &quot;skyblue&quot;) + scale_x_continuous(&quot;y&quot;, limits = c(40, 110)) + scale_y_continuous(NULL, breaks = NULL) Finally, combine the subplots, and annotate a bit. ((p1 | p2) / (p3 | p4) / (p5 | p6) / (p7 | p8)) + plot_annotation(title = &quot;This is our final plot, together.&quot;, caption = expression(atop(italic(&quot;Upper quartet&quot;)*&quot;: Censored data omitted from analysis; parameter estimates are too small. &quot;, italic(&quot;Lower quartet&quot;)*&quot;: Censored data imputed in known bins; parameter estimates are accurate.&quot;))) &amp; theme(plot.caption = element_text(hjust = 0)) To learn more about censored data, check out the nice lecture by Gordon Fox, Introduction to analysis of censored and truncated data. 25.4.4 Bonus: Truncation. Though Kruschke didn’t cover it, truncation is often confused with censoring. When you have actually collected all your values, but some are just less certain than you’d like, you have a censoring issue. That is, you can think of censoring as an issue with measurement precision. Truncation has to do with data collection. When there’s some level in your criterion variable that you haven’t collected data on, that’s a truncation issue. For example, imagine you wanted to predict scores on an IQ test with socioeconomic status (SES). There might be a relation, there. But now imagine you only collected data on people with an IQ above 110. Those IQ data are severely left truncated. If you only care about the relation of SES and IQ for those with high IQ scores, your data are fine. But if you want to make general statements across the full range of IQ values, standard regression methods will likely produce biased results. Happily, brms allows users to accommodate truncated criterion variables with the trunc() function, which works in a similar way to the cens() function. For details on fitting truncated regression models with trunc(), check out the Additional response information subsection of the brmsformula section of the brms reference manual. The end of the Fox lecture, above, briefly covers truncated regression. You can also find a brief vignette on truncation by the good folks at the UCLA Institute for Digital Research and Education. 25.5 What Next? “If you have made it this far and you are looking for more, you might peruse posts at [Kruschke’s] blog, https://doingbayesiandataanalysis.blogspot.com/, and search there for topics that interest you.” In addition to the other references Kruschke mentioned, you might also check out McElreath’s Statistical rethinking, both first (2015) and second (2020) editions. Much like this ebook, I have recoded both editions of Statistical rethinking in a bookdown form (Kurz, 2021, 2020). Click here for the first and here for the second. You can find other pedagogical material at my academic blog, https://solomonkurz.netlify.com/post/. For assistance on brms-related issues, check out the brms section on the Stan forums at https://discourse.mc-stan.org/c/interfaces/brms/36. Nicenboim, Schad, and Vasishth also have a nice new (2021) ebook called An introduction to Bayesian data analysis for cognitive science, which highlights brms in a way that could compliment the material we have practiced together. Happy modeling, friends! Session info sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur/Monterey 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.1.2 tidybayes_3.0.2 ggthemes_4.2.4 brms_2.18.0 ## [5] Rcpp_1.0.9 forcats_0.5.1 stringr_1.4.1 dplyr_1.0.10 ## [9] purrr_0.3.4 readr_2.1.2 tidyr_1.2.1 tibble_3.1.8 ## [13] ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.4.1 backports_1.4.1 plyr_1.8.7 ## [4] igraph_1.3.4 svUnit_1.0.6 sp_1.5-0 ## [7] splines_4.2.0 crosstalk_1.2.0 TH.data_1.1-1 ## [10] rstantools_2.2.0 inline_0.3.19 digest_0.6.30 ## [13] htmltools_0.5.3 fansi_1.0.3 magrittr_2.0.3 ## [16] checkmate_2.1.0 googlesheets4_1.0.1 tzdb_0.3.0 ## [19] modelr_0.1.8 RcppParallel_5.1.5 matrixStats_0.62.0 ## [22] xts_0.12.1 sandwich_3.0-2 prettyunits_1.1.1 ## [25] colorspace_2.0-3 rvest_1.0.2 ggdist_3.2.0 ## [28] haven_2.5.1 xfun_0.35 callr_3.7.3 ## [31] crayon_1.5.2 jsonlite_1.8.3 lme4_1.1-31 ## [34] survival_3.4-0 zoo_1.8-10 glue_1.6.2 ## [37] gtable_0.3.1 gargle_1.2.0 emmeans_1.8.0 ## [40] distributional_0.3.1 pkgbuild_1.3.1 rstan_2.21.7 ## [43] abind_1.4-5 scales_1.2.1 mvtnorm_1.1-3 ## [46] emo_0.0.0.9000 DBI_1.1.3 miniUI_0.1.1.1 ## [49] xtable_1.8-4 HDInterval_0.2.2 stats4_4.2.0 ## [52] StanHeaders_2.21.0-7 DT_0.24 htmlwidgets_1.5.4 ## [55] httr_1.4.4 threejs_0.3.3 arrayhelpers_1.1-0 ## [58] posterior_1.3.1 ellipsis_0.3.2 pkgconfig_2.0.3 ## [61] loo_2.5.1 farver_2.1.1 sass_0.4.2 ## [64] dbplyr_2.2.1 utf8_1.2.2 labeling_0.4.2 ## [67] tidyselect_1.1.2 rlang_1.0.6 reshape2_1.4.4 ## [70] later_1.3.0 munsell_0.5.0 cellranger_1.1.0 ## [73] tools_4.2.0 cachem_1.0.6 cli_3.5.0 ## [76] generics_0.1.3 broom_1.0.1 ggridges_0.5.3 ## [79] evaluate_0.18 fastmap_1.1.0 processx_3.8.0 ## [82] knitr_1.40 fs_1.5.2 nlme_3.1-159 ## [85] mime_0.12 projpred_2.2.1 xml2_1.3.3 ## [88] compiler_4.2.0 bayesplot_1.9.0 shinythemes_1.2.0 ## [91] rstudioapi_0.13 gamm4_0.2-6 reprex_2.0.2 ## [94] bslib_0.4.0 stringi_1.7.8 highr_0.9 ## [97] ps_1.7.2 Brobdingnag_1.2-8 lattice_0.20-45 ## [100] Matrix_1.4-1 nloptr_2.0.3 markdown_1.1 ## [103] shinyjs_2.1.0 tensorA_0.36.2 vctrs_0.5.1 ## [106] pillar_1.8.1 lifecycle_1.0.3 jquerylib_0.1.4 ## [109] bridgesampling_1.1-2 estimability_1.4.1 raster_3.5-15 ## [112] httpuv_1.6.5 R6_2.5.1 bookdown_0.28 ## [115] promises_1.2.0.1 gridExtra_2.3 codetools_0.2-18 ## [118] boot_1.3-28 colourpicker_1.1.1 MASS_7.3-58.1 ## [121] gtools_3.9.3 assertthat_0.2.1 withr_2.5.0 ## [124] shinystan_2.6.0 multcomp_1.4-20 mgcv_1.8-40 ## [127] parallel_4.2.0 hms_1.1.1 terra_1.5-21 ## [130] grid_4.2.0 coda_0.19-4 minqa_1.2.5 ## [133] rmarkdown_2.16 googledrive_2.0.0 shiny_1.7.2 ## [136] lubridate_1.8.0 base64enc_0.1-3 dygraphs_1.1.1.6 References Aczel, B., Hoekstra, R., Gelman, A., Wagenmakers, E.-J., Klugkist, I. G., Rouder, J. N., Vandekerckhove, J., Lee, M. D., Morey, R. D., Vanpaemel, W., Dienes, Z., &amp; van Ravenzwaaij, D. (2020). Discussion points for Bayesian inference. Nature Human Behaviour, 1–3. https://doi.org/10.1038/s41562-019-0807-z Bürkner, P.-C. (2022d). brms reference manual, Version 2.17.0. https://CRAN.R-project.org/package=brms/brms.pdf Chen, M.-H., He, X., Shao, Q.-M., &amp; Xu, H. (2003). A Monte Carlo gap test in computing HPD regions. In Development of Modern Statistics and Related Topics: Vols. Volume 1 (pp. 38–52). World Scientific. https://doi.org/10.1142/9789812796707_0004 Chen, M.-H., &amp; Shao, Q.-M. (1999). Monte Carlo estimation of Bayesian credible and HPD intervals. Journal of Computational and Graphical Statistics, 8(1), 69–92. https://doi.org/10.1080/10618600.1999.10474802 Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., &amp; Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378 Hyndman, R. J. (1996). Computing and graphing highest density regions. The American Statistician, 50(2), 120–126. https://doi.org/10.1080/00031305.1996.10474359 Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Hofelich Mohr, A., IJzerman, H., Nilsonne, G., Vanpaemel, W., &amp; Frank, M. C. (2018). A practical guide for transparency in psychological science. Collabra: Psychology, 4(1), 1–15. https://doi.org/10.1525/collabra.158 Kruschke, J. K. (2013). Posterior predictive checks can and should be Bayesian: Comment on Gelman and Shalizi, “Philosophy and the practice of Bayesian statistics.” British Journal of Mathematical and Statistical Psychology, 66(1), 45–56. https://doi.org/10.1111/j.2044-8317.2012.02063.x Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Kruschke, J. K. (2021). Bayesian analysis reporting guidelines. Nature Human Behaviour, 5(10), 1282–1291. https://doi.org/10.1038/s41562-021-01177-7 Kruschke, J. K., &amp; Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin &amp; Review, 25(1), 178–206. https://doi.org/10.3758/s13423-016-1221-4 Kurz, A. S. (2021). Statistical rethinking with brms, ggplot2, and the tidyverse: Second Edition (version 0.2.0). https://bookdown.org/content/4857/ Kurz, A. S. (2020). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 Martone, M. E., Garcia-Castro, A., &amp; VandenBos, G. R. (2018). Data sharing in psychology. The American Psychologist, 73(2), 111–125. https://doi.org/10.1037/amp0000242 McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/ Nicenboim, B., Schad, D., &amp; Vasishth, S. (2021). An introduction to Bayesian data analysis for cognitive science. https://vasishth.github.io/bayescogsci/book/ Rouder, J. N. (2016). The what, why, and how of born-open data. Behavior Research Methods, 48(3), 1062–1069. https://doi.org/10.3758/s13428-015-0630-z Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2022). R Markdown: The definitive guide. Chapman and Hall/CRC. https://bookdown.org/yihui/rmarkdown/ "],["references.html", "References", " References Aczel, B., Hoekstra, R., Gelman, A., Wagenmakers, E.-J., Klugkist, I. G., Rouder, J. N., Vandekerckhove, J., Lee, M. D., Morey, R. D., Vanpaemel, W., Dienes, Z., &amp; van Ravenzwaaij, D. (2020). Discussion points for Bayesian inference. Nature Human Behaviour, 1–3. https://doi.org/10.1038/s41562-019-0807-z Agresti, A. (2015). Foundations of linear and generalized linear models. John Wiley &amp; Sons. https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034 Allan, A., Cook, D., Gayler, R., Kirk, H., Peng, R., &amp; Saber, E. (2021). ochRe: Australia-themed colour palettes [Manual]. https://github.com/ropenscilabs/ochRe Arnold, J. B. (2021). ggthemes: Extra themes, scales and geoms for ’ggplot2’. https://CRAN.R-project.org/package=ggthemes Atkins, D. C., Baldwin, S. A., Zheng, C., Gallop, R. J., &amp; Neighbors, C. (2013). A tutorial on count regression and zero-altered count models for longitudinal substance use data. Psychology of Addictive Behaviors, 27(1), 166. https://doi.org/10.1037/a0029508 Attali, D., &amp; Baker, C. (2022). ggExtra: Add marginal histograms to ’ggplot2’, and more ’ggplot2’ enhancements. https://CRAN.R-project.org/package=ggExtra Auguie, B. (2017). gridExtra: Miscellaneous functions for \"grid\" graphics. https://CRAN.R-project.org/package=gridExtra Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01 Bates, D., Maechler, M., Bolker, B., &amp; Steven Walker. (2021). lme4: Linear mixed-effects models using Eigen’ and S4. https://CRAN.R-project.org/package=lme4 Bayes, T. (1763). LII. An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, FRS communicated by Mr. Price, in a letter to John Canton, AMFR S. Philosophical Transactions of the Royal Society of London, 53, 370–418. https://royalsocietypublishing.org/doi/pdf/10.1098/rstl.1763.0053 BibTeX. (2020). http://www.bibtex.org/ Bliss, C. I. (1934). The method of probits. Science. https://doi.org/10.1126/science.79.2037.38 Bolger, N., Zee, K. S., Rossignac-Milon, M., &amp; Hassin, R. R. (2019). Causal processes in psychology are heterogeneous. Journal of Experimental Psychology: General, 148(4), 601–618. https://doi.org/10.1037/xge0000558 Braumoeller, B. F. (2004). Hypothesis testing and multiplicative interaction terms. International Organization, 58(4), 807–820. https://doi.org/10.1017/S0020818304040251 Bryan, J., the STAT 545 TAs, &amp; Hester, J. (2020). Happy Git and GitHub for the useR. https://happygitwithr.com Bürkner, P.-C. (2020). Bayesian item response modeling in R with brms and Stan. http://arxiv.org/abs/1905.09501 Bürkner, P.-C. (2022). Define custom response distributions with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html Bürkner, P.-C. (2022a). Estimating distributional models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html Bürkner, P.-C. (2022b). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html Bürkner, P.-C. (2022). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html Bürkner, P.-C. (2022c). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html Bürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01 Bürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms. The R Journal, 10(1), 395–411. https://doi.org/10.32614/RJ-2018-017 Bürkner, P.-C. (2022d). brms reference manual, Version 2.17.0. https://CRAN.R-project.org/package=brms/brms.pdf Bürkner, P.-C. (2022e). brms: Bayesian regression models using ’Stan’. https://CRAN.R-project.org/package=brms Bürkner, P.-C., Gabry, J., Kay, M., &amp; Vehtari, A. (2021). posterior: Tools for working with posterior distributions [Manual]. Bürkner, P.-C., &amp; Vuorre, M. (2019). Ordinal regression models in psychology: A tutorial. Advances in Methods and Practices in Psychological Science, 2(1), 77–101. https://doi.org/10.1177/2515245918823199 Campbell, H., &amp; Gustafson, P. (2021). Re: Linde et al.(2021) factor, HDI-ROPE and frequentist equivalence testing are actually all equivalent. https://arxiv.org/abs/2104.07834 Carifio, J., &amp; Perla, R. (2008). Resolving the 50-year debate around using and misusing Likert scales. Medical Education, 42(12), 1150–1152. https://doi.org/10.1111/j.1365-2923.2008.03172.x Carifio, J., &amp; Perla, R. J. (2007). Ten common misunderstandings, misconceptions, persistent myths and urban legends about Likert scales and Likert response formats and their antidotes. Journal of Social Sciences, 3(3), 106–116. https://thescipub.com/pdf/10.3844/jssp.2007.106.116.pdf Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., &amp; Riddell, A. (2017). Stan: A probabilistic programming language. Journal of Statistical Software, 76(1). https://doi.org/10.18637/jss.v076.i01 Carvalho, C. M., Polson, N. G., &amp; Scott, J. G. (2009). Handling sparsity via the horseshoe. Artificial Intelligence and Statistics, 73–80. http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf Chandramouli, S. H., &amp; Shiffrin, R. M. (2019). Commentary on Gronau and Wagenmakers. Computational Brain &amp; Behavior, 2(1), 12–21. https://doi.org/10.1007/s42113-018-0017-1 Chen, M.-H., He, X., Shao, Q.-M., &amp; Xu, H. (2003). A Monte Carlo gap test in computing HPD regions. In Development of Modern Statistics and Related Topics: Vols. Volume 1 (pp. 38–52). World Scientific. https://doi.org/10.1142/9789812796707_0004 Chen, M.-H., &amp; Shao, Q.-M. (1999). Monte Carlo estimation of Bayesian credible and HPD intervals. Journal of Computational and Graphical Statistics, 8(1), 69–92. https://doi.org/10.1080/10618600.1999.10474802 Chung, Y., Rabe-Hesketh, S., Dorie, V., Gelman, A., &amp; Liu, J. (2013). A nondegenerate penalized likelihood estimator for variance parameters in multilevel models. Psychometrika, 78(4), 685–709. https://doi.org/10.1007/s11336-013-9328-2 Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd Edition). Routledge. https://doi.org/10.4324/9780203771587 Cumming, G. (2012). Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis. Routledge. https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682 Dale, A. I. (2012). A history of inverse probability: From Thomas Bayes to Karl Pearson. Springer Science &amp; Business Media. https://www.springer.com/gp/book/9780387988078 Duane, S., Kennedy, A. D., Pendleton, B. J., &amp; Roweth, D. (1987). Hybrid Monte Carlo. Physics Letters B, 195(2), 216–222. https://doi.org/10.1016/0370-2693(87)91197-X Eckhardt, R. (1987). Stan Ulam, John von Neumann and the Monte Carlo method. Argonne, USA. https://library.sciencemadness.org/lanl1_a/lib-www/pubs/00326867.pdf Efron, B., &amp; Morris, C. (1977). Stein’s paradox in statistics. Scientific American, 236(5), 119–127. https://doi.org/10.1038/scientificamerican0577-119 Ellison., S. L. R. (2018). metRology: Support for metrological applications. https://CRAN.R-project.org/package=metRology Enders, C. (2013). Centering predictors and contextual effects. In M. Scott, J. Simonoff, &amp; B. Marx (Eds.), The SAGE Handbook of Multilevel Modeling (pp. 89–108). SAGE Publications Ltd. https://doi.org/10.4135/9781446247600.n6 Enders, C. K., &amp; Tofighi, D. (2007). Centering predictor variables in cross-sectional multilevel models: A new look at an old issue. Psychological Methods, 12(2), 121. https://doi.org/10.1037/1082-989X.12.2.121 Fernandes, M., Walls, L., Munson, S., Hullman, J., &amp; Kay, M. (2018). Uncertainty displays using quantile dotplots or CDFs improve transit decision-making. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (pp. 1–12). Association for Computing Machinery. https://doi.org/10.1145/3173574.3173718 Fernández i Marín, X. (2016). ggmcmc: Analysis of MCMC samples and Bayesian inference. Journal of Statistical Software, 70(9), 1–20. https://doi.org/10.18637/jss.v070.i09 Fernández i Marín, X. (2021). ggmcmc: Tools for analyzing MCMC simulations from Bayesian inference [Manual]. https://CRAN.R-project.org/package=ggmcmc Firke, S. (2020). janitor: Simple tools for examining and cleaning dirty data. https://CRAN.R-project.org/package=janitor Fisher, R. A. (1925). Statistical methods for research workers, 11th ed. rev. Edinburgh. https://psycnet.apa.org/record/1925-15003-000 Gabry, J. (2022). Graphical posterior predictive checks using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/graphical-ppcs.html Gabry, J., &amp; Mahr, T. (2022). bayesplot: Plotting for Bayesian models. https://CRAN.R-project.org/package=bayesplot Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., &amp; Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378 Garnier, S. (2021). viridis: Default color maps from ’matplotlib’ [Manual]. https://CRAN.R-project.org/package=viridis Gelman, A. (2005). Analysis of variance–Why it is more important than ever. Annals of Statistics, 33(1), 1–53. https://doi.org/10.1214/009053604000001048 Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper). Bayesian Analysis, 1(3), 515–534. https://doi.org/10.1214/06-BA117A Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). Bayesian data analysis (Third Edition). CRC press. https://stat.columbia.edu/~gelman/book/ Gelman, A., Goodrich, B., Gabry, J., &amp; Vehtari, A. (2019). R-squared for Bayesian regression models. The American Statistician, 73(3), 307–309. https://doi.org/10.1080/00031305.2018.1549100 Gelman, A., Hill, J., &amp; Yajima, M. (2012). Why we (usually) don’t have to worry about multiple comparisons. Journal of Research on Educational Effectiveness, 5(2), 189–211. https://doi.org/10.1080/19345747.2011.618213 Grolemund, G., &amp; Wickham, H. (2017). R for data science. O’Reilly. https://r4ds.had.co.nz Gronau, Q. F., &amp; Wagenmakers, E.-J. (2019). Limitations of Bayesian leave-one-out cross-validation for model selection. Computational Brain &amp; Behavior, 2(1), 1–11. https://doi.org/10.1007/s42113-018-0011-7 Gronau, Q. F., &amp; Wagenmakers, E.-J. (2019). Rejoinder: More limitations of Bayesian leave-one-out cross-validation. Computational Brain &amp; Behavior, 2(1), 35–47. https://doi.org/10.1007/s42113-018-0022-4 Guber, L., Deborah. (1999). Getting what you pay for: The debate over equity in public school expenditures. Journal of Statistics Education, 7(2). https://www.semanticscholar.org/paper/Getting-What-You-Pay-For-The-Debate-Over-Equity-in-Guber/29c30e9dc77b56340faa5e6ad35e0741a5a83d49 Hamaker, E. L. (2012). Why researchers should think \"within-person\": A paradigmatic rationale. In Handbook of research methods for studying daily life (pp. 43–61). The Guilford Press. https://www.guilford.com/books/Handbook-of-Research-Methods-for-Studying-Daily-Life/Mehl-Conner/9781462513055 Hanley, J., A, &amp; Shapiro, S., H. (1994). Sexual activity and the lifespan of male fruitflies: A dataset that gets attention. Journal of Statistics Education, 2(1), null. https://doi.org/10.1080/10691898.1994.11910467 Henry, L., &amp; Wickham, H. (2020). purrr: Functional programming tools. https://CRAN.R-project.org/package=purrr Heyns, E. (2020). Better BibTeX for zotero. https://retorque.re/zotero-better-bibtex/ Hocking, T. D. (2021). Directlabels: Direct labels for multicolor plots [Manual]. https://CRAN.R-project.org/package=directlabels Hokusai, K. (1820–1831). The great wave off Kanagawa. Hugh-Jones, D. (2020). santoku: A versatile cutting tool. https://CRAN.R-project.org/package=santoku Hyndman, R. J. (1996). Computing and graphing highest density regions. The American Statistician, 50(2), 120–126. https://doi.org/10.1080/00031305.1996.10474359 Jean, J. (2009). RIFT SCULL. Jeffreys, H. (1961). Theory of probability. Oxford University Press. https://global.oup.com/academic/product/theory-of-probability-9780198503682?cc=us&amp;lang=en&amp; Kass, R. E., &amp; Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90(430), 773–795. https://www.stat.washington.edu/raftery/Research/PDF/kass1995.pdf Kay, M. (2022). Slab + interval stats and geoms. https://mjskay.github.io/ggdist/articles/slabinterval.html Kay, M. (2021). Extracting and visualizing tidy draws from brms models. https://mjskay.github.io/tidybayes/articles/tidy-brms.html Kay, M. (2020). tidybayes: Tidy data and ’geoms’ for Bayesian models. https://mjskay.github.io/tidybayes/ Kay, M., Kola, T., Hullman, J. R., &amp; Munson, S. A. (2016). When (ish) is my bus? User-centered visualizations of uncertainty in everyday, mobile predictive systems. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, 5092–5103. https://doi.org/10.1145/2858036.2858558 Kelley, K., &amp; Preacher, K. J. (2012). On effect size. Psychological Methods, 17(2), 137. https://doi.org/10.1037/a0028086 Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Hofelich Mohr, A., IJzerman, H., Nilsonne, G., Vanpaemel, W., &amp; Frank, M. C. (2018). A practical guide for transparency in psychological science. Collabra: Psychology, 4(1), 1–15. https://doi.org/10.1525/collabra.158 Kolmogorov, A. N., &amp; Bharucha-Reid, A. T. (1956). Foundations of the theory of probability: Second English Edition. Chelsea Publishing Company. https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations.pdf Kruschke, J. K. (2013). Posterior predictive checks can and should be Bayesian: Comment on Gelman and Shalizi, “Philosophy and the practice of Bayesian statistics.” British Journal of Mathematical and Statistical Psychology, 66(1), 45–56. https://doi.org/10.1111/j.2044-8317.2012.02063.x Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Kruschke, J. K. (2021). Bayesian analysis reporting guidelines. Nature Human Behaviour, 5(10), 1282–1291. https://doi.org/10.1038/s41562-021-01177-7 Kruschke, J. K., &amp; Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin &amp; Review, 25(1), 178–206. https://doi.org/10.3758/s13423-016-1221-4 Kurz, A. S. (2021). Statistical rethinking with brms, ggplot2, and the tidyverse: Second Edition (version 0.2.0). https://bookdown.org/content/4857/ Kurz, A. S. (2020). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 Lakens, D., &amp; Delacre, M. (2018). Equivalence testing and the second generation p-value. https://doi.org/10.31234/osf.io/7k6ay Lakens, D., McLatchie, N., Isager, P. M., Scheel, A. M., &amp; Dienes, Z. (2020). Improving inferences about null effects with Bayes factors and equivalence tests. The Journals of Gerontology: Series B, 75(1), 45–57. https://doi.org/10.1093/geronb/gby065 Lakens, D., Scheel, A. M., &amp; Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963 Lawlor, J. (2020). PNWColors: Color palettes inspired by nature in the US Pacific Northwest [Manual]. https://CRAN.R-project.org/package=PNWColors Lee, M. D., &amp; Webb, M. R. (2005). Modeling individual differences in cognition. Psychonomic Bulletin &amp; Review, 12(4), 605–621. https://doi.org/10.3758/BF03196751 Legler, J., &amp; Roback, P. (2019). Broadening your statistical horizons: Generalized linear models and multilevel models. https://bookdown.org/roback/bookdown-bysh/ Likert, R. (1932). A technique for the measurement of attitudes. Archives of Psychology, 22 140, 55–55. https://legacy.voteview.com/pdf/Likert_1932.pdf Linde, M., Tendeiro, J. N., Selker, R., Wagenmakers, E.-J., &amp; van Ravenzwaaij, D. (2021). Decisions about equivalence: A comparison of TOST, HDI-ROPE, and the Bayes factor. Psychological Methods. https://doi.org/10.1037/met0000402 Littlefield, T. (2020). lisa: Color palettes from color lisa [Manual]. https://CRAN.R-project.org/package=lisa Liu, C. C., &amp; Aitkin, M. (2008). Bayes factors: Prior sensitivity and model generalizability. Journal of Mathematical Psychology, 52(6), 362–375. https://doi.org/10.1016/j.jmp.2008.03.002 Lucas, T. (2016). palettetown: Use Pokemon inspired colour palettes [Manual]. https://CRAN.R-project.org/package=palettetown Luce, R. D. (2012). Individual choice behavior: A theoretical analysis. Courier Corporation. https://books.google.com?id=ERQsKkPiKkkC Luce, R. D. (2008). Luce’s choice axiom. Scholarpedia, 3(12), 8077. https://doi.org/10.4249/scholarpedia.8077 MacKay, D. J. (2003). Information theory, inference and learning algorithms. Cambridge University Press. https://www.inference.org.uk/itprnn/book.pdf Martone, M. E., Garcia-Castro, A., &amp; VandenBos, G. R. (2018). Data sharing in psychology. The American Psychologist, 73(2), 111–125. https://doi.org/10.1037/amp0000242 McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/ McGrayne, S. B. (2011). The theory that would not die: How Bayes’ rule cracked the enigma code, hunted down Russian submarines, &amp; emerged triumphant from two centuries of controversy. Yale University Press. https://yalebooks.yale.edu/book/9780300188226/theory-would-not-die McWhite, C. D., &amp; Wilke, C. O. (2021). colorblindr: Simulate colorblindness in R figures [Manual]. https://github.com/clauswilke/colorblindr Merkle, E. C., &amp; Rosseel, Y. (2018). blavaan: Bayesian structural equation models via parameter expansion. Journal of Statistical Software, 85(4), 1–30. https://doi.org/10.18637/jss.v085.i04 Merkle, E. C., Rosseel, Y., &amp; Goodrich, B. (2021). blavaan: Bayesian latent variable analysis. https://CRAN.R-project.org/package=blavaan Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953). Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 21(6), 1087–1092. https://doi.org/10.1063/1.1699114 Miller, D. L. (2021). beyonce: Beyoncé colour palettes for R. https://github.com/dill/beyonce Miller, J. (2009). What is the probability of replicating a statistically significant effect? Psychonomic Bulletin &amp; Review, 16(4), 617–640. https://doi.org/10.3758/PBR.16.4.617 Müller, K., &amp; Wickham, H. (2020). tibble: Simple data frames. https://CRAN.R-project.org/package=tibble Nakagawa, S., &amp; Foster, T. M. (2004). The case against retrospective statistical power analyses with an introduction to power analysis. Acta Ethologica, 7(2), 103–108. https://doi.org/10.1007/s10211-004-0095-z Navarro, D. (2019). Learning statistics with R. https://learningstatisticswithr.com Navarro, D. J. (2019). Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection. Computational Brain &amp; Behavior, 2(1), 28–34. https://doi.org/10.1007/s42113-018-0019-z Neal, R. (2011). MCMC using Hamiltonian dynamics. In S. Brooks, A. Gelman, G. Jones, &amp; X.-L. Meng (Eds.), Handbook of Markov chain Monte Carlo (pp. 116–162). London, United Kingdom: Chapman &amp; Hall/CRC Press. https://arxiv.org/pdf/1206.1901.pdf Neal, R. M. (1994). An improved acceptance procedure for the hybrid Monte Carlo algorithm. Journal of Computational Physics, 111(1), 194–203. https://doi.org/10.1006/jcph.1994.1054 Nelder, J. A., &amp; Wedderburn, R. W. (1972). Generalized linear models. Journal of the Royal Statistical Society: Series A (General), 135(3), 370–384. https://doi.org/10.2307/2344614 Nicenboim, B., Schad, D., &amp; Vasishth, S. (2021). An introduction to Bayesian data analysis for cognitive science. https://vasishth.github.io/bayescogsci/book/ Norman, G. (2010). Likert scales, levels of measurement and the “laws” of statistics. Advances in Health Sciences Education, 15(5), 625–632. https://doi.org/10.1007/s10459-010-9222-y O’Keefe, D. J. (2007). Brief report: Post hoc power, observed power, a priori power, retrospective power, prospective power, achieved power: Sorting out appropriate uses of statistical power analyses. Communication Methods and Measures, 1(4), 291–299. https://doi.org/10.1080/19312450701641375 Pedersen, Thomas Lin. (n.d.). Draw polygons with expansion/contraction and/or rounded corners geom_shape. Retrieved September 11, 2020, from https://ggforce.data-imaginist.com/reference/geom_shape.html Pedersen, Thomas L. (2020). Adding annotation and style. https://patchwork.data-imaginist.com/articles/guides/annotation.html Pedersen, Thomas Lin. (2020). patchwork: The composer of plots. https://CRAN.R-project.org/package=patchwork Pedersen, Thomas L. (2020). Plot assembly. https://patchwork.data-imaginist.com/articles/guides/assembly.html Pedersen, Thomas Lin. (2021). ggforce: Accelerating ’ggplot2’ [Manual]. https://CRAN.R-project.org/package=ggforce Pedersen, Thomas Lin, &amp; Crameri, F. (2021). scico: Colour palettes based on the scientific colour-maps [Manual]. https://CRAN.R-project.org/package=scico Pek, J., &amp; Flora, D. B. (2018). Reporting effect sizes in original psychological research: A discussion and tutorial. Psychological Methods, 23(2), 208. https://doi.org/https://doi.apa.org/fulltext/2017-10871-001.html Peng, R. D. (2020). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/ Piironen, J., &amp; Vehtari, A. (2017). Sparsity information and regularization in the horseshoe and other shrinkage priors. Electronic Journal of Statistics, 11(2), 5018–5051. https://doi.org/10.1214/17-EJS1337SI Plummer, M., Best, N., Cowles, K., &amp; Vines, K. (2006). CODA: Convergence diagnosis and output analysis for MCMC. R News, 6(1), 7–11. https://journal.r-project.org/archive/ Plummer, M., Best, N., Cowles, K., Vines, K., Sarkar, D., Bates, D., Almond, R., &amp; Magnusson, A. (2020). coda: Output analysis and diagnostics for MCMC [Manual]. https://CRAN.R-project.org/package=coda R Core Team. (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Revelle, W. (2022). psych: Procedures for psychological, psychometric, and personality research. https://CRAN.R-project.org/package=psych Ripley, B. (2021). MASS: Support functions and datasets for venables and Ripley’s MASS. https://CRAN.R-project.org/package=MASS Rosa, L., Rosa, E., Sarner, L., &amp; Barrett, S. (1998). A close look at therapeutic touch. JAMA, 279(13), 1005–1010. https://doi.org/10.1001/jama.279.13.1005 Rouder, J. N. (2016). The what, why, and how of born-open data. Behavior Research Methods, 48(3), 1062–1069. https://doi.org/10.3758/s13428-015-0630-z Rouder, J. N., Morey, R. D., Speckman, P. L., &amp; Province, J. M. (2012). Default Bayes factors for ANOVA designs. Journal of Mathematical Psychology, 56(5), 356–374. https://doi.org/10.1016/j.jmp.2012.08.001 Roy Rosenzweig Center for History and New Media. (2020). Zotero. https://www.zotero.org/ Schiettekatte, N. M. D., Brandl, S. J., &amp; Casey, J. M. (2022). fishualize: Color palettes based on fish species [Manual]. https://CRAN.R-project.org/package=fishualize Schloerke, B., Crowley, J., Di Cook, Briatte, F., Marbach, M., Thoen, E., Elberg, A., &amp; Larmarange, J. (2021). GGally: Extension to ’ggplot2’. https://CRAN.R-project.org/package=GGally Skinner, B. F. (1956). A case history in scientific method. American Psychologist, 11(5), 221–233. https://doi.org/10.1037/h0047662 Snee, R. D. (1974). Graphical display of two-way contingency tables. The American Statistician, 28(1), 9–12. https://doi.org/10.1080/00031305.1974.10479053 Stan Development Team. (2022a). Accessing the contents of a stanfit object. https://CRAN.R-project.org/package=rstan/vignettes/stanfit-objects.html Stan Development Team. (2022b). Stan reference manual, Version 2.29. https://mc-stan.org/docs/2_29/reference-manual/ Stan Development Team. (2022c). Stan user’s guide, Version 2.29. https://mc-stan.org/docs/2_29/stan-users-guide/index.html Steidl, R. J., Hayes, J. P., &amp; Schauber, E. (1997). Statistical power analysis in wildlife research. The Journal of Wildlife Management, 61(2), 270. https://doi.org/10.2307/3802582 Sun, S., Pan, W., &amp; Wang, L. L. (2011). Rethinking observed power: Concept, practice, and implications. Methodology, 7(3), 81–87. https://doi.org/10.1027/1614-2241/a000025 Thomas, L. (1997). Retrospective power analysis. Conservation Biology, 11(1), 276–280. https://doi.org/10.1046/j.1523-1739.1997.96102.x Vanpaemel, W. (2010). Prior sensitivity in theory testing: An apologia for the Bayes factor. Journal of Mathematical Psychology, 54(6), 491–498. https://doi.org/10.1016/j.jmp.2010.07.003 Vehtari, A., &amp; Gabry, J. (2022a). Using the loo Package (Version \\(&gt;\\)= 2.0.0). https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html Vehtari, A., &amp; Gabry, J. (2022b, March 23). Bayesian stacking and pseudo-BMA weights using the loo package. https://CRAN.R-project.org/package=loo/vignettes/loo2-weights.html Vehtari, A., Gabry, J., Magnusson, M., Yao, Y., &amp; Gelman, A. (2022). loo: Efficient leave-one-out cross-validation and WAIC for bayesian models. https://CRAN.R-project.org/package=loo/ Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; Bürkner, P.-C. (2021). Rank-normalization, folding, and localization: An improved $\\widehatR$ for assessing convergence of MCMC (with Discussion). Bayesian Analysis, 16(2), 667–718. https://doi.org/10.1214/20-BA1221 Vehtari, A., Simpson, D. P., Yao, Y., &amp; Gelman, A. (2019). Limitations of “Limitations of Bayesian leave-one-out cross-validation for model selection.” Computational Brain &amp; Behavior, 2(1), 22–27. https://doi.org/10.1007/s42113-018-0020-6 Venables, W. N., &amp; Ripley, B. D. (2002). Modern applied statistics with S (Fourth Edition). Springer. http://www.stats.ox.ac.uk/pub/MASS4 Wagenmakers, E.-J. (2007). A practical solution to the pervasive problems of p values. Psychonomic Bulletin &amp; Review, 14(5), 779–804. https://doi.org/10.3758/BF03194105 Wagenmakers, E.-J., Lodewyckx, T., Kuriyal, H., &amp; Grasman, R. (2010). Bayesian hypothesis testing for psychologists: A tutorial on the Savage method. Cognitive Psychology, 60(3), 158–189. https://doi.org/10.1016/j.cogpsych.2009.12.001 Weber, S., &amp; Bürkner, P.-C. (2022). Running brms models with within-chain parallelization. https://CRAN.R-project.org/package=brms/vignettes/brms_threading.html Wetzels, R., Grasman, R. P. P. P., &amp; Wagenmakers, E.-J. (2012). A default Bayesian hypothesis test for ANOVA designs. The American Statistician, 66(2), 104–111. https://doi.org/10.1080/00031305.2012.695956 Wetzels, R., Matzke, D., Lee, M. D., Rouder, J. N., Iverson, G. J., &amp; Wagenmakers, E.-J. (2011). Statistical evidence in experimental psychology: An empirical comparison using 855 t tests. Perspectives on Psychological Science, 6(3), 291–298. https://doi.org/10.1177/1745691611406923 Wickham, H. (2007). Reshaping data with the reshape package. Journal of Statistical Software, 21(12), 1–20. https://doi.org/10.18637/jss.v021.i12 Wickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2-book.org/ Wickham, H. (2019). stringr: Simple, consistent wrappers for common string operations. https://CRAN.R-project.org/package=stringr Wickham, H. (2020a). cubelyr: A data cube ’dplyr’ backend. https://CRAN.R-project.org/package=cubelyr Wickham, H. (2020b). forcats: Tools for working with categorical variables (factors). https://CRAN.R-project.org/package=forcats Wickham, H. (2020c). reshape2: Flexibly reshape data: A reboot of the reshape package. https://CRAN.R-project.org/package=reshape2 Wickham, H. (2020d). The tidyverse style guide. https://style.tidyverse.org/ Wickham, H. (2021). tidyverse: Easily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H., &amp; Dunnington, D. (2021). ggplot2: Create elegant data visualisations using the grammar of graphics. https://CRAN.R-project.org/package=ggplot2 Wickham, H., François, R., Henry, L., &amp; Müller, K. (2020). dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr Wickham, H., &amp; Henry, L. (2020). tidyr: Tidy messy data. https://CRAN.R-project.org/package=tidyr Wickham, H., Hester, J., &amp; Francois, R. (2018). readr: Read rectangular text data. https://CRAN.R-project.org/package=readr Wilke, C. O. (2020). Themes. https://wilkelab.org/cowplot/articles/themes.html Wilke, C. O. (2019). Fundamentals of data visualization. https://clauswilke.com/dataviz/ Wilke, C. O. (2020). cowplot: Streamlined plot theme and plot annotations for ggplot2 [Manual]. https://wilkelab.org/cowplot/ Wilke, C. O. (2021). ggridges: Ridgeline Plots in ’ggplot2’. https://CRAN.R-project.org/package=ggridges Williams, Donald R., Martin, S. R., Liu, S., &amp; Rast, P. (2021). Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity. European Journal of Psychological Assessment. https://doi.org/10.1027/1015-5759/a000624 Williams, Donald R., Zimprich, D. R., &amp; Rast, P. (2019). A Bayesian nonlinear mixed-effects location scale model for learning. Behavior Research Methods, 51(5), 1968–1986. https://doi.org/10.3758/s13428-019-01255-9 Xie, Y. (2022). Bookdown: Authoring books and technical documents with R Markdown. Chapman and Hall/CRC. https://bookdown.org/yihui/bookdown/ Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2022). R Markdown: The definitive guide. Chapman and Hall/CRC. https://bookdown.org/yihui/rmarkdown/ Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A. (2018). Using stacking to average Bayesian predictive distributions (with discussion). Bayesian Analysis, 13(3), 917–1007. https://doi.org/10.1214/17-BA1091 Zhu, M., &amp; Lu, A. Y. (2004). The counter-intuitive non-informative prior for the Bernoulli family. Journal of Statistics Education, 12(2), 3. https://doi.org/10.1080/10691898.2004.11910734 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
