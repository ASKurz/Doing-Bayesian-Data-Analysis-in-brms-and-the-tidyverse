[
["index.html", "Doing Bayesian Data Analysis in brms and the tidyverse version 0.2.0 What and why We have updates Thank-you’s are in order", " Doing Bayesian Data Analysis in brms and the tidyverse version 0.2.0 A Solomon Kurz 2020-05-19 What and why Kruschke began his text with “This book explains how to actually do Bayesian data analysis, by real people (like you), for realistic data (like yours).” In the same way, this project is designed to help those real people do Bayesian data analysis. My contribution is converting Kruschke’s JAGS and Stan code for use in Bürkner’s brms package (Bürkner, 2017, 2018, 2020f), which makes it easier to fit Bayesian regression models in R (R Core Team, 2020) using Hamiltonian Monte Carlo. I also prefer plotting and data wrangling with the packages from the tidyverse (Wickham, 2019b; Wickham, Averick, et al., 2019). So we’ll be using those methods, too. This project is not meant to stand alone. It’s a supplement to the second edition of Kruschke’s (2015) Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Please give the source material some love. We have updates For a brief rundown of the version history, we have: Version 0.1.0. I released the 0.1.0 version of this project in February 17, 2020. It was the first [fairly] complete draft including material from all the chapters in Kruschke’s text. The supermajority of Kruschke’s JAGS and Stan models were fit brms 2.11.5. The results were saved in the fits folder on GitHub and most of the results are quite comparable to those in the original text. We also reproduced most of the data-related figures and tables and little subpoints and examples sprinkled throughout Kruschke’s prose. Version 0.2.0. Welcome to version 0.2.0! Noteworthy changes include: finally reproducing the simulation necessary for Figure 7.3 (see GitHub issue #14) with help from Cardy Moten III (@cmoten); with guidance from Bjørn Peare Bartholdy (@bbartholdy), Mladen Jovanović (@mladenjovanovic), Cory Whitney (@CWWhitney), and Brenton M. Wiernik (@bwiernik), we have improved in-text citations and reference sections using BibTex (BibTeX, 2020), Better BibTeX (Better BibTeX for Zotero, 2020), and zotero (Zotero | Your Personal Research Assistant, 2020); increased the plot resolution with fig.retina = 2.5; small updates to broken ggplot2 code; refreshed hyperlinks; and various typo corrections. We’re not done yet and I could use your help. There are some minor improvements I’d like to add in future versions, such as prettying up the plots with custom themes. More importantly, I’d like to patch up the content holes. A few simulations, figures, and models are beyond my current skill set. I’ve opened separate GitHub issues for the most important ones and they are as follows: the effective-sample-size simulations in Section 7.5.2 and the corresponding plots in Figures 7.13 and 7.14 (issue #15), several of the simulations in Sections 11.1.4, 11.3.1, and 11.3.2 and their corresponding figures (issues #16, #17, #18, and #19), the stopping-rule simulations in Section 13.3.2 and their corresponding figures (issue #20), the data necessary to properly reproduce the HMC proposal schematic presented in Section 14.1 and Figures 14.1 through 14.3 (issue #21), and the conditional logistic models of Section 22.3.3.2 (issue #22). If you know how to conquer any of these unresolved challenges, I’d love to hear all about it. In addition, please feel free to open a new issue if you find any flaws in the other sections of the project. Thank-you’s are in order Before we enter the primary text, I’d like to thank the following for their helpful contributions: Bjørn Peare Bartholdy (@bbartholdy), Paul-Christian Bürkner (@paul-buerkner), Andrew Gelman (@andrewgelman), Mladen Jovanović (@mladenjovanovic), Matthew Kay (@mjskay), TJ Mahr (@tjmahr), Cardy Moten III (@cmoten), Lukas Neugebauer (@LukasNeugebauer), Demetri Pananos (@Dpananos), Aki Vehtari (@avehtari), Matti Vuorre (@mvuorre), Cory Whitney (@CWWhitney), and Brenton M. Wiernik (@bwiernik). References "],
["whats-in-this-book-read-this-first.html", "1 What’s in This Book (Read This First!) 1.1 Real people can read this book 1.2 What’s in this book 1.3 What’s new in the second edition 1.4 Gimme feedback (be polite) 1.5 Thank you! Session info", " 1 What’s in This Book (Read This First!) 1.1 Real people can read this book Kruschke began his (2015, p. 1) text with “This book explains how to actually do Bayesian data analysis, by real people (like you), for realistic data (like yours).” Agreed. Similarly, this project is designed to help those real people do Bayesian data analysis. While I’m at it, I may as well explicate my assumptions about you. If you’re looking at this project, I’m guessing you’re a graduate student, a post-graduate academic or researcher of some sort. Which means I’m presuming you have at least a 101-level foundation in statistics. In his text, it seems like Kruschke presumed his readers would have a good foundation in calculus, too. I make no such presumption. But if your stats 101 chops are rusty, check out Legler and Roback’s free (2019) bookdown text, Broadening your statistical horizons or Navarro’s free (2019) text, Learning statistics with R: A tutorial for psychology students and other beginners. I presume a basic working fluency in R and a vague idea about what the tidyverse is. Kruschke does some R warm-up in Chapter 3, and I follow suit. But if you’re totally new to R, you might also consider starting with Peng’s (2019) R programming for data science. The best introduction to the tidyvese-style of data analysis I’ve found is Grolemund and Wickham’s (2017) R for data science. 1.2 What’s in this book This project is not meant to stand alone. It’s a supplement to the second edition of Kruschke’s (2015) Doing Bayesian data analysis. I follow the structure of his text, chapter by chapter, translating his analyses into brms and tidyverse code. However, many of the sections in the text are composed entirely of equations and prose, leaving us nothing to translate. When we run into those sections, the corresponding sections in this project might be blank or even missing. Also beware the content herein will depart at times from the source material. Bayesian data analysis with HMC is an active area of development in terms of both statistical methods and software implementation. There will also be times when my thoughts and preferences on Bayesian data analysis diverge a bit from Kruschke’s. In those places of divergence, I will often provide references and explanations. In this project, I use a handful of formatting conventions gleaned from R4DS and Xie, Allaire, and Grolemund’s (2020) R markdown: The definitive guide. I put R and R packages (e.g., brms) in boldface. R code blocks and their output appear in a gray background. E.g., 2 + 2 ## [1] 4 Did you notice how there were two strips of gray background, there? The first one designated the actual code. The second one was the output of that code. The output of a code block often begins with ##. Functions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., brm()). When I want to make explicit what packages a given function comes from, I insert the double-colon operator :: between the package name and the function (e.g., tidyr::pivot_longer()). R objects, such as data or function arguments, are in typewriter font atop a gray background (e.g., d or size = 2). Hyperlinks are denoted by their typical blue-colored font. 1.3 What’s new in the second edition This is my first attempt at this project. There’s nothing new from my end. 1.4 Gimme feedback (be polite) I am not a statistician and I have no formal background in computer science. I just finished my PhD in clinical psychology in 2018. During my graduate training I developed an unexpected interest in applied statistics and, more recently, programming. I became an R user in 2015 and started learning about Bayesian statistics around 2013. There is still so much to learn, so my apologies for when my code appears dated or inelegant. There will also be occasions in which I’m not yet sure how to reproduce models or plots in the text. Which is all to say, suggestions on how to improve my code are welcome. If you’d like to learn more about me, you can find my website at https://solomonkurz.netlify.com. 1.5 Thank you! While in grad school, I benefitted tremendously from free online content. This project and others like it (e.g., here or here or here) are my attempts to pay it forward. As soon as you’ve gained a little proficiency, do consider doing to same. I addition to great texts like Kruschke’s, I’d like to point out a few other important resources that have allowed me to complete a project like this: Jenny Bryan’s Happy Git and GitHub for the useR (Bryan et al., 2020) is the reference that finally got me working on Github. Again and again, I return to Grolemund and Wickham’s (2017) R for data science to learn about the tidyverse way of coding. Yihui Xie’s (2016) bookdown: Authoring books and technical documents with R markdown is the primary source from which I learned how to make online books like this. While you’re at it, also check out Xie, Allaire, and Grolemund’s (2020) R markdown: The definitive guide to learn more about how you can mix your R code with well-formatted prose. If you haven’t already, bookmark these resources and share them with your friends. With time and practice, these resources might help you write books and other online projects with R. Session info At the end of every chapter, I use the sessionInfo() function to help make my results more reproducible. sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.3 magrittr_1.5 bookdown_0.18 htmltools_0.4.0 ## [5] tools_3.6.3 yaml_2.2.1 Rcpp_1.0.4.6 stringi_1.4.6 ## [9] rmarkdown_2.1 knitr_1.28 stringr_1.4.0 digest_0.6.25 ## [13] xfun_0.13 rlang_0.4.6 evaluate_0.14 References "],
["introduction-credibility-models-and-parameters.html", "2 Introduction: Credibility, Models, and Parameters 2.1 Bayesian inference is reallocation of credibility across possibilities 2.2 Possibilities are parameter values in descriptive models 2.3 The steps of Bayesian data analysis Session info", " 2 Introduction: Credibility, Models, and Parameters The goal of this chapter is to introduce the conceptual framework of Bayesian data analysis. Bayesian data analysis has two foundational ideas. The first idea is that Bayesian inference is reallocation of credibility across possibilities. The second foundational idea is that the possibilities, over which we allocate credibility, are parameter values in meaningful mathematical models. (Kruschke, 2015, p. 15) 2.1 Bayesian inference is reallocation of credibility across possibilities The first step toward making Figure 2.1 is putting together a data object. And to help with that, we’ll open up the tidyverse. library(tidyverse) d &lt;- crossing(iteration = 1:3, stage = factor(c(&quot;Prior&quot;, &quot;Posterior&quot;), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) %&gt;% expand(nesting(iteration, stage), Possibilities = LETTERS[1:4]) %&gt;% mutate(Credibility = c(rep(.25, times = 4), 0, rep(1/3, times = 3), 0, rep(1/3, times = 3), rep(c(0, .5), each = 2), rep(c(0, .5), each = 2), rep(0, times = 3), 1)) When making data with man repetitions in the rows, it’s good to have the tidyr::expand() function up your sleeve. Go here to learn more. We can take a look at the top few rows of the data with head(). head(d) ## # A tibble: 6 x 4 ## iteration stage Possibilities Credibility ## &lt;int&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Prior A 0.25 ## 2 1 Prior B 0.25 ## 3 1 Prior C 0.25 ## 4 1 Prior D 0.25 ## 5 1 Posterior A 0 ## 6 1 Posterior B 0.333 Before we attempt Figure 2.1, we’ll need two supplemental data frames. The first one, d_text, will supply the coordinates for the annotation in the plot. The second, d_arrow, will supply the coordinates for the arrows. text &lt;- tibble(Possibilities = &quot;B&quot;, Credibility = .75, label = str_c(LETTERS[1:3], &quot; is\\nimpossible&quot;), iteration = 1:3, stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) arrow &lt;- tibble(Possibilities = LETTERS[1:3], iteration = 1:3) %&gt;% expand(nesting(Possibilities, iteration), Credibility = c(0.6, 0.01)) %&gt;% mutate(stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) Now we’re ready to code our version of Figure 2.1. d %&gt;% ggplot(aes(x = Possibilities, y = Credibility)) + geom_col(color = &quot;grey30&quot;, fill = &quot;grey30&quot;) + # annotation in the bottom row geom_text(data = text, aes(label = label)) + # arrows in the bottom row geom_line(data = arrow, arrow = arrow(length = unit(0.30, &quot;cm&quot;), ends = &quot;first&quot;, type = &quot;closed&quot;)) + facet_grid(stage ~ iteration) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank(), strip.text.x = element_blank()) We will take a similar approach to make our version of Figure 2.2. But this time, we’ll define our supplemental data sets directly in geom_text() and geom_line(). It’s good to have both methods up your sleeve. Also notice how we simply fed our primary data set directly into ggplot() without saving it, either. # primary data crossing(Possibilities = LETTERS[1:4], stage = factor(c(&quot;Prior&quot;, &quot;Posterior&quot;), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) %&gt;% mutate(Credibility = c(rep(0.25, times = 4), rep(0, times = 3), 1)) %&gt;% # plot! ggplot(aes(x = Possibilities, y = Credibility)) + geom_col(color = &quot;grey30&quot;, fill = &quot;grey30&quot;) + # annotation in the bottom panel geom_text(data = tibble( Possibilities = &quot;B&quot;, Credibility = .8, label = &quot;D is\\nresponsible&quot;, stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ), aes(label = label) ) + # the arrow geom_line(data = tibble( Possibilities = LETTERS[c(4, 4)], Credibility = c(.25, .99), stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ), arrow = arrow(length = unit(0.30, &quot;cm&quot;), ends = &quot;last&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + facet_wrap(~stage, ncol = 1) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) 2.1.1 Data are noisy and inferences are probabilistic. Now on to Figure 2.3. I’m pretty sure the curves in the plot are Gaussian, which we’ll make with the dnorm() function. After a little trial and error, their standard deviations look to be 1.2. However, it’s tricky placing those curves in along with the probabilities, because the probabilities for the four discrete sizes (i.e., 1 through 4) are in a different metric than the Gaussian density curves. Since the probability metric for the four discrete sizes are the primary metric of the plot, we need to rescale the curves using a little algebra. We do that in the data code, below. After that, the code for the plot is relatively simple. # data tibble(mu = 1:4, p = .25) %&gt;% expand(nesting(mu, p), x = seq(from = -2, to = 6, by = .1)) %&gt;% mutate(density = dnorm(x, mean = mu, sd = 1.2)) %&gt;% mutate(d_max = max(density)) %&gt;% mutate(rescale = p / d_max) %&gt;% mutate(density = density * rescale) %&gt;% # plot! ggplot(aes(x = x)) + geom_col(data = . %&gt;% distinct(mu, p), aes(x = mu, y = p), fill = &quot;grey67&quot;, width = 1/3) + geom_line(aes(y = density, group = mu)) + scale_x_continuous(breaks = 1:4) + scale_y_continuous(breaks = 0:5 / 5) + coord_cartesian(xlim = c(0, 5), ylim = c(0, 1)) + labs(title = &quot;Prior&quot;, x = &quot;Possibilities&quot;, y = &quot;Credibility&quot;) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) We can use the same basic method to make the bottom panel. The new consideration is choosing the relative probabilities for the different mu values–keeping in mind they have to sum to 1. I just eyeballed them. The only other notable change from the previous plot is our addition of a geom_point() section, in which we defined the data on the fly. tibble(mu = 1:4, p = c(.1, .58, .3, .02)) %&gt;% expand(nesting(mu, p), x = seq(from = -2, to = 6, by = .1)) %&gt;% mutate(density = dnorm(x, mean = mu, sd = 1.2)) %&gt;% mutate(d_max = max(density)) %&gt;% mutate(rescale = p / d_max) %&gt;% mutate(density = density * rescale) %&gt;% # plot! ggplot() + geom_col(data = . %&gt;% distinct(mu, p), aes(x = mu, y = p), fill = &quot;grey67&quot;, width = 1/3) + geom_line(aes(x = x, y = density, group = mu)) + geom_point(data = tibble(x = c(1.75, 2.25, 2.75), y = 0), aes(x = x, y = y), size = 3, color = &quot;grey33&quot;, alpha = 3/4) + scale_x_continuous(breaks = 1:4) + scale_y_continuous(breaks = 0:5 / 5) + coord_cartesian(xlim = c(0, 5), ylim = c(0, 1)) + labs(title = &quot;Posterior&quot;, x = &quot;Possibilities&quot;, y = &quot;Credibility&quot;) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) In summary, the essence of Bayesian inference is reallocation of credibility across possibilities. The distribution of credibility initially reflects prior knowledge about the possibilities, which can be quite vague. Then new data are observed, and the credibility is re-allocated. Possibilities that are consistent with the data garner more credibility, while possibilities that are not consistent with the data lose credibility. Bayesian analysis is the mathematics of re-allocating credibility in a logically coherent and precise way. (p. 22) 2.2 Possibilities are parameter values in descriptive models “A key step in Bayesian analysis is defining the set of possibilities over which credibility is allocated. This is not a trivial step, because there might always be possibilities beyond the ones we include in the initial set” (p. 22, emphasis added). In the last section, we used the dnorm() function to make curves following the Normal distribution. Here we’ll do that again, but also use the rnorm() function to simulate actual data from that same Normal distribution. Behold Figure 2.4.a. # set the seed to make the simulation reproducible set.seed(2) # simulate the data with `rnorm()` d &lt;- tibble(x = rnorm(2000, mean = 10, sd = 5)) # plot! ggplot(data = d, aes(x = x)) + geom_histogram(aes(y = ..density..), binwidth = 1, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/10) + geom_line(data = tibble(x = seq(from = -6, to = 26, by = .01)), aes(x = x, y = dnorm(x, mean = 10, sd = 5)), color = &quot;grey33&quot;) + coord_cartesian(xlim = c(-5, 25)) + labs(subtitle = &quot;The candidate normal distribution\\nhas a mean of 10 and SD of 5.&quot;, x = &quot;Data Values&quot;, y = &quot;Data Probability&quot;) + theme(panel.grid = element_blank()) Did you notice how we made the data for the density curve within geom_line()? That’s one way to do it. In our next plot, we’ll take a different and more elegant approach with stat_function(). Here’s our Figure 2.4.b. ggplot(data = d, aes(x = x)) + geom_histogram(aes(y = ..density..), binwidth = 1, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + stat_function(fun = dnorm, n = 101, args = list(mean = 8, sd = 6), color = &quot;grey33&quot;, linetype = 2) + coord_cartesian(xlim = c(-5, 25)) + labs(subtitle = &quot;The candidate normal distribution\\nhas a mean of 8 and SD of 6.&quot;, x = &quot;Data Values&quot;, y = &quot;Data Probability&quot;) + theme(panel.grid = element_blank()) 2.3 The steps of Bayesian data analysis In general, Bayesian analysis of data follows these steps: Identify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors? Define a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis. Specify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists. Use Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step). Check that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a “posterior predictive check”). If not, then consider a different descriptive model. Perhaps the best way to explain these steps is with a realistic example of Bayesian data analysis. The discussion that follows is abbreviated for purposes of this introductory chapter, with many technical details suppressed. (p. 25) I will show you a few more details than Kruschke did in the text. But just has he did, we’ll cover this workflow in much more detail in the chapters to come. In order to recreate Figure 2.5, we need to generate the data and fit the model. In his HtWtDataDenerator.R script, Kruschke provided the code for a function that will generate height/weight data of the kind in his text. Here is the code in full: HtWtDataGenerator &lt;- function(nSubj, rndsd = NULL, maleProb = 0.50) { # Random height, weight generator for males and females. Uses parameters from # Brainard, J. &amp; Burmaster, D. E. (1992). Bivariate distributions for height and # weight of men and women in the United States. Risk Analysis, 12(2), 267-275. # Kruschke, J. K. (2011). Doing Bayesian data analysis: # A Tutorial with R and BUGS. Academic Press / Elsevier. # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition: # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier. # require(MASS) # Specify parameters of multivariate normal (MVN) distributions. # Men: HtMmu &lt;- 69.18 HtMsd &lt;- 2.87 lnWtMmu &lt;- 5.14 lnWtMsd &lt;- 0.17 Mrho &lt;- 0.42 Mmean &lt;- c(HtMmu, lnWtMmu) Msigma &lt;- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd, Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2) # Women cluster 1: HtFmu1 &lt;- 63.11 HtFsd1 &lt;- 2.76 lnWtFmu1 &lt;- 5.06 lnWtFsd1 &lt;- 0.24 Frho1 &lt;- 0.41 prop1 &lt;- 0.46 Fmean1 &lt;- c(HtFmu1, lnWtFmu1) Fsigma1 &lt;- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1, Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2) # Women cluster 2: HtFmu2 &lt;- 64.36 HtFsd2 &lt;- 2.49 lnWtFmu2 &lt;- 4.86 lnWtFsd2 &lt;- 0.14 Frho2 &lt;- 0.44 prop2 &lt;- 1 - prop1 Fmean2 &lt;- c(HtFmu2, lnWtFmu2) Fsigma2 &lt;- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2, Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2) # Randomly generate data values from those MVN distributions. if (!is.null(rndsd)) {set.seed(rndsd)} datamatrix &lt;- matrix(0, nrow = nSubj, ncol = 3) colnames(datamatrix) &lt;- c(&quot;male&quot;, &quot;height&quot;, &quot;weight&quot;) maleval &lt;- 1; femaleval &lt;- 0 # arbitrary coding values for (i in 1:nSubj) { # Flip coin to decide sex sex &lt;- sample(c(maleval, femaleval), size = 1, replace = TRUE, prob = c(maleProb, 1 - maleProb)) if (sex == maleval) {datum = MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)} if (sex == femaleval) { Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2)) if (Fclust == 1) {datum = MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)} if (Fclust == 2) {datum = MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)} } datamatrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1)) } return(datamatrix) } # end function Now we have the HtWtDataGenerator() function, all we need to do is determine how many values to generate and how probable we want the values to be based on those from men. These are controlled by the nSubj and maleProb parameters. # set your seed to make the data generation reproducible set.seed(57) d &lt;- HtWtDataGenerator(nSubj = 57, maleProb = .5) %&gt;% as_tibble() d %&gt;% head() ## # A tibble: 6 x 3 ## male height weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 68.8 133. ## 2 1 70 187. ## 3 0 63.2 154 ## 4 0 61.4 145. ## 5 0 66.1 130. ## 6 1 71.5 271 We’re about ready for the model. We will fit it with HMC via the brms package. library(brms) The traditional use of diffuse and noninformative priors is discouraged with HMC, as is the uniform distribution for sigma. Instead, we’ll use weakly-regularizing priors for the intercept and slope and a half Cauchy with a fairly large scale parameter for \\(\\sigma\\). fit2.1 &lt;- brm(data = d, family = gaussian, weight ~ 1 + height, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = b), prior(cauchy(0, 10), class = sigma)), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 2, file = &quot;fits/fit02.01&quot;) If you wanted a quick model summary, you could execute print(fit1). We’ll walk through that and other diagnostics in greater detail starting in Chapter 8. But for now, here’s how we might make Figure 2.5.a. # extract the posterior draws post &lt;- posterior_samples(fit2.1) # this will streamline some of the code, below n_lines &lt;- 150 # plot! d %&gt;% ggplot(aes(x = height, y = weight)) + geom_abline(intercept = post[1:n_lines, 1], slope = post[1:n_lines, 2], color = &quot;grey50&quot;, size = 1/4, alpha = .3) + geom_point(alpha = 2/3) + # the `eval(substitute(paste()))` trick came from: https://www.r-bloggers.com/value-of-an-r-object-in-an-expression/ labs(subtitle = eval(substitute(paste(&quot;Data with&quot;, n_lines, &quot;credible regression lines&quot;))), x = &quot;Height in inches&quot;, y = &quot;Weight in pounds&quot;) + coord_cartesian(xlim = c(55, 80), ylim = c(50, 250)) + theme(panel.grid = element_blank()) For Figure 2.5.b., we’ll use the handy stat_histintervalh() function from the tidybayes package (Kay, 2020c) to mark off the mode and 95% HDIs underneath a histogram. library(tidybayes) post %&gt;% ggplot(aes(x = b_height, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .2, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 8)) + labs(title = &quot;The posterior distribution&quot;, subtitle = &quot;The mode and 95% HPD intervals are\\nthe dot and horizontal line at the bottom.&quot;, x = expression(paste(beta[1], &quot; (slope)&quot;))) + theme(panel.grid = element_blank()) Here’s Figure 2.6. We’ll go over the brms::predict() function later. nd &lt;- tibble(height = seq(from = 53, to = 81, length.out = 20)) predict(fit2.1, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = height)) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), color = &quot;grey67&quot;, shape = 20) + geom_point(data = d, aes(y = weight), alpha = 2/3) + labs(subtitle = &quot;Data with the percentile-based 95% intervals and\\nthe means of the posterior predictions&quot;, x = &quot;Height in inches&quot;, y = &quot;Weight in inches&quot;) + theme(panel.grid = element_blank()) The posterior predictions might be easier to depict with a ribbon and line, instead. nd &lt;- tibble(height = seq(from = 53, to = 81, length.out = 30)) predict(fit2.1, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = height)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey75&quot;) + geom_line(aes(y = Estimate), color = &quot;grey92&quot;) + geom_point(data = d, aes(y = weight), alpha = 2/3) + labs(subtitle = &quot;Data with the percentile-based 95% intervals and\\nthe means of the posterior predictions&quot;, x = &quot;Height in inches&quot;, y = &quot;Weight in inches&quot;) + theme(panel.grid = element_blank()) Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.3.9000 brms_2.12.0 Rcpp_1.0.4.6 ## [4] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 ## [7] purrr_0.3.4 readr_1.3.1 tidyr_1.0.2 ## [10] tibble_3.0.1 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.4.1 rstudioapi_0.11 farver_2.0.3 ## [10] rstan_2.19.3 svUnit_1.0.3 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 ## [16] xml2_1.3.1 bridgesampling_1.0-0 knitr_1.28 ## [19] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 ## [22] broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 ## [25] compiler_3.6.3 httr_1.4.1 backports_1.1.6 ## [28] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] cli_2.0.2 later_1.0.0 prettyunits_1.1.1 ## [34] htmltools_0.4.0 tools_3.6.3 igraph_1.2.5 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.4.0 ## [40] reshape2_1.4.4 cellranger_1.1.0 vctrs_0.3.0 ## [43] nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ## [46] ps_1.3.3 rvest_0.3.5 mime_0.9 ## [49] miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 ## [52] MASS_7.3-51.5 zoo_1.8-7 scales_1.1.1 ## [55] colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [58] Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 ## [64] StanHeaders_2.21.0-1 loo_2.2.0 stringi_1.4.6 ## [67] dygraphs_1.1.1.6 pkgbuild_1.0.8 rlang_0.4.6 ## [70] pkgconfig_2.0.3 matrixStats_0.56.0 HDInterval_0.2.0 ## [73] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [76] htmlwidgets_1.5.1 labeling_0.3 tidyselect_1.0.0 ## [79] processx_3.4.2 plyr_1.8.6 magrittr_1.5 ## [82] bookdown_0.18 R6_2.4.1 generics_0.0.2 ## [85] DBI_1.1.0 pillar_1.4.4 haven_2.2.0 ## [88] withr_2.2.0 xts_0.12-0 abind_1.4-5 ## [91] modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 ## [94] utf8_1.1.4 rmarkdown_2.1 grid_3.6.3 ## [97] readxl_1.3.1 callr_3.4.3 threejs_0.3.3 ## [100] reprex_0.3.0 digest_0.6.25 xtable_1.8-4 ## [103] httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 ## [106] shinyjs_1.1 References "],
["the-r-programming-language.html", "3 The R Programming Language 3.1 Get the software 3.2 A simple example of R in action 3.3 Basic commands and operators in R 3.4 Variable types 3.5 Loading and saving data 3.6 Some utility functions 3.7 Programming in R 3.8 Graphical plots: Opening and saving Session info", " 3 The R Programming Language The material in this chapter is rather dull reading because it basically amounts to a list (although a carefully scaffolded list) of basic commands in R along with illustrative examples. After reading the first few pages and nodding off, you may be tempted to skip ahead, and I wouldn’t blame you. But much of the material in this chapter is crucial, and all of it will eventually be useful, so you should at least skim it all so you know where to return when the topics arise later. (Kruschke, 2015, p. 35) Most, but not all, of this part of my project will mirror what’s in the text. However, I do add tidyverse-oriented content, such as a small walk through of plotting with the ggplot2 package (Wickham, 2016; Wickham, Chang, et al., 2020). 3.1 Get the software In addition to R and RStudio (RStudio Team, 2020), I make use of a variety of R packages in this project. You can get the heaviest hitters by executing this code block. install.packages(&quot;devtools&quot;) install.packages(&quot;tidyverse&quot;, dependencies = T) install.packages(&quot;brms&quot;, dependencies = T) install.packages(&quot;tidybayes&quot;, dependencies = T) 3.1.1 A look at RStudio. The R programming language comes with its own basic user interface that is adequate for modest applications. But larger applications become unwieldy in the basic R user interface, and therefore it helps to install a more sophisticated R-friendly editor. There are a number of useful editors available, many of which are free, and they are constantly evolving. At the time of this writing, I recommend RStudio, which can be obtained from http://www.rstudio.com/ (p. 35). I completely agree. R programing is easier with RStudio. 3.2 A simple example of R in action Basic arithmetic is straightforward in R. 2 + 3 ## [1] 5 Algebra is simple, too. x &lt;- 2 x + x ## [1] 4 Behold Figure 3.1. library(tidyverse) d &lt;- tibble(x = seq(from = -2, to = 2, by = .1)) %&gt;% mutate(y = x^2) ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) + theme(panel.grid = element_blank()) If you’re new to the tidyverse and/or making figures with ggplot2, it’s worthwhile to walk that code out. With the first line, library(tidyverse), we opened up the core packages within the tidyverse, which are: ggplot2 (Wickham, 2016; Wickham, Chang, et al., 2020), dplyr (Wickham, François, et al., 2020), tidyr (Wickham &amp; Henry, 2020), readr (Wickham et al., 2018), purrr (Henry &amp; Wickham, 2020), tibble (Müller &amp; Wickham, 2020), stringr (Wickham, 2019a), and forcats (Wickham, 2020a). With the next block d &lt;- tibble(x = seq(from = -2, to = 2, by = .1)) %&gt;% mutate(y = x^2) we made our tibble. In R, data frames are one of the primary types of data objects (see subsection 3.4.4., below). We’ll make extensive use of data frames in this project. Tibbles are a particular type of data frame, which you might learn more about in the tibbles section of Grolemund and Wickham’s (2017) R4DS. With those first two lines, we determined what the name of our tibble would be, d, and made the first column, x. Note the %&gt;% operator at the end of the second line. In prose, we call that the pipe. As explained in chapter 5 of R4DS, “a good way to pronounce %&gt;% when reading code is ‘then.’” So in words, the those first two lines indicate “Make an object, d, which is a tibble with a variable, x, defined by the seq() function, then…” In the portion after then (i.e., the %&gt;%), we changed d. The mutate() function let us add another variable, y, which is a function of our first variable, x. With the next 4 lines of code, we made our plot. When plotting with ggplot2, the first line is always with the ggplot() function. This is where you typically tell ggplot2 what data object you’re using–which must be a data frame or tibble–and what variables you want on your axes. The interesting thing about ggplot2 is that the code is modular. So if we only coded the ggplot() portion, we’d get: ggplot(data = d, aes(x = x, y = y)) Although ggplot2 knows which variables to put on which axes, it has no idea how we’d like to express the data. The result is an empty coordinate system. The next line of code is the main event. With geom_line() we told ggplot2 to connect the data points with a line. With the color argument, we made that line skyblue. [Here’s a great list of the named colors available in ggplot2.] Also, notice the + operator at the end of the ggplot() function. With ggplot2, you add functions by placing the + operator on the right of the end of one function, which will then append the next function. ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) Personally, I’m not a fan of gridlines. They occasionally have their place and I do use them from time to time. But on the whole, I prefer to omit them from my plots. The final theme() function allowed me to do so. ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) + theme(panel.grid = element_blank()) Chapter 3 of R4DS is a great introduction to plotting with ggplot2. If you want to dive deeper, see the references at the bottom of this page. And of course, you might read up in Wickham’s (2016) ggplot2: Elegant Graphics for Data Analysis. 3.2.1 Get the programs used with this book. This subtitle has a double meaning, here. Yes, you should probably get Kruschke’s scripts from the book’s website. You may have noticed this already, but unlike in Kruschke’s text, I will usually show all my code. Indeed, the purpose of my project is to make coding these kinds of models and visualizations easier. But if you’re ever curious, you can always find my script files in their naked form, here. Later in this subsection, Kruschke mentioned working directories. If you don’t know what your current working directory is, just execute getwd(). I’ll have more to say on this topic later on when I make my pitch for RStudio projects. 3.3 Basic commands and operators in R In addition to the resource link Kruschke provided in the text, Grolemund and Wickham’s R4DS is an excellent general introduction to the kinds of R functions you’ll want to succeed with your data analysis. Other than that, I’ve learned the most when I had a specific data problem to solve and then sought out the specific code/techniques required to solve it. If already have your own data or can get your hands on some sexy data, learn these techniques by playing around with them. This isn’t the time to worry about rigor, preregistration, or all of that. This is time to play. 3.3.1 Getting help in R. As with plot() you can learn more about the ggplot() function with ?. ?ggplot help.start() can be nice, too. help.start() ??geom_line() can help us learn more about the geom_line() function. ??geom_line() 3.3.2 Arithmetic and logical operators. With arithmetic, the order of operations is: power first, then multiplication, then addition. 1 + 2 * 3^2 ## [1] 19 With parentheses, you can force addition before multiplication. (1 + 2) * 3^2 ## [1] 27 Operations inside parentheses get done before power operations. (1 + 2 * 3)^2 ## [1] 49 One can nest parentheses. ((1 + 2) * 3)^2 ## [1] 81 ?Syntax We can use R to perform a variety of logical tests, such as negation. !TRUE ## [1] FALSE We can do conjunction. TRUE &amp; FALSE ## [1] FALSE And we can do disjunction. TRUE | FALSE ## [1] TRUE Conjunction has precedence over disjunction. TRUE | TRUE &amp; FALSE ## [1] TRUE However, with parentheses we can force disjunction first. (TRUE | TRUE) &amp; FALSE ## [1] FALSE 3.3.3 Assignment, relational operators, and tests of equality. In contrast to Kruschke’s preference, I will use the arrow operator, &lt;-, to assign values to named variables.1 x = 1 x &lt;- 1 Yep, this ain’t normal math. (x = 1) ## [1] 1 (x = x + 1) ## [1] 2 Here we use == to test for equality. (x = 2) ## [1] 2 x == 2 ## [1] TRUE Using !=, we can check whether the value of x is NOT equal to 3. x != 3 ## [1] TRUE We can use &lt; to check whether the value of x is less than 3. x &lt; 3 ## [1] TRUE Similarly, we can use &gt; to check whether the value of x is greater than 3. x &gt; 3 ## [1] FALSE This normal use of the &lt;- operator x &lt;- 3 is not the same as x &lt; - 3 ## [1] FALSE The limited precision of a computer’s memory can lead to odd results. x &lt;- 0.5 - 0.3 y &lt;- 0.3 - 0.1 Although mathematically TRUE, this is FALSE for limited precision. x == y ## [1] FALSE However, they are equal up to the precision of a computer. all.equal(x, y) ## [1] TRUE 3.4 Variable types If you’d like to learn more about the differences among vectors, matrices, lists, data frames and so on, you might check out Roger Peng’s (2019) R Programming for data science, chapter 4. 3.4.1 Vector. 3.4.1.1 The combine function. The combine function is c(). c(2.718, 3.14, 1.414) ## [1] 2.718 3.140 1.414 x &lt;- c(2.718, 3.14, 1.414) You’ll note the equivalence. x == c(2.718, 3.14, 1.414) ## [1] TRUE TRUE TRUE This leads to the next subsection. 3.4.1.2 Component-by-component vector operations. We can multiply two vectors, component by component. c(1, 2, 3) * c(7, 6, 5) ## [1] 7 12 15 If you have a sole number, a scaler, you can multiply an entire vector by it like: 2 * c(1, 2, 3) ## [1] 2 4 6 which is a more compact way to perform this. c(2, 2, 2) * c(1, 2, 3) ## [1] 2 4 6 The same sensibilities hold for other operations, such as addition. 2 + c(1, 2, 3) ## [1] 3 4 5 3.4.1.3 The colon operator and sequence function. The colon operator has precedence over addition. 2 + 3:6 ## [1] 5 6 7 8 Parentheses override default precedence. (2 + 3):6 ## [1] 5 6 The power operator has precedence over the colon operator. 1:3^2 ## [1] 1 2 3 4 5 6 7 8 9 And parentheses override default precedence. (1:3)^2 ## [1] 1 4 9 The seq() function is quite handy. If you don’t specify the length of the output, it will figure that out the logical consequence of the other arguments. seq(from = 0, to = 3, by = 0.5) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 This sequence won’t exceed to = 3. seq(from = 0, to = 3, by = 0.5001) ## [1] 0.0000 0.5001 1.0002 1.5003 2.0004 2.5005 In each of the following examples, we’ll omit one of the core seq() arguments: from, to, by, and length.out. Here we do not define the end point. seq(from = 0, by = 0.5, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 This time we fail to define the increment. seq(from = 0, to = 3, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 And this time we omit a starting point. seq(to = 3, by = 0.5, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.4.1.4 The replicate function. We’ll define our pre-replication vector with the &lt;- operator. abc &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) With times, we repeat the vector as a unit. rep(abc, times = 2) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; But if we mix times with c(), we can repeat individual components of abc differently. rep(abc, times = c(4, 2, 1)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; With the each argument, we repeat the individual components of abc one at a time. rep(abc, each = 2) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; And you can even combine each and length, repeating each element until the length requirement has been fulfilled. rep(abc, each = 2, length = 10) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; You can also combine each and times. rep(abc, each = 2, times = 3) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; I tend to do things like the above as two separate steps. One way to do so is by nesting one rep() function within another. rep(rep(abc, each = 2), times = 3) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; As Kruschke points out, this can look confusing. rep(abc, each = 2, times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; But breaking the results up into two steps might be easier to understand, rep(rep(abc, each = 2), times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; And especially earlier in my R career, it helped quite a bit to break operation sequences like this up by saving and assessing the intermediary steps. step_1 &lt;- rep(abc, each = 2) step_1 ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; rep(step_1, times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; 3.4.1.5 Getting at elements of a vector. Behold our exemplar vector, x. x &lt;- c(2.718, 3.14, 1.414, 47405) The straightforward way to extract the second and fourth elements is x[c(2, 4)] ## [1] 3.14 47405.00 Or you might use reverse logic and omit the first and third elements. x[c(-1, -3 )] ## [1] 3.14 47405.00 It’s handy to know that T is a stand in for TRUE and F is a stand in for FALSE. You’ll probably notice I tend to use the abbreviations most of the time. x[c(F, T, F, T)] ## [1] 3.14 47405.00 The names() function makes it easy to name the components of a vector. names(x) &lt;- c(&quot;e&quot;, &quot;pi&quot;, &quot;sqrt2&quot;, &quot;zipcode&quot;) x ## e pi sqrt2 zipcode ## 2.718 3.140 1.414 47405.000 Now we can call the components with their names. x[c(&quot;pi&quot;, &quot;zipcode&quot;)] ## pi zipcode ## 3.14 47405.00 Here’s Kruschke’s review: # define a vector x &lt;- c(2.718, 3.14, 1.414, 47405) # name the components names(x) &lt;- c(&quot;e&quot;, &quot;pi&quot;, &quot;sqrt2&quot;, &quot;zipcode&quot;) # you can indicate which elements you&#39;d like to include x[c(2, 4)] ## pi zipcode ## 3.14 47405.00 # you can decide which to exclude x[c(-1, -3)] ## pi zipcode ## 3.14 47405.00 # or you can use logical tests x[c(F, T, F, T)] ## pi zipcode ## 3.14 47405.00 # and you can use the names themselves x[c(&quot;pi&quot;, &quot;zipcode&quot;)] ## pi zipcode ## 3.14 47405.00 3.4.2 Factor. Here are our five-person SES status data. x &lt;- c(&quot;high&quot;, &quot;medium&quot;, &quot;low&quot;, &quot;high&quot;, &quot;medium&quot;) x ## [1] &quot;high&quot; &quot;medium&quot; &quot;low&quot; &quot;high&quot; &quot;medium&quot; The factor() function turns them into a factor, which will return the levels when called. xf &lt;- factor(x) xf ## [1] high medium low high medium ## Levels: high low medium Here are the factor levels as numerals. as.numeric(xf) ## [1] 1 3 2 1 3 With the levels and ordered arguments, we can order the factor elements. xfo &lt;- factor(x, levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), ordered = T) xfo ## [1] high medium low high medium ## Levels: low &lt; medium &lt; high Now “high” is a larger integer. as.numeric(xfo) ## [1] 3 2 1 3 2 We’ve already specified xf. xf ## [1] high medium low high medium ## Levels: high low medium And we know how it’s been coded numerically. as.numeric(xf) ## [1] 1 3 2 1 3 We can have levels and labels. xfol &lt;- factor(x, levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), ordered = T, labels = c(&quot;Bottom SES&quot;, &quot;Middle SES&quot;, &quot;Top SES&quot;)) xfol ## [1] Top SES Middle SES Bottom SES Top SES Middle SES ## Levels: Bottom SES &lt; Middle SES &lt; Top SES 3.4.3 Matrix and array. Kruschke uses these more often than I do. I’m more of a vector and data frame kinda guy. Even so, here’s an example of a matrix. matrix(1:6, ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 We can get the same thing using nrow. matrix(1:6, nrow = 2) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 Note how the numbers got ordered by rows within each column? We can specify them to be ordered across columns, first. matrix(1:6, nrow = 2, byrow = T) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 We can name the dimensions. I’m not completely consistent, but I’ve been moving in the direction of following The tidyverse style guide (Wickham, 2020c) for naming my R objects and their elements. From the Syntax chapter in the guide, we read Variable and function names should use only lowercase letters, numbers, and _. Use underscores (_) (so called snake case) to separate words within a name. By those sensibilities, we’ll name our rows and columns as matrix(1:6, nrow = 2, dimnames = list(TheRowDimName = c(&quot;row_1_name&quot;, &quot;row_2_name&quot;), TheColDimName = c(&quot;col_1_name&quot;, &quot;col_2_name&quot;, &quot;col_3_name&quot;))) ## TheColDimName ## TheRowDimName col_1_name col_2_name col_3_name ## row_1_name 1 3 5 ## row_2_name 2 4 6 You’ve also probably noticed that I “always put a space after a comma, never before, just like in regular English,” as well as “put a space before and after = when naming arguments in function calls.” IMO, this makes code easier to read. You do you. We’ll name our matrix x. x &lt;- matrix(1:6, nrow = 2, dimnames = list(TheRowDimName = c(&quot;row_1_name&quot;, &quot;row_2_name&quot;), TheColDimName = c(&quot;col_1_name&quot;, &quot;col_2_name&quot;, &quot;col_3_name&quot;))) Since there are 2 dimensions, we’ll subset with two dimensions. Numerical indices work. x[2, 3] ## [1] 6 Row and column names work, too. Just make sure to use quotation marks, &quot;&quot;, for those. x[&quot;row_2_name&quot;, &quot;col_3_name&quot;] ## [1] 6 Here we specify the range of columns to include. x[2, 1:3] ## col_1_name col_2_name col_3_name ## 2 4 6 Leaving that argument blank returns them all. x[2, ] ## col_1_name col_2_name col_3_name ## 2 4 6 And leaving the row index blank returns all row values within the specified column(s). x[, 3] ## row_1_name row_2_name ## 5 6 Mind your commas! This produces the second row, returned as a vector. x[2, ] ## col_1_name col_2_name col_3_name ## 2 4 6 This returns both rows of the 2nd column. x[, 2] ## row_1_name row_2_name ## 3 4 Leaving out the comma will return the numbered element. x[2] ## [1] 2 It’ll be important in your brms career to have a sense of 3-dimensional arrays. Several brms convenience functions often return them (e.g., ranef() in multilevel models). a &lt;- array(1:24, dim = c(3, 4, 2), # 3 rows, 4 columns, 2 layers dimnames = list(RowDimName = c(&quot;r1&quot;, &quot;r2&quot;, &quot;r3&quot;), ColDimName = c(&quot;c1&quot;, &quot;c2&quot;, &quot;c3&quot;, &quot;c4&quot;), LayDimName = c(&quot;l1&quot;, &quot;l2&quot;))) a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 Since these have 3 dimensions, you have to use 3-dimensional indexing. As with 2-dimensional objects, leaving the indices for a dimension blank will return all elements within that dimension. For example, this code returns all columns of r3 and l2, as a vector. a[&quot;r3&quot;, , &quot;l2&quot;] ## c1 c2 c3 c4 ## 15 18 21 24 And this code returns all layers of r3 and c4, as a vector. a[&quot;r3&quot;, &quot;c4&quot;, ] ## l1 l2 ## 12 24 3.4.4 List and data frame. Here’s my_list. my_list &lt;- list(&quot;a&quot; = 1:3, &quot;b&quot; = matrix(1:6, nrow = 2), &quot;c&quot; = &quot;Hello, world.&quot;) my_list ## $a ## [1] 1 2 3 ## ## $b ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## $c ## [1] &quot;Hello, world.&quot; To return the contents of the a portion of my_list, just execute this. my_list$a ## [1] 1 2 3 We can index further within a. my_list$a[2] ## [1] 2 To return the contents of the first item in our list with the double bracket, [[]], do: my_list[[1]] ## [1] 1 2 3 You can index further to return only the second element of the first list item. my_list[[1]][2] ## [1] 2 But double brackets, [][], are no good, here. my_list[1][2] ## $&lt;NA&gt; ## NULL To learn more, Jenny Bryan has a great talk discussing the role of lists within data wrangling. There’s also this classic pic from Hadley Wickham: Indexing lists in #rstats. Inspired by the Residence Inn But here’s a data frame. d &lt;- data.frame(integers = 1:3, number_names = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) d ## integers number_names ## 1 1 one ## 2 2 two ## 3 3 three With data frames, we can continue indexing with the $ operator. d$number_names ## [1] one two three ## Levels: one three two We can also use the double bracket. d[[2]] ## [1] one two three ## Levels: one three two Notice how the single bracket with no comma indexes columns rather than rows. d[2] ## number_names ## 1 one ## 2 two ## 3 three But adding the comma returns the factor-level information when indexing columns. d[, 2] ## [1] one two three ## Levels: one three two It works a touch differently when indexing by row. d[2, ] ## integers number_names ## 2 2 two Let’s try with a tibble, instead. t &lt;- tibble(integers = 1:3, number_names = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) t ## # A tibble: 3 x 2 ## integers number_names ## &lt;int&gt; &lt;chr&gt; ## 1 1 one ## 2 2 two ## 3 3 three One difference is that tibbles default to assigning text columns as character strings rather than factors. Another difference occurs when printing large data frames versus large tibbles. Tibbles yield more compact glimpses. For more, check out R4DS Chapter 10. It’s also worthwhile pointing out that within the tidyverse, you can pull out a specific column with the select() function. Here we select number_names. t %&gt;% select(number_names) ## # A tibble: 3 x 1 ## number_names ## &lt;chr&gt; ## 1 one ## 2 two ## 3 three Go here learn more about select(). 3.5 Loading and saving data 3.5.1 The read.csv read_csv() and read.table read_table() functions. Although read.csv() is the default CSV reader in R, the read_csv() function from the readr package (i.e., one of the core tidyverse packages) is a new alternative. In comparison to base R’s read.csv(), readr::read_csv() is faster and returns tibbles (as opposed to data frames with read.csv()). The same general points hold for base R’s read.table() versus readr::read_table(). Using Kruschke’s HGN.csv example, we’d load the CSV with read_csv() like this: hgn &lt;- read_csv(&quot;data.R/HGN.csv&quot;) Note again that read_csv() defaults to returning columns with character information as characters, not factors. hgn$Hair ## [1] &quot;black&quot; &quot;brown&quot; &quot;blond&quot; &quot;black&quot; &quot;black&quot; &quot;red&quot; &quot;brown&quot; See? As a character variable, Hair no longer has factor level information. But if you knew you wanted to treat Hair as a factor, you could easily convert it with dplyr::mutate(). hgn &lt;- hgn %&gt;% mutate(Hair = factor(Hair)) hgn$Hair ## [1] black brown blond black black red brown ## Levels: black blond brown red And here’s a tidyverse way to reorder the levels for the Hair factor. hgn &lt;- hgn %&gt;% mutate(Hair = factor(Hair, levels = c(&quot;red&quot;, &quot;blond&quot;, &quot;brown&quot;, &quot;black&quot;))) hgn$Hair ## [1] black brown blond black black red brown ## Levels: red blond brown black as.numeric(hgn$Hair) ## [1] 4 3 2 4 4 1 3 Since we imported hgn with read_csv(), the Name column is already a character vector, which we can verify with the str() function. hgn$Name %&gt;% str() ## chr [1:7] &quot;Alex&quot; &quot;Betty&quot; &quot;Carla&quot; &quot;Diane&quot; &quot;Edward&quot; &quot;Frank&quot; &quot;Gabrielle&quot; Note how using as.vector() did nothing in our case. Name was already a character vector. hgn$Name %&gt;% as.vector() %&gt;% str() ## chr [1:7] &quot;Alex&quot; &quot;Betty&quot; &quot;Carla&quot; &quot;Diane&quot; &quot;Edward&quot; &quot;Frank&quot; &quot;Gabrielle&quot; The Group column was imported as composed of integers. hgn$Group %&gt;% str() ## num [1:7] 1 1 1 2 2 2 2 Switching Group to a factor is easy enough. hgn &lt;- hgn %&gt;% mutate(Group = factor(Group)) hgn$Group ## [1] 1 1 1 2 2 2 2 ## Levels: 1 2 3.5.2 Saving data from R. Yeah you guessed it, readr has a write_csv() function, too. The arguments are as follows: write_csv(x, path, na = &quot;NA&quot;, append = FALSE, col_names = !append). Saving hgn in your working directory is as easy as: write_csv(hgn, &quot;hgn.csv&quot;) You could also use save(). save(hgn, file = &quot;hgn.Rdata&quot; ) Once we start fitting Bayesian models, this method will be an important way to save the results of those models. The load() function is simple. load(&quot;hgn.Rdata&quot; ) The ls() function works very much the same way as the more verbosely-named objects() function. ls() ## [1] &quot;a&quot; &quot;abc&quot; &quot;d&quot; &quot;hgn&quot; &quot;my_list&quot; &quot;step_1&quot; &quot;t&quot; ## [8] &quot;x&quot; &quot;xf&quot; &quot;xfo&quot; &quot;xfol&quot; &quot;y&quot; 3.6 Some utility functions # this is a more compact way to replicate 100 1&#39;s, 200 2&#39;s, and 300 3&#39;s x &lt;- rep(1:3, times = c(100, 200, 300)) summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 2.500 2.333 3.000 3.000 We can use the pipe to convert and then summarize x. x %&gt;% factor() %&gt;% summary() ## 1 2 3 ## 100 200 300 head() and tail() are quite useful. head(x) ## [1] 1 1 1 1 1 1 tail(x) ## [1] 3 3 3 3 3 3 Within the tidyverse, the slice() function serves a similar role. In order to use slice(), we’ll want to convert x, which is just a vector of integers, into a data frame. Then we’ll use slice() to return a subset of the rows. x &lt;- x %&gt;% data.frame() x %&gt;% slice(1:6) ## . ## 1 1 ## 2 1 ## 3 1 ## 4 1 ## 5 1 ## 6 1 So that was analogous to what we accomplished with head(). Here’s the analogue to tail(). x %&gt;% slice(595:600) ## . ## 1 3 ## 2 3 ## 3 3 ## 4 3 ## 5 3 ## 6 3 The downside of that code was we had to do the math to determine that \\(600 - 6 = 595\\) in order to get the last six rows, as returned by tail(). A more general approach is to use n(), which will return the total number of rows in the tibble. x %&gt;% slice((n() - 6):n()) ## . ## 1 3 ## 2 3 ## 3 3 ## 4 3 ## 5 3 ## 6 3 ## 7 3 To unpack (n() - 6):n(), because n() = 600, (n() - 6) = 600 - 6 = 595. Therefore (n() - 6):n() was equivalent to having coded 595:600. Instead of having to do the math ourselves, n() did it for us. It’s often easier to just go with head() or tail(). But the advantage of this more general approach is that it allows one take more complicated slices of the data, such as returning the first three and last three rows. x %&gt;% slice(c(1:3, (n() - 3):n())) ## . ## 1 1 ## 2 1 ## 3 1 ## 4 3 ## 5 3 ## 6 3 ## 7 3 We’ve already used the handy str() function a bit. It’s also nice to know that tidyverse::glimpse() performs a similar function. x %&gt;% str() ## &#39;data.frame&#39;: 600 obs. of 1 variable: ## $ .: int 1 1 1 1 1 1 1 1 1 1 ... x %&gt;% glimpse() ## Rows: 600 ## Columns: 1 ## $ . &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… Within the tidyverse, we’d use group_by() and then summarize() as alternatives to the aggregate() function. With group_by() we group the observations first by Hair and then by Gender within Hair. After that, we summarize the groups by taking the median() values of their Number. hgn %&gt;% group_by(Hair, Gender) %&gt;% summarize(median = median(Number)) ## # A tibble: 5 x 3 ## # Groups: Hair [4] ## Hair Gender median ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 red M 7 ## 2 blond F 3 ## 3 brown F 7 ## 4 black F 7 ## 5 black M 1.5 One of the nice things about this workflow is that the code reads somewhat like how we’d explain what we were doing. We, in effect, told R to Take hgn, then group the data by Hair and Gender within Hair, and then summarize() those groups by their median() Number values. There’s also the nice quality that we don’t have to continually tell R where the data are coming from the way the aggregate() function required Kruschke to prefix each of his variables with HGNdf$. We also didn’t have to explicitly rename the output columns the way Kruschke had to. I’m not aware that our group_by() %&gt;% summarize() workflow has a formula format the way aggregate() does. To count how many levels we had in a grouping factor, we’d use the n() function in summarize(). hgn %&gt;% group_by(Hair, Gender) %&gt;% summarize(n = n()) ## # A tibble: 5 x 3 ## # Groups: Hair [4] ## Hair Gender n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 red M 1 ## 2 blond F 1 ## 3 brown F 2 ## 4 black F 1 ## 5 black M 2 Alternatively, we could switch out the group_by() and summarize() lines with count(). hgn %&gt;% count(Hair, Gender) ## # A tibble: 5 x 3 ## Hair Gender n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 red M 1 ## 2 blond F 1 ## 3 brown F 2 ## 4 black F 1 ## 5 black M 2 We could then use spread() to convert that output to a format similar to Kruschke’s table of counts. hgn %&gt;% count(Hair, Gender) %&gt;% spread(key = Hair, value = n) ## # A tibble: 2 x 5 ## Gender red blond brown black ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 F NA 1 2 1 ## 2 M 1 NA NA 2 With this method, the NAs are stand-ins for 0s. a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 apply() is part of a family of functions that offer a wide array of uses. You can learn more about the apply() family here or here. apply(a, MARGIN = c(2, 3), FUN = sum) ## LayDimName ## ColDimName l1 l2 ## c1 6 42 ## c2 15 51 ## c3 24 60 ## c4 33 69 Here’s a. a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 The reshape2 package (Wickham, 2007, 2020b) is a precursor to the tidyr package (i.e., one of the core tidyverse packages). The reshape2::melt() function is a quick way to transform the 3-dimensional a matrix into a tidy data frame. a %&gt;% reshape2::melt() ## RowDimName ColDimName LayDimName value ## 1 r1 c1 l1 1 ## 2 r2 c1 l1 2 ## 3 r3 c1 l1 3 ## 4 r1 c2 l1 4 ## 5 r2 c2 l1 5 ## 6 r3 c2 l1 6 ## 7 r1 c3 l1 7 ## 8 r2 c3 l1 8 ## 9 r3 c3 l1 9 ## 10 r1 c4 l1 10 ## 11 r2 c4 l1 11 ## 12 r3 c4 l1 12 ## 13 r1 c1 l2 13 ## 14 r2 c1 l2 14 ## 15 r3 c1 l2 15 ## 16 r1 c2 l2 16 ## 17 r2 c2 l2 17 ## 18 r3 c2 l2 18 ## 19 r1 c3 l2 19 ## 20 r2 c3 l2 20 ## 21 r3 c3 l2 21 ## 22 r1 c4 l2 22 ## 23 r2 c4 l2 23 ## 24 r3 c4 l2 24 We have an alternative if you wanted to stay within the tidyverse. To my knowledge, the fastest way to make the transformation is to first use as.tbl_cube() and follow that up with as_tibble(). The as.tbl_cube() function will convert the a matrix into a tbl_cube. We will use the met_name argument to determine the name of the measure assessed in the data. Since the default is for as.tbl_cube() to name the measure name as ., it seemed value was a more descriptive choice. We’ll then use the as_tibble() function to convert our tbl_cube object into a tidy tibble. a %&gt;% as.tbl_cube(met_name = &quot;value&quot;) %&gt;% as_tibble() ## # A tibble: 24 x 4 ## RowDimName ColDimName LayDimName value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 r1 c1 l1 1 ## 2 r2 c1 l1 2 ## 3 r3 c1 l1 3 ## 4 r1 c2 l1 4 ## 5 r2 c2 l1 5 ## 6 r3 c2 l1 6 ## 7 r1 c3 l1 7 ## 8 r2 c3 l1 8 ## 9 r3 c3 l1 9 ## 10 r1 c4 l1 10 ## # … with 14 more rows Notice how the first three columns are returned as characters instead of factors. If you really wanted those to be factors, you could always follow up the code with mutate_if(is.character, as.factor). 3.7 Programming in R It’s worthy to note that this project was done with R Markdown, which is an alternative to an R script. As Grolemund and Wickham point out R Markdown integrates a number of R packages and external tools. This means that help is, by-and-large, not available through ?. Instead, as you work through this chapter, and use R Markdown in the future, keep these resources close to hand: R Markdown Cheat Sheet: Help &gt; Cheatsheets &gt; R Markdown Cheat Sheet, R Markdown Reference Guide: Help &gt; Cheatsheets &gt; R Markdown Reference Guide. Both cheatsheets are also available at https://rstudio.com/resources/cheatsheets/. I also strongly recommend checking out R Notebooks, which is a kind of R Markdown document but with a few bells a whistles that make it more useful for working scientists. You can learn more about it here and here. And for a more comprehensive overview, check out Xie, Allaire, and Grolemund’s (2020) R markdown: The definitive guide. 3.7.1 Variable names in R. Kruschke prefers to use camelBack notation for his variable and function names. Though I initially hated it, I’ve been moving in the direction of snake_case. It seems easier to read_prose_in_snake_case than it is to readProseInCamelBack. To each their own. 3.7.2 Running a program. See R4DS Chapter 8 for a nice overview on working directories within the context of an RStudio project. 3.7.3 Programming a function. Here’s our simple a_sq_plus_b function. a_sq_plus_b &lt;- function(a, b = 1) { c &lt;- a^2 + b return(c) } If you explicitly denote your arguments, everything works fine. a_sq_plus_b(a = 3, b = 2) ## [1] 11 Keep things explicit and you can switch up the order of the arguments. a_sq_plus_b(b = 2, a = 3) ## [1] 11 But here’s what happens when you are less explicit. # this a_sq_plus_b(3, 2) ## [1] 11 # is not the same as this a_sq_plus_b(2, 3) ## [1] 7 Since we gave b a default value, we can be really lazy. a_sq_plus_b(a = 2) ## [1] 5 But we can’t be lazy with a. This a_sq_plus_b(b = 1) yielded this warning on my computer: “Error in a_sq_plus_b(b = 1) : argument”a&quot; is missing, with no default&quot;. If we’re completely lazy, a_sq_plus_b() presumes our sole input value is for the a argument and it uses the default value of 1 for b. a_sq_plus_b(2) ## [1] 5 The lesson is important because it’s good practice to familiarize yourself with the defaults of the functions you use in statistics and data analysis, more generally. 3.7.4 Conditions and loops. Here’s our starting point for if() and else(). if(x &lt;= 3) { # if x is less than or equal to 3 show(&quot;small&quot;) # display the word &quot;small&quot; } else { # otherwise show(&quot;big&quot;) # display the word &quot;big&quot; } # end of ’else’ clause ## Warning in if (x &lt;= 3) {: the condition has length &gt; 1 and only the first ## element will be used ## [1] &quot;small&quot; Yep, this is no good. if (x &lt;= 3) {show(&quot;small&quot;)} else {show(&quot;big&quot;)} On my computer, it returned this message: “the condition has length &gt; 1 and only the first element will be used[1]”small&quot; Error: unexpected ‘else’ in “else”&quot;. Here we use the loop. for (count_down in 5:1) { show(count_down) } ## [1] 5 ## [1] 4 ## [1] 3 ## [1] 2 ## [1] 1 for (note in c(&quot;do&quot;, &quot;re&quot;, &quot;mi&quot;)) { show(note) } ## [1] &quot;do&quot; ## [1] &quot;re&quot; ## [1] &quot;mi&quot; It’s also useful to understand how to use the ifelse() function within the context of a data frame. Recall how x is a data frame. x &lt;- tibble(x = 1:5) x ## # A tibble: 5 x 1 ## x ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 We can use the mutate() function to make a new variable, size, which is itself a function of the original variable, x. We’ll use the ifelse() function to return “small” if x &lt;= 3, but to return “big” otherwise. x %&gt;% mutate(size = ifelse(x &lt;= 3, &quot;small&quot;, &quot;big&quot;)) ## # A tibble: 5 x 2 ## x size ## &lt;int&gt; &lt;chr&gt; ## 1 1 small ## 2 2 small ## 3 3 small ## 4 4 big ## 5 5 big You should also know there’s a dplyr alternative, called if_else(). It works quite similarily, but is stricter about type consistency. If you ever get into a situation where you need to do many ifelse() statements or a many-layered ifelse() statement, you might check out dplyr::case_when(). 3.7.5 Measuring processing time. This will be nontrivial to consider in your Bayesian career. Here’s the loop. start_time &lt;- proc.time() y &lt;- vector(mode = &quot;numeric&quot;, length = 1.0E6) for (i in 1:1.0E6) {y[i] &lt;- log(i)} stop_time &lt;- proc.time() elapsed_time_loop &lt;- stop_time - start_time show(elapsed_time_loop) ## user system elapsed ## 0.061 0.002 0.063 Now we use a vector. start_time &lt;- proc.time() y &lt;- log(1:1.0E6) stop_time &lt;- proc.time() elapsed_time_vector &lt;- stop_time - start_time show(elapsed_time_vector) ## user system elapsed ## 0.009 0.003 0.012 Here we compare the two times. elapsed_time_vector[1] / elapsed_time_loop[1] ## user.self ## 0.147541 For my computer, the vectorized approach took about 14.8% the time the loop approach did. When using R, avoid loops for vectorized approaches whenever possible. As an alternative, I tend to just use Sys.time() when I’m doing analyses like these. I’m not going to walk them out, here. But as we go along, you might notice I sometimes use functions from the purrr::map() family in places where Kruschke used loops. I think they’re pretty great. 3.7.6 Debugging. This should be no surprise by now, but in addition to Kruschke’s good advice, I also recommend checking out R4DS. I reference it often. 3.8 Graphical plots: Opening and saving For making and saving plots with ggplot2, I recommend reviewing R4DS Chapters 3 and 28 or Wickham’s ggplot2: Elegant graphics for data analysis. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 ## [5] readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ggplot2_3.3.0 ## [9] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.0.0 xfun_0.13 reshape2_1.4.4 haven_2.2.0 ## [5] lattice_0.20-38 colorspace_1.4-1 vctrs_0.3.0 generics_0.0.2 ## [9] htmltools_0.4.0 yaml_2.2.1 utf8_1.1.4 rlang_0.4.6 ## [13] pillar_1.4.4 glue_1.4.0 withr_2.2.0 DBI_1.1.0 ## [17] dbplyr_1.4.2 modelr_0.1.6 readxl_1.3.1 plyr_1.8.6 ## [21] lifecycle_0.2.0 munsell_0.5.0 gtable_0.3.0 cellranger_1.1.0 ## [25] rvest_0.3.5 evaluate_0.14 labeling_0.3 knitr_1.28 ## [29] fansi_0.4.1 highr_0.8 broom_0.5.5 Rcpp_1.0.4.6 ## [33] scales_1.1.1 backports_1.1.6 jsonlite_1.6.1 farver_2.0.3 ## [37] fs_1.4.1 hms_0.5.3 digest_0.6.25 stringi_1.4.6 ## [41] bookdown_0.18 grid_3.6.3 cli_2.0.2 tools_3.6.3 ## [45] magrittr_1.5 crayon_1.3.4 pkgconfig_2.0.3 ellipsis_0.3.0 ## [49] xml2_1.3.1 reprex_0.3.0 lubridate_1.7.8 assertthat_0.2.1 ## [53] rmarkdown_2.1 httr_1.4.1 rstudioapi_0.11 R6_2.4.1 ## [57] nlme_3.1-144 compiler_3.6.3 References "],
["what-is-this-stuff-called-probability.html", "4 What is This Stuff Called Probability? 4.1 The set of all possible events 4.2 Probability: Outside or inside the head 4.3 Probability distributions 4.4 Two-way distributions Session info", " 4 What is This Stuff Called Probability? Inferential statistical techniques assign precise measures to our uncertainty about possibilities. Uncertainty is measured in terms of probability, and therefore we must establish the properties of probability before we can make inferences about it. This chapter introduces the basic ideas of probability. (Kruschke, 2015, p. 71, emphasis in the original) 4.1 The set of all possible events This snip from page 72 is important (emphasis in the original): Whenever we ask about how likely an outcome is, we always ask with a set of possible outcomes in mind. This set exhausts all possible outcomes, and the outcomes are all mutually exclusive. This set is called the sample space. 4.2 Probability: Outside or inside the head It’s worthwhile to quote this section in full. Sometimes we talk about probabilities of outcomes that are “out there” in the world. The face of a flipped coin is such an outcome: We can observe the flip, and the probability of coming up heads can be estimated by observing several flips. But sometimes we talk about probabilities of things that are not so clearly “out there,” and instead are just possible beliefs “inside the head.” Our belief about the fairness of a coin is an example of something inside the head. The coin may have an intrinsic physical bias, but now I am referring to our belief about the bias. Our beliefs refer to a space of mutually exclusive and exhaustive possibilities. It might be strange to say that we randomly sample from our beliefs, like we randomly sample from a sack of coins. Nevertheless, the mathematical properties of probabilities outside the head and beliefs inside the head are the same in their essentials, as we will see. (pp. 73–74, emphasis in the original) 4.2.1 Outside the head: Long-run relative frequency. For events outside the head, it’s intuitive to think of probability as being the long-run relative frequency of each possible outcome… We can determine the long-run relative frequency by two different ways. One way is to approximate it by actually sampling from the space many times and tallying the number of times each event happens. A second way is by deriving it mathematically. These two methods are now explored in turn. (p. 74) 4.2.1.1 Simulating a long-run relative frequency. Before we try coding the simulation, we’ll first load the tidyverse. library(tidyverse) Now run the simulation. n &lt;- 500 # specify the total number of flips p_heads &lt;- 0.5 # specify underlying probability of heads # Kruschke reported this was the seed he used at the top of page 94 set.seed(47405) # here we use that seed to flip a coin n times and compute the running proportion of heads at each flip. # we generate a random sample of n flips (heads = 1, tails = 0) d &lt;- tibble(flip_sequence = sample(x = c(0, 1), prob = c(1 - p_heads, p_heads), size = n, replace = T)) %&gt;% mutate(n = 1:n, r = cumsum(flip_sequence)) %&gt;% mutate(run_prop = r / n) end_prop &lt;- d %&gt;% select(run_prop) %&gt;% slice(n()) %&gt;% round(digits = 3) %&gt;% pull() Now we’re ready to make Figure 4.1. d %&gt;% filter(n &lt; 1000) %&gt;% # this step cuts down on the time it takes to make the plot ggplot(aes(x = n, y = run_prop)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_line(color = &quot;grey50&quot;) + geom_point(color = &quot;grey50&quot;, alpha = 1/4) + scale_x_log10(breaks = c(1, 2, 5, 10, 20, 50, 200, 500)) + coord_cartesian(xlim = c(1, 500), ylim = c(0, 1)) + labs(title = &quot;Running proportion of heads&quot;, subtitle = paste(&quot;Our end proportion =&quot;, end_prop), x = &quot;Flip number&quot;, y = &quot;Proportion of heads&quot;) + theme(panel.grid = element_blank()) 4.2.1.2 Deriving a long-run relative frequency. Sometimes, when the situation is simple enough mathematically, we can derive the exact long-run relative frequency. The case of the fair coin is one such simple situation. The sample space of the coin consists of two possible outcomes, head and tail. By the assumption of fairness, we know that each outcome is equally likely. Therefore, the long-run relative frequency of heads should be exactly one out of two, i.e., 1/2, and the long-run relative frequency of tails should also be exactly 1/2. (p. 76) 4.2.2 Inside the head: Subjective belief. To specify our subjective beliefs, we have to specify how likely we think each possible outcome is. It can be hard to pin down mushy intuitive beliefs. In the next section, we explore one way to “calibrate” subjective beliefs, and in the subsequent section we discuss ways to mathematically describe degrees of belief. (p. 76) 4.2.3 Probabilities assign numbers to possibilities. In general, a probability, whether it’s outside the head or inside the head, is just a way of assigning numbers to a set of mutually exclusive possibilities. The numbers, called “probabilities,” merely need to satisfy three properties (Kolmogorov &amp; Bharucha-Reid, 1956): A probability value must be nonnegative (i.e., zero or positive). The sum of the probabilities across all events in the entire sample space must be 1.0 (i.e., one of the events in the space must happen, otherwise the space does not exhaust all possibilities). For any two mutually exclusive events, the probability that one or the other occurs is the sum of their individual probabilities. For example, the probability that a fair six-sided die comes up 3-dots or 4-dots is 1/6 + 1/6 = 2/6. Any assignment of numbers to events that respects those three properties will also have all the properties of probabilities that we will discuss below. (pp. 77–78, emphasis in the original) 4.3 Probability distributions “A probability distribution is simply a list of all possible outcomes and their corresponding probabilities” (p. 78, emphasis in the original) 4.3.1 Discrete distributions: Probability mass. When the sample space consists of discrete outcomes, then we can talk about the probability of each distinct outcome. For example, the sample space of a flipped coin has two discrete outcomes, and we talk about the probability of head or tail… For continuous outcome spaces, we can discretize the space into a finite set of mutually exclusive and exhaustive “bins.” (p. 78, emphasis in the original) In order to recreate Figure 4.2, we need to generate the heights data. In his HtWtDataDenerator.R script, Kruschke provided the code for a function that will generate height data of the kind in his text. Here is the code: HtWtDataGenerator &lt;- function(n_subj, rndsd = NULL, male_prob = 0.50) { # Random height, weight generator for males and females. Uses parameters from # Brainard, J. &amp; Burmaster, D. E. (1992). Bivariate distributions for height and # weight of men and women in the United States. Risk Analysis, 12(2), 267-275. # Kruschke, J. K. (2011). Doing Bayesian data analysis: # A Tutorial with R and BUGS. Academic Press / Elsevier. # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition: # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier. # require(MASS) # Specify parameters of multivariate normal (MVN) distributions. # Men: HtMmu &lt;- 69.18 HtMsd &lt;- 2.87 lnWtMmu &lt;- 5.14 lnWtMsd &lt;- 0.17 Mrho &lt;- 0.42 Mmean &lt;- c(HtMmu, lnWtMmu) Msigma &lt;- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd, Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2) # Women cluster 1: HtFmu1 &lt;- 63.11 HtFsd1 &lt;- 2.76 lnWtFmu1 &lt;- 5.06 lnWtFsd1 &lt;- 0.24 Frho1 &lt;- 0.41 prop1 &lt;- 0.46 Fmean1 &lt;- c(HtFmu1, lnWtFmu1) Fsigma1 &lt;- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1, Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2) # Women cluster 2: HtFmu2 &lt;- 64.36 HtFsd2 &lt;- 2.49 lnWtFmu2 &lt;- 4.86 lnWtFsd2 &lt;- 0.14 Frho2 &lt;- 0.44 prop2 &lt;- 1 - prop1 Fmean2 &lt;- c(HtFmu2, lnWtFmu2) Fsigma2 &lt;- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2, Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2) # Randomly generate data values from those MVN distributions. if (!is.null(rndsd)) {set.seed(rndsd)} data_matrix &lt;- matrix(0, nrow = n_subj, ncol = 3) colnames(data_matrix) &lt;- c(&quot;male&quot;, &quot;height&quot;, &quot;weight&quot;) maleval &lt;- 1; femaleval &lt;- 0 # arbitrary coding values for (i in 1:n_subj) { # Flip coin to decide sex sex = sample(c(maleval, femaleval), size = 1, replace = TRUE, prob = c(male_prob, 1 - male_prob)) if (sex == maleval) {datum &lt;- MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)} if (sex == femaleval) { Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2)) if (Fclust == 1) {datum &lt;- MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)} if (Fclust == 2) {datum &lt;- MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)} } data_matrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1)) } return(data_matrix) } # end function Now we have the HtWtDataGenerator() function, all we need to do is determine how many values are generated and how probable we want the values to be based on those from men. These are controlled by the n_subj and male_prob parameters. set.seed(4) d &lt;- HtWtDataGenerator(n_subj = 10000, male_prob = .5) %&gt;% data.frame() %&gt;% mutate(person = 1:n()) d %&gt;% head() ## male height weight person ## 1 1 76.0 221.5 1 ## 2 0 59.5 190.0 2 ## 3 0 60.2 117.9 3 ## 4 1 64.1 137.7 4 ## 5 1 69.3 147.6 5 ## 6 1 69.1 165.9 6 For Figure 4.2, we’ll make extensive use of the case_when() syntax, which you can learn more about from hrbrmstr’s blog post, Making a case for case_when. d_bin &lt;- d %&gt;% mutate(bin = case_when( height &lt; 51 ~ 51, between(height, 51, 53) ~ 53, between(height, 53, 55) ~ 55, between(height, 55, 57) ~ 57, between(height, 57, 59) ~ 59, between(height, 59, 61) ~ 61, between(height, 61, 63) ~ 63, between(height, 63, 65) ~ 65, between(height, 65, 67) ~ 67, between(height, 67, 69) ~ 69, between(height, 69, 71) ~ 71, between(height, 71, 73) ~ 73, between(height, 73, 75) ~ 75, between(height, 75, 77) ~ 77, between(height, 77, 79) ~ 79, between(height, 79, 81) ~ 71, between(height, 81, 83) ~ 83, height &gt; 83 ~ 85) ) %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% mutate(height = bin - 1) d %&gt;% ggplot(aes(x = height, y = person)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 51, to = 83, by = 2), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Person #&quot;) + theme(panel.grid = element_blank()) Because we’re simulating and we don’t know what seed number Kruschke used for his plot, ours will differ a little from his. But the overall pattern is the same. It’s a little less work to make Figure 4.2.b. d %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = stat(density)), binwidth = 2, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + scale_y_continuous(&quot;Probability density&quot;, breaks = c(0, .04, .08)) + xlab(&quot;Height (inches)&quot;) + coord_cartesian(xlim = c(51, 83)) + theme(panel.grid = element_blank()) Our data binning approach for Figure 4.2.c will be a little different than what we did, above. Here we’ll make our bins with the round() function. d_bin &lt;- d %&gt;% mutate(bin = round(height, digits = 0)) %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% mutate(height = bin - 0.5) d %&gt;% ggplot(aes(x = height, y = person)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 51, to = 83, by = 1), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n, angle = 90), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Person #&quot;) + theme(panel.grid = element_blank()) However, our method for Figure 4.2.d will be like what we did, before. d %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = stat(density)), boundary = 0, binwidth = 1, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + scale_y_continuous(&quot;Probability density&quot;, breaks = c(0, .04, .08)) + xlab(&quot;Height (inches)&quot;) + coord_cartesian(xlim = c(51, 83)) + theme(panel.grid = element_blank()) The probability of a discrete outcome, such as the probability of falling into an interval on a continuous scale, is referred to as a probability mass. Loosely speaking, the term “mass” refers the amount of stuff in an object. When the stuff is probability and the object is an interval of a scale, then the mass is the proportion of the outcomes in the interval. (p. 80, emphasis in the original) 4.3.2 Continuous distributions: Rendezvous with density. If you think carefully about a continuous outcome space, you realize that it becomes problematic to talk about the probability of a specific value on the continuum, as opposed to an interval on the continuum… Therefore, what we will do is make the intervals infinitesimally narrow, and instead of talking about the infinitesimal probability mass of each infinitesimal interval, we will talk about the ratio of the probability mass to the interval width. That ratio is called the probability density. Loosely speaking, density is the amount of stuff per unit of space it takes up. Because we are measuring amount of stuff by its mass, then density is the mass divided by the amount space it occupies. (p. 80, emphasis in the original) To make Figure 4.3, we’ll need new data. set.seed(4) d &lt;- tibble(height = rnorm(1e4, mean = 84, sd = .1)) %&gt;% mutate(door = 1:n()) d %&gt;% head() ## # A tibble: 6 x 2 ## height door ## &lt;dbl&gt; &lt;int&gt; ## 1 84.0 1 ## 2 83.9 2 ## 3 84.1 3 ## 4 84.1 4 ## 5 84.2 5 ## 6 84.1 6 To make the bins for our version of Figure 4.3.a, we could use the case_when() approach from above. However, that would require some tedious code. Happily, we have an alternative in the santoku package (Hugh-Jones, 2020), which I learned about with help from the great Mara Averick, Tyson Barrett, and Omar Wasow. We can use the santoku::chop() function to discretize our height values. Here we’ll walk through the first part. # devtools::install_github(&quot;hughjonesd/santoku&quot;) library(santoku) d_bin &lt;- d %&gt;% mutate(bin = chop(height, breaks = seq(from = 83.6, to = 84.4, length.out = 31), labels = seq(from = 83.6, to = 84.4, length.out = 31)[-1])) head(d_bin) ## # A tibble: 6 x 3 ## height door bin ## &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; ## 1 84.0 1 84.0266666666667 ## 2 83.9 2 83.9466666666667 ## 3 84.1 3 84.1066666666667 ## 4 84.1 4 84.08 ## 5 84.2 5 84.1866666666667 ## 6 84.1 6 84.08 We’ve labeled our bin levels by their upper bounds. Note how they are saved as factors. To make use of those values in our plot, we’ll need to convert them to numerals. Here we make that conversion and complete the data wrangling. d_bin &lt;- d_bin %&gt;% mutate(bin = as.character(bin) %&gt;% as.double()) %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% mutate(height = bin - (83.62667 - 83.6) / 2) head(d_bin) ## # A tibble: 6 x 3 ## bin n height ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 83.7 5 83.6 ## 2 83.7 5 83.7 ## 3 83.7 7 83.7 ## 4 83.7 24 83.7 ## 5 83.8 37 83.7 ## 6 83.8 86 83.8 Now we plot. d %&gt;% ggplot(aes(x = height, y = door)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 83.6, to = 84.4, length.out = 31), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n, angle = 90), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Door #&quot;) + theme(panel.grid = element_blank()) Figure 4.3.b is a breeze. d %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = stat(density)), boundary = 0, binwidth = .025, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + scale_y_continuous(&quot;Probability density&quot;, breaks = 0:4) + xlab(&quot;Height (inches)&quot;) + coord_cartesian(xlim = c(83.6, 84.4)) + theme(panel.grid = element_blank()) 4.3.2.1 Properties of probability density functions. In general, for any continuous value that is split up into intervals, the sum of the probability masses of the intervals must be 1, because, by definition of making a measurement, some value of the measurement scale must occur. (p. 82) 4.3.2.2 The normal probability density function. “Perhaps the most famous probability density function is the normal distribution, also known as the Gaussian distribution” (p. 83). We’ll use dnorm() again to make our version of Figure 4.4. tibble(x = seq(from = -.8, to = .8, by = .02)) %&gt;% mutate(p = dnorm(x, mean = 0, sd = .2)) %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = p), color = &quot;grey50&quot;, size = 1.25) + geom_linerange(aes(ymin = 0, ymax = p), size = 1/3) + labs(title = &quot;Normal probability density&quot;, subtitle = expression(paste(mu, &quot; = 0 and &quot;, sigma, &quot; = 0.2&quot;)), y = &quot;p(x)&quot;) + coord_cartesian(xlim = c(-.61, .61)) + theme(panel.grid = element_blank()) The equation for the normal probability density follows the form \\[ p(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\Bigg ( - \\frac{1}{2} \\bigg [ \\frac{x - \\mu}{\\sigma}^2 \\bigg ] \\Bigg ), \\] where \\(\\mu\\) governs the mean and \\(\\sigma\\) governs the standard deviation. 4.3.3 Mean and variance of a distribution. The mean of a probability distribution is also called the expected value, which follow the form \\[E[x] = \\sum_x p(x) x\\] when \\(x\\) is discrete. For continuous \\(x\\) values, the formula is \\[E[x] = \\int \\text d x \\; p(x) x.\\] The variance is defined as the mean squared deviation from the mean, \\[\\text{var}_x = \\int \\text d x \\; p(x) (x - E[x])^2.\\] If you take the square root of the variance, you get the standard deviation. 4.3.4 Highest density interval (HDI). The HDI indicates which points of a distribution are most credible, and which cover most of the distribution. Thus, the HDI summarizes the distribution by specifying an interval that spans most of the distribution, say 95% of it, such that every point inside the interval has higher credibility than any point outside the interval. (p. 87) In Chapter 10 (p. 294), Kruschke briefly mentions his HDIofICDF() function, the code for which you can find in his DBDA2E-utilities.R file. It’s a handy function which we’ll put to use from time to time. Here’s a mild reworking of his code. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { # Arguments: # `name` is R&#39;s name for the inverse cumulative density function # of the distribution. # `width` is the desired mass of the HDI region. # `tol` is passed to R&#39;s optimize function. # Return value: # Highest density iterval (HDI) limits in a vector. # Example of use: For determining HDI of a beta(30, 12) distribution, type # `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)` # Notice that the parameters of the `name` must be explicitly stated; # e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work. # Adapted and corrected from Greg Snow&#39;s TeachingDemos package. incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Now we already know from the text, and perhaps from prior experience, what the 95% HDIs for the unit normal. But it’s nice to be able to confirm that with a function. h &lt;- hdi_of_icdf(name = qnorm, mean = 0, sd = 1) h ## [1] -1.959964 1.959964 Now we’ve saved those values in h, we can use then to make our version of Figure 4.5.a. tibble(x = seq(from = -3.5, to = 3.5, by = .05)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = dnorm(x, mean = 0, sd = 1))) + geom_ribbon(fill = &quot;grey75&quot;) + geom_ribbon(data = . %&gt;% filter(x &gt;= h[1] &amp; x &lt;= h[2]), fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1] + .02, h[2] - .02), y = c(.059, .059)), aes(y = y), arrow = arrow(length = unit(.2, &quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(geom = &quot;text&quot;, x = 0, y = .09, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + xlim(-3.1, 3.1) + ylab(&quot;p(x)&quot;) + theme(panel.grid = element_blank()) As far as I could tell, Figure 4.5.b is of a beta distribution, which Kruschke covers in greater detail starting in Chapter 6. I got the shape1 and shape2 values from playing around. If you have a more principled approach, do share. But anyway, we can use our hdi_of_icdf() funciton to ge the correct values. h &lt;- hdi_of_icdf(name = qbeta, shape1 = 15, shape2 = 4) h ## [1] 0.6103498 0.9507510 Let’s put those h values to work. tibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = dbeta(x, shape1 = 15, shape2 = 4))) + geom_ribbon(fill = &quot;grey75&quot;) + geom_ribbon(data = . %&gt;% filter(x &gt;= h[1] &amp; x &lt;= h[2]), fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1] + .01, h[2] - .002), y = c(.75, .75)), aes(y = y), arrow = arrow(length = unit(.2, &quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(geom = &quot;text&quot;, x = .8, y = 1.1, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + xlim(.4, 1) + ylab(&quot;p(x)&quot;) + theme(panel.grid = element_blank()) Figure 4.5.c was also a lot of trial and error. It seemed the easiest way to reproduce the shape was to mash two Gaussians together. After playing around with rnorm(), I ended up with this. set.seed(4) d &lt;- tibble(x = c(rnorm(6e5, mean = 1.50, sd = .5), rnorm(4e5, mean = 4.75, sd = .5))) glimpse(d) ## Rows: 1,000,000 ## Columns: 1 ## $ x &lt;dbl&gt; 1.6083774, 1.2287537, 1.9455723, 1.7979903, 2.3178090, 1.8446377, 0… As you’ll see, it’s not exactly right. But it’s close enough to give you a sense of what’s going on. But anyway, since we’re working with simulated data rather than an analytic solution, we’ll want to use a powerful convenience function from the tidybayes package. library(tidybayes) Kay’s tidybayes package provides a family of functions for generating point summaries and intervals from draws in a tidy format. These functions follow the naming scheme [median|mean|mode]_[qi|hdi], for example, median_qi, mean_qi, mode_hdi, and so on. The first name (before the _) indicates the type of point summary, and the second name indicates the type of interval. qi yields a quantile interval (a.k.a. equi-tailed interval, central interval, or percentile interval) and hdi yields a highest (posterior) density interval. (Kay, 2020a, “Point summaries and intervals”) Here we’ll use mode_hdi() to compute the HDIs and put them in a tibble. We’ll be using a lot of mode_hdi() in this project. h &lt;- d %&gt;% mode_hdi() h ## # A tibble: 2 x 6 ## x .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1.50 0.468 2.54 0.95 mode hdi ## 2 1.50 3.81 5.68 0.95 mode hdi Usually, mode_hdi() will return a tibble with just one row. But in this case, since we had a bimodal distribution, it returned two rows—one for each of the two distinct regions. Oh, and in case it wasn’t clear, that first column x is the measure of central tendency—the mode, in this case. Though I acknowledge, it’s a little odd to speak of central tendency in a bimodal distribution. Again, this won’t happen much. In order to fill the bimodal density with the split HDIs, you need to use the density() function to transform the d data to a tibble with the values for the \\(x\\)-axis in an x vector and the corresponding density values in a y vector. dens &lt;- d$x %&gt;% density() %&gt;% with(tibble(x, y)) head(dens) ## # A tibble: 6 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.11 0.0000000503 ## 2 -1.09 0.0000000822 ## 3 -1.08 0.000000131 ## 4 -1.06 0.000000201 ## 5 -1.04 0.000000304 ## 6 -1.03 0.000000449 We’re finally ready to plot. Forgive me. It’s a monster. ggplot(data = dens, aes(x = x, y = y, ymin = 0, ymax = y)) + geom_ribbon(size = 0, fill = &quot;grey75&quot;) + # note the use of `pull()`, which extracts the values, rather than return a tibble geom_ribbon(data = dens %&gt;% filter(x &gt; h[1, 2] %&gt;% pull() &amp; x &lt; h[1, 3] %&gt;% pull()), size = 0, fill = &quot;grey50&quot;) + geom_ribbon(data = dens %&gt;% filter(x &gt; h[2, 2] %&gt;% pull() &amp; x &lt; h[2, 3] %&gt;% pull()), size = 0, fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1, 2] %&gt;% pull(), h[1, 3] %&gt;% pull()), y = c(.06, .06)), arrow = arrow(length = unit(.2,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + geom_line(data = tibble(x = c(h[2, 2] %&gt;% pull(), h[2, 3] %&gt;% pull()), y = c(.06, .06)), arrow = arrow(length = unit(.2,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(geom = &quot;text&quot;, x = 1.5, y = .1, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + annotate(geom = &quot;text&quot;, x = 4.75, y = .1, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + scale_x_continuous(breaks = 0:6, limits = c(0, 6.3)) + scale_y_continuous(&quot;p(x)&quot;, breaks = c(0, .1, .2, .3, .4, .5)) + theme(panel.grid = element_blank()) When the distribution refers to credibility of values, then the width of the HDI is another way of measuring uncertainty of beliefs. If the HDI is wide, then beliefs are uncertain. If the HDI is narrow, then beliefs are relatively certain. (p. 89) 4.4 Two-way distributions In the note below Table 4.1, Kruschke indicated the data came from Snee (1974), Graphical display of two-way contingency tables. Kruschke has those data saved as the HairEyeColor.csv file. d &lt;- read_csv(&quot;data.R/HairEyeColor.csv&quot;) glimpse(d) ## Rows: 16 ## Columns: 3 ## $ Hair &lt;chr&gt; &quot;Black&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Blond&quot;, &quot;Blond&quot;, &quot;Blond&quot;, … ## $ Eye &lt;chr&gt; &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green&quot;, &quot;Hazel&quot;, &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green&quot;, &quot;H… ## $ Count &lt;dbl&gt; 20, 68, 5, 15, 94, 7, 16, 10, 84, 119, 29, 54, 17, 26, 14, 14 We’ll need to transform Hair and Eye just a bit to ensure our output matches the order in Table 4.1. d &lt;- d %&gt;% mutate(Hair = if_else(Hair == &quot;Brown&quot;, &quot;Brunette&quot;, Hair) %&gt;% factor(., levels = c(&quot;Black&quot;, &quot;Brunette&quot;, &quot;Red&quot;, &quot;Blond&quot;)), Eye = factor(Eye, levels = c(&quot;Brown&quot;, &quot;Blue&quot;, &quot;Hazel&quot;, &quot;Green&quot;))) Here we’ll use the tabyl() and adorn_totals() functions from the janitor package (Firke, 2020) to help make the table of proportions by Eye and Hair. library(janitor) d &lt;- d %&gt;% uncount(weights = Count, .remove = F) %&gt;% tabyl(Eye, Hair) %&gt;% adorn_totals(c(&quot;row&quot;, &quot;col&quot;)) %&gt;% data.frame() %&gt;% mutate_if(is.double, ~(. / 592)) d %&gt;% mutate_if(is.double, round, digits = 2) ## Eye Black Brunette Red Blond Total ## 1 Brown 0.11 0.20 0.04 0.01 0.37 ## 2 Blue 0.03 0.14 0.03 0.16 0.36 ## 3 Hazel 0.03 0.09 0.02 0.02 0.16 ## 4 Green 0.01 0.05 0.02 0.03 0.11 ## 5 Total 0.18 0.48 0.12 0.21 1.00 4.4.1 Conditional probability. We often want to know the probability of one outcome, given that we know another outcome is true. For example, suppose I sample a person at random from the population referred to in Table 4.1. Suppose I tell you that this person has blue eyes. Conditional on that information, what is the probability that the person has blond hair (or any other particular hair color)? It is intuitively clear how to compute the answer: We see from the blue-eye row of Table 4.1 that the total (i.e., marginal) amount of blue-eyed people is 0.36, and that 0.16 of the population has blue eyes and blond hair. (p. 91) Kruschke then showed how to compute such conditional probabilities by hand in Table 4.2. Here’s a slightly reformatted version of that information. d %&gt;% filter(Eye == &quot;Blue&quot;) %&gt;% pivot_longer(Black:Blond, names_to = &quot;Hair&quot;, values_to = &quot;proportion&quot;) %&gt;% rename(`Eye color` = Eye, `proportion Eyes == &quot;Blue&quot;` = Total, `Hair color` = Hair) %&gt;% mutate(`conditional probability` = proportion / `proportion Eyes == &quot;Blue&quot;`) ## # A tibble: 4 x 5 ## `Eye color` `proportion Eyes == … `Hair color` proportion `conditional probab… ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Blue 0.363 Black 0.0338 0.0930 ## 2 Blue 0.363 Brunette 0.142 0.391 ## 3 Blue 0.363 Red 0.0287 0.0791 ## 4 Blue 0.363 Blond 0.159 0.437 The only reason our values differ from those in Table 4.2 is because Kruschke rounded. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] janitor_2.0.1 tidybayes_2.0.3.9000 forcats_0.5.0 ## [4] stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 ## [7] readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ## [10] ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.4.6 lubridate_1.7.8 santoku_0.3.0 lattice_0.20-38 ## [5] assertthat_0.2.1 digest_0.6.25 utf8_1.1.4 R6_2.4.1 ## [9] cellranger_1.1.0 plyr_1.8.6 backports_1.1.6 HDInterval_0.2.0 ## [13] reprex_0.3.0 evaluate_0.14 coda_0.19-3 httr_1.4.1 ## [17] pillar_1.4.4 rlang_0.4.6 readxl_1.3.1 rstudioapi_0.11 ## [21] rmarkdown_2.1 labeling_0.3 munsell_0.5.0 broom_0.5.5 ## [25] compiler_3.6.3 modelr_0.1.6 xfun_0.13 pkgconfig_2.0.3 ## [29] htmltools_0.4.0 tidyselect_1.0.0 bookdown_0.18 arrayhelpers_1.1-0 ## [33] fansi_0.4.1 crayon_1.3.4 dbplyr_1.4.2 withr_2.2.0 ## [37] MASS_7.3-51.5 grid_3.6.3 nlme_3.1-144 jsonlite_1.6.1 ## [41] gtable_0.3.0 lifecycle_0.2.0 DBI_1.1.0 pacman_0.5.1 ## [45] magrittr_1.5 scales_1.1.1 cli_2.0.2 stringi_1.4.6 ## [49] farver_2.0.3 fs_1.4.1 snakecase_0.11.0 xml2_1.3.1 ## [53] ellipsis_0.3.0 generics_0.0.2 vctrs_0.3.0 tools_3.6.3 ## [57] svUnit_1.0.3 glue_1.4.0 hms_0.5.3 yaml_2.2.1 ## [61] colorspace_1.4-1 rvest_0.3.5 knitr_1.28 haven_2.2.0 References "],
["bayes-rule.html", "5 Bayes’ Rule 5.1 Bayes’ rule 5.2 Applied to parameters and data 5.3 Complete examples: Estimating bias in a coin 5.4 Why Bayesian inference can be difficult Session info", " 5 Bayes’ Rule “Bayes’ rule is merely the mathematical relation between the prior allocation of credibility and the posterior reallocation of credibility conditional on data” (pp. 99–100). 5.1 Bayes’ rule Thomas Bayes (1702–1761) was a mathematician and Presbyterian minister in England. His famous theorem was published posthumously in 1763, thanks to the extensive editorial efforts of his friend, Richard Price (Bayes, 1763). The simple rule has vast ramifications for statistical inference, and therefore as long as his name is attached to the rule, we’ll continue to see his name in textbooks. But Bayes himself probably was not fully aware of these ramifications, and many historians argue that it is Bayes’ successor, Pierre-Simon Laplace (1749–1827), whose name should really label this type of analysis, because it was Laplace who independently rediscovered and extensively developed the methods (e.g., Dale, 2012; McGrayne, 2011). (Kruschke, 2015, p. 100) I do recommend checking out McGrayne’s book It’s an easy and entertaining read. For a sneak preview, why not listen to her discuss the main themes she covered in the book? 5.1.1 Derived from definitions of conditional probability. With Equations 5.5 and 5.6, Kruschke gave us Bayes’ rule in terms of \\(c\\) and \\(r\\). Equation 5.5 was \\[p(c|r) = \\frac{p(r|c)p(c)}{p(r)}.\\] Since \\(p(r) = \\sum_{c^*}p(r|c^*)p(c^*)\\), we can re-express that as Equation 5.6: \\[p(c|r) = \\frac{p(r|c)p(c)}{\\sum_{c^*}p(r|c^*)p(c^*)},\\] where \\(c^*\\) “in the denominator is a variable that takes on all possible values” of \\(c\\) (p. 101). 5.2 Applied to parameters and data Here we get those equations re-expressed in the terms data analysts tend to think with, parameters (i.e., \\(\\theta\\)) and data (i.e., \\(D\\)). \\[\\begin{align*} p(\\theta|D) &amp; = \\frac{p(D|\\theta)p(\\theta)}{p(D)} \\;\\; \\text{and since} \\\\ p(D) &amp; = \\sum\\limits_{\\theta^*}p(D|\\theta^*)p(\\theta^*) \\;\\; \\text{it&#39;s also the case that} \\\\ p(\\theta|D) &amp; = \\frac{p(D|\\theta)p(\\theta)}{\\sum\\limits_{\\theta^*}p(D|\\theta^*)p(\\theta^*)}. \\end{align*}\\] As in the previous section where we spoke in terms of \\(r\\) and \\(c\\), our updated \\(\\theta^*\\) notation is meant to indicate all possible values of \\(\\theta\\). For practice, it’s worth repeating how Kruschke broke this down with Equation 5.7, \\[ \\underbrace{p(\\theta|D)}_\\text{posterior} \\; = \\; \\underbrace{p(D|\\theta)}_\\text{likelihood} \\;\\; \\underbrace{p(\\theta)}_\\text{prior} \\; / \\; \\underbrace{p(D)}_\\text{evidence}. \\] The “prior,” \\(p(\\theta)\\), is the credibility of the \\(\\theta\\) values without the data \\(D\\). The “posterior,” \\(p(\\theta|D)\\), is the credibility of \\(\\theta\\) values with the data \\(D\\) taken into account. The “likelihood,” \\(p(D|\\theta)\\), is the probability that the data could be generated by the model with parameter value \\(\\theta\\). The “evidence” for the model, \\(p(D)\\), is the overall probability of the data according to the model, determined by averaging across all possible parameter values weighted by the strength of belief in those parameter values. (pp. 106–107) And don’t forget, “evidence” is short for “marginal likelihood,” which is the term we’ll use in some of our code, below. 5.3 Complete examples: Estimating bias in a coin Here’s a way to make Figure 5.1.a. library(tidyverse) tibble(theta = seq(from = 0, to = 1, by = .1), prior = c(seq(from = 0, to = .2, length.out = 6), seq(from = .16, to = 0, length.out = 5))) %&gt;% ggplot(aes(x = theta, ymin = -0.0005, ymax = prior)) + geom_linerange(size = 4, color = &quot;grey50&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Prior&quot;, y = expression(paste(&quot;p(&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) If you were curious, it is indeed the case that those prior values sum to 1. tibble(prior = c(seq(from = 0, to = .2, length.out = 6), seq(from = .16, to = 0, length.out = 5))) %&gt;% summarise(s = sum(prior)) ## # A tibble: 1 x 1 ## s ## &lt;dbl&gt; ## 1 1 If we follow Kruschke’s equation 5.10 (i.e., the Bernoulli function) closely, we can express it as a function in R. bernoulli &lt;- function(theta, y) { return(theta^y * (1 - theta)^(1 - y)) } To get a sense of how it works, consider a single coin flip of heads when heads is considered a successful trial. We’ll call the single successful trial y = 1. We can use our custom bernoulli() function to compute the likelihood of different values of \\(\\theta\\). We’ll look at 11 candidate \\(\\theta\\) values, which we’ll call theta_sequence. theta_sequence &lt;- seq(from = 0, to = 1, by = .1) bernoulli(theta = theta_sequence, y = 1) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Notice how our theta_sequence corresponds nicely with the sequence of \\(\\theta\\) values on the x-axes of Figure 5.1. We can combine theta_sequence and our bernoulli() function to make the middle panel of Figure 5.1 tibble(x = theta_sequence) %&gt;% mutate(likelihood = bernoulli(theta = theta_sequence, y = 1)) %&gt;% ggplot(aes(x = x, y = likelihood)) + geom_col(width = .025, color = &quot;grey50&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Likelihood&quot;, y = expression(paste(&quot;p(D|&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) In order to compute \\(p(D)\\) (i.e., the evidence or the marginal likelihood), we’ll need to multiply our respective prior and likelihood values for each point in our theta sequence and then sum all that up. That sum will be our marginal likelihood. With that cleared up, we can make Figure 5.1.c. tibble(theta = theta_sequence, prior = c(seq(from = 0, to = .2, length.out = 6), seq(from = .16, to = 0, length.out = 5))) %&gt;% mutate(likelihood = bernoulli(theta = theta_sequence, y = 1)) %&gt;% mutate(marginal_likelihood = sum(prior * likelihood)) %&gt;% mutate(posterior = (prior * likelihood) / marginal_likelihood) %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_col(width = .025, color = &quot;grey50&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Posterior&quot;, y = expression(paste(&quot;p(&quot;, theta, &quot;|D)&quot;))) + theme(panel.grid = element_blank()) 5.3.1 Influence of sample size on the posterior. In order to follow along with this section, we’re going to have to update our Bernoulli likelihood function so it can accommodate more than a single trial. We’ll anticipate Chapter 6 and call our more general function the bernoulli_likelihood(). bernoulli_likelihood &lt;- function(theta, data) { # `theta` = success probability parameter ranging from 0 to 1 # `data` = the vector of data (i.e., a series of 0s and 1s) n &lt;- length(data) return(theta^sum(data) * (1 - theta)^(n - sum(data))) } Here’s the work required to make our version of the left portion of Figure 5.2. small_data &lt;- rep(0:1, times = c(3, 1)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = small_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + ylab(&quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) We’ll follow the same procedure to make the right portion of Figure 5.2. The only difference is how we switched from small_data to large_data. large_data &lt;- rep(0:1, times = c(30, 10)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = large_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + ylab(&quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) With just an \\(N = 40\\), the likelihood already dominated the posterior. But this is also a function of our fairly gentle prior. 5.3.2 Influence of prior on the posterior. It’s not immediately obvious how Kruschke made his prior distributions for Figure 5.3. However, hidden away in his BernGridExample.R file he indicated that to get the distribution for the left side of Figure 5.3, you simply raise the prior from the left of Figure 5.2 to the 0.1 power. small_data &lt;- rep(0:1, times = c(3, 1)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% # here&#39;s the important line of code mutate(Prior = Prior^0.1 / sum(Prior^0.1)) %&gt;% mutate(Likelihood = bernoulli_likelihood(theta = theta, data = small_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + ylab(&quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) The trick is similar for the right half of Figure 5.3. large_data &lt;- rep(0:1, times = c(30, 10)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = large_data)) %&gt;% # here&#39;s the important line of code mutate(Prior = Prior^10) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) + ylab(&quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) Bayesian inference is intuitively rational: With a strongly informed prior that uses a lot of previous data to put high credibility over a narrow range of parameter values, it takes a lot of novel contrary data to budge beliefs away from the prior. But with a weakly informed prior that spreads credibility over a wide range of parameter values, it takes relatively little data to shift the peak of the posterior distribution toward the data (although the posterior will be relatively wide and uncertain). (p. 114) 5.4 Why Bayesian inference can be difficult Determining the posterior distribution directly from Bayes, rule involves computing the evidence (a.k.a. marginal likelihood) in Equations 5.8 and 5.9. In the usual case of continuous parameters, the integral in Equation 5.9 can be impossible to solve analytically. Historically, the difficulty of the integration was addressed by restricting models to relatively simple likelihood functions with corresponding formulas for prior distributions, called conjugate priors, that “played nice” with the likelihood function to produce a tractable integral. (p. 115, emphasis in the original) However, the simple model + conjugate prior approach has its limitations. As we’ll see, we often want to fit complex models without shackling ourselves with conjugate priors—which can be quite a pain to work with. Happily, another kind of approximation involves randomly sampling a large number of representative combinations of parameter values from the posterior distribution. In recent decades, many such algorithms have been developed, generally referred to as Markov chain Monte Carlo (MCMC) methods. What makes these methods so useful is that they can generate representative parameter-value combinations from the posterior distribution of complex models without computing the integral in Bayes’ rule. It is the development of these MCMC methods that has allowed Bayesian statistical methods to gain practical use. (pp. 115–116, emphasis in the original) Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 ## [5] readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ggplot2_3.3.0 ## [9] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.0.0 xfun_0.13 haven_2.2.0 lattice_0.20-38 ## [5] colorspace_1.4-1 vctrs_0.3.0 generics_0.0.2 htmltools_0.4.0 ## [9] yaml_2.2.1 utf8_1.1.4 rlang_0.4.6 pillar_1.4.4 ## [13] glue_1.4.0 withr_2.2.0 DBI_1.1.0 dbplyr_1.4.2 ## [17] modelr_0.1.6 readxl_1.3.1 lifecycle_0.2.0 munsell_0.5.0 ## [21] gtable_0.3.0 cellranger_1.1.0 rvest_0.3.5 evaluate_0.14 ## [25] labeling_0.3 knitr_1.28 fansi_0.4.1 broom_0.5.5 ## [29] Rcpp_1.0.4.6 scales_1.1.1 backports_1.1.6 jsonlite_1.6.1 ## [33] farver_2.0.3 fs_1.4.1 hms_0.5.3 digest_0.6.25 ## [37] stringi_1.4.6 bookdown_0.18 grid_3.6.3 cli_2.0.2 ## [41] tools_3.6.3 magrittr_1.5 crayon_1.3.4 pkgconfig_2.0.3 ## [45] ellipsis_0.3.0 xml2_1.3.1 reprex_0.3.0 lubridate_1.7.8 ## [49] assertthat_0.2.1 rmarkdown_2.1 httr_1.4.1 rstudioapi_0.11 ## [53] R6_2.4.1 nlme_3.1-144 compiler_3.6.3 References "],
["inferring-a-binomial-probability-via-exact-mathematical-analysis.html", "6 Inferring a Binomial Probability via Exact Mathematical Analysis 6.1 The likelihood function: The Bernoulli distribution 6.2 A description of credibilities: The beta distribution 6.3 The posterior beta 6.4 Examples 6.5 Summary Session info", " 6 Inferring a Binomial Probability via Exact Mathematical Analysis This chapter presents an example of how to do Bayesian inference using pure analytical mathematics without any approximations. Ultimately, we will not use the pure analytical approach for complex applications, but this chapter is important for two reasons. First, the relatively simple mathematics in this chapter nicely reveal the underlying concepts of Bayesian inference on a continuous parameter. The simple formulas show how the continuous allocation of credibility changes systematically as data accumulate. The examples provide an important conceptual foundation for subsequent approximation methods, because the examples give you a clear sense of what is being approximated. Second, the distributions introduced in this chapter, especially the beta distribution, will be used repeatedly in subsequent chapters. (Kruschke, 2015, p. 123, emphasis added) 6.1 The likelihood function: The Bernoulli distribution If we denote a set of possible outcomes as \\(\\{y_i\\}\\) Kruschke’s Bernoulli likelihood function for a set of \\(N\\) trials follows the form \\[p(\\{y_i\\} | \\theta) = \\theta^z \\cdot (1 - \\theta) ^ {N - z},\\] where \\(z\\) is the number of 1s in the data (i.e., heads in a series of coin flips) and the sole parameter a given observation will be a 1 is \\(\\theta\\) (i.e., the probability; \\(p(y_i = 1 | \\theta)\\)). If you follow that equation closely, here is how we might express it in R. bernoulli_likelihood &lt;- function(theta, data) { # `theta` = success probability parameter ranging from 0 to 1 # `data` = the vector of data (i.e., a series of 0s and 1s) n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } This will come in handy in just a bit. 6.2 A description of credibilities: The beta distribution In this chapter, we use purely mathematical analysis, with no numerical approximation, to derive the mathematical form of the posterior credibilities of parameter values. To do this, we need a mathematical description of the prior allocation of credibilities… In principle, we could use any probability density function supported on the interval [0, 1]. When we intend to apply Bayes’ rule (Equation 5.7, p. 106), however, there are two desiderata for mathematical tractability. First, it would be convenient if the product of \\(p(y | \\theta)\\) and \\(p(\\theta)\\), which is in the numerator of Bayes’ rule, results in a function of the same form as \\(p(\\theta)\\)… Second, we desire the denominator of Bayes’ rule (Equation 5.9, p. 107), namely \\(\\int \\text d \\; \\theta p(y | \\theta) p(\\theta)\\), to be solvable analytically. This quality also depends on how the form of the function \\(p(\\theta)\\) relates to the form of the function \\(p(y | \\theta)\\). When the forms of \\(p(y | \\theta)\\) and \\(p(\\theta)\\) combine so that the posterior distribution has the same form as the prior distribution, then \\(p(\\theta)\\) is called a conjugate prior for \\(p(y | \\theta)\\). (p. 127 emphasis in the original) When we want a conjugate prior for \\(\\theta\\) of the Bernoulli likelihood, the beta distribution is a handy choice. Beta has two parameters, \\(a\\) and \\(b\\) (also sometimes called \\(\\alpha\\) and \\(\\beta\\)), and the density is defined as \\[\\begin{align*} p(\\theta | a, b) &amp; = \\operatorname{beta} (\\theta | a, b) \\\\ &amp; = \\frac{\\theta^{(a - 1)} (1 - \\theta)^{(b - 1)}}{B(a, b)}, \\end{align*}\\] where \\(B(a, b)\\) is a normalizing constant, keeping the results in a probability metric, and \\(B(.)\\) is the Beta function. Kruschke then clarified that the beta distribution and the Beta function are not the same. In R, we use the beta density with the dbeta() function, whereas we use the Beta function with beta(). In this project, we’ll primarily use dbeta(). But to give a sense, notice that when given the same input for \\(a\\) and \\(b\\), the two functions return very different values. theta &lt;- .5 a &lt;- 3 b &lt;- 3 dbeta(theta, a, b) ## [1] 1.875 beta(a, b) ## [1] 0.03333333 The \\(a\\) and \\(b\\) parameters are also called shape parameters. And indeed, if we look at the parameters of the dbeta() function in R, we’ll see that \\(a\\) is called shape1 and \\(b\\) is called shape2. print(dbeta) ## function (x, shape1, shape2, ncp = 0, log = FALSE) ## { ## if (missing(ncp)) ## .Call(C_dbeta, x, shape1, shape2, log) ## else .Call(C_dnbeta, x, shape1, shape2, ncp, log) ## } ## &lt;bytecode: 0x7f9995c5d278&gt; ## &lt;environment: namespace:stats&gt; You can learn more about the dbeta() function here. Before we make Figure 6.1, we’ll need some data. library(tidyverse) length &lt;- 1e4 d &lt;- crossing(shape1 = c(.1, 1:4), shape2 = c(.1, 1:4)) %&gt;% expand(nesting(shape1, shape2), x = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(a = str_c(&quot;a = &quot;, shape1), b = str_c(&quot;b = &quot;, shape2), group = rep(1:length, each = 25)) head(d) ## # A tibble: 6 x 6 ## shape1 shape2 x a b group ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 0.1 0.1 0 a = 0.1 b = 0.1 1 ## 2 0.1 0.1 0.000100 a = 0.1 b = 0.1 1 ## 3 0.1 0.1 0.000200 a = 0.1 b = 0.1 1 ## 4 0.1 0.1 0.000300 a = 0.1 b = 0.1 1 ## 5 0.1 0.1 0.000400 a = 0.1 b = 0.1 1 ## 6 0.1 0.1 0.000500 a = 0.1 b = 0.1 1 Now we’re ready for our Figure 6.1. d %&gt;% ggplot(aes(x = x, group = group)) + geom_line(aes(y = dbeta(x, shape1 = shape1, shape2 = shape2)), color = &quot;grey50&quot;, size = 1.25) + scale_x_continuous(expression(theta), breaks = c(0, .5, 1)) + coord_cartesian(ylim = c(0, 3)) + ylab(expression(p(theta*&quot;|&quot;*a*&quot;, &quot;*b))) + theme(panel.grid = element_blank()) + facet_grid(b~a) Notice that as \\(a\\) gets bigger (left to right across columns of Figure 6.1), the bulk of the distribution moves rightward over higher values of \\(\\theta\\), but as \\(b\\) gets bigger (top to bottom across rows of Figure 6.1), the bulk of the distribution moves leftward over lower values of \\(\\theta\\). Notice that as \\(a\\) and \\(b\\) get bigger together, the beta distribution gets narrower. (p. 127). We have a lot of practice with the beta distribution waiting for us in the chapters to come. But if you like informal tutorials, you might also check out Karin Knudson’s nice blog post, Beta distributions, Dirichlet distributions and Dirichlet processes. 6.2.1 Specifying a beta prior. It is useful to know the central tendency and spread of the beta distribution expressed in terms of \\(a\\) and \\(b\\). It turns out that the mean of the \\(\\text{beta} (\\theta | a, b)\\) distribution is \\(\\mu = a / (a + b)\\) and the mode is \\(\\omega = (a − 1) / (a + b − 2)\\) for \\(a &gt; 1\\) and \\(b &gt; 1\\) (\\(\\mu\\) is Greek letter mu and \\(\\omega\\) is Greek letter omega)… The spread of the beta distribution is related to the “concentration” \\(\\kappa = a + b\\) (\\(\\kappa\\) is Greek letter kappa). You can see from Figure 6.1 that as \\(\\kappa = a + b\\) gets larger, the beta distribution gets narrower or more concentrated. (p. 129) As such, if you’d like to specify a beta distribution in terms of \\(\\omega\\) and \\(\\kappa\\), it’d follow the form \\[\\operatorname{beta} (\\alpha = \\omega (\\kappa - 2) + 1, \\beta = (1 - \\omega) \\cdot (\\kappa - 2) + 1),\\] as long as \\(\\kappa &gt; 2\\). Kruschke further clarified: The value we choose for the prior \\(\\kappa\\) can be thought of this way: It is the number of new flips of the coin that we would need to make us teeter between the new data and the prior belief about \\(\\mu\\). If we would only need a few new flips to sway our beliefs, then our prior beliefs should be represented by a small \\(\\kappa\\). If we would need a large number of new flips to sway us away from our prior beliefs about \\(\\mu\\), then our prior beliefs are worth a very large \\(\\kappa\\). (p. 129) He went on to clarify why we might prefer the mode to the mean when discussing the central tendency of a beta distribution. The mode can be more intuitive than the mean, especially for skewed distributions, because the mode is where the distribution reaches its tallest height, which is easy to visualize. The mean in a skewed distribution is somewhere away from the mode, in the direction of the longer tail. (pp. 129–130) Figure 6.2 helped contrast the mean and mode for beta. We’ll use the same process for Figure 6.2 and create the data, first. d &lt;- tibble(shape1 = c(5.6, 17.6, 5, 17), shape2 = c(1.4, 4.4, 2, 5)) %&gt;% mutate(a = str_c(&quot;a = &quot;, shape1), b = str_c(&quot;b = &quot;, shape2), kappa = rep(c(&quot;kappa = 7&quot;, &quot;kappa = 22&quot;), times = 2), mu_omega = rep(c(&quot;mu = 0.8&quot;, &quot;omega = 0.8&quot;), each = 2)) %&gt;% mutate(kappa = factor(kappa, levels = c(&quot;kappa = 7&quot;, &quot;kappa = 22&quot;))) %&gt;% expand(nesting(shape1, shape2, a, b, kappa, mu_omega), x = seq(from = 0, to = 1, length.out = length)) head(d) ## # A tibble: 6 x 7 ## shape1 shape2 a b kappa mu_omega x ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 5 2 a = 5 b = 2 kappa = 7 omega = 0.8 0 ## 2 5 2 a = 5 b = 2 kappa = 7 omega = 0.8 0.000100 ## 3 5 2 a = 5 b = 2 kappa = 7 omega = 0.8 0.000200 ## 4 5 2 a = 5 b = 2 kappa = 7 omega = 0.8 0.000300 ## 5 5 2 a = 5 b = 2 kappa = 7 omega = 0.8 0.000400 ## 6 5 2 a = 5 b = 2 kappa = 7 omega = 0.8 0.000500 Here’s Figure 6.2. d %&gt;% ggplot(aes(x = x)) + geom_vline(xintercept = .8, color = &quot;white&quot;) + geom_line(aes(y = dbeta(x, shape1 = shape1, shape2 = shape2)), color = &quot;grey50&quot;, size = 1.25) + scale_x_continuous(expression(theta), breaks = c(0, .8, 1)) + ylab(expression(p(theta*&quot;|&quot;*a*&quot;, &quot;*b))) + coord_cartesian(ylim = c(0, 5)) + theme(panel.grid = element_blank()) + facet_grid(mu_omega~kappa) In lines 264 to 290 in his DBDA2E-utilities.R file, Kruschke provided a series of betaABfrom...() functions that will allow us to compute the \\(a\\) and \\(b\\) parameters from measures of central tendency (i.e., mean and mode) and of spread (i.e., \\(\\kappa\\) and \\(\\sigma\\)). Here are those bits of his code. # Shape parameters from central tendency and scale: betaABfromMeanKappa &lt;- function(mean, kappa) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (kappa &lt;= 0) stop(&quot;kappa must be &gt; 0&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } betaABfromModeKappa &lt;- function(mode, kappa) { if (mode &lt;= 0 | mode &gt;= 1) stop(&quot;must have 0 &lt; mode &lt; 1&quot;) if (kappa &lt;= 2) stop(&quot;kappa must be &gt; 2 for mode parameterization&quot;) a &lt;- mode * (kappa - 2) + 1 b &lt;- (1.0 - mode) * (kappa - 2) + 1 return(list(a = a, b = b)) } betaABfromMeanSD &lt;- function(mean, sd) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) kappa &lt;- mean * (1 - mean)/sd^2 - 1 if (kappa &lt;= 0) stop(&quot;invalid combination of mean and sd&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } You can use them like so. betaABfromMeanKappa(mean = .25, kappa = 4) ## $a ## [1] 1 ## ## $b ## [1] 3 betaABfromModeKappa(mode = .25, kappa = 4) ## $a ## [1] 1.5 ## ## $b ## [1] 2.5 betaABfromMeanSD(mean = .5, sd = .1) ## $a ## [1] 12 ## ## $b ## [1] 12 You can also save the results as an object, which can then be indexed by parameter. beta_param &lt;- betaABfromModeKappa(mode = .25, kappa = 4) beta_param$a ## [1] 1.5 beta_param$b ## [1] 2.5 6.3 The posterior beta I’m not going to reproduce all of Formula 6.8. But this a fine opportunity to re-express Bayes’ rule in terms of \\(z\\) and \\(N\\), \\[p(\\theta | z, N) = \\frac{p(z, N | \\theta) p(\\theta)}{p(z, N)}.\\] 6.3.1 Posterior is compromise of prior and likelihood. You might wonder how Kruschke computed the HDI values for Figure 6.3. Remember our hdi_of_icdf() function from back in Chapter 4? Yep, that’s how. Here’s that code, again. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { # Arguments: # `name` is R&#39;s name for the inverse cumulative density function # of the distribution. # `width` is the desired mass of the HDI region. # `tol` is passed to R&#39;s optimize function. # Return value: # Highest density iterval (HDI) limits in a vector. # Example of use: For determining HDI of a beta(30, 12) distribution, type # `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)` # Notice that the parameters of the `name` must be explicitly stated; # e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work. # Adapted and corrected from Greg Snow&#39;s TeachingDemos package. incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Recall it’s based off of the HDIofICDF() function from Kruschke’s DBDA2E-utilities.R file. I’ve altered Kruschke’s formatting a little bit, but the guts of the code are unchanged. Our hdi_of_icdf() function will take the name of an “inverse cumulative density function” and its parameters and then return an HDI range, as defined by the width parameter. Since the prior at the top of Figure 6.3 is \\(\\operatorname{beta} (5, 5)\\), we can use hdi_of_icdf() to calculate the HDI like so. hdi_of_icdf(name = qbeta, shape1 = 5, shape2 = 5, width = .95) ## [1] 0.2120085 0.7879915 Here they are for the posterior distribution at the bottom of the figure. hdi_of_icdf(name = qbeta, shape1 = 6, shape2 = 14) ## [1] 0.1142339 0.4967144 Note that since we set width = .95 as the default, we can leave it out if we want to stick with the conventional 95% intervals. Here are the mean calculations from the last paragraph on page 134. n &lt;- 10 z &lt;- 1 a &lt;- 5 b &lt;- 5 (proportion_heads &lt;- z / n) ## [1] 0.1 (prior_mean &lt;- a / (a + b)) ## [1] 0.5 (posterior_mean &lt;- (z + a) / (n + a + b)) ## [1] 0.3 In order to make the plots for Figure 6.3, we’ll want to compute the prior, likelihood, and posterior density values across a densely-packed range of \\(\\theta\\) values. trial_data &lt;- c(rep(0, 9), 1) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 100)) %&gt;% mutate(`Prior (beta)` = dbeta(theta, shape1 = a, shape2 = b), `Likelihood (Bernoulli)` = bernoulli_likelihood(theta = theta, data = trial_data), `Posterior (beta)` = dbeta(theta, shape1 = 6, shape2 = 14)) glimpse(d) ## Rows: 100 ## Columns: 4 ## $ theta &lt;dbl&gt; 0.00000000, 0.01010101, 0.02020202, 0.030303… ## $ `Prior (beta)` &lt;dbl&gt; 0.000000e+00, 6.297429e-06, 9.670878e-05, 4.… ## $ `Likelihood (Bernoulli)` &lt;dbl&gt; 0.000000000, 0.009218977, 0.016812166, 0.022… ## $ `Posterior (beta)` &lt;dbl&gt; 0.000000e+00, 1.500163e-05, 4.201284e-04, 2.… To make things easier on ourselves, we’ll also make two additional data objects to annotate the plots with lines and text. # the data for the in-plot lines line &lt;- tibble(theta = c(.212 + .008, .788 - .008, .114 + .004, .497 - .005), value = rep(c(.51, .66), each = 2), xintercept = c(.212, .788, .114, .497), key = rep(c(&quot;Prior (beta)&quot;, &quot;Posterior (beta)&quot;), each = 2)) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior (beta)&quot;, &quot;Likelihood (Bernoulli)&quot;, &quot;Posterior (beta)&quot;))) # the data for the annotation text &lt;- tibble(theta = c(.5, .3), value = c(.8, 1.125), label = &quot;95% HDI&quot;, key = c(&quot;Prior (beta)&quot;, &quot;Posterior (beta)&quot;)) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior (beta)&quot;, &quot;Likelihood (Bernoulli)&quot;, &quot;Posterior (beta)&quot;))) Finally, here’s our Figure 6.3. d %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior (beta)&quot;, &quot;Likelihood (Bernoulli)&quot;, &quot;Posterior (beta)&quot;))) %&gt;% ggplot(aes(x = theta, y = value, )) + # densities geom_ribbon(aes(ymin = 0, ymax = value), fill = &quot;grey67&quot;) + # dashed vertical lines geom_vline(data = line, aes(xintercept = xintercept), linetype = 2, color = &quot;grey92&quot;) + # arrows geom_line(data = line, arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + # text geom_text(data = text, aes(label = label), color = &quot;grey92&quot;) + labs(x = expression(theta), y = NULL) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) + theme(panel.grid = element_blank()) 6.4 Examples 6.4.1 Prior knowledge expressed as a beta distribution. Here are the results Kruschke reported in the first paragraph of this subsection. beta_param &lt;- betaABfromModeKappa(mode = .5, kappa = 500) beta_param$a ## [1] 250 beta_param$b ## [1] 250 Confusingly, Kruschke switched from dbeta(250, 250) in the prose to dbeta(100, 100) in Figure 6.4.a, which he acknowledged in his Corrigenda. We’ll stick with dbeta(100, 100). beta_param &lt;- betaABfromModeKappa(mode = .5, kappa = 200) beta_param$a ## [1] 100 beta_param$b ## [1] 100 ggplot(data = tibble(x = seq(from = 0, to = 1, by = .001)), aes(x = x, ymin = 0, ymax = dbeta(x, shape1 = beta_param$a, shape2 = beta_param$b))) + geom_ribbon(fill = &quot;grey75&quot;) + geom_ribbon(data = tibble(x = seq(from = .5 - .069, to = .5 + .069, by = .001)), fill = &quot;grey67&quot;) + geom_line(data = tibble(x = c(.5 - .069 + .005, .5 + .069 - .005), y = 1.7), aes(y = y), arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(geom = &quot;text&quot;, x = .5, y = 3.5, label = &quot;95% HDI&quot;, color = &quot;grey0&quot;) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*100*&quot;, &quot;*100))) + coord_cartesian(ylim = c(0, 12)) + theme(panel.grid = element_blank()) Here are those HDI values. hdi_of_icdf(name = qbeta, shape1 = 100, shape2 = 100, width = .95) ## [1] 0.4309509 0.5690491 Here are our \\(a\\) and \\(b\\) parameters for Kruschke’s free throw example beta_param &lt;- betaABfromModeKappa(mode = .75, kappa = 25) beta_param$a ## [1] 18.25 beta_param$b ## [1] 6.75 Behold Figure 6.4.b. ggplot(data = tibble(x = seq(from = 0, to = 1, by = .001)), aes(x = x, ymin = 0, ymax = dbeta(x, shape1 = beta_param$a, shape2 = beta_param$b))) + geom_ribbon(fill = &quot;grey75&quot;) + geom_ribbon(data = tibble(x = seq(from = .558, to = .892, by = .001)), fill = &quot;grey67&quot;) + geom_line(data = tibble(x = c(.558 + .005, .892 - .005), y = .75), aes(y = y), arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(geom = &quot;text&quot;, x = .73, y = 1.5, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + ylim(0, 6) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*18.25*&quot;, &quot;*6.75))) + theme(panel.grid = element_blank()) Here are those HDI values. hdi_of_icdf(name = qbeta, shape1 = beta_param$a, shape2 = beta_param$b, width = .95) ## [1] 0.5581935 0.8915815 But we can be a little more organized and plot the top row of Figure 6.4 all at once. # the data for the in-plot lines line &lt;- tibble(theta = c(.431 + .005, .569 - .005, .558 + .005, .892 - .005), value = rep(c(1.9, .8), each = 2), xintercept = c(.431, .569, .558, .892), exemplar = rep(c(&quot;dbeta(theta, 100, 100)&quot;, &quot;dbeta(theta, 18.25, 6.75)&quot;), each = 2)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 100, 100)&quot;, &quot;dbeta(theta, 18.25, 6.75)&quot;, &quot;dbeta(theta, 1, 1)&quot;))) # the data for the annotation text &lt;- tibble(theta = c(.5, .735), value = c(3.6, 1.45), label = &quot;95% HDI&quot;, exemplar = c(&quot;dbeta(theta, 100, 100)&quot;, &quot;dbeta(theta, 18.25, 6.75)&quot;)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 100, 100)&quot;, &quot;dbeta(theta, 18.25, 6.75)&quot;, &quot;dbeta(theta, 1, 1)&quot;))) # our main data tibble tibble(theta = seq(from = 0, to = 1, length.out = 100) %&gt;% rep(., times = 3), a = rep(c(100, 18.25, 1), each = 100), b = rep(c(100, 6.75, 1), each = 100)) %&gt;% mutate(exemplar = ifelse(a == 18.25, str_c(&quot;dbeta(theta, &quot;, a, &quot;, &quot;, b, &quot;)&quot;), str_c(&quot;dbeta(theta, &quot;, a %&gt;% round(0), &quot;, &quot;, b %&gt;% round(0), &quot;)&quot;)), density = dbeta(theta, shape1 = a, shape2 = b)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 100, 100)&quot;, &quot;dbeta(theta, 18.25, 6.75)&quot;, &quot;dbeta(theta, 1, 1)&quot;))) %&gt;% # finally, the plot code! ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = density), size = 0, fill = &quot;grey67&quot;) + geom_vline(data = line, aes(xintercept = xintercept), linetype = 2, color = &quot;grey92&quot;) + geom_line(data = line, aes(y = value), arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + geom_text(data = text, aes(y = value, label = label), color = &quot;grey0&quot;) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) + theme(panel.grid = element_blank()) + facet_wrap(~exemplar, scales = &quot;free_y&quot;, ncol = 3) If you look closely, you’ll notice the middle row is the same for each column. So we’ll just plot it once. n &lt;- 20 z &lt;- 17 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) tibble(theta = seq(from = 0, to = 1, length.out = 100)) %&gt;% mutate(likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = likelihood)) + geom_ribbon(size = 0, fill = &quot;grey67&quot;) + labs(title = &quot;Likelihood (Bernoulli)&quot;, subtitle = &quot;This is the same for all 3 exemplars, so\\nthere&#39;s no need to plot this thrice.&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) + theme(panel.grid = element_blank()) The bottom row: # the data for the in-plot lines line &lt;- tibble(theta = c(.466 + .005, .597 - .005, .663 + .004, .897 - .005, .660 + .005, .959 - .003), value = rep(c(1.9, 1.1, .85), each = 2), xintercept = c(.466, .597, .663, .897, .660, .959), exemplar = rep(c(&quot;dbeta(theta, 117, 103)&quot;, &quot;dbeta(theta, 35.25, 9.75)&quot;, &quot;dbeta(theta, 18, 4)&quot;), each = 2)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 117, 103)&quot;, &quot;dbeta(theta, 35.25, 9.75)&quot;, &quot;dbeta(theta, 18, 4)&quot;))) # the data for the annotation text &lt;- tibble(theta = c(.532, .78, .82), value = c(3.5, 2, 1.6), label = &quot;95% HDI&quot;, exemplar = c(&quot;dbeta(theta, 117, 103)&quot;, &quot;dbeta(theta, 35.25, 9.75)&quot;, &quot;dbeta(theta, 18, 4)&quot;)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 117, 103)&quot;, &quot;dbeta(theta, 35.25, 9.75)&quot;, &quot;dbeta(theta, 18, 4)&quot;))) # our main data tibble tibble(theta = seq(from = 0, to = 1, length.out = 100) %&gt;% rep(., times = 3), a = rep(c(117, 35.25, 18), each = 100), b = rep(c(103, 9.75, 4), each = 100)) %&gt;% mutate(exemplar = ifelse(a == 35.25, str_c(&quot;dbeta(theta, &quot;, a, &quot;, &quot;, b, &quot;)&quot;), str_c(&quot;dbeta(theta, &quot;, a %&gt;% round(0), &quot;, &quot;, b %&gt;% round(0), &quot;)&quot;)), density = dbeta(theta, shape1 = a, shape2 = b)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 117, 103)&quot;, &quot;dbeta(theta, 35.25, 9.75)&quot;, &quot;dbeta(theta, 18, 4)&quot;))) %&gt;% # the plot ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = density), size = 0, fill = &quot;grey67&quot;) + geom_vline(data = line, aes(xintercept = xintercept), linetype = 2, color = &quot;grey92&quot;) + geom_line(data = line, aes(y = value), arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + geom_text(data = text, aes(y = value, label = label), color = &quot;grey0&quot;) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(p(theta*&quot;|&quot;*D))) + theme(panel.grid = element_blank()) + facet_wrap(~exemplar, scales = &quot;free_y&quot;, ncol = 3) And if you wanted those HDI values in bulk, you might wrap the hdi_of_icdf() into another function for use in purrr::map2(). # new function get_hdi &lt;- function(a, b) { hdi_of_icdf(name = qbeta, shape1 = a, shape2 = b, width = .95) %&gt;% as_tibble() %&gt;% mutate(key = c(&quot;lower level&quot;, &quot;upper level&quot;)) %&gt;% spread(key = key, value = value) } # put it to work tibble(`figure position` = c(&quot;left&quot;, &quot;middle&quot;, &quot;right&quot;), a = c(117, 35.25, 18), b = c(103, 9.75, 4)) %&gt;% mutate(hdi = purrr::map2(a, b, get_hdi)) %&gt;% unnest(hdi) ## # A tibble: 3 x 5 ## `figure position` a b `lower level` `upper level` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 left 117 103 0.466 0.597 ## 2 middle 35.2 9.75 0.663 0.897 ## 3 right 18 4 0.660 0.959 6.4.2 Prior knowledge that cannot be expressed as a beta distribution. The beauty of using a beta distribution to express prior knowledge is that the posterior distribution is again exactly a beta distribution, and therefore, no matter how much data we include, we always have an exact representation of the posterior distribution and a simple way of computing it. But not all prior knowledge can be expressed by a beta distribution, because the beta distribution can only be in the forms illustrated by Figure 6.1. If the prior knowledge cannot be expressed as a beta distribution, then we must use a different method to derive the posterior. In particular, we might revert to grid approximation as was explained in Section 5.5 (p. 116). For such a small section in the text, the underlying code is a bit of a beast. Fir kicks, we’ll practice two ways. First we’ll follow the code Kruschke used in the text. Our second attempt will be in a more tidyverse sort of way. 6.4.2.1 Figure 6.5 in Kruschke style. # Fine teeth for Theta theta &lt;- seq(0, 1, length = 1000) # Two triangular peaks on a small non-zero floor p_theta &lt;- c(rep(1, 200), seq(1, 100, length = 50), seq(100, 1, length = 50), rep(1, 200)) %&gt;% rep(., times = 2) # Make p_theta sum to 1.0 p_theta &lt;- p_theta / sum(p_theta) Here’s Kruschke’s BernGrid() code in all its glory. BernGrid = function( Theta , pTheta , Data , plotType=c(&quot;Points&quot;,&quot;Bars&quot;)[2] , showCentTend=c(&quot;Mean&quot;,&quot;Mode&quot;,&quot;None&quot;)[3] , showHDI=c(TRUE,FALSE)[2] , HDImass=0.95 , showpD=c(TRUE,FALSE)[2] , nToPlot=length(Theta) ) { # Theta is vector of values between 0 and 1. # pTheta is prior probability mass at each value of Theta # Data is vector of 0&#39;s and 1&#39;s. # Check for input errors: if ( any( Theta &gt; 1 | Theta &lt; 0 ) ) { stop(&quot;Theta values must be between 0 and 1&quot;) } if ( any( pTheta &lt; 0 ) ) { stop(&quot;pTheta values must be non-negative&quot;) } if ( !isTRUE(all.equal( sum(pTheta) , 1.0 )) ) { stop(&quot;pTheta values must sum to 1.0&quot;) } if ( !all( Data == 1 | Data == 0 ) ) { stop(&quot;Data values must be 0 or 1&quot;) } # Create summary values of Data z = sum( Data ) # number of 1&#39;s in Data N = length( Data ) # Compute the Bernoulli likelihood at each value of Theta: pDataGivenTheta = Theta^z * (1-Theta)^(N-z) # Compute the evidence and the posterior via Bayes&#39; rule: pData = sum( pDataGivenTheta * pTheta ) pThetaGivenData = pDataGivenTheta * pTheta / pData # Plot the results. layout( matrix( c( 1,2,3 ) ,nrow=3 ,ncol=1 ,byrow=FALSE ) ) # 3x1 panels par( mar=c(3,3,1,0) , mgp=c(2,0.7,0) , mai=c(0.5,0.5,0.3,0.1) ) # margins cexAxis = 1.33 cexLab = 1.75 # convert plotType to notation used by plot: if ( plotType==&quot;Points&quot; ) { plotType=&quot;p&quot; } if ( plotType==&quot;Bars&quot; ) { plotType=&quot;h&quot; } dotsize = 5 # how big to make the plotted dots barsize = 5 # how wide to make the bar lines # If the comb has a zillion teeth, it&#39;s too many to plot, so plot only a # thinned out subset of the teeth. nteeth = length(Theta) if ( nteeth &gt; nToPlot ) { thinIdx = round( seq( 1, nteeth , length=nteeth ) ) } else { thinIdx = 1:nteeth } # Plot the prior. yLim = c(0,1.1*max(c(pTheta,pThetaGivenData))) plot( Theta[thinIdx] , pTheta[thinIdx] , type=plotType , pch=&quot;.&quot; , cex=dotsize , lwd=barsize , xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis , xlab=bquote(theta) , ylab=bquote(p(theta)) , cex.lab=cexLab , main=&quot;Prior&quot; , cex.main=1.5 , col=&quot;skyblue&quot; ) if ( showCentTend != &quot;None&quot; ) { if ( showCentTend == &quot;Mean&quot; ) { meanTheta = sum( Theta * pTheta ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mean=&quot; * .(signif(meanTheta,3)) ) , cex=2.0 , adj=textadj ) } if ( showCentTend == &quot;Mode&quot; ) { modeTheta = Theta[ which.max( pTheta ) ] if ( modeTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mode=&quot; * .(signif(modeTheta,3)) ) , cex=2.0 , adj=textadj ) } } # Mark the highest density interval. HDI points are not thinned in the plot. if ( showHDI ) { HDIinfo = HDIofGrid( pTheta , credMass=HDImass ) points( Theta[ HDIinfo$indices ] , rep( HDIinfo$height , length( HDIinfo$indices ) ) , pch=&quot;-&quot; , cex=1.0 ) text( mean( Theta[ HDIinfo$indices ] ) , HDIinfo$height , bquote( .(100*signif(HDIinfo$mass,3)) * &quot;% HDI&quot; ) , adj=c(0.5,-1.5) , cex=1.5 ) # Mark the left and right ends of the waterline. # Find indices at ends of sub-intervals: inLim = HDIinfo$indices[1] # first point for ( idx in 2:(length(HDIinfo$indices)-1) ) { if ( ( HDIinfo$indices[idx] != HDIinfo$indices[idx-1]+1 ) | # jumps on left, OR ( HDIinfo$indices[idx] != HDIinfo$indices[idx+1]-1 ) ) { # jumps on right inLim = c(inLim,HDIinfo$indices[idx]) # include idx } } inLim = c(inLim,HDIinfo$indices[length(HDIinfo$indices)]) # last point # Mark vertical lines at ends of sub-intervals: for ( idx in inLim ) { lines( c(Theta[idx],Theta[idx]) , c(-0.5,HDIinfo$height) , type=&quot;l&quot; , lty=2 , lwd=1.5 ) text( Theta[idx] , HDIinfo$height , bquote(.(round(Theta[idx],3))) , adj=c(0.5,-0.1) , cex=1.2 ) } } # Plot the likelihood: p(Data|Theta) plot( Theta[thinIdx] , pDataGivenTheta[thinIdx] , type=plotType , pch=&quot;.&quot; , cex=dotsize , lwd=barsize , xlim=c(0,1) , ylim=c(0,1.1*max(pDataGivenTheta)) , cex.axis=cexAxis , xlab=bquote(theta) , ylab=bquote( &quot;p(D|&quot; * theta * &quot;)&quot; ) , cex.lab=cexLab , main=&quot;Likelihood&quot; , cex.main=1.5 , col=&quot;skyblue&quot; ) if ( z &gt; .5*N ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx ,1.0*max(pDataGivenTheta) ,cex=2.0 ,bquote( &quot;Data: z=&quot; * .(z) * &quot;,N=&quot; * .(N) ) ,adj=textadj ) if ( showCentTend != &quot;None&quot; ) { if ( showCentTend == &quot;Mean&quot; ) { meanTheta = sum( Theta * pDataGivenTheta ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , 0.7*max(pDataGivenTheta) , bquote( &quot;mean=&quot; * .(signif(meanTheta,3)) ) , cex=2.0 , adj=textadj ) } if ( showCentTend == &quot;Mode&quot; ) { modeTheta = Theta[ which.max( pDataGivenTheta ) ] if ( modeTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , 0.7*max(pDataGivenTheta) , bquote( &quot;mode=&quot; * .(signif(modeTheta,3)) ) , cex=2.0 , adj=textadj ) } } # Plot the posterior. yLim = c(0,1.1*max(c(pTheta,pThetaGivenData))) plot( Theta[thinIdx] , pThetaGivenData[thinIdx] , type=plotType , pch=&quot;.&quot; , cex=dotsize , lwd=barsize , xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis , xlab=bquote(theta) , ylab=bquote( &quot;p(&quot; * theta * &quot;|D)&quot; ) , cex.lab=cexLab , main=&quot;Posterior&quot; , cex.main=1.5 , col=&quot;skyblue&quot; ) if ( showCentTend != &quot;None&quot; ) { if ( showCentTend == &quot;Mean&quot; ) { meanTheta = sum( Theta * pThetaGivenData ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mean=&quot; * .(signif(meanTheta,3)) ) , cex=2.0 , adj=textadj ) } if ( showCentTend == &quot;Mode&quot; ) { modeTheta = Theta[ which.max( pThetaGivenData ) ] if ( modeTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mode=&quot; * .(signif(modeTheta,3)) ) , cex=2.0 , adj=textadj ) } } # Plot marginal likelihood pData: if ( showpD ) { meanTheta = sum( Theta * pThetaGivenData ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , 0.75*max(pThetaGivenData) , cex=2.0 , bquote( &quot;p(D)=&quot; * .(signif(pData,3)) ) ,adj=textadj ) } # Mark the highest density interval. HDI points are not thinned in the plot. if ( showHDI ) { HDIinfo = HDIofGrid( pThetaGivenData , credMass=HDImass ) points( Theta[ HDIinfo$indices ] , rep( HDIinfo$height , length( HDIinfo$indices ) ) , pch=&quot;-&quot; , cex=1.0 ) text( mean( Theta[ HDIinfo$indices ] ) , HDIinfo$height , bquote( .(100*signif(HDIinfo$mass,3)) * &quot;% HDI&quot; ) , adj=c(0.5,-1.5) , cex=1.5 ) # Mark the left and right ends of the waterline. # Find indices at ends of sub-intervals: inLim = HDIinfo$indices[1] # first point for ( idx in 2:(length(HDIinfo$indices)-1) ) { if ( ( HDIinfo$indices[idx] != HDIinfo$indices[idx-1]+1 ) | # jumps on left, OR ( HDIinfo$indices[idx] != HDIinfo$indices[idx+1]-1 ) ) { # jumps on right inLim = c(inLim,HDIinfo$indices[idx]) # include idx } } inLim = c(inLim,HDIinfo$indices[length(HDIinfo$indices)]) # last point # Mark vertical lines at ends of sub-intervals: for ( idx in inLim ) { lines( c(Theta[idx],Theta[idx]) , c(-0.5,HDIinfo$height) , type=&quot;l&quot; , lty=2 , lwd=1.5 ) text( Theta[idx] , HDIinfo$height , bquote(.(round(Theta[idx],3))) , adj=c(0.5,-0.1) , cex=1.2 ) } } # return( pThetaGivenData ) } # end of function You plot using Kruschke’s method, like so. Data &lt;- c(rep(0, 13), rep(1, 14)) BernGrid(theta, p_theta, Data, plotType = &quot;Bars&quot;, showCentTend = &quot;None&quot;, showHDI = FALSE, showpD = FALSE) The method works fine. But, I’m not a fan. It’s clear Kruschke put a lot of thought into the BernGrid() function. However, its inner workings are too opaque, for me, which leads to our next section… 6.4.2.2 Figure 6.5 in tidyverse style. Here we’ll be plotting with ggplot2. But let’s first get the data into a tibble. # we need these to compute the likelihood n &lt;- 27 z &lt;- 14 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) # (i.e., Data) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000), # (i.e., Theta) Prior = c(rep(1, 200), # (i.e., pTheta) seq(1, 100, length = 50), seq(100, 1, length = 50), rep(1, 200)) %&gt;% rep(., times = 2)) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, # (i.e., pDataGivenTheta) data = trial_data)) %&gt;% mutate(evidence = sum(Likelihood * Prior)) %&gt;% # (i.e., pData) mutate(Posterior = Likelihood * Prior / evidence) # (i.e., pThetaGivenData) glimpse(d) ## Rows: 1,000 ## Columns: 5 ## $ theta &lt;dbl&gt; 0.000000000, 0.001001001, 0.002002002, 0.003003003, 0.0040… ## $ Prior &lt;dbl&gt; 9.174312e-05, 9.174312e-05, 9.174312e-05, 9.174312e-05, 9.… ## $ Likelihood &lt;dbl&gt; 0.000000e+00, 1.000988e-42, 1.618784e-38, 4.664454e-36, 2.… ## $ evidence &lt;dbl&gt; 3.546798e-10, 3.546798e-10, 3.546798e-10, 3.546798e-10, 3.… ## $ Posterior &lt;dbl&gt; 0.000000e+00, 2.589202e-37, 4.187221e-33, 1.206529e-30, 6.… With our nice tibble in hand, we’ll plot the prior, likelihood, and posterior one at a time. # prior (p1 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Prior), fill = &quot;grey50&quot;) + labs(title = &quot;Prior&quot;, x = expression(theta), y = expression(p(theta))) + theme(panel.grid = element_blank()) ) # likelihood (p2 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Likelihood), fill = &quot;grey50&quot;) + labs(title = &quot;Likelihood&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) + theme(panel.grid = element_blank()) ) # posterior (p3 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Posterior), fill = &quot;grey50&quot;) + labs(title = &quot;Posterior&quot;, x = expression(theta), y = expression(p(theta*&quot;|&quot;*D))) + theme(panel.grid = element_blank()) ) Note how we saved each the plots as objects. There are many ways to combine multiple ggplots, such as stacking them one atop another like they’re presented in Figure 6.5. One of the earliest methods I learned was the good old multiplot() function. For a long time I relied on grid.arrange() from the gridExtra package (Auguie, 2017). But it’s hard to beat the elegant syntax from Thomas Lin Pedersen’s (2019) patchwork package. library(patchwork) p1 / p2 / p3 You can learn more about how to use patchwork here. Plus we’ll have many more opportunities to practice as we progress through the chapters. 6.5 Summary The main point of this chapter was to demonstrate how Bayesian inference works when Bayes’ rule can be solved analytically, using mathematics alone, without numerical approximation… Unfortunately, there are two severe limitations with this approach… Thus, although it is interesting and educational to see how Bayes’ rule can be solved analytically, we will have to abandon exact mathematical solutions when doing complex applications. We will instead use Markov chain Monte Carlo (MCMC) methods. (p. 139) And if you’re looking at this project, I imagine that’s exactly what you’re looking for. We want to use the power of a particular kind of MCMC, Hamiltonian Monte Carlo, through the interface of the brms package. Get excited. It’s coming. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.0.0 forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 ## [5] purrr_0.3.4 readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ## [9] ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.0.0 xfun_0.13 haven_2.2.0 lattice_0.20-38 ## [5] colorspace_1.4-1 vctrs_0.3.0 generics_0.0.2 htmltools_0.4.0 ## [9] yaml_2.2.1 utf8_1.1.4 rlang_0.4.6 pillar_1.4.4 ## [13] glue_1.4.0 withr_2.2.0 DBI_1.1.0 dbplyr_1.4.2 ## [17] modelr_0.1.6 readxl_1.3.1 lifecycle_0.2.0 munsell_0.5.0 ## [21] gtable_0.3.0 cellranger_1.1.0 rvest_0.3.5 evaluate_0.14 ## [25] labeling_0.3 knitr_1.28 fansi_0.4.1 broom_0.5.5 ## [29] Rcpp_1.0.4.6 scales_1.1.1 backports_1.1.6 jsonlite_1.6.1 ## [33] farver_2.0.3 fs_1.4.1 hms_0.5.3 digest_0.6.25 ## [37] stringi_1.4.6 bookdown_0.18 grid_3.6.3 cli_2.0.2 ## [41] tools_3.6.3 magrittr_1.5 crayon_1.3.4 pkgconfig_2.0.3 ## [45] ellipsis_0.3.0 xml2_1.3.1 reprex_0.3.0 lubridate_1.7.8 ## [49] assertthat_0.2.1 rmarkdown_2.1 httr_1.4.1 rstudioapi_0.11 ## [53] R6_2.4.1 nlme_3.1-144 compiler_3.6.3 References "],
["markov-chain-monte-carlo.html", "7 Markov Chain Monte Carlo 7.1 Approximating a distribution with a large sample 7.2 A simple case of the Metropolis algorithm 7.3 The Metropolis algorithm more generally 7.4 Toward Gibbs sampling: Estimating two coin biases 7.5 MCMC representativeness, accuracy, and efficiency Session info", " 7 Markov Chain Monte Carlo This chapter introduces the methods we will use for producing accurate approximations to Bayesian posterior distributions for realistic applications. The class of methods is called Markov chain Monte Carlo (MCMC), for reasons that will be explained later in the chapter. It is MCMC algorithms and software, along with fast computer hardware, that allow us to do Bayesian data analysis for realistic applications that would have been effectively impossible 30 years ago. (Kruschke, 2015, p. 144) Statistician David Draper covered some of the history of MCMC in his lecture, Bayesian Statistical Reasoning. 7.1 Approximating a distribution with a large sample The concept of representing a distribution by a large representative sample is foundational for the approach we take to Bayesian analysis of complex models. The idea is applied intuitively and routinely in everyday life and in science. For example, polls and surveys are founded on this concept: By randomly sampling a subset of people from a population, we estimate the underlying tendencies in the entire population. The larger the sample, the better the estimation. What is new in the present application is that the population from which we are sampling is a mathematically defined distribution, such as a posterior probability distribution. (p. 145) Like in Chapters 4 and 6, we need to define our hdi_of_icdf() function. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Our hdi_of_icdf() function will compute the analytic 95% HDIs for the distribution under consideration in Figure 7.1, \\(\\operatorname{beta} (\\theta | 15, 7)\\). ( h &lt;- hdi_of_icdf(name = qbeta, shape1 = 15, shape2 = 7) ) ## [1] 0.4907001 0.8639305 Using an equation from Chapter 6, \\(\\omega = (a − 1) / (a + b − 2)\\), we can compute the corresponding mode. (omega &lt;- (15 - 1) / (15 + 7 - 2)) ## [1] 0.7 To get the density in the upper left panel of Figure 7.1, we’ll make use of dbeta(). And we’ll also make use of our h[1:2] and omega values. library(tidyverse) tibble(theta = seq(from = 0, to = 1, length.out = 100)) %&gt;% ggplot() + geom_ribbon(aes(x = theta, ymin = 0, ymax = dbeta(theta, shape1 = 15, shape2 = 7)), fill = &quot;grey67&quot;) + geom_segment(aes(x = h[1], xend = h[2], y = 0, yend = 0), size = .75) + geom_point(aes(x = omega, y = 0), size = 1.5, shape = 19) + annotate(geom = &quot;text&quot;, x = .675, y = .4, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + scale_x_continuous(breaks = c(0, h, omega, 1), labels = c(&quot;0&quot;, h %&gt;% round(2), omega, &quot;1&quot;)) + labs(title = &quot;Exact distribution&quot;, x = expression(theta), y = expression(p(theta))) + coord_cartesian(ylim = c(-.125, 4)) + theme(panel.grid = element_blank()) The remaining panels in Figure 7.1 require we simulate the data. set.seed(7) d &lt;- tibble(n = c(500, 5000, 50000)) %&gt;% mutate(theta = map(n, ~rbeta(., shape1 = 15, shape2 = 7))) %&gt;% unnest(theta) %&gt;% mutate(key = str_c(&quot;Sample N = &quot;, n)) head(d) ## # A tibble: 6 x 3 ## n theta key ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 500 0.806 Sample N = 500 ## 2 500 0.756 Sample N = 500 ## 3 500 0.727 Sample N = 500 ## 4 500 0.784 Sample N = 500 ## 5 500 0.782 Sample N = 500 ## 6 500 0.590 Sample N = 500 With the data in hand, we’re ready to plot the remaining panels for Figure 7.1. This time, we’ll use the handy stat_pointintervalh() function from the tidybayes package to mark off the mode and 95% HDIs. library(tidybayes) d %&gt;% ggplot(aes(x = theta, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_x_continuous(expression(theta), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, ncol = 3, scales = &quot;free&quot;) If we want the exact values for the mode and 95% HDIs, we can use the tidybayes::mode_hdi() function. d %&gt;% group_by(key) %&gt;% mode_hdi(theta) ## # A tibble: 3 x 7 ## key theta .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Sample N = 500 0.695 0.511 0.868 0.95 mode hdi ## 2 Sample N = 5000 0.688 0.497 0.870 0.95 mode hdi ## 3 Sample N = 50000 0.711 0.490 0.863 0.95 mode hdi If you wanted a better sense of the phenomena, you could do a simulation. We’ll make a custom simulation function to compute the modes from many random draws from our \\(\\operatorname{beta} (\\theta | 15, 7)\\) distribution, with varying \\(N\\) values. my_mode_simulation &lt;- function(seed) { set.seed(seed) tibble(n = c(500, 5000, 50000)) %&gt;% mutate(theta = map(n, ~rbeta(., shape1 = 15, shape2 = 7))) %&gt;% unnest(theta) %&gt;% mutate(key = str_c(&quot;Sample N = &quot;, n)) %&gt;% group_by(key) %&gt;% mode_hdi(theta) } Here we put our my_mode_simulation() function to work. # we need an index of the values we set our seed with in our `my_mode_simulation()` function sim &lt;- tibble(seed = 1:1e3) %&gt;% group_by(seed) %&gt;% # inserting our subsamples mutate(modes = map(seed, my_mode_simulation)) %&gt;% # unnesting allows us to access our model results unnest(modes) sim %&gt;% ggplot(aes(x = theta, y = key)) + geom_vline(xintercept = .7, color = &quot;white&quot;) + geom_halfeyeh(.width = c(.95, .5)) + labs(title = expression(paste(&quot;Variability of the mode for simulations of &quot;, beta, &quot;(&quot;, theta, &quot;|15, 7), the true mode of which is .7&quot;)), subtitle = &quot;For each sample size, the dot is the median, the inner thick line is the percentile-based 50% interval,\\nand the outer thin line the percentile-based 95% interval. Although the central tendency\\napproximates the true value for all three conditions, the variability of the mode estimate is inversely\\nrelated to the sample size.&quot;, x = &quot;mode&quot;, y = NULL) + coord_cartesian(xlim = c(.6, .8), ylim = c(1.25, 3.5)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) 7.2 A simple case of the Metropolis algorithm Our goal in Bayesian inference is to get an accurate representation of the posterior distribution. One way to do that is to sample a large number of representative points from the posterior. The question then becomes this: How can we sample a large number of representative values from a distribution? (p. 146). The answer, my friends, is MCMC. 7.2.1 A politician stumbles upon the Metropolis algorithm. If we denote \\(P_\\text{proposed}\\) as the population of the proposed island and \\(P_\\text{current}\\) as the population of the current island, then \\[p_\\text{move} = \\frac{P_\\text{proposed}}{P_\\text{current}}.\\] “What’s amazing about this heuristic is that it works: In the long run, the probability that the politician is on any one of the islands exactly matches the relative population of the island” (p. 147)! 7.2.2 A random walk. The code below will allow us to reproduce Kruschke’s random walk. To give credit where it’s due, this is a mild amendment to the code from Chapter 8 of McElreath’s (2015) text, Statistical rethinking: A Bayesian course with examples in R and Stan. set.seed(7) num_days &lt;- 5e4 positions &lt;- rep(0, num_days) current &lt;- 4 for (i in 1:num_days) { # record current position positions[i] &lt;- current # flip coin to generate proposal proposal &lt;- current + sample(c(-1, 1), size = 1) # now make sure he loops around from 7 back to 1 if (proposal &lt; 1) proposal &lt;- 7 if (proposal &gt; 7) proposal &lt;- 1 # move? prob_accept_the_proposal &lt;- proposal/current current &lt;- ifelse(runif(1) &lt; prob_accept_the_proposal, proposal, current) } If you missed it, positions is the main product of our simulation. Here we’ll put positions in a tibble and reproduce the top portion of Figure 7.2. tibble(theta = positions) %&gt;% ggplot(aes(x = theta)) + geom_bar() + scale_x_continuous(expression(theta), breaks = 1:7) + theme(panel.grid = element_blank()) Here’s the middle portion of Figure 7.2. tibble(t = 1:5e4, theta = positions) %&gt;% slice(1:500) %&gt;% ggplot(aes(x = theta, y = t)) + geom_path(size = 1/4, color = &quot;grey50&quot;) + geom_point(size = 1/2, alpha = 1/2) + scale_x_continuous(expression(theta), breaks = 1:7) + scale_y_log10(&quot;Time Step&quot;, breaks = c(1, 2, 5, 20, 100, 500)) + theme(panel.grid = element_blank()) And now we make the bottom. tibble(x = 1:7, y = 1:7) %&gt;% ggplot(aes(x = x, y = y)) + geom_col(width = .2) + scale_x_continuous(expression(theta), breaks = 1:7) + ylab(expression(p(theta))) + theme(panel.grid = element_blank()) 7.2.3 General properties of a random walk. The tajectory shown in Figure 7.2 is just one possible sequence of positions when the movement heuristic is applied. At each time step, the direction of the proposed move is random, and if the relative probability of the proposed position is less than that of the current position, then acceptance of the proposed move is also random. Because of the randomness, if the process were started over again, then the specific trajectory would almost certainly be different. Regardless of the specific trajectory, in the long run the relative frequency of visits mimics the target distribution. Figure 7.3 shows the probability of being in each position as a function of time. (p. 149) I was initially stumped on how to reproduce the simulation depicted in Figure 7.3. However, Cardy Moten III kindly shared a solution which was itself based on Kruschke’s blog post from 2012, Metropolis Algorithm: Discrete position probabilities. Here’s a mild reworking of their solutions. First, we simulate. nslots &lt;- 7 p_target &lt;- 1:7 p_target &lt;- p_target / sum(p_target) # construct the transition matrix proposal_matrix &lt;- matrix(0, nrow = nslots, ncol = nslots) for(from_idx in 1:nslots) { for(to_idx in 1:nslots) { if(to_idx == from_idx - 1) {proposal_matrix[from_idx, to_idx] &lt;- 0.5} if(to_idx == from_idx + 1) {proposal_matrix[from_idx, to_idx] &lt;- 0.5} } } # construct the acceptance matrix acceptance_matrix &lt;- matrix(0, nrow = nslots, ncol = nslots) for(from_idx in 1:nslots) { for(to_idx in 1:nslots) { acceptance_matrix[from_idx, to_idx] &lt;- min(p_target[to_idx] / p_target[from_idx], 1) } } # compute the matrix of move probabilities move_matrix &lt;- proposal_matrix * acceptance_matrix # compute the transition matrix, including the probability of staying in place transition_matrix &lt;- move_matrix for (diag_idx in 1:nslots) { transition_matrix[diag_idx, diag_idx] = 1.0 - sum(move_matrix[diag_idx, ]) } # specify starting position vector: position_vec &lt;- rep(0, nslots) position_vec[round(nslots / 2)] &lt;- 1.0 p &lt;- list() data &lt;- tibble(position = 1:nslots, prob = position_vec) # loop through the requisite time indexes # update the data and transition vector for(time_idx in 1:15) { p[[time_idx]] &lt;- data # update the position vec position_vec &lt;- position_vec %*% transition_matrix # update the data data &lt;- NULL data &lt;- tibble(position = 1:nslots, prob = t(position_vec)) } Now we wrangle and plot. p %&gt;% as_tibble_col() %&gt;% mutate(facet = str_c(&quot;italic(t)==&quot;, 1:15)) %&gt;% unnest(value) %&gt;% bind_rows( tibble(position = 1:nslots, prob = p_target, facet = &quot;target&quot;) ) %&gt;% mutate(facet = factor(facet, levels = c(str_c(&quot;italic(t)==&quot;, 1:15), &quot;target&quot;))) %&gt;% # plot! ggplot(aes(x = position, y = prob, fill = facet == &quot;target&quot;)) + geom_col(width = .2) + scale_fill_manual(values = c(&quot;grey50&quot;, &quot;goldenrod2&quot;)) + scale_x_continuous(expression(theta), breaks = 1:7) + ylab(expression(italic(p)(theta))) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~facet, scales = &quot;free_y&quot;, labeller = label_parsed) 7.2.4 Why we care. Through the simple magic of the random walk procedure, we are able to do indirectly something we could not necessarily do directly: We can generate random samples from the target distribution. Moreover, we can generate those random samples from the target distribution even when the target distribution is not normalized. This technique is profoundly useful when the target distribution \\(P(\\theta)\\) is a posterior proportional to \\(p(D | \\theta) p(\\theta)\\). Merely by evaluating \\(p(D | \\theta) p(\\theta)\\), without normalizing it by \\(p(D)\\), we can generate random representative values from the posterior distribution. This result is wonderful because the method obviates direct computation of the evidence \\(p(D)\\), which, as you’ll recall, is one of the most difficult aspects of Bayesian inference. By using MCMC techniques, we can do Bayesian inference in rich and complex models. It has only been with the development of MCMC algorithms and software that Bayesian inference is applicable to complex data analysis, and it has only been with the production of fast and cheap computer hardware that Bayesian inference is accessible to a wide audience. (p. 152, emphasis in the original) 7.3 The Metropolis algorithm more generally “The procedure described in the previous section was just a special case of a more general procedure known as the Metropolis algorithm, named after the first author of a famous article (Metropolis et al., 1953)” (p. 156). Here’s how to generate a proposed jump from a zero-mean normal distribuiton with a standard deviation of 0.2. rnorm(1, mean = 0, sd = 0.2) ## [1] -0.1985524 To get a sense of what draws from rnorm() looks like in the long run, we might plot. mu &lt;- 0 sigma &lt;- 0.2 # how many proposals would you like? n &lt;- 500 set.seed(7) tibble(proposed_jump = rnorm(n, mean = mu, sd = sigma)) %&gt;% ggplot(aes(x = proposed_jump, y = 0)) + geom_jitter(width = 0, height = .1, size = 1/2, alpha = 1/2) + # this is the idealized distribution stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), color = &quot;grey50&quot;) + scale_x_continuous(breaks = seq(from = -0.6, to = 0.6, length.out = 7)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Jump proposals&quot;, subtitle = &quot;The gray line shows the data generating distribution.&quot;) + theme(panel.grid = element_blank()) Anyway, having generated a proposed new position, the algorithm then decides whether or not to accept the proposal. The decision rule is exactly what was already specified in Equation 7.1. In detail, this is accomplished by computing the ratio \\(p_\\text{move} = P(\\theta_\\text{proposed}) / P(\\theta_\\text{current})\\). Then a random number from the uniform interval \\([0, 1]\\) is generated; in R, this can be accomplished with the command runif(1). If the random number is between 0 and pmove, then the move is accepted. (p. 157) We’ll see what that might look like in the next section. In the meantime, here’s how to use runif(). runif(1) ## [1] 0.2783186 Just for kicks, here’s what that looks like in bulk. # how many proposals would you like? n &lt;- 500 set.seed(7) tibble(draw = runif(n)) %&gt;% ggplot(aes(x = draw, y = 0)) + geom_jitter(width = 0, height = 1/4, size = 1/2, alpha = 1/2) + stat_function(fun = dunif, color = &quot;grey50&quot;) + scale_y_continuous(NULL, breaks = NULL, limits = c(-1/3, 5/3)) + labs(title = &quot;Uniform draws&quot;, subtitle = &quot;The gray line shows the data generating distribution.&quot;) + theme(panel.grid = element_blank()) We do not see a concentration towards the mean, this time. The draws are uniformly distributed across the parameter space. 7.3.1 Metropolis algorithm applied to Bernoulli likelihood and beta prior. You can find Kruschke’s code in the BernMetrop.R file. I’m going to break it up a little. # specify the data, to be used in the likelihood function. my_data &lt;- c(rep(0, 6), rep(1, 14)) # define the Bernoulli likelihood function, p(D|theta). # the argument theta could be a vector, not just a scalar likelihood &lt;- function(theta, data) { z &lt;- sum(data) n &lt;- length(data) p_data_given_theta &lt;- theta^z * (1 - theta)^(n - z) # the theta values passed into this function are generated at random, # and therefore might be inadvertently greater than 1 or less than 0. # the likelihood for theta &gt; 1 or for theta &lt; 0 is zero p_data_given_theta[theta &gt; 1 | theta &lt; 0] &lt;- 0 return(p_data_given_theta) } # define the prior density function. prior_d &lt;- function(theta) { p_theta &lt;- dbeta(theta, 1, 1) # the theta values passed into this function are generated at random, # and therefore might be inadvertently greater than 1 or less than 0. # the prior for theta &gt; 1 or for theta &lt; 0 is zero p_theta[theta &gt; 1 | theta &lt; 0] = 0 return(p_theta) } # define the relative probability of the target distribution, # as a function of vector theta. for our application, this # target distribution is the unnormalized posterior distribution target_rel_prob &lt;- function(theta, data) { target_rel_prob &lt;- likelihood(theta, data) * prior_d(theta) return(target_rel_prob) } # specify the length of the trajectory, i.e., the number of jumps to try: traj_length &lt;- 50000 # this is just an arbitrary large number # initialize the vector that will store the results trajectory &lt;- rep(0, traj_length) # specify where to start the trajectory: trajectory[1] &lt;- 0.01 # another arbitrary value # specify the burn-in period burn_in &lt;- ceiling(0.0 * traj_length) # arbitrary number, less than `traj_length` # initialize accepted, rejected counters, just to monitor performance: n_accepted &lt;- 0 n_rejected &lt;- 0 That first part follows what Kruschke put in his script. I’m going to bundel the next large potion in a fucntion, my_metropolis() which will make it easier to plug the code into the purrr::map() function. my_metropolis &lt;- function(proposal_sd) { # now generate the random walk. the &#39;t&#39; index is time or trial in the walk. # specify seed to reproduce same random walk set.seed(47405) ## I&#39;m taking this section out and will replace it # # specify standard deviation of proposal distribution # proposal_sd &lt;- c(0.02, 0.2, 2.0)[2] ## end of the section I took out for (t in 1:(traj_length - 1)) { current_position &lt;- trajectory[t] # use the proposal distribution to generate a proposed jump proposed_jump &lt;- rnorm(1, mean = 0, sd = proposal_sd) # compute the probability of accepting the proposed jump prob_accept &lt;- min(1, target_rel_prob(current_position + proposed_jump, my_data) / target_rel_prob(current_position, my_data)) # generate a random uniform value from the interval [0, 1] to # decide whether or not to accept the proposed jump if (runif(1) &lt; prob_accept) { # accept the proposed jump trajectory[t + 1] &lt;- current_position + proposed_jump # increment the accepted counter, just to monitor performance if (t &gt; burn_in) {n_accepted &lt;- n_accepted + 1} } else { # reject the proposed jump, stay at current position trajectory[t + 1] &lt;- current_position # increment the rejected counter, just to monitor performance if (t &gt; burn_in) {n_rejected &lt;- n_rejected + 1} } } # extract the post-burn_in portion of the trajectory accepted_traj &lt;- trajectory[(burn_in + 1) : length(trajectory)] tibble(accepted_traj = accepted_traj, n_accepted = n_accepted, n_rejected = n_rejected) # end of Metropolis algorithm } Now we have my_metropolis(), we can run the analysis based on the three proposal_sd values, nesting the results in a tibble. d &lt;- tibble(proposal_sd = c(0.02, 0.2, 2.0)) %&gt;% mutate(accepted_traj = map(proposal_sd, my_metropolis)) %&gt;% unnest(accepted_traj) glimpse(d) ## Rows: 150,000 ## Columns: 4 ## $ proposal_sd &lt;dbl&gt; 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.0… ## $ accepted_traj &lt;dbl&gt; 0.01000000, 0.01000000, 0.01000000, 0.01000000, 0.01149173, 0.02550380, 0.0… ## $ n_accepted &lt;dbl&gt; 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801… ## $ n_rejected &lt;dbl&gt; 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 319… Now we have d in hand, here’s the top portion of Figure 7.4. d &lt;- d %&gt;% mutate(proposal_sd = str_c(&quot;Proposal SD = &quot;, proposal_sd), iter = rep(1:50000, times = 3)) d %&gt;% ggplot(aes(x = accepted_traj, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_x_continuous(expression(theta), breaks = 0:5 * 0.2) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ proposal_sd, ncol = 3) The modes are the points and the lines depict the 95% HDIs. Here’s the middle of Figure 7.4. d %&gt;% ggplot(aes(x = accepted_traj, y = iter)) + geom_path(size = 1/4, color = &quot;grey50&quot;) + geom_point(size = 1/2, alpha = 1/2) + scale_x_continuous(expression(theta), breaks = 0:5 * 0.2, limits = c(0, 1)) + scale_y_continuous(&quot;Step in Chain&quot;, limits = c(49900, 50000)) + ggtitle(&quot;End of Chain&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~proposal_sd, ncol = 3) The bottom: d %&gt;% ggplot(aes(x = accepted_traj, y = iter)) + geom_path(size = 1/4, color = &quot;grey50&quot;) + geom_point(size = 1/2, alpha = 1/2) + scale_x_continuous(expression(theta), breaks = 0:5 * 0.2, limits = c(0, 1)) + scale_y_continuous(&quot;Step in Chain&quot;, limits = c(1, 100)) + ggtitle(&quot;End of Chain&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~proposal_sd, ncol = 3) 7.3.2 Summary of Metropolis algorithm. The motivation for methods like the Metropolis algorithm is that they provide a high-resolution picture of the posterior distribution, even though in complex models we cannot explicitly solve the mathematical integral in Bayes’ rule. The idea is that we get a handle on the posterior distribution by generating a large sample of representative values. The larger the sample, the more accurate is our approximation. As emphasized previously, this is a sample of representative credible parameter values from the posterior distribution; it is not a resampling of data (there is a fixed data set). The cleverness of the method is that representative parameter values can be randomly sampled from complicated posterior distributions without solving the integral in Bayes’ rule, and by using only simple proposal distributions for which efficient random number generators already exist. (p. 161) 7.4 Toward Gibbs sampling: Estimating two coin biases “The Metropolis method is very useful, but it can be inefficient. Other methods can be more efficient in some situations” (p. 162). 7.4.1 Prior, likelihood and posterior for two biases. We are considering situations in which there are two underlying biases, namely \\(\\theta_1\\) and \\(\\theta_2\\), for the two coins. We are trying to determine what we should believe about these biases after we have observed some data from the two coins. Recall that [Kruschke used] the term “bias” as the name of the parameter \\(\\theta\\), and not to indicate that the value of \\(\\theta\\) deviates from 0.5… What we have to do next is specify a particular mathematical form for the prior distribution. We will work through the mathematics of a particular case for two reasons: First, it will allow us to explore graphical displays of two-dimensional parameter spaces, which will inform our intuitions about Bayes’ rule and sampling from the posterior distribution. Second, the mathematics will set the stage for a specific example of Gibbs sampling. Later in the book when we do applied Bayesian analysis, we will not be doing any of this sort of mathematics. We are doing the math now, for simple cases, to understand how the methods work so we can properly interpret their outputs in realistically complex cases. (pp. 163–165, emphasis in the original) 7.4.2 The posterior via exact formal analysis. The plots in the left column of Figure 7.5 are outside of my skill set. I believe they are referred to as wireframe plots and it’s my understanding that ggplot2 does not support wireframe plots at this time. However, I can reproduce versions of the right hand column. For our initial attempt for the upper right corner, we’ll simulate. set.seed(7) betas &lt;- tibble(theta_1 = rbeta(1e5, shape1 = 2, shape2 = 2), theta_2 = rbeta(1e5, shape1 = 2, shape2 = 2)) betas %&gt;% ggplot(aes(x = theta_1, y = theta_2)) + stat_density_2d() + labs(x = expression(theta[1]), y = expression(theta[2])) + coord_equal() + theme(panel.grid = element_blank()) Instead of the contour lines, one might use color to depict the density variable, instead. betas %&gt;% ggplot(aes(x = theta_1, y = theta_2, fill = stat(density))) + stat_density_2d(geom = &quot;raster&quot;, contour = F) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(theta[2])) + coord_equal() + theme(panel.grid = element_blank()) With careful use of dbeta(), we can be more precise. theta_sequence &lt;- seq(from = 0, to = 1, by = .01) tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(prior_1 = dbeta(x = theta_1, shape1 = 2, shape2 = 2), prior_2 = dbeta(x = theta_2, shape1 = 2, shape2 = 2)) %&gt;% expand(nesting(theta_1, prior_1), nesting(theta_2, prior_2)) %&gt;% ggplot(aes(x = theta_1, y = theta_2, fill = prior_1 * prior_2)) + geom_tile() + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(theta[2])) + coord_equal() + theme(panel.grid = element_blank()) We’ll need the bernoulli_likelihood() function from back in Chapter 6 for the middle right of Figure 7.5. bernoulli_likelihood &lt;- function(theta, data) { # theta = success probability parameter ranging from 0 to 1 # data = the vector of data (i.e., a series of 0s and 1s) n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } With our trusty bernoulli_likelihood() function in hand, we can now make a version of the middle right panel of Figure 7.5. theta_1_data &lt;- rep(0:1, times = c(8 - 6, 6)) theta_2_data &lt;- rep(0:1, times = c(7 - 2, 2)) tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_sequence, data = theta_1_data), likelihood_2 = bernoulli_likelihood(theta = theta_sequence, data = theta_2_data)) %&gt;% expand(nesting(theta_1, likelihood_1), nesting(theta_2, likelihood_2)) %&gt;% ggplot(aes(x = theta_1, y = theta_2, fill = likelihood_1 * likelihood_2)) + geom_tile() + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(theta[2])) + coord_equal() + theme(panel.grid = element_blank()) Here’s the two-dimensional posterior, the lower right panel of Figure 7.5. # we&#39;ve already defined these, but here they are again theta_sequence &lt;- seq(from = 0, to = 1, by = .01) theta_1_data &lt;- rep(0:1, times = c(8 - 6, 6)) theta_2_data &lt;- rep(0:1, times = c(7 - 2, 2)) # this is a redo from two plots up, but saved as `d_prior` d_prior &lt;- tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(prior_1 = dbeta(x = theta_1, shape1 = 2, shape2 = 2), prior_2 = dbeta(x = theta_2, shape1 = 2, shape2 = 2)) %&gt;% expand(nesting(theta_1, prior_1), nesting(theta_2, prior_2)) # this is a redo from one plot up, but saved as `d_likelihood` d_likelihood &lt;- tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_sequence, data = theta_1_data), likelihood_2 = bernoulli_likelihood(theta = theta_sequence, data = theta_2_data)) %&gt;% expand(nesting(theta_1, likelihood_1), nesting(theta_2, likelihood_2)) # here we cobine `d_prior` and `d_likelihood` d_prior %&gt;% left_join(d_likelihood, by = c(&quot;theta_1&quot;, &quot;theta_2&quot;)) %&gt;% # we need the marginal likelihood, the denominator in Bayes&#39; rule mutate(marginal_likelihood = sum(prior_1 * prior_2 * likelihood_1 * likelihood_2)) %&gt;% # finally, the two-dimensional posterior mutate(posterior = (prior_1 * prior_2 * likelihood_1 * likelihood_2) / marginal_likelihood) %&gt;% # plot! ggplot(aes(x = theta_1, y = theta_2, fill = posterior)) + geom_tile() + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(theta[2])) + coord_equal() + theme(panel.grid = element_blank()) That last plot, my friends, is a depiction of \\[p(\\theta_1, \\theta_2 | D) = \\frac{p(D | \\theta_1, \\theta_2) p(\\theta_1, \\theta_2)}{p(D)}.\\] 7.4.3 The posterior via the Metropolis algorithm. I’ve got nothing on this. But we’re here to learn HMC anyways. Read on. 7.4.4 Gibbs Hamiltonian Monte Carlo sampling. Figure 7.7 is still out of my skill set. But let’s fit the model with our primary package, brms. First we need to laod brms. library(brms) These, recall, are the data. d &lt;- tibble(z1 = 6, z2 = 2, n1 = 8, n2 = 7) d ## # A tibble: 1 x 4 ## z1 z2 n1 n2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 2 8 7 Kruschke said he was starting us out simply. But within the brms context, this is an intercepts-only multivariate model, which isn’t the simplest of things to code into brms. There are a couple ways to code a multivariate model in brms (Bürkner, 2020d). With this one, it makes sense to specify the model for each sequence of flips separately. This results in two models, which we’ll call model_1 and model_2. model_1 &lt;- bf(z1 | trials(n1) ~ 1) model_2 &lt;- bf(z2 | trials(n2) ~ 1) Before we fit, we’ll have to address a technicality. The brms package does allow for multivariate Bernoulli models. However, it does not support such models with different numbers of trials across the variables. Since our first variable is of 8 trials and the second is of 7, brms will not support this model using the Bernoulli likelihood. However, we can fit the model in brms as an aggregated binomial model. The main difficulty is that the regularizing beta(2, 2) prior won’t make sense, here. So we’ll opt for the regularizing normal(0, 1), instead. fit7.1 &lt;- brm(data = d, family = binomial(), model_1 + model_2, prior(normal(0, 1), class = Intercept), iter = 25500, warmup = 500, cores = 1, chains = 1, seed = 7, file = &quot;fits/fit07.01&quot;) Here is a summary of the results. print(fit7.1) ## Family: MV(binomial, binomial) ## Links: mu = logit ## mu = logit ## Formula: z1 | trials(n1) ~ 1 ## z2 | trials(n2) ~ 1 ## Data: d (Number of observations: 1) ## Samples: 1 chains, each with iter = 25500; warmup = 500; thin = 1; ## total post-warmup samples = 25000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## z1_Intercept 0.72 0.62 -0.46 1.97 1.00 25094 17061 ## z2_Intercept -0.59 0.63 -1.88 0.62 1.00 20552 16585 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As we’ll learn in later chapters, the parameters of a typical aggregated binomial model are in the log-odds scale. Over time, you will learn how to interpret them. But for now, just be happy that brms offers the inv_logit_scaled() function, with which we might convert our results back to the probability scale. fixef(fit7.1)[, 1] %&gt;% inv_logit_scaled() ## z1_Intercept z2_Intercept ## 0.6727986 0.3565870 Here we’ll use posterior_samples() to collect out posterior draws and save them as a data frame, which we’ll name post. post &lt;- posterior_samples(fit7.1, add_chain = T) With post in hand, we’re ready to make our version of Figure 7.8. To reduce the overplotting, we’re only looking at the first 500 post-warmup iterations. post %&gt;% mutate(theta_1 = b_z1_Intercept %&gt;% inv_logit_scaled(), theta_2 = b_z2_Intercept %&gt;% inv_logit_scaled()) %&gt;% filter(iter &lt; 1001) %&gt;% ggplot(aes(x = theta_1, y = theta_2)) + geom_point(alpha = 1/4) + geom_path(size = 1/10, alpha = 1/2) + labs(x = expression(theta[1]), y = expression(theta[2])) + coord_equal(xlim = c(0, 1), ylim = c(0, 1)) + theme(panel.grid = element_blank()) Just for kicks and giggles, we’ll plot the marginal posterior densities. You’ll note that even though we didn’t use beta priors, the posteriors look quite beta like. post %&gt;% mutate(`theta[1]` = b_z1_Intercept %&gt;% inv_logit_scaled(), `theta[2]` = b_z2_Intercept %&gt;% inv_logit_scaled()) %&gt;% pivot_longer(`theta[1]`:`theta[2]`) %&gt;% ggplot(aes(x = value, y = name)) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.5, .95)) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + xlab(&quot;posterior&quot;) + theme(panel.grid = element_blank()) 7.4.5 Is there a difference between biases? The difference distribution from our brms-based multivariate aggregated binomial model, \\(\\theta_1 - \\theta_2\\), is pretty similar to the ones in Figure 7.9. post %&gt;% mutate(theta_1 = b_z1_Intercept %&gt;% inv_logit_scaled(), theta_2 = b_z2_Intercept %&gt;% inv_logit_scaled()) %&gt;% transmute(`theta_1 - theta_2` = theta_1 - theta_2) %&gt;% ggplot(aes(x = `theta_1 - theta_2`, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_x_continuous(expression(theta[1]-theta[2]), limits = c(-.5, .9)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Here are the exact estimates of the mode and 95% HDIs for our difference distribution, \\(\\theta_1 - \\theta_2\\). post %&gt;% mutate(theta_1 = b_z1_Intercept %&gt;% inv_logit_scaled(), theta_2 = b_z2_Intercept %&gt;% inv_logit_scaled()) %&gt;% transmute(`theta_1 - theta_2` = theta_1 - theta_2) %&gt;% mode_hdi() ## theta_1 - theta_2 .lower .upper .width .point .interval ## 1 0.3109545 -0.07194696 0.6407353 0.95 mode hdi Given that we used both a different likelihood function, which necessitated a different prior, I think we did pretty good complimenting the results in the text. 7.4.6 Terminology: MCMC. Any simulation that samples a lot of random values from a distribution is called a Monte Carlo simulation, named after the dice and spinners and shufflings of the famous casino locale. The appellation “Monte Carlo” is attributed (Eckhardt, 1987) to the mathematicians Stanislaw Ulam (1909–1984) and John von Neumann (1903–1957). (p. 177) In case you didn’t know, brms is a user-friendly interface for the Stan probabilistic programing language (Stan; Carpenter et al., 2017) and Stan is named after Stanislaw Ulam. 7.5 MCMC representativeness, accuracy, and efficiency We have three main goals in generating an MCMC sample from the posterior distribution: The values in the chain must be representative of the posterior distribution. They should not be unduly influenced by the arbitrary initial value of the chain, and they should fully explore the range of the posterior distribution without getting stuck. The chain should be of sufficient size so that estimates are accurate and stable. In particular, the estimates of the central tendency (such as median or mode), and the limits of the 95% HDI, should not be much different if the MCMC analysis is run again (using different seed states for the pseudorandom number generators). The chain should be generated efficiently, with as few steps as possible, so not to exceed our patience or computing power. (p. 178, emphasis in the original) 7.5.1 MCMC representativeness. Here are our data. z &lt;- 35 n &lt;- 50 d &lt;- tibble(y = rep(0:1, times = c(n - z, z))) Here we fit the model. Note how since we’re just univariate, it’s easy to switch back to directly modeling with the Bernoulli likelihood. fit7.2 &lt;- brm(data = d, family = bernoulli(link = identity), y ~ 1, prior(beta(2, 2), class = Intercept), iter = 10000, warmup = 500, cores = 3, chains = 3, control = list(adapt_delta = 0.9), seed = 7, file = &quot;fits/fit07.02&quot;) On page 179, Kruschke discussed burn-in steps within the Gibbs framework: The preliminary steps, during which the chain moves from its unrepresentative initial value to the modal region of the posterior, is called the burn-in period. For realistic applications, it is routine to apply a burn-in period of several hundred to several thousand steps. For each HMC chain, the first \\(n\\) iterations are calles “warmups.” In this example, \\(n = 500\\) (i.e., warmup = 500). Within the Stan-HMC paradigm, warmups are somewhat analogous to but not synonymous with burn-in iterations as done by the Gibbs sampling in JAGS. But HMC warmups are like Gibbs burn-ins in that both are discarded and not used to describe the posterior. For more on warmup, check out McElreath’s lecture, starting here or, for more detail, the HMC Algorithm Parameters section (15.2) of the Stan reference manual, version 2.23 (Stan Development Team, 2020b). It appears that the upshot of all this is some of the packages in the Stan ecosystem don’t make it easy to extract the warmup values. For example, the brms::plot() function excludes them from the trace plot without the option to include them. plot(fit7.2) Notice how the x-axis on the trace plot ranges from 0 to 9,500. Now recall that our model code included iter = 10000, warmup = 500. Those 9,500 iterations in the trace plot are excluding the first 500 warmup iterations. This code is a little janky, but if you really want those warmup iterations, you can extract them from the fit2 object like this. warmups &lt;- c(fit7.2$fit@sim$samples[[1]]$b_Intercept[1:500], fit7.2$fit@sim$samples[[2]]$b_Intercept[1:500], fit7.2$fit@sim$samples[[3]]$b_Intercept[1:500]) %&gt;% # since these come from lists, here we&#39;ll convert them to a data frame as.data.frame() %&gt;% rename(b_Intercept = &quot;.&quot;) %&gt;% # we&#39;ll need to recapture the iteration and chain information mutate(iter = rep(1:500, times = 3), chain = factor(rep(1:3, each = 500), levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;))) warmups %&gt;% head() ## b_Intercept iter chain ## 1 0.2981264 1 1 ## 2 0.2981264 2 1 ## 3 0.2981264 3 1 ## 4 0.2981264 4 1 ## 5 0.2991483 5 1 ## 6 0.2937265 6 1 The bayesplot package (Gabry et al., 2019; Gabry &amp; Mahr, 2019) makes it easier to reproduce some of the plots in Figure 7.10. library(bayesplot) We’ll reproduce the upper left panel with mcmc_trace(). mcmc_trace(warmups, pars = &quot;b_Intercept&quot;) As an alternative, we can also extract the warmup draws from a brm() fit with the ggmcmc package (Fernández i Marín, 2016; Marín, 2020). # install.packages(&quot;ggmcmc&quot;, dependencies = T) library(ggmcmc) The ggmcmc package has a variety of convenience functions for working with MCMC chains. The ggs() function extracts the posterior draws, including warmup, and arranges them in a tidy tibble. With those in hand, we can now make a trace plot with warmup draws. ggs(fit7.2) %&gt;% filter(Iteration &lt; 501 &amp; Parameter == &quot;b_Intercept&quot;) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x = Iteration, y = value, color = chain)) + geom_line() + scale_color_viridis_d(end = .8) + labs(title = &quot;My custom trace plots with warmups via ggmcmc::ggs()&quot;, x = NULL, y = NULL) + theme(panel.grid = element_blank()) It appears our HMC warmup iterations found the posterior quite quickly. Here’s the autocorrelation plot. mcmc_acf(warmups, pars = &quot;b_Intercept&quot;, lags = 25) Our autocorrelation plots indicate substantially lower autocorrelations yielded by HMC as implemented by Stan than what Kruschke generated with the MH algorithm. This is one of the reasons folks using HMC tend to use fewer iterations than those using MH or Gibbs. If you were unhappy with the way mcmc_acf() defaults to faceting the plot by chain, you could always extract the data from the function and use them to make the plot the way you prefer. E.g., mcmc_acf(warmups)$data %&gt;% as_tibble() %&gt;% filter(Parameter == &quot;b_Intercept&quot;) %&gt;% ggplot(aes(x = Lag, y = AC, color = Chain %&gt;% as.factor())) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point(size = 2/3) + geom_line() + scale_color_viridis_d(end = .8) + ylab(&quot;Autocorrelation&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Here are the overlaid densities. mcmc_dens_overlay(warmups, pars = c(&quot;b_Intercept&quot;)) The densities aren’t great, but they still appear nicer than those in for the burn-in iterations in the text. With our warmups in their current state, I’m not aware how we might conveniently make a shrink factor plot, as seen in the lower left of Figure 7.10. So it goes… Figure 7.11 examined the post-burn-in iterations. We’ll follow suit with our post-warmup iterations. post &lt;- posterior_samples(fit7.2, add_chain = T) mcmc_trace(post, pars = &quot;b_Intercept&quot;) The autocorrelation plots: mcmc_acf(post, pars = &quot;b_Intercept&quot;, lags = 40) As with the warmups, above, the post-warmup autocorrelation plots indicate substantially lower autocorrelations yielded by HMC as implemented by Stan than what Kruschke generated with the MH algorithm. This is one of the reasons folks using HMC tend to use fewer iterations than those using MH or Gibbs. Here are the overlaid densities. mcmc_dens_overlay(post, pars = c(&quot;b_Intercept&quot;)) And now that we’re focusing on the post-warmup iterations, we can make a shrink factor plot. We’ll do so with the coda::gelman.plot() function. But you can’t just dump your brm() fit object into coda::gelman.plot(). It’s the wrong object type. However, brms offers the as.mcmc() function which will convert brm() objects for use in coda package functions. fit7.2_c &lt;- as.mcmc(fit7.2) fit7.2_c %&gt;% glimpse() ## List of 3 ## $ : &#39;mcmc&#39; num [1:9500, 1:2] 0.78 0.822 0.699 0.648 0.702 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:2] &quot;b_Intercept&quot; &quot;lp__&quot; ## ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1 ## $ : &#39;mcmc&#39; num [1:9500, 1:2] 0.76 0.754 0.725 0.725 0.672 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:2] &quot;b_Intercept&quot; &quot;lp__&quot; ## ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1 ## $ : &#39;mcmc&#39; num [1:9500, 1:2] 0.808 0.788 0.803 0.65 0.65 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:2] &quot;b_Intercept&quot; &quot;lp__&quot; ## ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1 ## - attr(*, &quot;class&quot;)= chr &quot;mcmc.list&quot; With our freshly-converted fit2_c object in hand, we’re ready to plot. coda::gelman.plot(fit7.2_c[, &quot;b_Intercept&quot;, ]) Looks great. As Kruschke explained on page 181, that plot is based on the potential scale reduction factor, or \\(\\hat R\\) as it’s typically referred to in the Stan ecosystem. Happily, brms reports the \\(\\hat R\\) values for the major model parameters using print() or summary(). print(fit7.2) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: d (Number of observations: 50) ## Samples: 3 chains, each with iter = 10000; warmup = 500; thin = 1; ## total post-warmup samples = 28500 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.69 0.06 0.56 0.80 1.00 9215 9302 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Instead of a running value, you get a single statistic in the ‘Rhat’ column. On page 181, Kruschke discussed how his overlaid density plots include the HDIs, by chain. The convenience functions from brms and bayesplot don’t easily get us there. But we can get those easy enough with a little help tidybayes::geom_halfeyeh(). post %&gt;% ggplot(aes(x = b_Intercept, y = chain, fill = chain)) + geom_halfeyeh(point_interval = mode_hdi, .width = .95) + scale_fill_viridis_d(begin = .35, end = .95) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) 7.5.2 MCMC accuracy. We’ll wrangle our post object a bit to make it easier to reproduce Figure 7.12. lagged_post &lt;- post %&gt;% filter(chain == 1) %&gt;% select(b_Intercept, iter) %&gt;% rename(lag_0 = b_Intercept) %&gt;% mutate(lag_1 = lag(lag_0, 1), lag_5 = lag(lag_0, 5), lag_10 = lag(lag_0, 10)) %&gt;% pivot_longer(-iter, names_to = &quot;key&quot;) head(lagged_post) ## # A tibble: 6 x 3 ## iter key value ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 501 lag_0 0.780 ## 2 501 lag_1 NA ## 3 501 lag_5 NA ## 4 501 lag_10 NA ## 5 502 lag_0 0.822 ## 6 502 lag_1 0.780 Here’s our version of the top row. p1 &lt;- lagged_post %&gt;% filter(key %in% c(&quot;lag_0&quot;, &quot;lag_1&quot;), iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = iter, y = value, color = key)) + geom_point() + geom_line() + scale_color_manual(values = c(&quot;black&quot;, &quot;grey67&quot;)) + labs(x = &quot;Index 1001:1071&quot;, title = &quot;Lag 1&quot;) p2 &lt;- lagged_post %&gt;% filter(key %in% c(&quot;lag_0&quot;, &quot;lag_5&quot;), iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = iter, y = value, color = key)) + geom_point() + geom_line() + scale_color_manual(values = c(&quot;black&quot;, &quot;grey67&quot;)) + labs(x = &quot;Index 1001:1071&quot;, title = &quot;Lag 5&quot;) p3 &lt;- lagged_post %&gt;% filter(key %in% c(&quot;lag_0&quot;, &quot;lag_10&quot;), iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = iter, y = value, color = key)) + geom_point() + geom_line() + scale_color_manual(values = c(&quot;black&quot;, &quot;grey67&quot;)) + labs(x = &quot;Index 1001:1071&quot;, title = &quot;Lag 10&quot;) library(patchwork) (p1 + p2 + p3) &amp; theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Here’s the middle row for Figure 7.12. lagged_post_wide &lt;- lagged_post %&gt;% spread(key = key, value = value) p1 &lt;- lagged_post_wide %&gt;% filter(iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = lag_1, y = lag_0)) + stat_smooth(method = &quot;lm&quot;) + geom_point() p2 &lt;- lagged_post_wide %&gt;% filter(iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = lag_5, y = lag_0)) + stat_smooth(method = &quot;lm&quot;) + geom_point() p3 &lt;- lagged_post_wide %&gt;% filter(iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = lag_10, y = lag_0)) + stat_smooth(method = &quot;lm&quot;) + geom_point() (p1 + p2 + p3) &amp; theme(panel.grid = element_blank()) For kicks and giggles, we used stat_smooth() to add an OLS regression line with its 95% confidence intervals to each plot. If you want the Pearson’s correlations among the lags, the lowerCor() function from the psych package (Revelle, 2020) can be handy. library(psych) lagged_post_wide %&gt;% select(-iter) %&gt;% filter(!is.na(lag_10)) %&gt;% lowerCor(digits = 3) ## lag_0 lag_1 lag_10 lag_5 ## lag_0 1.000 ## lag_1 0.455 1.000 ## lag_10 0.021 0.016 1.000 ## lag_5 0.053 0.080 0.053 1.000 For our version of the bottom of Figure 7.12, we’ll use the bayesplot::mcmc_acf_bar() function to get the autocorrelation bar plot, by chain. mcmc_acf_bar(post, pars = &quot;b_Intercept&quot;, lags = 20) All three rows of our versions for Figure 7.12 indicate in their own way how much lower our autocorrelations were than the ones in the text. If you’re curious of the effective sample sizes for the parameters in your brms models, just look at the model summary using either summary() or print(). print(fit7.2) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: d (Number of observations: 50) ## Samples: 3 chains, each with iter = 10000; warmup = 500; thin = 1; ## total post-warmup samples = 28500 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.69 0.06 0.56 0.80 1.00 9215 9302 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Look at the last two columns in the Intercept summary. Earlier versions of brms had one column named Eff.Sample, which reported the effect sample size as discussed by Kruschke. Starting with version 2.10.0, brms now returns Bulk_ESS and Tail_ESS, instead. These originate from a (2019) preprint by Stan-team all-stars Vehtari, Gelman, Simpson, Carpenter, and Bürkner. From their paper, we read: If you plan to report quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3 we show that convergence of Markov chains is not uniform across the parameter space and propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as the “bulk-ESS”), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (p. 5, emphasis in the original) For more technical details, see the paper. The Bulk_ESS column in current versions of brms is what was previously referred to as Eff.Sample. This is what corresponds to what Kruschke meant when referring to effective sample size. Now rather than focusing solely on ‘the center of the’ posterior distribution’ as indexed by Bulk_ESS, we also gauge the effective sample size in the posterior intervals using Tail_ESS. Anyway, I’m not quite sure how to reproduce Kruschke’s MCMC ESS simulation studies. If you’ve got it figured out, please share your code in my GitHub issue #15. If you’re interested in the Monte Carlo standard error (MCSE) for your brms parameters, the easiest way is to tack $fit onto your fit object. fit7.2$fit ## Inference for Stan model: 5351a09d3d5d34fa50f6376e2c847de5. ## 3 chains, each with iter=10000; warmup=500; thin=1; ## post-warmup draws per chain=9500, total post-warmup draws=28500. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept 0.69 0.00 0.06 0.56 0.64 0.69 0.73 0.80 9156 1 ## lp__ -30.79 0.01 0.68 -32.75 -30.96 -30.53 -30.35 -30.31 8163 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Feb 4 19:51:28 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). This returns an rstan-like summary (Stan Development Team, 2020a). The ‘se_mean’ column is the MCSE. 7.5.3 MCMC efficiency. Kruschke wrote: “It is often the case in realistic applications that there is strong autocorrelation for some parameters, and therefore, an extremely long chain is required to achieve an adequate ESS or MCSE” (p. 187). As we’ll see, this is generally less of a problem for HMC than for MH or Gibbs. But it does still crop up, particularly in complicated models. As he wrote on the following page, “one sampling method that can be relatively efficient is Hamiltonian Monte Carlo.” Indeed. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] psych_1.9.12.31 patchwork_1.0.0 ggmcmc_1.4.1 bayesplot_1.7.1 ## [5] brms_2.12.0 Rcpp_1.0.4.6 tidybayes_2.0.3.9000 forcats_0.5.0 ## [9] stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 readr_1.3.1 ## [13] tidyr_1.0.2 tibble_3.0.1 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 ## [9] farver_2.0.3 rstan_2.19.3 svUnit_1.0.3 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 ## [17] splines_3.6.3 bridgesampling_1.0-0 mnormt_1.5-6 knitr_1.28 ## [21] shinythemes_1.1.2 jsonlite_1.6.1 broom_0.5.5 dbplyr_1.4.2 ## [25] shiny_1.4.0.2 compiler_3.6.3 httr_1.4.1 backports_1.1.6 ## [29] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 ## [33] later_1.0.0 prettyunits_1.1.1 htmltools_0.4.0 tools_3.6.3 ## [37] igraph_1.2.5 coda_0.19-3 gtable_0.3.0 glue_1.4.0 ## [41] reshape2_1.4.4 cellranger_1.1.0 vctrs_0.3.0 nlme_3.1-144 ## [45] crosstalk_1.1.0.1 xfun_0.13 ps_1.3.3 rvest_0.3.5 ## [49] mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 ## [53] MASS_7.3-51.5 zoo_1.8-7 scales_1.1.1 colourpicker_1.0 ## [57] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 ## [61] inline_0.3.15 RColorBrewer_1.1-2 shinystan_2.5.0 yaml_2.2.1 ## [65] gridExtra_2.3 StanHeaders_2.21.0-1 loo_2.2.0 reshape_0.8.8 ## [69] stringi_1.4.6 dygraphs_1.1.1.6 pkgbuild_1.0.8 rlang_0.4.6 ## [73] pkgconfig_2.0.3 matrixStats_0.56.0 HDInterval_0.2.0 evaluate_0.14 ## [77] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 ## [81] processx_3.4.2 tidyselect_1.0.0 GGally_1.5.0 plyr_1.8.6 ## [85] magrittr_1.5 bookdown_0.18 R6_2.4.1 generics_0.0.2 ## [89] DBI_1.1.0 mgcv_1.8-31 pillar_1.4.4 haven_2.2.0 ## [93] withr_2.2.0 xts_0.12-0 abind_1.4-5 modelr_0.1.6 ## [97] crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 rmarkdown_2.1 ## [101] grid_3.6.3 readxl_1.3.1 callr_3.4.3 threejs_0.3.3 ## [105] reprex_0.3.0 digest_0.6.25 xtable_1.8-4 httpuv_1.5.2 ## [109] stats4_3.6.3 munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.1 References "],
["jags-brms.html", "8 JAGS brms 8.1 JAGS brms and its relation to R 8.2 A complete example 8.3 Simplified scripts for frequently used analyses 8.4 Example: Difference of biases 8.5 Sampling from the prior distribution in JAGS brms 8.6 Probability distributions available in JAGS brms 8.7 Faster sampling with parallel processing in runjags brms::brm() 8.8 Tips for expanding JAGS brms models Session info", " 8 JAGS brms We, of course, will be using brms in place of JAGS. 8.1 JAGS brms and its relation to R In the opening prargraph in his GitHub repository for brms, Bürkner explained: The brms package provides an interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan, which is a C++ package for performing full Bayesian inference (see http://mc-stan.org/). The formula syntax is very similar to that of the package lme4 to provide a familiar and simple interface for performing regression analyses. A wide range of response distributions are supported, allowing users to fit – among others – linear, robust linear, count data, survival, response times, ordinal, zero-inflated, and even self-defined mixture models all in a multilevel context. Further modeling options include non-linear and smooth terms, auto-correlation structures, censored data, missing value imputation, and quite a few more. In addition, all parameters of the response distribution can be predicted in order to perform distributional regression. Multivariate models (i.e., models with multiple response variables) can be fit, as well. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. Model fit can easily be assessed and compared with posterior predictive checks, cross-validation, and Bayes factors. (emphasis in the original) Bürkner’s brms repository includes many helpful links, such as to where brms lives on CRAN, a list of blog posts highlighting brms, and a forum where users can ask questions about brms in specific or about Stan in general. You can install the current official version of brms in the same way you would any other R package (i.e., install.packages(&quot;brms&quot;, dependencies = T)). If you want the current developmental version, you could download it from GitHub by executing the following. if (!requireNamespace(&quot;devtools&quot;)) { install.packages(&quot;devtools&quot;) } devtools::install_github(&quot;paul-buerkner/brms&quot;) 8.2 A complete example We express the likelihood for our coin toss example as \\[y_{i} \\sim \\operatorname{Bernoulli} (\\theta).\\] Our prior will be \\[\\theta \\sim \\operatorname{Beta} (\\alpha, \\beta).\\] 8.2.1 Load data. “Logically, models of data start with the data. We must know their basic scale and structure to conceive of a descriptive model” (p. 197). Here we load the data with the readr::read_csv() function, the tidyverse version of base R read.csv(). library(tidyverse) my_data &lt;- read_csv(&quot;data.R/z15N50.csv&quot;) Unlike what Kruschke wrote about JAGS, the brms package does not require us to convert the data into a list. It can handle data in lists or data frames, of which tibbles are a special case. Here are what the data look like. head(my_data) ## # A tibble: 6 x 1 ## y ## &lt;dbl&gt; ## 1 0 ## 2 1 ## 3 0 ## 4 0 ## 5 0 ## 6 0 We might visualize them in a bar plot. my_data %&gt;% mutate(y = y %&gt;% as.character()) %&gt;% ggplot(aes(x = y)) + geom_bar() + theme(panel.grid = element_blank()) If you wanted to compute “Ntotal”, the number of rows in our tibble, one way is with count(). my_data %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 50 However, we’re not going to do anything with an “Ntotal” value. For brms, the data are fine in their current data frame form. No need for a dataList. 8.2.2 Specify model. Let’s open brms. library(brms) The brms package doesn’t have code blocks following the JAGS format or the sequence in Kurschke’s diagrams. Rather, its syntax is modeled in part after the popular frequentist mixed-effects package, lme4. To learn more about how brms compares to lme4, see Bürkner’s (2017) overview, brms: An R package for Bayesian multilevel models using Stan. The primary function in brms is brm(). Into this one function we will specify the data, the model, the likelihood function, the prior(s), and any technical settings such as the number of MCMC chains, iterations, and so forth. You can order the arguments in any way you like. My typical practice is to start with data, family (i.e., the likelihood function), the model formula, and my priors. If there are any technical specifications such as the number of MCMC iterations I’d like to change from their default values, I usually do that last. Here’s how to fit the model. fit8.1 &lt;- brm(data = my_data, family = bernoulli(link = identity), formula = y ~ 1, prior(beta(2, 2), class = Intercept), iter = 500 + 3334, warmup = 500, chains = 3, seed = 8, file = &quot;fits/fit08.01&quot;) For a more detailed explanation of the brms::brm() function, spend some time with the brm section of the brms reference manual (Bürkner, 2020f). 8.2.3 Initialize chains. In Stan, and in brms by extension, the initial values have default settings. In the Initialization section of the Program Execution chapter in the Stan reference manual, Version 2.23 (Stan Development Team, 2020b) we read If there are no user-supplied initial values, the default initialization strategy is to initialize the unconstrained parameters directly with values drawn uniformly from the interval \\((−2, 2)\\). The bounds of this initialization can be changed but it is always symmetric around 0. The value of 0 is special in that it represents the median of the initialization. An unconstrained value of 0 corresponds to different parameter values depending on the constraints declared on the parameters. In general, I do not recommend setting custom initial values in brms or Stan. Under the hood, Stan will transform the parameters to the unconstrained space in models where they are bounded. In our Bernoulli model, \\(\\theta\\) is bounded at 0 and 1. A little further down in the same section, we read For parameters bounded above and below, the initial value of 0 on the unconstrained scale corresponds to a value at the midpoint of the constraint interval. For probability parameters, bounded below by 0 and above by 1, the transform is the inverse logit, so that an initial unconstrained value of 0 corresponds to a constrained value of 0.5, -2 corresponds to 0.12 and 2 to 0.88. Bounds other than 0 and 1 are just scaled and translated. If you want to play around with this, have at it. In my experience, the only time it helps to set these manually is when you want to fix them to zero. You can do that by specifying inits = 0 within brm(). 8.2.4 Generate chains. By default, brms will use 4 chains of 2,000 iterations each. The type of MCMC brms uses is Hamiltonian Monte Carlo (HMC). You can learn more about HMC at the Stan website, https://mc-stan.org, which includes resources such as the Stan user’s guide (Stan Development Team, 2020c), the Stan reference manual (Stan Development Team, 2020b), and a list of tutorials. McElreath has a nice intro lecture on MCMC in general and HMC in particular. Michael Bentacourt has some good lectures on Stan and HMC, such as here and here. And, of course, we will cover HMC with Kruschke in Chapter 14. Within each HMC chain, the first \\(n\\) iterations are warmups. Within the Stan-HMC paradigm, warmups are somewhat analogous to but not synonymous with burn-in iterations as done by the Gibbs sampling in JAGS. But HMC warmups are like Gibbs burn-ins in that both are discarded and not used to describe the posterior. As such, the brms default settings yield 1,000 post-warmup iterations for each of the 4 HMC chains. However, we specified iter = 500 + 3334, warmup = 500, chains = 3. Thus instead of defaults, we have 3 HMC chains. Each chain has 500 + 3,334 = 3,834 total iterations, of which 500 were discarded warmup iterations. To learn more about the warmup stage in Stan, check out the HMC Algorithm Parameters section of the MCMC Sampling chapter of the Stan reference manual. 8.2.5 Examine chains. The brms::plot() function returns a density and trace plot for each model parameter. plot(fit8.1) If you want to display each chain as its own density, you can use the handy mcmc_dens_overlay() function from the bayesplot package. library(bayesplot) But before we do so, we’ll need to export the posterior samples into a data frame, for which we’ll employ posterior_samples(). post &lt;- posterior_samples(fit8.1, add_chain = T) Note the add_chain = T argument, which will allow us to differentiate the draws by their chain of origin. But anyway, here are the overlaid densities. mcmc_dens_overlay(post, pars = c(&quot;b_Intercept&quot;)) + theme(panel.grid = element_blank()) The bayesplot::mcmc_acf() function will give us the autocorrelation plots. mcmc_acf(post, pars = &quot;b_Intercept&quot;, lags = 35) With brms functions, we get a sole \\(\\hat R\\) value for each parameter rather than a running vector. rhat(fit8.1)[&quot;b_Intercept&quot;] ## b_Intercept ## 1.00023 We’ll have to employ brms::as.mcmc() and coda::gelman.plot() to make our running \\(\\hat R\\) plot. fit8.1_c &lt;- as.mcmc(fit8.1) coda::gelman.plot(fit8.1_c[, &quot;b_Intercept&quot;, ]) 8.2.5.1 The plotPost function How to plot your brms posterior distributions. We’ll get into plotting in just a moment. But before we do, here’s a summary of the model. print(fit8.1) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: my_data (Number of observations: 50) ## Samples: 3 chains, each with iter = 3834; warmup = 500; thin = 1; ## total post-warmup samples = 10002 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.32 0.06 0.20 0.44 1.00 3678 4655 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). To summarize a posterior in terms of central tendency, brms defaults to the mean value (i.e., the value in the ‘Estimate’ column of the print() output). In many of the other convenience functions, you can also request the median instead. For example, we can use the robust = T argument to get the ‘Estimate’ in terms of the median. posterior_summary(fit8.1, robust = T) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.314162 0.06271794 0.2005124 0.4431792 ## lp__ -30.522541 0.29765456 -32.6877345 -30.3052625 Across functions, the intervals default to 95%. With print() and summary() you can adjust the level with a prob argument. For example, here we’ll use 50% intervals. print(fit8.1, prob = .5) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: my_data (Number of observations: 50) ## Samples: 3 chains, each with iter = 3834; warmup = 500; thin = 1; ## total post-warmup samples = 10002 ## ## Population-Level Effects: ## Estimate Est.Error l-50% CI u-50% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.32 0.06 0.27 0.36 1.00 3678 4655 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). But in many other brms convenience functions, you can use the probs argument to request specific percentile summaries. posterior_summary(fit8.1, probs = c(.025, .25, .75, .975)) ## Estimate Est.Error Q2.5 Q25 Q75 Q97.5 ## b_Intercept 0.3160697 0.06246065 0.2005124 0.2726817 0.357283 0.4431792 ## lp__ -30.7894587 0.68273263 -32.6877345 -30.9497898 -30.354684 -30.3052625 Regardless of what prob or probs levels you use, brms functions always return percentile-based estimates. All this central tendency and interval talk will be important in a moment… When plotting the posterior distribution of a parameter estimated with brms, you typically do so working with the results of an object returned by posterior_samples(). Recall we already saved those samples as post. head(post) ## b_Intercept lp__ chain iter ## 1 0.3885602 -31.04295 1 501 ## 2 0.4013501 -31.28580 1 502 ## 3 0.3733681 -30.79754 1 503 ## 4 0.3764648 -30.84373 1 504 ## 5 0.3862548 -31.00268 1 505 ## 6 0.2774596 -30.42087 1 506 With post in hand, we can use ggplot2 to do the typical distributional plots, such as with geom_histogram(). post %&gt;% ggplot(aes(x = b_Intercept)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Theta via ggplot2::geom_histogram()&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) The bayesplot::mcmc_areas() function offers a nice way to depict the posterior densities, along with their percentile-based 50% and 95% ranges. mcmc_areas( post, pars = c(&quot;b_Intercept&quot;), prob = 0.5, prob_outer = 0.95, point_est = &quot;mean&quot; ) + scale_y_discrete(NULL, breaks = NULL) + labs(title = &quot;Theta via bayesplot::mcmc_areas()&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) ## Warning: `expand_scale()` is deprecated; use `expansion()` instead. brms doesn’t have a convenient way to compute the posterior mode or HDIs. Base R is no help, either. But Matthew Kay’s tidybayes package makes it easy to compute posterior modes and HDIs with the handy geom_halfeyeh() function. library(tidybayes) post %&gt;% ggplot(aes(x = b_Intercept, y = 0)) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Theta via tidybayes::geom_halfeyeh()&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) The tidybayes::geom_halfeyeh() function returns a density with a measure of the posterior’s central tendency in a dot and one or multiple interval bands as horizontal lines at the base of the density. Using the point_interval = mode_hdi argument, we asked for the mode to be our measure of central tendency and the highest posterior density intervals to be our type intervals. With .width = c(.95, .5), we requested our HDIs be at both the 95% and 50% levels. To be more congruent with Kruschke’s plotting sensibilities, we can use tidybayes::stat_histintervalh(). # this is unnecessary, but makes for nicer x-axis breaks my_breaks &lt;- mode_hdi(post$b_Intercept)[, 1:3] %&gt;% gather(key, breaks) %&gt;% mutate(labels = breaks %&gt;% round(digits = 3)) # here&#39;s the main plot code post %&gt;% ggplot(aes(x = b_Intercept, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_x_continuous(breaks = my_breaks$breaks, labels = my_breaks$labels) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Theta via tidybayes::stat_histintervalh()&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) With the point_interval argument within the stat_histintervalh() function, we can request different combinations of measures of central tendency (i.e., mean, median, mode) and interval types (i.e., percentile-based and HDIs). Although all of these are legitimate ways to summarize a posterior, they can yield somewhat different results. For example, here we’ll contrast our mode + HDI summary with a median + percentile-based interval summary using `tidybayes::stat_pointintervalh(). post %&gt;% ggplot(aes(x = b_Intercept)) + stat_pointintervalh(aes(y = 1), point_interval = median_qi, .width = c(.95, .5)) + stat_pointintervalh(aes(y = 2), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = 1:2, labels = c(&quot;median_qi&quot;, &quot;mode_hdi&quot;)) + coord_cartesian(ylim = c(0, 3)) + labs(title = &quot;Theta via tidybayes::stat_pointintervalh()&quot;, x = expression(theta)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank(), title = element_text(size = 10.5)) Similar, yet distinct. To get a sense of the full variety of ways tidybayes allows users to summarize and plot the results of a Bayesian model, check out Kay’s (2020b) vignette, Slab + interval stats and geoms. 8.3 Simplified scripts for frequently used analyses A lot has happened in R for Bayesian analysis since Kruschke wrote his (2015) text. In addition to our use of the tidyverse, the brms, bayesplot, and tidybayes packages offer an array of useful convenience functions. We can and occasionally will write our own. But really, the rich R ecosystem already has us pretty much covered. 8.4 Example: Difference of biases Here are our new data. my_data &lt;- read_csv(&quot;data.R/z6N8z2N7.csv&quot;) glimpse(my_data) ## Rows: 15 ## Columns: 2 ## $ y &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0 ## $ s &lt;chr&gt; &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Re… They look like this. my_data %&gt;% mutate(y = y %&gt;% as.character()) %&gt;% ggplot(aes(x = y)) + geom_bar() + theme(panel.grid = element_blank()) + facet_wrap(~s) Here we fit the model with brms::brm(). fit8.2 &lt;- brm(data = my_data, family = bernoulli(identity), y ~ 0 + s, prior = c(prior(beta(2, 2), class = b, coef = sReginald), prior(beta(2, 2), class = b, coef = sTony)), iter = 6000, warmup = 5000, cores = 4, chains = 4, # this line isn&#39;t always necessary, but it will let us use `prior_samples()` later sample_prior = T, control = list(adapt_delta = .999), seed = 8, file = &quot;fits/fit08.02&quot;) More typically, we’d parameterize the model as y ~ 1 + s. This form would yield an intercept and a slope. Behind the scenes, brms would treat the nominal s variable as an 0-1 coded dummy variable. One of the nominal levels would become the reverence category, depicted by the Intercept, and the difference between that and the other category would be the s slope. However, with our y ~ 0 + s syntax, we’ve suppressed the typical model intercept. The consequence is that each level of the nominal variable s gets its own intercept or [i] index, if you will. This is analogous to Kruschke’s y[i] ∼ dbern(theta[s[i]]) code. Also, notice our use of the control = list(adapt_delta = .999) argument. By default, adapt_delta = .8. Leaving it at its default for this model resulted in “divergent transitions after warmup” warnings, which urged me to increase “adapt_delta above 0.8.” The model fit well after raising it to .999 and increasing the number of warmup samples. See the brms reference manual (Bürkner, 2020g) for more on adapt_delta. All that aside, here are the chains. plot(fit8.2) The model summary() is as follows: summary(fit8.2) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 0 + s ## Data: my_data (Number of observations: 15) ## Samples: 4 chains, each with iter = 6000; warmup = 5000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sReginald 0.67 0.13 0.39 0.89 1.00 1760 1821 ## sTony 0.36 0.14 0.13 0.65 1.00 1837 1590 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The brms::pairs() function gets us the bulk of Figure 8.6. pairs(fit8.2, off_diag_args = list(size = 1/3, alpha = 1/3)) But to get at that difference-score distribution, we’ll have extract the posterior iterations with posterior_samples(), make difference score with mutate(), and manually plot with ggplot2. post &lt;- posterior_samples(fit8.2) post &lt;- post %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% mutate(`theta_Reginald - theta_Tony` = theta_Reginald - theta_Tony) glimpse(post) ## Rows: 4,000 ## Columns: 6 ## $ theta_Reginald &lt;dbl&gt; 0.5287682, 0.6097360, 0.7972532, 0.7778351, 0.7044172, 0.40… ## $ theta_Tony &lt;dbl&gt; 0.3324897, 0.3057314, 0.2492853, 0.2449084, 0.3836122, 0.40… ## $ prior_b_sReginald &lt;dbl&gt; 0.34506299, 0.69256517, 0.44661952, 0.48778682, 0.54056976,… ## $ prior_b_sTony &lt;dbl&gt; 0.1271459, 0.4352422, 0.2385522, 0.6839308, 0.5565150, 0.54… ## $ lp__ &lt;dbl&gt; -8.862774, -8.446904, -8.677803, -8.594283, -8.303235, -10.… ## $ `theta_Reginald - theta_Tony` &lt;dbl&gt; 0.1962784909, 0.3040045478, 0.5479678103, 0.5329266827, 0.3… gathered_post &lt;- post %&gt;% select(starts_with(&quot;theta&quot;)) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;theta_Reginald&quot;, &quot;theta_Tony&quot;, &quot;theta_Reginald - theta_Tony&quot;))) gathered_post %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;) Here’s a way to get the numeric summaries out of post. gathered_post %&gt;% group_by(key) %&gt;% mode_hdi() ## # A tibble: 3 x 7 ## key value .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 theta_Reginald 0.707 0.411 0.907 0.95 mode hdi ## 2 theta_Tony 0.297 0.109 0.633 0.95 mode hdi ## 3 theta_Reginald - theta_Tony 0.336 -0.0612 0.680 0.95 mode hdi In this context, the mode_hdi() summary yields: key (i.e., the name we used to denote the parameters) value (i.e., the value of the measure of central tendency) .lower (i.e., the lower level of the 95% HDI) .upper (i.e., the upper level…) .width (i.e., what interval we used) .point (i.e., the type of measure of central tendency) .interval (i.e., the type of interval) 8.5 Sampling from the prior distribution in JAGS brms The sample_prior = T argument in our brm() code allowed us to extract prior samples with the well-named prior_samples() function. prior &lt;- prior_samples(fit8.2) head(prior) ## b_sReginald b_sTony ## 1 0.3450630 0.1271459 ## 2 0.6925652 0.4352422 ## 3 0.4466195 0.2385522 ## 4 0.4877868 0.6839308 ## 5 0.5405698 0.5565150 ## 6 0.2326884 0.5445573 With prior in hand, we’re almost ready to make the prior histograms of Figure 8.7. But first we’ll want to determine the \\(z/N\\) values in order to mark them off in the plots. [You’ll note Kruschke did so with gray plus marks in his.] my_data %&gt;% group_by(s) %&gt;% summarise(z = sum(y), N = n()) %&gt;% mutate(`z/N` = z / N) ## # A tibble: 2 x 4 ## s z N `z/N` ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Reginald 6 8 0.75 ## 2 Tony 2 7 0.286 d_line &lt;- tibble(value = c(.75, .286, .75 - .286), key = factor(c(&quot;theta_Reginald&quot;, &quot;theta_Tony&quot;, &quot;theta_Reginald - theta_Tony&quot;), levels = c(&quot;theta_Reginald&quot;, &quot;theta_Tony&quot;, &quot;theta_Reginald - theta_Tony&quot;))) Behold the histograms of Figure 8.7. prior %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% mutate(`theta_Reginald - theta_Tony` = theta_Reginald - theta_Tony) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;theta_Reginald&quot;, &quot;theta_Tony&quot;, &quot;theta_Reginald - theta_Tony&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + geom_vline(data = d_line, aes(xintercept = value), color = &quot;white&quot;, size = 1) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 35, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme_grey() + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;) Here’s how to make the scatter plot. prior %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% ggplot(aes(x = theta_Reginald, y = theta_Tony)) + geom_point(alpha = 1/4, color = &quot;grey50&quot;) + coord_equal() + theme(panel.grid = element_blank()) Or you could always use a two-dimensional density plot with stat_density_2d(). prior %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% ggplot(aes(x = theta_Reginald, y = theta_Tony)) + stat_density_2d(aes(fill = stat(density)), geom = &quot;raster&quot;, contour = F) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(x = expression(theta[1]), y = expression(theta[2])) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) 8.6 Probability distributions available in JAGS brms [brms] has a large collection of frequently used probability distributions that are built-in. These distributions include the beta, gamma, normal, Bernoulli, and binomial along with many others. A complete list of distributions, and their [brms] names, can be found in [Bürkner’s (2020h) vignette Parameterization of response distributions in brms]. (Kruschke, 2015, pp. 213–214, emphasis added) 8.6.1 Defining new likelihood functions. In addition to all the likelihood functions listed in above mentioned vignette, you can also make your own likelihood functions. Bürkner explained the method in his (2020b) vignette, Define custom response distributions with brms. 8.7 Faster sampling with parallel processing in runjags brms::brm() We don’t need to open another package to sample in parallel in brms. In fact, we’ve already been doing that. Take another look at the code use used for the last model, fit8.2. fit8.2 &lt;- brm(data = my_data, family = bernoulli(identity), y ~ 0 + s, prior = c(prior(beta(2, 2), class = b, coef = sReginald), prior(beta(2, 2), class = b, coef = sTony)), iter = 6000, warmup = 5000, cores = 4, chains = 4, sample_prior = T, control = list(adapt_delta = .999), seed = 8, file = &quot;fits/fit08.02&quot;) See the cores = 4, chains = 4 arguments? With that bit of code, we told brms::brm() we wanted 4 chains, which we ran in parallel across 4 cores. 8.8 Tips for expanding JAGS brms models I’m in complete agreement with Kruschke, here: Often, the process of programming a model is done is stages, starting with a simple model and then incrementally incorporating complexifications. At each step, the model is checked for accuracy and efficiency. This procedure of incremental building is useful for creating a desired complex model from scratch, for expanding a previously created model for a new application, and for expanding a model that has been found to be inadequate in a posterior predictive check. (p. 218) Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.3.9000 bayesplot_1.7.1 brms_2.12.0 Rcpp_1.0.4.6 ## [5] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 ## [9] readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ggplot2_3.3.0 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 ## [9] farver_2.0.3 rstan_2.19.3 svUnit_1.0.3 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 ## [17] bridgesampling_1.0-0 knitr_1.28 shinythemes_1.1.2 jsonlite_1.6.1 ## [21] broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 compiler_3.6.3 ## [25] httr_1.4.1 backports_1.1.6 assertthat_0.2.1 Matrix_1.2-18 ## [29] fastmap_1.0.1 cli_2.0.2 later_1.0.0 htmltools_0.4.0 ## [33] prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 cellranger_1.1.0 ## [41] vctrs_0.3.0 nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ## [45] ps_1.3.3 rvest_0.3.5 mime_0.9 miniUI_0.1.1.1 ## [49] lifecycle_0.2.0 gtools_3.8.2 MASS_7.3-51.5 zoo_1.8-7 ## [53] scales_1.1.1 colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [57] Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 loo_2.2.0 StanHeaders_2.21.0-1 ## [65] stringi_1.4.6 dygraphs_1.1.1.6 pkgbuild_1.0.8 rlang_0.4.6 ## [69] pkgconfig_2.0.3 matrixStats_0.56.0 HDInterval_0.2.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 ## [77] tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 magrittr_1.5 ## [81] bookdown_0.18 R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [85] pillar_1.4.4 haven_2.2.0 withr_2.2.0 xts_0.12-0 ## [89] abind_1.4-5 modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 ## [93] utf8_1.1.4 rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 ## [97] callr_3.4.3 threejs_0.3.3 reprex_0.3.0 digest_0.6.25 ## [101] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 ## [105] viridisLite_0.3.0 shinyjs_1.1 References "],
["hierarchical-models.html", "9 Hierarchical Models 9.1 A single coin from a single mint 9.2 Multiple coins from a single mint 9.3 Shrinkage in hierarchical models 9.4 Speeding up JAGS brms 9.5 Extending the hierarchy: Subjects within categories Session info", " 9 Hierarchical Models As Kruschke put it, “There are many realistic situations that involve meaningful hierarchical structure. Bayesian modeling software makes it straightforward to specify and analyze complex hierarchical models” (2015, p. 221). IMO, brms makes it even easier than JAGS. Further down, we read: The parameters at different levels in a hierarchical model are all merely parameters that coexist in a joint parameter space. We simply apply Bayes’ rule to the joint parameter space, as we did for example when estimating two coin biases back in Figure 7.5, p. 167. To say it a little more formally with our parameters \\(\\theta\\) and \\(\\omega\\), Bayes’ rule applies to the joint parameter space: \\(p(\\theta, \\omega | D) \\propto p(D | \\theta, \\omega) p(\\theta, \\omega)\\). What is special to hierarchical models is that the terms on the right-hand side can be factored into a chain of dependencies, like this: \\[\\begin{align*} p(\\theta, \\omega | D) &amp; \\propto p(D | \\theta, \\omega) \\; p(\\theta, \\omega) \\\\ &amp; = p(D | \\theta) \\; p(\\theta | \\omega) \\; p(\\omega) \\end{align*}\\] The refactoring in the second line means that the data depend only on the value of \\(\\theta\\), in the sense that when the value \\(\\theta\\) is set then the data are independent of all other parameter values. Moreover, the value of \\(\\theta\\) depends on the value of \\(\\omega\\) and the value of \\(\\theta\\) is conditionally independent of all other parameters. Any model that can be factored into a chain of dependencies like [this] is a hierarchical model. (pp. 222–223) 9.1 A single coin from a single mint Recall from the last chapter that our likelihood is the Bernoulli distribution, \\[y_i \\sim \\operatorname{Bernoulli} (\\theta).\\] We’ll use the beta density for our prior distribution for \\(\\theta\\), \\[\\theta \\sim \\operatorname{beta} (\\alpha, \\beta).\\] And we can re-express \\(\\alpha\\) and \\(\\beta\\) in terms of the mode \\(\\omega\\) and concentration \\(\\kappa\\), such that \\[\\alpha = \\omega(\\kappa - 2) + 1 \\;\\;\\; \\textrm{and} \\;\\;\\; \\beta = (1 - \\omega)(\\kappa - 2) + 1.\\] As a consequence, we can re-express \\(\\theta\\) as \\[\\theta \\sim \\operatorname{beta} (\\omega(\\kappa - 2) + 1, (1 - \\omega)(\\kappa - 2) + 1).\\] On page 224, Kruschke wrote: “The value of \\(\\kappa\\) governs how near \\(\\theta\\) is to \\(\\omega\\), with larger values of \\(\\kappa\\) generating values of \\(\\theta\\) more concentrated near \\(\\omega\\).” To give a sense of that, we’ll simulate 20 beta distributions, all with \\(\\omega = .25\\) but with \\(\\theta\\) increasing from 10 to 200, by 10. We’ll then plot them with a little help from the ggridges package (Wilke, 2020). library(tidyverse) library(ggridges) beta_by_k &lt;- function(k) { w &lt;- .25 tibble(x = seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(theta = dbeta(x = x, shape1 = w * (k - 2) + 1, shape2 = (1 - w) * (k - 2) + 1)) } tibble(k = seq(from = 10, to = 200, by = 10)) %&gt;% mutate(theta = map(k, beta_by_k)) %&gt;% unnest(theta) %&gt;% ggplot(aes(x = x, y = k, height = theta, group = k, fill = k)) + geom_vline(xintercept = c(0, .25, .5), color = &quot;grey85&quot;, size = 1/2) + geom_ridgeline(size = 1/5, color = &quot;grey92&quot;, scale = 2) + scale_fill_viridis_c(expression(kappa), option = &quot;A&quot;) + scale_y_continuous(expression(kappa), breaks = seq(from = 10, to = 200, by = 10)) + xlab(expression(theta)) + theme(panel.grid = element_blank()) Holding \\(\\omega\\) constant, the density gets more concentrated around \\(\\omega\\) as \\(\\kappa\\) increases. But back to the text&quot; “Now we make the essential expansion of our scenario into the realm of hierarchical models. Instead of thinking of \\(\\omega\\) as fixed by prior knowledge, we think of it as another parameter to be estimated” (p. 224). 9.1.1 Posterior via grid approximation. When the parameters extend over a finite domain, and there are not too many of them, then we can approximate the posterior via grid approximation. In our present situation, we have the parameters \\(\\theta\\) and \\(\\omega\\) that both have finite domains, namely the interval \\([0, 1]\\). Therefore, a grid approximation is tractable and the distributions can be readily graphed. (p. 226) Given \\(\\alpha\\) and \\(\\beta\\), we can compute the corresponding mode \\(\\omega\\). To foreshadow, consider \\(\\text{beta} (2, 2)\\). alpha &lt;- 2 beta &lt;- 2 (alpha - 1) / (alpha + beta - 2) ## [1] 0.5 That is, the mode of \\(\\operatorname{beta} (2, 2)\\) is \\(.5\\). We won’t be able to make the wireframe plots on the left of Figure 9.2, but we can make the others. We’ll make the initial data following Kruschke’s (p. 226) formulas. \\[p(\\theta, \\omega) = p(\\theta | \\omega) \\; p(\\omega) = \\operatorname{beta} (\\theta | \\omega (100 - 2) + 1, (1 - \\omega) (100 - 2) + 1) \\; \\operatorname{beta} (\\omega | 2, 2)\\] First, we’ll make a custom function, make_prior() based on the formulas. make_prior &lt;- function(theta, omega, alpha, beta, kappa) { # p(theta | omega) t &lt;- dbeta(x = theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) # p(omega) o &lt;- dbeta(x = omega, shape1 = alpha, shape2 = beta) # p(theta, omega) = p(theta | omega) * p(omega) return(t * o) } Next we’ll define the parameter space as a tightly-spaced sequence of values ranging from 0 to 1. parameter_space &lt;- seq(from = 0, to = 1, by = .01) Now we’ll use parameter_space to define the ranges for the two variables, theta and omega, which we’ll save in a tibble. We’ll then sequentially feed those theta and omega values into our make_prior() while manually specifying the desired values for alpha, beta, and kappa. d &lt;- # here we define the grid for our grid approximation crossing(theta = parameter_space, omega = parameter_space) %&gt;% # compute the joint prior mutate(prior = make_prior(theta, omega, alpha = 2, beta = 2, kappa = 100)) %&gt;% # convert the prior from the density metric to the probability metric mutate(prior = prior / sum(prior)) head(d) ## # A tibble: 6 x 3 ## theta omega prior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 ## 2 0 0.01 0 ## 3 0 0.02 0 ## 4 0 0.03 0 ## 5 0 0.04 0 ## 6 0 0.05 0 Now we’re ready to plot the top middle panel of Figure 9.2. d %&gt;% ggplot(aes(x = theta, y = omega, fill = prior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Note, you could also make this with geom_tile(). But geom_raster() with the interpolate = T argument smooths the color transitions. If we collapse “the joint prior across \\(\\theta\\)” (i.e., group_by(omega) and then sum(prior)), we plot the marginal distribution for \\(p(\\omega)\\) as seen in the top right panel. d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(Marginal~p(omega))) + coord_flip(ylim = c(0, .03)) + theme(panel.grid = element_blank()) We’ll follow a similar procedure to get the marginal probability distribution for theta. d %&gt;% group_by(theta) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(Marginal~p(theta))) + coord_cartesian(ylim = c(0, .03)) + theme(panel.grid = element_blank()) We’ll use the filter() function to take the two slices from the posterior grid. Since we’re taking slices, we’re no longer working with the joint probability distribution. As such, our two marginal prior distributions for theta no longer sum to 1, which means they’re no longer in a probability metric. No worries. After we group by omega, we can simply divide prior by the sum() of prior which renormalizes the two slices “so that they are individually proper probability densities that sum to 1.0 over \\(\\theta\\)” (p. 226). d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(prior = prior / sum(prior)) %&gt;% mutate(label = factor(str_c(&quot;omega == &quot;, omega), levels = c(&quot;omega == 0.75&quot;, &quot;omega == 0.25&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(p(theta*&quot;|&quot;*omega))) + coord_cartesian() + theme(panel.grid = element_blank()) + facet_wrap(~label, ncol = 1, labeller = label_parsed) As Kruschke pointed out at the top of page 228, these are indeed beta densities. Here’s proof. # we&#39;ll want this for the annotation text &lt;- tibble(theta = c(.75, .25), y = 10.5, label = c(&quot;beta(74.5, 25.5)&quot;, &quot;beta(25.5, 74.5)&quot;), omega = letters[1:2]) # here&#39;s the primary data for the plot tibble(theta = rep(parameter_space, times = 2), alpha = rep(c(74.5, 25.5), each = 101), beta = rep(c(25.5, 74.5), each = 101), omega = rep(letters[1:2], each = 101)) %&gt;% # the plot ggplot(aes(x = theta, fill = omega)) + geom_ribbon(aes(ymin = 0, ymax = dbeta(x = theta, shape1 = alpha, shape2 = beta))) + geom_text(data = text, aes(y = y, label = label, color = omega)) + scale_fill_viridis_d(option = &quot;A&quot;, begin = .3, end = .7) + scale_color_viridis_d(option = &quot;A&quot;, begin = .3, end = .7) + labs(x = expression(theta), y = &quot;density&quot;) + coord_cartesian(ylim = c(0, 12)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) But back on track, we need the Bernoulli likelihood function for the lower three rows of Figure 9.2. bernoulli_likelihood &lt;- function(theta, data) { n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } Time to feed theta and our data into the bernoulli_likelihood() function, which will allow us to make the 2-dimensional density plot in the middle of Figure 9.2. # define the data n &lt;- 12 z &lt;- 9 trial_data &lt;- rep(0:1, times = c(n - z, z)) # compute the likelihood d &lt;- d %&gt;% mutate(likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) # plot d %&gt;% ggplot(aes(x = theta, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Note how this plot demonstrates how the likelihood is solely dependent on \\(\\theta\\); it’s orthogonal to \\(\\omega\\). This is the visual consequence of Kruschke’s Formula 9.6, \\[\\begin{align*} p (\\theta, \\omega | y) &amp; = \\frac{p (y | \\theta, \\omega) \\; p (\\theta, \\omega)}{p (y)} \\\\ &amp; = \\frac{p (y | \\theta) \\; p (\\theta | \\omega) \\; p (\\omega)}{p (y)}. \\end{align*}\\] That is, in the second line of the equation, the probability of \\(y\\) was only conditional on \\(\\theta\\). But the reason we call this a hierarchical model is because the probability of \\(\\theta\\) itself is conditioned on \\(\\omega\\). The prior itself had a prior. From Formula 9.1, the posterior \\(p(\\theta, \\omega | D)\\) is proportional to \\(p(D | \\theta) \\; p(\\theta | \\omega) \\; p(\\omega)\\). Divide that by the normalizing constant and we’ll have it in a proper probability metric. Recall that we’ve already saved the results of \\(p(\\theta | \\omega) \\; p(\\omega)\\) in the prior column. So we just need to multiply prior by likelihood and divide by their sum. Our first depiction will be the middle panel of the second row from the bottom. d &lt;- d %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) d %&gt;% ggplot(aes(x = theta, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Although the likelihood was orthogonal to \\(\\omega\\), conditioning the prior for \\(\\theta\\) on \\(\\omega\\) resulted in a posterior that was conditioned on both \\(\\theta\\) and \\(\\omega\\). Making the marginal plots for posterior is much like when making them for prior, above. # for omega d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(Marginal~p(omega*&quot;|&quot;*D))) + coord_flip() + theme(panel.grid = element_blank()) # for theta d %&gt;% group_by(theta) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(Marginal~p(theta*&quot;|&quot;*D))) + coord_cartesian() + theme(panel.grid = element_blank()) Note that after we slice with filter(), the next two wrangling lines renormalize those posterior slices into probability metrics. That is, when we take a slice through the joint posterior at a particular value of \\(\\omega\\), and renormalize by dividing the sum of discrete probability masses in that slice, we get the conditional distribution \\(p(\\theta | \\omega, D)\\). d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(posterior = posterior / sum(posterior)) %&gt;% mutate(label = factor(str_c(&quot;omega == &quot;, omega), levels = c(&quot;omega == 0.75&quot;, &quot;omega == 0.25&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(p(theta*&quot;|&quot;*omega))) + theme(panel.grid = element_blank()) + facet_wrap(~label, ncol = 1, labeller = label_parsed, scales = &quot;free&quot;) To repeat the process for Figure 9.3, we’ll first compute the new joint prior. d &lt;- crossing(theta = parameter_space, omega = parameter_space) %&gt;% mutate(prior = make_prior(theta, omega, alpha = 20, beta = 20, kappa = 6)) %&gt;% mutate(prior = prior / sum(prior)) Here’s the initial data and the 2-dimensional density plot for the prior, the middle plot in the top row of Figure 9.3. d %&gt;% ggplot(aes(x = theta, y = omega, fill = prior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) That higher certainty in \\(\\omega\\) resulted in a two-dimensional density plot where the values on the y-axis were concentrated near .5. This will have down-the-road consequences for the posterior. But before we get there, we’ll average over omega and theta to plot their marginal prior distributions. # for omega d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(Marginal~p(omega))) + coord_flip() + theme(panel.grid = element_blank()) # for theta d %&gt;% group_by(theta) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(Marginal~p(theta))) + coord_cartesian(ylim = c(0, .03)) + theme(panel.grid = element_blank()) Here are the two short plots in the right panel of the second row from the top of Figure 9.3. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(prior = prior / sum(prior)) %&gt;% mutate(label = factor(str_c(&quot;omega == &quot;, omega), levels = c(&quot;omega == 0.75&quot;, &quot;omega == 0.25&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(p(theta*&quot;|&quot;*omega))) + coord_cartesian(ylim = c(0, .0375)) + theme(panel.grid = element_blank()) + facet_wrap(~label, ncol = 1, labeller = label_parsed) Now we’re ready for the likelihood. # compute d &lt;- d %&gt;% mutate(likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) # plot d %&gt;% ggplot(aes(x = theta, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Now on to the posterior. Our first depiction will be the middle panel of the second row from the bottom of Figure 9.3. This will be \\(p(\\theta, \\omega | y)\\). # compute the posterior d &lt;- d %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) # plot d %&gt;% ggplot(aes(x = theta, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Here are the marginal plots for the two dimensions in our posterior. # for omega d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(Marginal~p(omega*&quot;|&quot;*D))) + coord_flip() + theme(panel.grid = element_blank()) # for theta d %&gt;% group_by(theta) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(Marginal~p(theta*&quot;|&quot;*D))) + coord_cartesian() + theme(panel.grid = element_blank()) And we’ll finish off with the plots of Figure 9.3’s lower right panel. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(posterior = posterior / sum(posterior)) %&gt;% mutate(label = factor(str_c(&quot;omega == &quot;, omega), levels = c(&quot;omega == 0.75&quot;, &quot;omega == 0.25&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(p(theta*&quot;|&quot;*omega))) + coord_cartesian(ylim = c(0, .0375)) + theme(panel.grid = element_blank()) + facet_wrap(~label, ncol = 1, labeller = label_parsed, scales = &quot;free&quot;) In summary, Bayesian inference in a hierarchical model is merely Bayesian inference on a joint parameter space, but we look at the joint distribution (e.g., \\(p(\\theta, \\omega)\\)) in terms of its marginal on a subset of parameters (e.g., \\(p(\\omega)\\)) and its conditional distribution for other parameters (e.g., p\\(p(\\theta | \\omega)\\). We do this primarily because it is meaningful in the context of particular models. (p. 230) 9.2 Multiple coins from a single mint What if we collect data from more than one coin created by the mint? If each coin has its own distinct bias \\(\\theta_s\\), then we are estimating a distinct parameter value for each coin, and using all the data to estimate \\(\\omega\\). (p. 230) 9.2.1 Posterior via grid approximation. Now we have two coins, the full prior distribution is a joint distribution over three parameters: \\(\\omega\\), \\(\\theta_1\\), and \\(\\theta_2\\). In a grid approximation, the prior is specified as a three-dimensional (3D) array that holds the prior probability at various grid points in the 3D space. (p. 233) So we’re going to have to update our make_prior() function. It was originally designed to handle two dimensions, \\(\\theta\\) and \\(\\omega\\). But now we have to update it to handle our three dimensions. make_prior &lt;- function(theta1, theta2, omega, alpha, beta, kappa) { # p(theta_1 | omega) t1 &lt;- dbeta(x = theta1, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) # p(theta_2 | omega) t2 &lt;- dbeta(x = theta2, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) # p(omega) o &lt;- dbeta(x = omega, shape1 = alpha, shape2 = beta) # p(theta1, theta2, omega) = p(theta1 | omega) * p(theta2 | omega) * p(omega) return(t1 * t2 * o) } Let’s make our new data object, d. d &lt;- crossing(theta_1 = parameter_space, theta_2 = parameter_space, omega = parameter_space) %&gt;% mutate(prior = make_prior(theta_1, theta_2, omega, alpha = 2, beta = 2, kappa = 5)) %&gt;% # here we normalize mutate(prior = prior / sum(prior)) glimpse(d) ## Rows: 1,030,301 ## Columns: 4 ## $ theta_1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ theta_2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ omega &lt;dbl&gt; 0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0… ## $ prior &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… Unlike what Kruschke said in the text, we’re not using a 3D data array. Rather, we’re just using a tibble with which prior has been expanded across all possible dimensions of the three indexing variables: theta_1, theta_2, and omega. As you can see from the ‘Observations’ count, above, this makes for a very long tibble. “Because the parameter space is 3D, a distribution on it cannot easily be displayed on a 2D page. Instead, Figure 9.5 shows various marginal distributions” (p. 234). The consequence of that is when we marginalize, we’ll have to group by the two variables we’d like to retain for the plot. For example, the plots in the left and middle columns of the top row are the same save for their indices. So let’s just do the plot for theta_1. In order to marginalize over theta_2, we’ll need to group_by(theta_1, omega) and then summarise(prior = sum(prior)). d %&gt;% group_by(theta_1, omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = prior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + coord_equal() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) But we just have to average over omega and theta_1 to plot their marginal prior distributions. # for omega d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(p(omega))) + coord_flip(ylim = c(0, .04)) + theme(panel.grid = element_blank()) # for theta d %&gt;% group_by(theta_1) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[1]), y = expression(p(theta[1]))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) Before we make the plots in the middle row of Figure 9.5, we need to add the likelihoods. Recall that we’re presuming the coin flips contained in \\(D_1\\) and \\(D_2\\) are independent. Kruschke explained in Chapter 7, section 4.1, that independence of the data across the two coins means that the data from coin 1 depend only on the bias in coin 1, and the data from coin 2 depend only on the bias in coin 2, which can be expressed formally as \\(p(y_1 | \\theta_1, \\theta_2) = p(y_1 | \\theta_1)\\) and \\(p(y_2 | \\theta_1, \\theta_2) = p(y_2 | \\theta_2)\\). (p. 164) The likelihood function for our two series of coin flips is then \\[p(D | \\theta_1, \\theta_2) = \\Big ( \\theta_1^{z_1} (1 - \\theta_1) ^ {N_1 - z_1} \\Big ) \\Big ( \\theta_2^{z_2} (1 - \\theta_2) ^ {N_2 - z_2} \\Big ).\\] The upshot is we can compute the likelihoods for \\(D_1\\) and \\(D_2\\) separately and just multiply them together. # D1: 3 heads, 12 tails n &lt;- 15 z &lt;- 3 trial_data_1 &lt;- rep(0:1, times = c(n - z, z)) # D2: 4 heads, 1 tail n &lt;- 5 z &lt;- 4 trial_data_2 &lt;- rep(0:1, times = c(n - z, z)) d &lt;- d %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_1, data = trial_data_1), likelihood_2 = bernoulli_likelihood(theta = theta_2, data = trial_data_2)) %&gt;% mutate(likelihood = likelihood_1 * likelihood_2) head(d) ## # A tibble: 6 x 7 ## theta_1 theta_2 omega prior likelihood_1 likelihood_2 likelihood ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 0 0 ## 2 0 0 0.01 0 0 0 0 ## 3 0 0 0.02 0 0 0 0 ## 4 0 0 0.03 0 0 0 0 ## 5 0 0 0.04 0 0 0 0 ## 6 0 0 0.05 0 0 0 0 Now after a little group_by() followed by summarise() we can plot the two marginal likelihoods, the two plots in the middle row of Figure 9.5. # likelihood_1 d %&gt;% group_by(theta_1, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + coord_equal() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) # likelihood_2 d %&gt;% group_by(theta_2, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[2]), y = expression(omega)) + coord_equal() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) The likelihoods look good. Next we compute the posterior in the same way we’ve done before: multiply the prior and the likelihood and then divide by their sum in order to convert the results to a probability metric. # compute d &lt;- d %&gt;% mutate(posterior = (prior * likelihood) / sum(prior * likelihood)) # posterior_1 d %&gt;% group_by(theta_1, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + coord_equal() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) # posterior_2 d %&gt;% group_by(theta_2, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[2]), y = expression(omega)) + coord_equal() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Here’s the right plot on the second row from the bottom, the posterior distribution for \\(\\omega\\). # for omega d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(p(omega*&quot;|&quot;*D))) + coord_flip(ylim = c(0, .04)) + theme(panel.grid = element_blank()) Now here are the marginal posterior plots on the bottom row of Figure 9.5. # for theta_1 d %&gt;% group_by(theta_1) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[1]), y = expression(p(theta[1]*&quot;|&quot;*D))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) # for theta_2 d %&gt;% group_by(theta_2) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[2]), y = expression(p(theta[2]*&quot;|&quot;*D))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) We’ll do this dog and pony one more time for Figure 9.6, which uses different priors on the same data. First, we make our new data object, d. d &lt;- crossing(theta_1 = parameter_space, theta_2 = parameter_space, omega = parameter_space) %&gt;% mutate(prior = make_prior(theta_1, theta_2, omega, alpha = 2, beta = 2, kappa = 75)) %&gt;% mutate(prior = prior / sum(prior)) Note how the only thing we changed from the last time was increasing kappa to 75. Also like last time, the plots in the left and middle columns of the top row are the same save for their indices. But unlike last time, we’ll make both in preparation for a grand plotting finale. You’ll see. p11 &lt;- d %&gt;% group_by(theta_1, omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = prior)) + geom_raster(interpolate = T) + annotate(geom = &quot;text&quot;, x = .05, y = .925, label = &quot;p(list(theta[1], omega))&quot;, parse = T, color = &quot;grey92&quot;, hjust = 0) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) p12 &lt;- d %&gt;% group_by(theta_2, omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = prior)) + geom_raster(interpolate = T) + annotate(geom = &quot;text&quot;, x = .05, y = .925, label = &quot;p(list(theta[2], omega))&quot;, parse = T, color = &quot;grey92&quot;, hjust = 0) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[2]), y = expression(omega)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) p11 p12 Now we’ll average over omega and theta to plot their marginal prior distributions. # for omega p13 &lt;- d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(p(omega))) + coord_flip(ylim = c(0, .04)) + theme(panel.grid = element_blank()) # for theta_1 p21 &lt;- d %&gt;% group_by(theta_1) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[1]), y = expression(p(theta[1]))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) # for theta_2 p22 &lt;- d %&gt;% group_by(theta_2) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_2, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[2]), y = expression(p(theta[2]))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) p13 p21 p22 Let’s get those likelihoods in there and plot. # D1: 3 heads, 12 tails n &lt;- 15 z &lt;- 3 trial_data_1 &lt;- rep(0:1, times = c(n - z, z)) # D2: 4 heads, 1 tail n &lt;- 5 z &lt;- 4 trial_data_2 &lt;- rep(0:1, times = c(n - z, z)) # compute the likelihoods d &lt;- d %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_1, data = trial_data_1), likelihood_2 = bernoulli_likelihood(theta = theta_2, data = trial_data_2)) %&gt;% mutate(likelihood = likelihood_1 * likelihood_2) # plot likelihood_1 p31 &lt;- d %&gt;% group_by(theta_1, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) # plot likelihood_2 p32 &lt;- d %&gt;% group_by(theta_2, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[2]), y = expression(omega)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) p31 p32 Compute the posterior and make the left and middle plots of the second row to the bottom of Figure 9.6. d &lt;- d %&gt;% mutate(posterior = (prior * likelihood) / sum(prior * likelihood)) # posterior_1 p41 &lt;- d %&gt;% group_by(theta_1, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = posterior)) + geom_raster(interpolate = T) + annotate(geom = &quot;text&quot;, x = .05, y = .925, label = expression(p(list(theta[1], omega)*&quot;|&quot;*D)), parse = T, color = &quot;grey92&quot;, hjust = 0) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) # posterior_2 p42 &lt;- d %&gt;% group_by(theta_2, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = posterior)) + geom_raster(interpolate = T) + annotate(geom = &quot;text&quot;, x = .05, y = .925, label = expression(p(list(theta[2], omega)*&quot;|&quot;*D)), parse = T, color = &quot;grey92&quot;, hjust = 0) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[2]), y = expression(omega)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) p41 p42 Here’s the right plot on the same row, the posterior distribution for \\(\\omega\\). # for omega p43 &lt;- d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(p(omega*&quot;|&quot;*D))) + coord_flip(ylim = c(0, .04)) + theme(panel.grid = element_blank()) p43 Finally, here are the marginal posterior plots on the bottom row of Figure 9.6. # for theta_1 p51 &lt;- d %&gt;% group_by(theta_1) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[1]), y = expression(p(theta[1]*&quot;|&quot;*D))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) # for theta_2 p52 &lt;- d %&gt;% group_by(theta_2) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[2]), y = expression(p(theta[2]*&quot;|&quot;*D))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) p51 p52 Did you notice how we saved each of plot from this last batch as objects? For our grand finale for this subsection, we’ll be stitching all those subplots together using syntax from the patchwork package. But before we do, we need to define three more subplots: the subplots with the annotation. text &lt;- tibble(x = 1, y = 10:8, label = c(&quot;Prior&quot;, &quot;list(A[omega]==2, B[omega]==2)&quot;, &quot;K==75&quot;), size = c(2, 1, 1)) p23 &lt;- text %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(aes(size = size), parse = T, hjust = 0, show.legend = F) + scale_size_continuous(range = c(3.5, 5.5)) + coord_cartesian(xlim = c(1, 2), ylim = c(4, 11)) + theme(axis.text = element_text(color = &quot;white&quot;), axis.ticks = element_blank(), panel.grid = element_blank(), text = element_text(color = &quot;white&quot;)) text &lt;- tibble(x = 1, y = 10:8, label = c(&quot;Likelihood&quot;, &quot;D1: 3 heads, 12 tails&quot;, &quot;D2: 4 heads, 1 tail&quot;), size = c(2, 1, 1)) p33 &lt;- text %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(aes(size = size), hjust = 0, show.legend = F) + scale_size_continuous(range = c(3.5, 5.5)) + coord_cartesian(xlim = c(1, 2), ylim = c(4, 11)) + theme(axis.text = element_text(color = &quot;white&quot;), axis.ticks = element_blank(), panel.grid = element_blank(), text = element_text(color = &quot;white&quot;)) p53 &lt;- ggplot() + annotate(geom = &quot;text&quot;, x = 1, y = 10, label = &quot;Posterior&quot;, size = 6, hjust = 0) + coord_cartesian(xlim = c(1, 2), ylim = c(3, 11)) + theme(axis.text = element_text(color = &quot;white&quot;), axis.ticks = element_blank(), panel.grid = element_blank(), text = element_text(color = &quot;white&quot;)) Okay, let’s make the full version of Figure 9.6. library(patchwork) (p11 / p21 / p31 / p41 / p51) | (p12 / p22 / p32 / p42 / p52) | (p13 / p23 / p33 / p43 / p53) Oh mamma! The grid approximation displayed in Figures 9.5 and 9.6 used combs of only [101] points on each parameter (\\(\\omega\\), \\(\\theta_1\\), and \\(\\theta_2\\)). This means that the 3D grid had [1013 = 1,030,301] points, which is a size that can be handled easily on an ordinary desktop computer of the early 21st century. It is interesting to remind ourselves that the grid approximation displayed in Figures 9.5 and 9.6 would have been on the edge of computability 50 years ago, and would have been impossible 100 years ago. The number of points in a grid approximation can get hefty in a hurry. If we were to expand the example by including a third coin, with its parameter \\(\\theta_3\\), then the grid would have [1014 = 104,060,401] points, which already strains small computers. Include a fourth coin, and the grid contains over [10 billion] points. Grid approximation is not a viable approach to even modestly large problems, which we encounter next. (p. 235) In case you didn’t catch it, we used different numbers of points to evaluate each parameter. Whereas Kruschke indicated in the text he only used 50, we used 101. That value of 101 came from how we defined our parameter_space with the code seq(from = 0, to = 1, by = .01). The reason we used a more densely-packed parameter space was to get smoother-looking 2D density plots. 9.2.2 A realistic model with MCMC. “Because the value of \\(\\kappa − 2\\) must be non-negative, the prior distribution on \\(\\kappa − 2\\) must not allow negative values” (p. 237). Gamma is one of the distributions with that property. The gamma distribution is defined by two parameters, its shape and rate. To get a sense of how those play out, here’ a look at the gamma densities of Figure 9.8. # how many points do you want in your sequence of x values? length &lt;- 100 # wrangle tibble(shape = c(.01, 1.56, 1, 6.25), rate = c(.01, .0312, .02, .125)) %&gt;% expand(nesting(shape, rate), x = seq(from = 0, to = 200, length.out = length)) %&gt;% mutate(mean = shape * 1 / rate, sd = sqrt(shape * (1 / rate)^2)) %&gt;% mutate(label = str_c(&quot;shape = &quot;, shape, &quot;, rate = &quot;, rate, &quot;\\nmean = &quot;, mean, &quot;, sd = &quot;, sd)) %&gt;% # plot ggplot(aes(x = x, ymin = 0, ymax = dgamma(x = x, shape = shape, rate = rate))) + geom_ribbon(fill = &quot;grey67&quot;) + scale_y_continuous(breaks = c(0, .01, .02)) + coord_cartesian(xlim = c(0, 150)) + labs(x = expression(kappa), y = expression(p(kappa*&quot;|&quot;*s*&quot;,&quot;*r))) + theme(panel.grid = element_blank()) + facet_wrap(~label) You can find the formulas for the mean and \\(SD\\) for a given gamma distribution here. We used those formulas in the second mutate() statement for the data-prep stage of that last figure. Using \\(s\\) for shape and \\(r\\) for rate, Kruschke’s Equations 9.7 and 9.8 are as follows: \\[ s = \\frac{\\mu^2}{\\sigma^2} \\;\\;\\; \\text{and} \\;\\;\\; r = \\frac{\\mu}{\\sigma^2} \\;\\;\\; \\text{for mean} \\;\\;\\; \\mu &gt; 0 \\\\ s = 1 + \\omega r \\;\\;\\; \\text{where} \\;\\;\\; r = \\frac{\\omega + \\sqrt{\\omega^2 + 4\\sigma^2}}{2\\sigma^2} \\;\\;\\; \\text{for mode} \\;\\;\\; \\omega &gt; 0. \\] With those in hand, we can follow Kruschke’s DBDA2E-utilities.R file to make a couple convenience functions. gamma_s_and_r_from_mean_sd &lt;- function(mean, sd) { if (mean &lt;= 0) stop(&quot;mean must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) shape &lt;- mean^2 / sd^2 rate &lt;- mean / sd^2 return(list(shape = shape, rate = rate)) } gamma_s_and_r_from_mode_sd &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } They’re easy to put to use: gamma_s_and_r_from_mean_sd(mean = 10, sd = 100) ## $shape ## [1] 0.01 ## ## $rate ## [1] 0.001 gamma_s_and_r_from_mode_sd(mode = 10, sd = 100) ## $shape ## [1] 1.105125 ## ## $rate ## [1] 0.01051249 Here’s a more detailed look at the structure of their output. gamma_param &lt;- gamma_s_and_r_from_mode_sd(mode = 10, sd = 100) str(gamma_param) ## List of 2 ## $ shape: num 1.11 ## $ rate : num 0.0105 9.2.3 Doing it with JAGS brms. Unlike JAGS, the brms formula will not correspond as closely to the schematic in Figure 9.7. You’ll see in just a bit. 9.2.4 Example: Therapeutic touch. Load the data from the TherapeuticTouchData.csv file (see Rosa et al., 1998). my_data &lt;- read_csv(&quot;data.R/TherapeuticTouchData.csv&quot;) glimpse(my_data) ## Rows: 280 ## Columns: 2 ## $ y &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0… ## $ s &lt;chr&gt; &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01… Here are what the data look like: my_data %&gt;% mutate(y = y %&gt;% as.character()) %&gt;% ggplot(aes(x = y)) + geom_bar(aes(fill = stat(count))) + scale_y_continuous(breaks = seq(from = 0, to = 9, by = 3)) + scale_fill_viridis_c(option = &quot;A&quot;, end = .7) + coord_flip() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~s, ncol = 7) And here’s our Figure 9.9. my_data %&gt;% group_by(s) %&gt;% summarize(mean = mean(y)) %&gt;% ggplot(aes(x = mean)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = .1) + coord_cartesian(xlim = c(0, 1)) + labs(x = &quot;Proportion Correct&quot;, y = &quot;# Practitioners&quot;) + theme(panel.grid = element_blank()) Let’s open brms. library(brms) In applied statistics, the typical way to model a Bernoulli variable is with logistic regression. Instead of going through the pain of setting up a model in brms that mirrors the one in the text, I’m going to set up a hierarchical logistic regression model, instead. Note the family = bernoulli(link = logit) argument. In work-a-day regression with vanilla Gaussian variables, the prediction space is unbounded. But when we want to model the probability of a success for a Bernoulli variable (i.e., \\(\\theta\\)), we need to constrain the model to only produce predictions between 0 and 1. With logistic regression, we use a link function to do just that. The consequence is that instead of modeling the probability, \\(\\theta\\), we’re modeling the logit probability. In case you’re curious, the logit of \\(\\theta\\) follows the formula \\[\\operatorname{logit} (\\theta) = \\log \\big (\\theta/(1 - \\theta) \\big ).\\] But anyway, we’ll be doing logistic regression using the logit link. Kruschke will cover this in detail in Chapter 21. The next new part of our syntax is (1 | s). As in the popular frequentist lme4 package (Bates et al., 2015, 2020), you specify random effects or group-level parameters with the (|) syntax in brms. On the left side of the |, you tell brms what parameters you’d like to make random (i.e., vary by group). On the right side of the |, you tell brms what variable you want to group the parameters by. In our case, we want the intercepts to vary over the grouping variable s. fit9.1 &lt;- brm(data = my_data, family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9, file = &quot;fits/fit09.01&quot;) As it turns out, the \\(N(0, 1.5)\\) prior is flat in the probability space for the intercept in a logistic regression model. We’ll explore that a little further down. The \\(N(0, 1)\\) prior for the random effect is actually a half Normal. That’s because brms defaults to bound \\(SD\\) parameters to zero and above. The half Normal prior for a hierarchical \\(SD\\) parameter in a logistic regression model is weakly regularizing and is conservative in the sense that it presumes some pooling is preferable to no pooling. If you wanted to take a lighter approach, you might use something like a cauchy(0, 5), instead. See the prior wiki by the Stan team for more ideas on priors. Here are the trace plots and posterior densities of the main parameters. plot(fit9.1) The trace plots indicate no problems with convergence. We’ll need to extract the posterior samples and open the bayesplot package before we can examine the autocorrelations. post &lt;- posterior_samples(fit9.1, add_chain = T) library(bayesplot) One of the nice things about bayesplot is it returns ggplot2 objects. As such, we can amend their theme settings to be consistent with our other ggplot2 plots. The theme_set() function will make that particularly easy. And since I prefer to plot without gridlines, we’ll slip in a line on panel.grid to suppress them by default for the remainder of this chapter. theme_set(theme_grey() + theme(panel.grid = element_blank())) Now we’re ready for bayesplot::mcmc_acf(). mcmc_acf(post, pars = c(&quot;b_Intercept&quot;, &quot;sd_s__Intercept&quot;), lags = 10) It appears fit9.1 had very low autocorrelations. Here we’ll examine the \\(N_{eff}/N\\) ratio. neff_ratio(fit9.1) %&gt;% mcmc_neff() The \\(N_{eff}/N\\) ratio values for our model parameters were excellent. And if you really wanted them, you could get the parameter labels on the y-axis by tacking + yaxis_text() on at the end of the plot block. Here’s a numeric summary of the model. print(fit9.1) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + (1 | s) ## Data: my_data (Number of observations: 280) ## Samples: 4 chains, each with iter = 20000; warmup = 1000; thin = 10; ## total post-warmup samples = 7600 ## ## Group-Level Effects: ## ~s (Number of levels: 28) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.28 0.18 0.02 0.68 1.00 7438 7638 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.25 0.14 -0.53 0.02 1.00 7237 7507 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll need brms::inv_logit_scaled() to convert the model parameters to predict \\(\\theta\\) rather than \\(\\operatorname{logit} (\\theta)\\). After the conversions, we’ll be ready to make the histograms in the lower portion of Figure 9.10. library(tidybayes) post_small &lt;- post %&gt;% # convert the linter model to the probability space with `inv_logit_scaled()` mutate(`theta[1]` = (b_Intercept + `r_s[S01,Intercept]`) %&gt;% inv_logit_scaled(), `theta[14]` = (b_Intercept + `r_s[S14,Intercept]`) %&gt;% inv_logit_scaled(), `theta[28]` = (b_Intercept + `r_s[S28,Intercept]`) %&gt;% inv_logit_scaled()) %&gt;% # make the difference distributions mutate(`theta[1] - theta[14]` = `theta[1]` - `theta[14]`, `theta[1] - theta[28]` = `theta[1]` - `theta[28]`, `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) %&gt;% select(starts_with(&quot;theta&quot;)) post_small %&gt;% gather() %&gt;% # this line is unnecessary, but will help order the plots mutate(key = factor(key, levels = c(&quot;theta[1]&quot;, &quot;theta[14]&quot;, &quot;theta[28]&quot;, &quot;theta[1] - theta[14]&quot;, &quot;theta[1] - theta[28]&quot;, &quot;theta[14] - theta[28]&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 3) If you wanted the specific values of the posterior modes and 95% HDIs, you could execute this. post_small %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 6 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 theta[1] 0.431 0.206 0.521 0.95 mode hdi ## 2 theta[1] - theta[14] -0.003 -0.284 0.117 0.95 mode hdi ## 3 theta[1] - theta[28] -0.01 -0.414 0.068 0.95 mode hdi ## 4 theta[14] 0.426 0.282 0.578 0.95 mode hdi ## 5 theta[14] - theta[28] -0.004 -0.329 0.105 0.95 mode hdi ## 6 theta[28] 0.452 0.357 0.697 0.95 mode hdi And here are the Figure 9.10 scatter plots. p1 &lt;- post_small %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[14]`)) + geom_abline(linetype = 1, color = &quot;white&quot;) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) p2 &lt;- post_small %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[28]`)) + geom_abline(linetype = 1, color = &quot;white&quot;) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) p3 &lt;- post_small %&gt;% ggplot(aes(x = `theta[14]`, y = `theta[28]`)) + geom_abline(linetype = 1, color = &quot;white&quot;) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) (p1 + p2 + p3) &amp; coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) This is posterior distribution for the population estimate for \\(\\theta\\), which roughly corresponds to the upper right histogram of \\(\\omega\\) in Figure 9.10. # this part makes it easier to set the break points in `scale_x_continuous()` labels &lt;- post %&gt;% transmute(theta = b_Intercept %&gt;% inv_logit_scaled()) %&gt;% mode_hdi() %&gt;% select(theta:.upper) %&gt;% gather() %&gt;% mutate(label = value %&gt;% round(3) %&gt;% as.character) %&gt;% slice(1:3) post %&gt;% mutate(theta = b_Intercept %&gt;% inv_logit_scaled()) %&gt;% ggplot(aes(x = theta, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_x_continuous(expression(theta), breaks = labels$value, labels = labels$label) + scale_y_continuous(NULL, breaks = NULL) I’m not aware there’s a straight conversion to get \\(\\sigma\\) in a probability metric. As far as I can tell, you have to first use coef() to “extract [the] model coefficients, which are the sum of population-level effects and corresponding group-level effects” (Bürkner, 2020g, p. 46). With the model coefficient draws in hand, you can index them by posterior iteration, group them by that index, compute the iteration-level \\(SD\\)s, and then plot the distribution of the \\(SD\\)s. # the tibble of the primary data sigmas &lt;- coef(fit9.1, summary = F)$s %&gt;% as_tibble() %&gt;% mutate(iter = 1:n()) %&gt;% group_by(iter) %&gt;% gather(key, value, -iter) %&gt;% mutate(theta = inv_logit_scaled(value)) %&gt;% summarise(sd = sd(theta)) # this, again, is just to customize `scale_x_continuous()` labels &lt;- sigmas %&gt;% mode_hdi(sd) %&gt;% select(sd:.upper) %&gt;% gather() %&gt;% mutate(label = value %&gt;% round(3) %&gt;% as.character) %&gt;% slice(1:3) # the plot sigmas %&gt;% ggplot(aes(x = sd, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_x_continuous(expression(paste(sigma, &quot; of &quot;, theta, &quot; in a probability metric&quot;)), breaks = labels$value, labels = labels$label) + scale_y_continuous(NULL, breaks = NULL) And now you have a sense of how to do all those by hand, bayesplot::mcmc_pairs() offers a fairly quick way to get a good portion of Figure 9.10. color_scheme_set(&quot;gray&quot;) coef(fit9.1, summary = F)$s %&gt;% inv_logit_scaled() %&gt;% as_tibble() %&gt;% rename(`theta[1]` = S01.Intercept, `theta[14]` = S14.Intercept, `theta[28]` = S28.Intercept) %&gt;% select(`theta[1]`, `theta[14]`, `theta[28]`) %&gt;% mcmc_pairs(off_diag_args = list(size = 1/8, alpha = 1/8)) Did you see how we slipped in that color_scheme_set(&quot;gray&quot;) line? When we used theme_set(), earlier, that changed the global theme settings for our ggplot2 plots. The color_scheme_set() function is specific to bayesplot plots and it sets the color palette within them. Setting the color palette “gray” changed the colors depicted in the dots and bars of the mcmc_pairs()-based scatter plots and histograms, respectively. Kruschke used a \\(\\operatorname{Beta} (1, 1)\\) prior for \\(\\omega\\). If you randomly draw from that prior and plot a histogram, you’ll see it was flat. set.seed(1) tibble(prior = rbeta(n = 1e5, 1, 1)) %&gt;% ggplot(aes(x = prior)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = .05, boundary = 0) + scale_x_continuous(expression(omega), labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 1)) + theme(legend.position = &quot;none&quot;) You’ll note that plot corresponds to the upper right panel of Figure 9.11. Recall that we used a logistic regression model with a normal(0, 1.5) prior on the intercept. If you sample from normal(0, 1.5) and then convert the draws using brms::inv_logit_scaled(), you’ll discover that our normal(0, 1.5) prior was virtually flat on the probability scale. Here we’ll show the consequence of a variety of zero-mean Gaussian priors for the intercept of a logistic regression model: # define a function r_norm &lt;- function(i, n = 1e4) { set.seed(1) rnorm(n = n, mean = 0, sd = i) %&gt;% inv_logit_scaled() } # simulate and wrangle tibble(sd = seq(from = .25, to = 3, by = .25)) %&gt;% group_by(sd) %&gt;% mutate(prior = map(sd, r_norm)) %&gt;% unnest(prior) %&gt;% ungroup() %&gt;% mutate(sd = str_c(&quot;sd = &quot;, sd)) %&gt;% # plot! ggplot(aes(x = prior)) + geom_histogram(fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = .2, binwidth = .05, boundary = 0) + scale_x_continuous(labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 1)) + facet_wrap(~sd) It appears that as \\(\\sigma\\) goes lower than 1.25, the prior becomes increasingly regularizing, pulling the estimate for \\(\\theta\\) to a neutral .5. However, as the prior’s \\(\\sigma\\) gets larger than 1.25, more and more of the probability mass ends up at extreme values. Next, Kruschke examined the prior distribution. There are a few ways to do this. The one we’ll explore involved adding the sample_prior = &quot;only&quot; argument to the brm() function. When you do so, the results of the model are just the prior. That is, brm() leaves out the likelihood. This returns a bunch of samples from the prior predictive distribution. fit9.1_prior &lt;- brm(data = my_data, family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9, sample_prior = &quot;only&quot;, file = &quot;fits/fit09.01_prior&quot;) If we feed fit1_prior into the posterior_samples() function, we’ll get back a data frame of samples from the prior, but with the same parameter names we’d get from the posterior. prior &lt;- posterior_samples(fit9.1_prior) %&gt;% as_tibble() head(prior) ## # A tibble: 6 x 31 ## b_Intercept sd_s__Intercept `r_s[S01,Interc… `r_s[S02,Interc… `r_s[S03,Interc… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -2.15 1.36 1.18 -0.892 0.326 ## 2 -0.825 0.0112 -0.00256 -0.0330 0.00620 ## 3 0.299 0.315 -0.00208 -0.535 -0.269 ## 4 2.75 0.654 0.0715 0.821 0.0239 ## 5 0.350 0.352 -0.127 0.0561 0.142 ## 6 -3.23 0.433 -0.680 0.0825 -0.406 ## # … with 26 more variables: `r_s[S04,Intercept]` &lt;dbl&gt;, ## # `r_s[S05,Intercept]` &lt;dbl&gt;, `r_s[S06,Intercept]` &lt;dbl&gt;, ## # `r_s[S07,Intercept]` &lt;dbl&gt;, `r_s[S08,Intercept]` &lt;dbl&gt;, ## # `r_s[S09,Intercept]` &lt;dbl&gt;, `r_s[S10,Intercept]` &lt;dbl&gt;, ## # `r_s[S11,Intercept]` &lt;dbl&gt;, `r_s[S12,Intercept]` &lt;dbl&gt;, ## # `r_s[S13,Intercept]` &lt;dbl&gt;, `r_s[S14,Intercept]` &lt;dbl&gt;, ## # `r_s[S15,Intercept]` &lt;dbl&gt;, `r_s[S16,Intercept]` &lt;dbl&gt;, ## # `r_s[S17,Intercept]` &lt;dbl&gt;, `r_s[S18,Intercept]` &lt;dbl&gt;, ## # `r_s[S19,Intercept]` &lt;dbl&gt;, `r_s[S20,Intercept]` &lt;dbl&gt;, ## # `r_s[S21,Intercept]` &lt;dbl&gt;, `r_s[S22,Intercept]` &lt;dbl&gt;, ## # `r_s[S23,Intercept]` &lt;dbl&gt;, `r_s[S24,Intercept]` &lt;dbl&gt;, ## # `r_s[S25,Intercept]` &lt;dbl&gt;, `r_s[S26,Intercept]` &lt;dbl&gt;, ## # `r_s[S27,Intercept]` &lt;dbl&gt;, `r_s[S28,Intercept]` &lt;dbl&gt;, lp__ &lt;dbl&gt; And here we’ll take a subset of the columns in prior, transform the results to the probability metric, and save the results as prior_samples. prior_samples &lt;- prior %&gt;% transmute(`theta[1]` = b_Intercept + `r_s[S01,Intercept]`, `theta[14]` = b_Intercept + `r_s[S14,Intercept]`, `theta[28]` = b_Intercept + `r_s[S28,Intercept]`) %&gt;% mutate_all(.funs = inv_logit_scaled) head(prior_samples) ## # A tibble: 6 x 3 ## `theta[1]` `theta[14]` `theta[28]` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.276 0.669 0.0393 ## 2 0.304 0.303 0.301 ## 3 0.574 0.527 0.565 ## 4 0.944 0.890 0.788 ## 5 0.556 0.711 0.505 ## 6 0.0197 0.0436 0.0305 Now we can use our prior_samples object to make the diagonal of the lower grid of Figure 9.11. prior_samples %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_histogram(fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = .2, binwidth = .05, boundary = 0) + scale_x_continuous(labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 1)) + facet_wrap(~key) With a little subtraction, we can reproduce the plots in the upper triangle. prior_samples %&gt;% transmute(`theta[1] - theta[14]` = `theta[1]` - `theta[14]`, `theta[1] - theta[28]` = `theta[1]` - `theta[28]`, `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_histogram(fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = .2, binwidth = .05, boundary = 0) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~key) Those plots clarify our hierarchical logistic regression model was a little more regularizing than Kruschke’s. The consequence of our priors was more aggressive regularization, greater shrinkage toward zero. The prose in the next section of the text clarifies this isn’t necessarily a bad thing. Finally, here are the plots for the lower triangle in Figure 9.11. p1 &lt;- prior_samples %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[14]`)) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) p2 &lt;- prior_samples %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[28]`)) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) p3 &lt;- prior_samples %&gt;% ggplot(aes(x = `theta[14]`, y = `theta[28]`)) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) (p1 + p2 + p3) &amp; geom_abline(linetype = 1, color = &quot;white&quot;) &amp; coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) In case you were curious, here are the Pearson’s correlation coefficients among the priors. cor(prior_samples) %&gt;% round(digits = 2) ## theta[1] theta[14] theta[28] ## theta[1] 1.00 0.73 0.73 ## theta[14] 0.73 1.00 0.72 ## theta[28] 0.73 0.72 1.00 9.3 Shrinkage in hierarchical models “In typical hierarchical models, the estimates of low-level parameters are pulled closer together than they would be if there were not a higher-level distribution. This pulling together is called shrinkage of the estimates” (p. 245, emphasis in the original) Further, shrinkage is a rational implication of hierarchical model structure, and is (usually) desired by the analyst because the shrunken parameter estimates are less affected by random sampling noise than estimates derived without hierarchical structure. Intuitively, shrinkage occurs because the estimate of each low-level parameter is influenced from two sources: (1) the subset of data that are directly dependent on the low-level parameter, and (2) the higher-level parameters on which the low-level parameter depends. The higher- level parameters are affected by all the data, and therefore the estimate of a low-level parameter is affected indirectly by all the data, via their influence on the higher-level parameters. (p. 247) Recall Formula 9.4 from page 223, \\[\\theta \\sim \\operatorname{beta} (\\omega(\\kappa - 2) + 1), (1 - \\omega)(\\kappa - 2) + 1).\\] With that formula, we can express dbeta()’s shape1 and shape2 in terms of \\(\\omega\\) and \\(\\kappa\\) and make the shapes in Figure 9.12. omega &lt;- 0.5 kappa1 &lt;- 2.1 kappa2 &lt;- 15.8 tibble(x = seq(from = 0, to = 1, by = .001)) %&gt;% mutate(`kappa = 2.1` = dbeta(x = x, shape1 = omega * (kappa1 - 2) + 1, shape2 = (1 - omega) * (kappa1 - 2) + 1), `kappa = 15.8` = dbeta(x = x, shape1 = omega * (kappa2 - 2) + 1, shape2 = (1 - omega) * (kappa2 - 2) + 1)) %&gt;% gather(key, value, - x) %&gt;% mutate(key = factor(key, levels = c(&quot;kappa = 2.1&quot;, &quot;kappa = 15.8&quot;))) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(Data~Proportion~or~theta~value), y = expression(dbeta(theta*&quot;|&quot;*omega*&quot;, &quot;*kappa))) + facet_wrap(~key) 9.4 Speeding up JAGS brms Here we’ll compare the time it takes to fit fit1 as either bernoulli(link = logit) or binomial(link = logit). # bernoulli start_time_bernoulli &lt;- proc.time() brm(data = my_data, family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9) stop_time_bernoulli &lt;- proc.time() # binomial start_time_binomial &lt;- proc.time() brm(data = my_data, family = binomial(link = logit), y | trials(1) ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9) stop_time_binomial &lt;- proc.time() See how we’re using proc.time() to record when we began and finished evaluating our brm() code? The last time we covered that was way back in Chapter 3. In Chapter 3 we also learned how subtracting the former from the latter yields the total elapsed time. stop_time_bernoulli - start_time_bernoulli ## user system elapsed ## 2.889 1.059 50.630 stop_time_binomial - start_time_binomial ## user system elapsed ## 53.496 4.285 144.122 If you wanted to be rigorous about this, you could do this multiple times in a mini simulation. As to the issue of parallel processing, we’ve been doing this all along. Note our chains = 4, cores = 4 code. 9.5 Extending the hierarchy: Subjects within categories Many data structures invite hierarchical descriptions that may have multiple levels. Software such as JAGS [brms] makes it easy to implement hierarchical models, and Bayesian inference makes it easy to interpret the parameter estimates, even for complex nonlinear hierarchical models. Here, we take a look at one type of extended hierarchical model. (p. 251) Here we depart a little from Kruschke, again. Though we will be fitting a hierarchical model with subjects \\(s\\) within categories \\(c\\), the higher-level parameters will not be \\(\\omega\\) and \\(\\kappa\\). As we’ll go over, below, we will use the binomial distribution within a more conventional hierarchical logistic regression paradigm. In this paradigm, we have an overall intercept, often called \\(\\alpha\\) or \\(\\beta_0\\), which will be our analogue to Kruschke’s overall \\(\\omega\\). For the two grouping categories, \\(s\\) and \\(c\\), we will have \\(\\sigma\\) estimates, which express the variability within those grouping. You’ll see when we get there. 9.5.1 Example: Baseball batting abilities by position. Here are the batting average data. my_data &lt;- read_csv(&quot;data.R/BattingAverage.csv&quot;) glimpse(my_data) ## Rows: 948 ## Columns: 6 ## $ Player &lt;chr&gt; &quot;Fernando Abad&quot;, &quot;Bobby Abreu&quot;, &quot;Tony Abreu&quot;, &quot;Dustin Ac… ## $ PriPos &lt;chr&gt; &quot;Pitcher&quot;, &quot;Left Field&quot;, &quot;2nd Base&quot;, &quot;2nd Base&quot;, &quot;1st Ba… ## $ Hits &lt;dbl&gt; 1, 53, 18, 137, 21, 0, 0, 2, 150, 167, 0, 128, 66, 3, 1,… ## $ AtBats &lt;dbl&gt; 7, 219, 70, 607, 86, 1, 1, 20, 549, 576, 1, 525, 275, 12… ## $ PlayerNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1… ## $ PriPosNumber &lt;dbl&gt; 1, 7, 4, 4, 3, 1, 1, 3, 3, 4, 1, 5, 4, 2, 7, 4, 6, 8, 9,… In his footnote #6, Kruschke indicated he retrieved the data from http://www.baseball-reference.com/leagues/MLB/2012-standard-batting.shtml on December of 2012. To give a sense of the data, here are the number of occasions by primary position, PriPos, with their median at bat, AtBats, values. my_data %&gt;% group_by(PriPos) %&gt;% summarise(n = n(), median = median(AtBats)) %&gt;% arrange(desc(n)) ## # A tibble: 9 x 3 ## PriPos n median ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Pitcher 324 4 ## 2 Catcher 103 170 ## 3 Left Field 103 164 ## 4 1st Base 81 265 ## 5 3rd Base 75 267 ## 6 2nd Base 72 228. ## 7 Center Field 67 259 ## 8 Shortstop 63 205 ## 9 Right Field 60 340. As these data are aggregated, we’ll fit with an aggregated binomial model. This is still logistic regression. The Bernoulli distribution is a special case of the binomial distribution when the number of trials in each data point is 1 (see Bürkner, 2020h for details). Since our data are aggregated, the information encoded in Hits is a combination of multiple trials, which requires us to jump up to the more general binomial likelihood. Note the Hits | trials(AtBats) syntax. With that bit, we instructed brms that our criterion, Hits, is an aggregate of multiple trials and the number of trials is encoded in AtBats. Also note the (1 | PriPos) + (1 | PriPos:Player) syntax. In this model, we have two grouping factors, PriPos and Player. Thus we have two (|) arguments. But since players are themselves nested within positions, we have encoded that nesting with the (1 | PriPos:Player) syntax. For more on this style of syntax, see Kristoffer Magnusson’s handy blog post, Using R and lme/lmer to fit different two- and three-level longitudinal models. Since brms syntax is based on that from the earlier lme4 package, the basic syntax rules apply. Bürkner (2020g), of course, also covered these topics in the brmsformula subsection of the brms reference manual. fit9.2 &lt;- brm(data = my_data, family = binomial(link = logit), Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = .99), seed = 9, file = &quot;fits/fit09.02&quot;) The chains look good. color_scheme_set(&quot;blue&quot;) plot(fit9.2) We might examine the autocorrelations within the chains. post &lt;- posterior_samples(fit9.2, add_chain = T) mcmc_acf(post, pars = c(&quot;b_Intercept&quot;, &quot;sd_PriPos__Intercept&quot;, &quot;sd_PriPos:Player__Intercept&quot;), lags = 8) Here’s a histogram of the \\(N_{eff}/N\\) ratios. fit9.2 %&gt;% neff_ratio() %&gt;% mcmc_neff_hist(binwidth = .1) + yaxis_text() Happily, most have a very favorable ratio. Here’s a numeric summary of the primary model parameters. print(fit9.2) ## Family: binomial ## Links: mu = logit ## Formula: Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player) ## Data: my_data (Number of observations: 948) ## Samples: 3 chains, each with iter = 3500; warmup = 500; thin = 1; ## total post-warmup samples = 9000 ## ## Group-Level Effects: ## ~PriPos (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.32 0.10 0.19 0.59 1.00 2660 4283 ## ## ~PriPos:Player (Number of levels: 948) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.14 0.01 0.12 0.15 1.00 3695 5911 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.17 0.11 -1.40 -0.94 1.00 1507 2262 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As far as I’m aware, brms offers three major ways to get the group-level parameters for a hierarchical model: using posterior_samples(), coef(), or fitted(). We’ll cover each, beginning with posterior_samples(). In order to look at the autocorrelation plots, above, we already saved the posterior_samples(fit9.2) output as post. Here we’ll look at its structure with head(). Before doing so we’ll convert post, which is currently saved as a data frame, into a tibble in order to keep the output from getting unwieldy. post &lt;- post %&gt;% as_tibble() head(post) ## # A tibble: 6 x 963 ## b_Intercept sd_PriPos__Inte… `sd_PriPos:Play… `r_PriPos[1st.B… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.11 0.213 0.145 -0.00504 ## 2 -1.13 0.262 0.147 0.101 ## 3 -1.18 0.351 0.149 0.169 ## 4 -1.18 0.279 0.130 0.141 ## 5 -1.16 0.408 0.137 0.0744 ## 6 -1.20 0.344 0.142 0.146 ## # … with 959 more variables: `r_PriPos[2nd.Base,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[3rd.Base,Intercept]` &lt;dbl&gt;, `r_PriPos[Catcher,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[Center.Field,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[Left.Field,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[Pitcher,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[Right.Field,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[Shortstop,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Adam.Dunn,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Adam.LaRoche,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Adam.Lind,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Adrian.Gonzalez,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Albert.Pujols,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Allen.Craig,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Anthony.Rizzo,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Aubrey.Huff,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Billy.Butler,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brandon.Allen,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brandon.Belt,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brandon.Moss,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brandon.Snyder,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brent.Lillibridge,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brett.Pill,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brett.Wallace,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Bryan.LaHair,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Carlos.Lee,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Carlos.Pena,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Casey.Kotchman,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Casey.McGehee,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Chad.Tracy,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Chris.Carter,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Chris.Davis,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Chris.Parmelee,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Corey.Hart,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Dan.Johnson,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Daric.Barton,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_David.Cooper,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_David.Ortiz,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Edwin.Encarnacion,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Eric.Hinske,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Eric.Hosmer,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Freddie.Freeman,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Gaby.Sanchez,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Garrett.Jones,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Hector.Luna,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Ike.Davis,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_James.Loney,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Jason.Giambi,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Jeff.Clement,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Jim.Thome,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Joe.Mahoney,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Joey.Votto,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Juan.Rivera,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Justin.Morneau,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Justin.Smoak,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Kendrys.Morales,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Kila.Kaaihue,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Kyle.Blanks,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Lance.Berkman,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Luke.Scott,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Lyle.Overbay,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mark.Reynolds,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mark.Teixeira,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mat.Gamel,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Matt.Adams,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Matt.Carpenter,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Matt.Downs,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Matt.Hague,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Matt.LaPorta,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mauro.Gomez,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Michael.Young,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Miguel.Cairo,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mike.Costanzo,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mike.Jacobs,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mike.Olt,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mitch.Moreland,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Nick.Johnson,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Paul.Goldschmidt,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Paul.Konerko,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Prince.Fielder,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Ryan.Howard,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Steven.Hill,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Taylor.Green,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Todd.Helton,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Travis.Ishikawa,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Ty.Wigginton,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Yan.Gomes,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Yonder.Alonso,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Zach.Lutz,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Aaron.Hill,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Adam.Rosales,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Adrian.Cardenas,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Alexi.Amarista,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Alexi.Casilla,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Blake.DeWitt,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Brandon.Phillips,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Brian.Roberts,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Brock.Holt,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Charlie.Culberson,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Chase.dArnaud,Intercept]` &lt;dbl&gt;, … In the text, Kruschke described the model as having 968 parameters. Our post tibble has one vector for each, with a couple others tacked onto the end. In the hierarchical logistic regression model, the group-specific parameters for the levels of PriPos are additive combinations of the global intercept vector, b_Intercept and each position-specific vector, r_PriPos[i.Base,Intercept], where i is a fill-in for the position of interest. And recall that since the linear model is of the logit of the criterion, we’ll need to use inv_logit_scaled() to convert that to the probability space. post_small &lt;- post %&gt;% transmute(`1st Base` = (b_Intercept + `r_PriPos[1st.Base,Intercept]`), Catcher = (b_Intercept + `r_PriPos[Catcher,Intercept]`), Pitcher = (b_Intercept + `r_PriPos[Pitcher,Intercept]`)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% # here we compute our difference distributions mutate(`Pitcher - Catcher` = Pitcher - Catcher, `Catcher - 1st Base` = Catcher - `1st Base`) head(post_small) ## # A tibble: 6 x 5 ## `1st Base` Catcher Pitcher `Pitcher - Catcher` `Catcher - 1st Base` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.246 0.237 0.135 -0.102 -0.00921 ## 2 0.264 0.251 0.134 -0.117 -0.0131 ## 3 0.267 0.242 0.122 -0.120 -0.0256 ## 4 0.262 0.242 0.126 -0.116 -0.0201 ## 5 0.252 0.241 0.133 -0.109 -0.0110 ## 6 0.259 0.243 0.137 -0.106 -0.0161 If you take a glance at Figures 9.14 through 9.16 in the text, we’ll be making a lot of histograms of the same basic structure. To streamline our code a bit, we can make a custom histogram plotting function. make_histogram &lt;- function(data, mapping, title, xlim, ...) { ggplot(data, mapping) + geom_histogram(fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = .2, bins = 30) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = title, x = expression(theta)) + coord_cartesian(xlim = xlim) + theme(legend.position = &quot;none&quot;) } We’ll do the same thing for the correlation plots. make_point &lt;- function(data, mapping, limits, ...) { ggplot(data, mapping) + geom_abline(color = &quot;white&quot;) + geom_point(color = &quot;grey50&quot;, size = 1/10, alpha = 1/20) + coord_cartesian(xlim = limits, ylim = limits) } To learn more about wrapping custom plots into custom functions, check out Chapter 16 of Wickham’s (2016) ggplot2: Elegant graphics for data analysis. Now we have our make_histogram() and make_point() functions, we’ll use grid.arrange() to paste together the left half of Figure 9.14. p1 &lt;- make_histogram(data = post_small, aes(x = Pitcher), title = &quot;Pitcher&quot;, xlim = c(.1, .25)) p2 &lt;- make_histogram(data = post_small, aes(x = `Pitcher - Catcher`), title = &quot;Pitcher - Catcher&quot;, xlim = c(-.15, 0)) p3 &lt;- make_point(data = post_small, aes(x = Pitcher, y = Catcher), limits = c(.12, .25)) p4 &lt;- make_histogram(data = post_small, aes(x = Catcher), title = &quot;Catcher&quot;, xlim = c(.1, .25)) library(patchwork) p1 + p2 + p3 + p4 We could follow the same procedure to make the right portion of Figure 9.14. But instead, let’s switch gears and explore the second way brms affords us for plotting group-level parameters. This time, we’ll use coef(). Up in Section 9.2.4, we learned that we can use coef() to “extract [the] model coefficients, which are the sum of population-level effects and corresponding group-level effects” (Bürkner, 2020g, p. 43). The grouping level we’re interested in is PriPos, so we’ll use that to index the information returned by coef(). Since coef() returns a matrix, we’ll use as_tibble() to convert it to a tibble. coef_primary_position &lt;- coef(fit9.2, summary = F)$PriPos %&gt;% as_tibble() str(coef_primary_position) ## tibble [9,000 × 9] (S3: tbl_df/tbl/data.frame) ## $ 1st Base.Intercept : num [1:9000] -1.12 -1.03 -1.01 -1.04 -1.09 ... ## $ 2nd Base.Intercept : num [1:9000] -1.059 -1.072 -1.067 -0.997 -1.169 ... ## $ 3rd Base.Intercept : num [1:9000] -1.01 -1.05 -1.04 -1.06 -1.07 ... ## $ Catcher.Intercept : num [1:9000] -1.17 -1.09 -1.14 -1.14 -1.15 ... ## $ Center Field.Intercept: num [1:9000] -1.05 -1.07 -1.06 -1.06 -1.03 ... ## $ Left Field.Intercept : num [1:9000] -1.08 -1.11 -1.06 -1.06 -1.12 ... ## $ Pitcher.Intercept : num [1:9000] -1.86 -1.87 -1.97 -1.94 -1.88 ... ## $ Right Field.Intercept : num [1:9000] -1.03 -1.05 -1.08 -1.09 -1.01 ... ## $ Shortstop.Intercept : num [1:9000] -1.16 -1.1 -1.14 -1.1 -1.12 ... Keep in mind that coef() returns the values in the logit scale when used for logistic regression models. So we’ll have to use brms::inv_logit_scaled() to convert the estimates to the probability metric. After we’re done converting the estimates, we’ll then make the difference distributions. coef_small &lt;- coef_primary_position %&gt;% select(`1st Base.Intercept`, Catcher.Intercept, Pitcher.Intercept) %&gt;% transmute(`1st Base` = `1st Base.Intercept`, Catcher = Catcher.Intercept, Pitcher = Pitcher.Intercept) %&gt;% mutate_all(inv_logit_scaled) %&gt;% # here we make the difference distributions mutate(`Pitcher - Catcher` = Pitcher - Catcher, `Catcher - 1st Base` = Catcher - `1st Base`) head(coef_small) ## # A tibble: 6 x 5 ## `1st Base` Catcher Pitcher `Pitcher - Catcher` `Catcher - 1st Base` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.246 0.237 0.135 -0.102 -0.00921 ## 2 0.264 0.251 0.134 -0.117 -0.0131 ## 3 0.267 0.242 0.122 -0.120 -0.0256 ## 4 0.262 0.242 0.126 -0.116 -0.0201 ## 5 0.252 0.241 0.133 -0.109 -0.0110 ## 6 0.259 0.243 0.137 -0.106 -0.0161 Now we’re ready for the right half of Figure 9.14. p1 &lt;- make_histogram(data = coef_small, aes(x = Catcher), title = &quot;Catcher&quot;, xlim = c(.22, .27)) p2 &lt;- make_histogram(data = coef_small, aes(x = `Catcher - 1st Base`), title = &quot;Catcher - 1st Base&quot;, xlim = c(-.04, .01)) p3 &lt;- make_point(data = coef_small, aes(x = Catcher, y = `1st Base`), limits = c(.22, .27)) p4 &lt;- make_histogram(data = coef_small, aes(x = `1st Base`), title = &quot;1st Base&quot;, xlim = c(.22, .27)) p1 + p2 + p3 + p4 And if you wanted the posterior modes and HDIs, you’d use mode_hdi() after a little wrangling. coef_small %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 5 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1st Base 0.252 0.245 0.263 0.95 mode hdi ## 2 Catcher 0.241 0.233 0.25 0.95 mode hdi ## 3 Catcher - 1st Base -0.013 -0.024 -0.001 0.95 mode hdi ## 4 Pitcher 0.13 0.12 0.14 0.95 mode hdi ## 5 Pitcher - Catcher -0.111 -0.124 -0.098 0.95 mode hdi While we’re at it, we should capitalize on the opportunity to show how these results are the same as those derived from our posterior_samples() approach, above. post_small %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 5 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1st Base 0.252 0.245 0.263 0.95 mode hdi ## 2 Catcher 0.241 0.233 0.25 0.95 mode hdi ## 3 Catcher - 1st Base -0.013 -0.024 -0.001 0.95 mode hdi ## 4 Pitcher 0.13 0.12 0.14 0.95 mode hdi ## 5 Pitcher - Catcher -0.111 -0.124 -0.098 0.95 mode hdi Success! For Figures 9.15 and 9.16, Kruschke drilled down further into the posterior. To drill along with him, we’ll take the opportunity to showcase fitted(), the third way brms affords us for plotting group-level parameters. # this will make life easier. just go with it name_list &lt;- c(&quot;Kyle Blanks&quot;, &quot;Bruce Chen&quot;, &quot;ShinSoo Choo&quot;, &quot;Ichiro Suzuki&quot;, &quot;Mike Leake&quot;, &quot;Wandy Rodriguez&quot;, &quot;Andrew McCutchen&quot;, &quot;Brett Jackson&quot;) # we&#39;ll define the data we&#39;d like to feed into `fitted()`, here nd &lt;- my_data %&gt;% filter(Player %in% name_list) %&gt;% # these last two lines aren&#39;t typically necessary, # but they allow us to arrange the rows in the same order we find the names in Figures 9.15 and 9.16 mutate(Player = factor(Player, levels = name_list)) %&gt;% arrange(Player) fitted_players &lt;- fitted(fit9.2, newdata = nd, scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% # rename the values as returned by `as_tibble()` set_names(name_list) %&gt;% # convert the values from the logit scale to the probability scale mutate_all(inv_logit_scaled) %&gt;% # in this last section, we make our difference distributions mutate(`Kyle Blanks - Bruce Chen` = `Kyle Blanks` - `Bruce Chen`, `ShinSoo Choo - Ichiro Suzuki` = `ShinSoo Choo` - `Ichiro Suzuki`, `Mike Leake - Wandy Rodriguez` = `Mike Leake` - `Wandy Rodriguez`, `Andrew McCutchen - Brett Jackson` = `Andrew McCutchen` - `Brett Jackson`) glimpse(fitted_players) ## Rows: 9,000 ## Columns: 12 ## $ `Kyle Blanks` &lt;dbl&gt; 0.2884479, 0.2444058, 0.3045453, 0… ## $ `Bruce Chen` &lt;dbl&gt; 0.1259121, 0.1468900, 0.1160687, 0… ## $ `ShinSoo Choo` &lt;dbl&gt; 0.2836534, 0.3041822, 0.2661348, 0… ## $ `Ichiro Suzuki` &lt;dbl&gt; 0.2631869, 0.2891847, 0.2999139, 0… ## $ `Mike Leake` &lt;dbl&gt; 0.1328278, 0.1611607, 0.1774467, 0… ## $ `Wandy Rodriguez` &lt;dbl&gt; 0.11374696, 0.12650915, 0.11822352… ## $ `Andrew McCutchen` &lt;dbl&gt; 0.3136231, 0.3139831, 0.2964375, 0… ## $ `Brett Jackson` &lt;dbl&gt; 0.2840384, 0.2368827, 0.2078027, 0… ## $ `Kyle Blanks - Bruce Chen` &lt;dbl&gt; 0.16253577, 0.09751580, 0.18847658… ## $ `ShinSoo Choo - Ichiro Suzuki` &lt;dbl&gt; 0.020466515, 0.014997563, -0.03377… ## $ `Mike Leake - Wandy Rodriguez` &lt;dbl&gt; 0.019080819, 0.034651509, 0.059223… ## $ `Andrew McCutchen - Brett Jackson` &lt;dbl&gt; 0.02958472, 0.07710036, 0.08863479… Note our use of the scale = &quot;linear&quot; argument in the fitted() function. By default, fitted() returns predictions on the scale of the criterion. But we don’t want a list of successes and failures; we want player-level parameters. When you specify scale = &quot;linear&quot;, you request fitted() return the values in the parameter scale. Here’s the left portion of Figure 9.15. p1 &lt;- make_histogram(data = fitted_players, aes(x = `Kyle Blanks`), title = &quot;Kyle Blanks (1st Base)&quot;, xlim = c(.05, .35)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `Kyle Blanks - Bruce Chen`), title = &quot;Kyle Blanks (1st Base) -\\nBruce Chen (Pitcher)&quot;, xlim = c(-.1, .25)) p3 &lt;- make_point(data = fitted_players, aes(x = `Kyle Blanks`, y = `Bruce Chen`), limits = c(.09, .35)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Bruce Chen`), title = &quot;Bruce Chen (Pitcher)&quot;, xlim = c(.05, .35)) p1 + p2 + p3 + p4 Figure 9.15, right: p1 &lt;- make_histogram(data = fitted_players, aes(x = `ShinSoo Choo`), title = &quot;ShinSoo Choo (Right Field)&quot;, xlim = c(.22, .34)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `ShinSoo Choo - Ichiro Suzuki`), title = &quot;ShinSoo Choo (Right Field) -\\nIchiro Suzuki (Right Field)&quot;, xlim = c(-.07, .07)) p3 &lt;- make_point(data = fitted_players, aes(x = `ShinSoo Choo`, y = `Ichiro Suzuki`), limits = c(.23, .32)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Ichiro Suzuki`), title = &quot;Ichiro Suzuki (Right Field)&quot;, xlim = c(.22, .34)) p1 + p2 + p3 + p4 Figure 9.16, left: p1 &lt;- make_histogram(data = fitted_players, aes(x = `Mike Leake`), title = &quot;Mike Leake (Pitcher)&quot;, xlim = c(.05, .35)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `Mike Leake - Wandy Rodriguez`), title = &quot;Mike Leake (Pitcher) -\\nWandy Rodriguez (Pitcher)&quot;, xlim = c(-.05, .25)) p3 &lt;- make_point(data = fitted_players, aes(x = `Mike Leake`, y = `Wandy Rodriguez`), limits = c(.07, .25)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Wandy Rodriguez`), title = &quot;Wandy Rodriguez (Pitcher)&quot;, xlim = c(.05, .35)) p1 + p2 + p3 + p4 Figure 9.16, right: p1 &lt;- make_histogram(data = fitted_players, aes(x = `Andrew McCutchen`), title = &quot;Andrew McCutchen (Center Field)&quot;, xlim = c(.15, .35)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `Andrew McCutchen - Brett Jackson`), title = &quot;Andrew McCutchen (Center Field) -\\nBrett Jackson (Center Field)&quot;, xlim = c(0, .20)) p3 &lt;- make_point(data = fitted_players, aes(x = `Andrew McCutchen`, y = `Brett Jackson`), limits = c(.15, .35)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Brett Jackson`), title = &quot;Brett Jackson (Center Field)&quot;, xlim = c(.15, .35)) p1 + p2 + p3 + p4 If you wanted the posterior modes and HDIs for any of the players and their contrasts, you’d use mode_hdi() after a little wrangling. fitted_players %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 12 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Andrew McCutchen 0.307 0.275 0.336 0.95 mode hdi ## 2 Andrew McCutchen - Brett Jackson 0.074 0.019 0.12 0.95 mode hdi ## 3 Brett Jackson 0.233 0.195 0.277 0.95 mode hdi ## 4 Bruce Chen 0.129 0.099 0.164 0.95 mode hdi ## 5 Ichiro Suzuki 0.275 0.246 0.306 0.95 mode hdi ## 6 Kyle Blanks 0.25 0.202 0.303 0.95 mode hdi ## 7 Kyle Blanks - Bruce Chen 0.117 0.059 0.179 0.95 mode hdi ## 8 Mike Leake 0.148 0.118 0.184 0.95 mode hdi ## 9 Mike Leake - Wandy Rodriguez 0.028 -0.015 0.069 0.95 mode hdi ## 10 ShinSoo Choo 0.275 0.244 0.304 0.95 mode hdi ## 11 ShinSoo Choo - Ichiro Suzuki 0 -0.042 0.043 0.95 mode hdi ## 12 Wandy Rodriguez 0.119 0.096 0.152 0.95 mode hdi To make our version of Figure 19.7, we’ll have to switch gears from player-specific effects to those specific to positions averaged over individual players. The fitted() approach will probably make this the easiest. This will require two caveats. Recall our model was based on aggregated data with the number of trials indexed by the AtBats column. In order to get the PriPos-specific estimates, we’ll have to assign each one an AtBats value. You could do this any number of ways, including a theoretically-meaningful value, a numerically-simple value (e.g., 100), the grand mean of the AtBats values, or even AtBats values based on the PriPos-specific means. We’ll take that last approach. Another consideration is if we’d like to use fitted() to average across one of the hierarchical grouping parameters (i.e., (1 | PriPos:Player)), we’ll need to employ the re_formula argument. With the line re_formula = ~ (1 | PriPos), we’ll instruct fitted() to return the PriPos-specific effects after averaging across levels of Player. The rest is quire similar to our method from above. nd &lt;- my_data %&gt;% group_by(PriPos) %&gt;% summarise(AtBats = mean(AtBats) %&gt;% round(digits = 0)) fitted_positions &lt;- fitted(fit9.2, newdata = nd, re_formula = ~ (1 | PriPos), scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% set_names(distinct(my_data, PriPos) %&gt;% arrange(PriPos) %&gt;% pull()) %&gt;% mutate_all(inv_logit_scaled) glimpse(fitted_positions) ## Rows: 9,000 ## Columns: 9 ## $ `1st Base` &lt;dbl&gt; 0.2464087, 0.2640264, 0.2671732, 0.2620118, 0.2522632,… ## $ `2nd Base` &lt;dbl&gt; 0.2574891, 0.2549343, 0.2559046, 0.2696126, 0.2369503,… ## $ `3rd Base` &lt;dbl&gt; 0.2676894, 0.2597608, 0.2605470, 0.2573531, 0.2555818,… ## $ Catcher &lt;dbl&gt; 0.2371965, 0.2509439, 0.2416116, 0.2419317, 0.2412558,… ## $ `Center Field` &lt;dbl&gt; 0.2595912, 0.2545440, 0.2575502, 0.2573838, 0.2637516,… ## $ `Left Field` &lt;dbl&gt; 0.2542200, 0.2471413, 0.2579201, 0.2575989, 0.2457442,… ## $ Pitcher &lt;dbl&gt; 0.1348607, 0.1335854, 0.1218640, 0.1257918, 0.1325010,… ## $ `Right Field` &lt;dbl&gt; 0.2638224, 0.2591208, 0.2539903, 0.2524098, 0.2660591,… ## $ Shortstop &lt;dbl&gt; 0.2390144, 0.2499422, 0.2428891, 0.2499811, 0.2462102,… Now we make and save the nine position-specific subplots for Figure 9.17. p1 &lt;- fitted_positions %&gt;% pivot_longer(everything(), values_to = &quot;theta&quot;) %&gt;% # though technically not needed, this line reorders the panels to match the text mutate(name = factor(name, levels = c(&quot;1st Base&quot;, &quot;Catcher&quot;, &quot;Pitcher&quot;, &quot;2nd Base&quot;, &quot;Center Field&quot;, &quot;Right Field&quot;, &quot;3rd Base&quot;, &quot;Left Field&quot;, &quot;Shortstop&quot;))) %&gt;% ggplot(aes(x = theta)) + geom_histogram(fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = .1, binwidth = .0025) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + coord_cartesian(xlim = c(.1, .28)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~name, nrow = 3, scales = &quot;free_y&quot;) In this code block, we’ll make the subplot for the overall batting average. Given the size of the model, it’s perhaps easiest to pull that information from the model with the fixef() function. p2 &lt;- fixef(fit9.2, summary = F) %&gt;% as_tibble() %&gt;% transmute(theta = inv_logit_scaled(Intercept), name = &quot;Overall&quot;) %&gt;% ggplot(aes(x = theta)) + geom_histogram(fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = .2, binwidth = .005) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + coord_cartesian(xlim = c(.1, .28)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~name, nrow = 3, scales = &quot;free_y&quot;) Now combine the plots with a little patchwork magic. p3 &lt;- plot_spacer() p1 + (p2 / p3 / p3) + plot_layout(widths = c(3, 1)) Do note that, unlike Kruschke’s Figure 9.17, our subplots are all based on \\(\\theta\\) rather than \\(\\omega\\). This is because of our use of a hierarchical aggregated binomial, rather than the approach Kruschke took. Even so, look how similar the results are. Finally, we have only looked at a tiny fraction of the relations among the 968 parameters. We could investigate many more comparisons among parameters if we were specifically interested. In traditional statistical testing based on \\(p\\)-values (which will be discussed in Chapter 11), we would pay a penalty for even intending to make more comparisons. This is because a \\(p\\) value depends on the space of counter-factual possibilities created from the testing intentions. In a Bayesian analysis, however, decisions are based on the posterior distribution, which is based only on the data (and the prior), not on the testing intention. More discussion of multiple comparisons can be found in Section 11.4. (pp. 259–260) Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.3.9000 bayesplot_1.7.1 brms_2.12.0 ## [4] Rcpp_1.0.4.6 patchwork_1.0.0 ggridges_0.5.2 ## [7] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 ## [10] purrr_0.3.4 readr_1.3.1 tidyr_1.0.2 ## [13] tibble_3.0.1 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 rsconnect_0.8.16 ## [4] markdown_1.1 base64enc_0.1-3 fs_1.4.1 ## [7] rstudioapi_0.11 farver_2.0.3 rstan_2.19.3 ## [10] svUnit_1.0.3 DT_0.13 fansi_0.4.1 ## [13] mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 ## [16] bridgesampling_1.0-0 knitr_1.28 shinythemes_1.1.2 ## [19] jsonlite_1.6.1 broom_0.5.5 dbplyr_1.4.2 ## [22] shiny_1.4.0.2 compiler_3.6.3 httr_1.4.1 ## [25] backports_1.1.6 assertthat_0.2.1 Matrix_1.2-18 ## [28] fastmap_1.0.1 cli_2.0.2 later_1.0.0 ## [31] prettyunits_1.1.1 htmltools_0.4.0 tools_3.6.3 ## [34] igraph_1.2.5 coda_0.19-3 gtable_0.3.0 ## [37] glue_1.4.0 reshape2_1.4.4 cellranger_1.1.0 ## [40] vctrs_0.3.0 nlme_3.1-144 crosstalk_1.1.0.1 ## [43] xfun_0.13 ps_1.3.3 rvest_0.3.5 ## [46] mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 ## [49] gtools_3.8.2 zoo_1.8-7 scales_1.1.1 ## [52] colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [55] Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 ## [58] shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 ## [61] StanHeaders_2.21.0-1 loo_2.2.0 stringi_1.4.6 ## [64] dygraphs_1.1.1.6 pkgbuild_1.0.8 rlang_0.4.6 ## [67] pkgconfig_2.0.3 matrixStats_0.56.0 HDInterval_0.2.0 ## [70] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [73] htmlwidgets_1.5.1 labeling_0.3 tidyselect_1.0.0 ## [76] processx_3.4.2 plyr_1.8.6 magrittr_1.5 ## [79] bookdown_0.18 R6_2.4.1 generics_0.0.2 ## [82] DBI_1.1.0 pillar_1.4.4 haven_2.2.0 ## [85] withr_2.2.0 xts_0.12-0 abind_1.4-5 ## [88] modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 ## [91] utf8_1.1.4 rmarkdown_2.1 grid_3.6.3 ## [94] readxl_1.3.1 callr_3.4.3 threejs_0.3.3 ## [97] reprex_0.3.0 digest_0.6.25 xtable_1.8-4 ## [100] httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 ## [103] viridisLite_0.3.0 shinyjs_1.1 References "],
["model-comparison-and-hierarchical-modeling.html", "10 Model Comparison and Hierarchical Modeling 10.1 General formula and the Bayes factor 10.2 Example: Two factories of coins 10.3 Solution by MCMC 10.4 Prediction: Model averaging 10.5 Model complexity naturally accounted for 10.6 Extreme sensitivity to the prior distribution 10.7 Bonus: There’s danger ahead Session info", " 10 Model Comparison and Hierarchical Modeling There are situations in which different models compete to describe the same set of data… …Bayesian inference is reallocation of credibility over possibilities. In model comparison, the focal possibilities are the models, and Bayesian model comparison reallocates credibility across the models, given the data. In this chapter, we explore examples and methods of Bayesian inference about the relative credibilities of models. (Kruschke, 2015, pp. 265–266) In the text, the emphasis is on the Bayes Factor paradigm. While we will discuss that, we will also present the alternatives available with information criteria, model averaging, and model stacking. 10.1 General formula and the Bayes factor So far we have spoken of the data, denoted by \\(D\\) or \\(y\\); the model parameters, generically denoted by \\(\\theta\\); the likelihood function, denoted by \\(p(D | \\theta)\\); and the prior distribution, denoted by \\(p(\\theta)\\). Now we add to that \\(m\\), which is a model index with \\(m = 1\\) standing for the first model, \\(m = 2\\) standing for the second model, and so on. So when we have more than one model in play, we might refer to the likelihood as \\(p_m(y | \\theta_m, m)\\) and the prior as \\(p_m(\\theta_m | m)\\). It’s also the case, then, that each model can be given a prior probability \\(p(m)\\). “The Bayes factor (BF) is the ratio of the probabilities of the data in models 1 and 2” (p. 268). This can be expressed simply as \\[\\text{BF} = \\frac{p(D | m = 1)}{p(D | m = 2)}.\\] Kruschke further explained that one convention for converting the magnitude of the BF to a discrete decision about the models is that there is “substantial” evidence for model \\(m = 1\\) when the BF exceeds 3.0 and, equivalently, “substantial” evidence for model \\(m = 2\\) when the BF is less than 1/3 (Jeffreys, 1961; Kass &amp; Raftery, 1995; Wetzels et al., 2011). However, as with \\(p\\)-values, effect sizes, and so on, BF values exist within continua and might should be evaluated in terms of degree more so than as ordered kinds. 10.2 Example: Two factories of coins Kruschke considered the coin bias of two factories, each described by the beta distribution. We can organize how to derive the \\(\\alpha\\) and \\(\\beta\\) parameters from \\(\\omega\\) and \\(\\kappa\\) with a tibble. library(tidyverse) d &lt;- tibble(factory = 1:2, omega = c(.25, .75), kappa = 12) %&gt;% mutate(alpha = omega * (kappa - 2) + 1, beta = (1 - omega) * (kappa - 2) + 1) d %&gt;% knitr::kable() factory omega kappa alpha beta 1 0.25 12 3.5 8.5 2 0.75 12 8.5 3.5 Thus given \\(\\omega_1 = .25\\), \\(\\omega_2 = .75\\) and \\(\\kappa = 12\\), we can describe the bias of the two coin factories as \\(\\operatorname{beta} (\\theta_{[m = 1]} | 3.5, 8.5)\\) and \\(\\operatorname{beta} (\\theta_{[m = 2]} | 8.5, 3.5)\\). With a little wrangling, we canuse our d tibble to make the densities of Figure 10.2. length &lt;- 101 d %&gt;% expand(nesting(factory, alpha, beta), theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(label = str_c(&quot;factory &quot;, factory)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = dbeta(x = theta, shape1 = alpha, shape2 = beta))) + geom_ribbon(fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + theme(panel.grid = element_blank()) + facet_wrap(~label) We might recreate the top panel with geom_col(). tibble(Model = c(&quot;1&quot;, &quot;2&quot;), y = 1) %&gt;% ggplot(aes(x = Model, y = y)) + geom_col(width = .75, fill = &quot;grey50&quot;) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(paste(italic(P)[italic(m)]))) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) Consider the Bernoulli bar plots in the bottom panels of Figure 10.2. The heights of the bars are arbitrary and just intended to give a sense of the Bernoulli distribution. If we wanted the heights to correspond to the Beta distributions above them, we might do so like this. crossing(factory = str_c(&quot;factory &quot;, 1:2), flip = factor(c(&quot;tails&quot;, &quot;heads&quot;), levels = c(&quot;tails&quot;, &quot;heads&quot;))) %&gt;% mutate(prob = c(.75, .25, .25, .75)) %&gt;% ggplot(aes(x = flip, y = prob)) + geom_col(width = .75, fill = &quot;grey50&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) + facet_wrap(~factory) But now suppose we flip the coin nine times and get six heads. Given those data, what are the posterior probabilities of the coin coming from the head-biased or tail-biased factories? We will pursue the answer three ways: via formal analysis, grid approximation, and MCMC. (p. 270) 10.2.1 Solution by formal analysis. Here we rehearse if we have \\(\\operatorname{beta} (\\theta, a, b)\\) prior for \\(\\theta\\) of the Bernoulli likelihood function, then the analytic solution for the posterior is \\(\\operatorname{beta} (\\theta | z + a, N – z + b)\\). Within this paradigm, if you would like to compute \\(p(D | m)\\), don’t use the following function. If suffers from underflow with large values. p_d &lt;- function(z, n, a, b) { beta(z + a, n - z + b) / beta(a, b) } This version is more robust. p_d &lt;- function(z, n, a, b) { exp(lbeta(z + a, n - z + b) - lbeta(a, b)) } You’d use it like this to compute \\(p(D|m_1)\\). p_d(z = 6, n = 9, a = 3.5, b = 8.5) ## [1] 0.0004993439 So to compute our BF, \\(\\frac{p(D|m_1)}{p(D|m_2)}\\), you might use the p_d() function like this. p_d_1 &lt;- p_d(z = 6, n = 9, a = 3.5, b = 8.5) p_d_2 &lt;- p_d(z = 6, n = 9, a = 8.5, b = 3.5) p_d_1 / p_d_2 ## [1] 0.2135266 And if we computed the BF the other way, it’d look like this. p_d_2 / p_d_1 ## [1] 4.683258 Since the BF itself is only \\(\\text{BF} = \\frac{p(D | m = 1)}{p(D | m = 2)}\\), we’d need to bring in the priors for the models themselves to get the posterior probabilities, which follows the form \\[\\frac{p(m = 1 | D)}{p(m = 2 | D)} = \\Bigg (\\frac{p(D | m = 1)}{p(D | m = 2)} \\Bigg ) \\Bigg ( \\frac{p(m = 1)}{p(m = 2)} \\Bigg).\\] If for both our models \\(p(m) = .5\\), then the BF is (p_d_1 * .5) / (p_d_2 * .5) ## [1] 0.2135266 As Kruschke pointed out, because we’re working in the probability metric, the sum of \\(p(m = 1 | D )\\) and \\(p(m = 2 | D )\\) must be 1. By simple algebra then, \\[p(m = 2 | D ) = 1 - p(m = 1 | D ).\\] Therefore, it’s also the case that \\[\\frac{p(m = 1 | D)}{1 - p(m = 1 | D)} = 0.2135266.\\] Thus, 0.2135266 is in an odds metric. If you want to convert odds to a probability, you follow the formula \\[\\text{odds} = \\frac{\\text{probability}}{1 - \\text{probability}}.\\] And with more algegraic manipulation, you can solve for the probability. \\[\\begin{align*} \\text{odds} &amp; = \\frac{\\text{probability}}{1 - \\text{probability}} \\\\ \\text{odds} - \\text{odds} \\cdot \\text{probability} &amp; = \\text{probability} \\\\ \\text{odds} &amp; = \\text{probability} + \\text{odds} \\cdot \\text{probability} \\\\ \\text{odds} &amp; = \\text{probability} (1 + \\text{odds}) \\\\ \\frac{\\text{odds}}{1 + \\text{odds}} &amp; = \\text{probability} \\end{align*}\\] Thus, the posterior probability for \\(m = 1\\) is \\[p(m = 1 | D) = \\frac{0.2135266}{1 + 0.2135266}.\\] We can express that in code like so. odds &lt;- (p_d_1 * .5) / (p_d_2 * .5) odds / (1 + odds) ## [1] 0.1759554 Relative to \\(m = 2\\), our posterior probability for \\(m = 1\\) is about .18. Therefore the posterior probability of \\(m = 2\\) is 1 minus that. 1 - (odds / (1 + odds)) ## [1] 0.8240446 Given the data, the two models and the prior assumption they were equally credible, we conclude \\(m = 2\\) is .82 probable. 10.2.2 Solution by grid approximation. We won’t be able to make the wireframe plots on the left of Figure 10.3, but we can do some of the others. Here’s the upper right panel. tibble(omega = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(m_p = ifelse(omega %in% c(.25, .75), 15, 0)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = m_p)) + geom_ribbon(fill = &quot;grey67&quot;, color = &quot;grey67&quot;) + labs(subtitle = &quot;Remember, the scale on the x is arbitrary.&quot;, x = expression(omega), y = expression(Marginal~p(omega))) + coord_flip(ylim = c(0, 25)) + theme(panel.grid = element_blank()) Building on that, here’s the upper middle panel of the “two [prior] dorsal fins” (p. 271). d &lt;- crossing(omega = seq(from = 0, to = 1, length.out = length), theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(prior = ifelse(omega == .25, dbeta(theta, 3.5, 8.5), ifelse(omega == .75, dbeta(theta, 8.5, 3.5), 0))) d %&gt;% ggplot(aes(x = theta, y = omega, fill = prior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) This time we’ll separate \\(p_{m = 1}(\\theta)\\) and \\(p_{m = 2}(\\theta)\\) into the two short plots on the right of the next row down. p1 &lt;- d %&gt;% filter(omega == .75) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(p(theta*&quot;|&quot;*omega==.75))) + theme(panel.grid = element_blank()) p2 &lt;- d %&gt;% filter(omega == .25) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(p(theta*&quot;|&quot;*omega==.25))) + theme(panel.grid = element_blank()) # we&#39;ll put them together with help from patchwork library(patchwork) p1 / p2 We can continue to build on those sensibilities for the middle panel of the same row. Here we’re literally adding \\(p_{m = 1}(\\theta)\\) to \\(p_{m = 2}(\\theta)\\) and taking their average. tibble(theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(d_75 = dbeta(x = theta, shape1 = 8.5, shape2 = 3.5), d_25 = dbeta(x = theta, shape1 = 3.5, shape2 = 8.5)) %&gt;% mutate(mean_prior = (d_75 + d_25) / 2) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = mean_prior)) + geom_ribbon(fill = &quot;grey67&quot;) + coord_cartesian(ylim = c(0, 3)) + labs(x = expression(theta), y = expression(Marginal~p(theta))) + theme(panel.grid = element_blank()) We need the Bernoulli likelihood function for the next step. bernoulli_likelihood &lt;- function(theta, data) { n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } Time to feed our data and the parameter space into bernoulli_likelihood(), which will allow us to make the 2-dimensional density plot at the heart of Figure 10.3. n &lt;- 9 z &lt;- 6 trial_data &lt;- rep(0:1, times = c(n - z, z)) d &lt;- d %&gt;% mutate(likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) d %&gt;% ggplot(aes(x = theta, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Now we just need the marginal likelihood, \\(p(D)\\), to compute the posterior. Our first depiction will be the middle panel of the second row from the bottom–the panel with the uneven dolphin fins. d &lt;- d %&gt;% mutate(marginal_likelihood = sum(prior * likelihood)) %&gt;% mutate(posterior = (prior * likelihood) / marginal_likelihood) d %&gt;% ggplot(aes(x = theta, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Here, then, is a way to get the panel in on the right of the second row from the bottom. d %&gt;% mutate(marginal = (posterior / max(posterior)) * 25) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = marginal)) + geom_ribbon(fill = &quot;grey67&quot;, color = &quot;grey67&quot;) + coord_flip(ylim = c(0, 25)) + labs(subtitle = &quot;Remember, the scale on the x is arbitrary.&quot;, x = expression(omega), y = expression(Marginal~p(omega*&quot;|&quot;*D))) + theme(panel.grid = element_blank()) To make the middle bottom panel of Figure 10.3, we have to average the posterior values of \\(\\theta\\) over the grid of \\(\\omega\\) values. That is, we have to marginalize. d %&gt;% group_by(theta) %&gt;% summarise(marginal_theta = mean(posterior)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = marginal_theta)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(Marginal~p(theta*&quot;|&quot;*D))) + theme(panel.grid = element_blank()) For the lower right panel of Figure 10.3, we’ll filter() to our two focal values of \\(\\omega\\) and then facet by them. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% mutate(omega = str_c(&quot;omega == &quot;, omega)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(Marginal~p(theta*&quot;|&quot;*omega))) + theme(panel.grid = element_blank()) + facet_wrap(~omega, ncol = 1, scales = &quot;free&quot;, labeller = label_parsed) Do note the different scales on the \\(y\\). Here’s what they’d look like on the same scale. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% mutate(omega = str_c(&quot;omega == &quot;, omega)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(Marginal~p(theta*&quot;|&quot;*omega))) + theme(panel.grid = element_blank()) + facet_wrap(~omega, ncol = 1, labeller = label_parsed) Hopefully that helps build the intuition of what Kruschke meant when he wrote “visual inspection suggests that the ratio of the heights is about 5 to 1, which matches the Bayes factor of 4.68 that we computed exactly in the previous section” (p. 273, emphasis in the original). Using the grid, you might compute that BF like this. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% summarise(sum_posterior = sum(posterior)) %&gt;% mutate(model = c(&quot;model_2&quot;, &quot;model_1&quot;)) %&gt;% pivot_wider(-omega, names_from = model, values_from = sum_posterior) %&gt;% summarise(BF = model_1 / model_2) ## # A tibble: 1 x 1 ## BF ## &lt;dbl&gt; ## 1 4.68 10.3 Solution by MCMC Kruschke started with: “For large, complex models, we cannot derive \\(p(D | m)\\) analytically or with grid approximation, and therefore we will approximate the posterior probabilities using MCMC methods” (p. 274). He’s not kidding. Welcome to modern Bayes. 10.3.1 Nonhierarchical MCMC computation of each model’s marginal likelihood. Before you get excited, Kruschke warned: “For complex models, this method might not be tractable. [But] for the simple application here, however, the method works well, as demonstrated in the next section” (p. 277). 10.3.1.1 Implementation with JAGS brms. Load brms. library(brms) Let’s save the trial_data as a tibble. trial_data &lt;- tibble(y = trial_data) Time to learn a new brms skill. When you want to enter variables into the parameters defining priors in brms::brm(), you need to specify them using the stanvar() function. Since we want to do this for two variables, we’ll use stanvar() twice and save the results as an object, conveniently named stanvars. omega &lt;- .75 kappa &lt;- 12 stanvars &lt;- stanvar( omega * (kappa - 2) + 1, name = &quot;my_alpha&quot;) + stanvar((1 - omega) * (kappa - 2) + 1, name = &quot;my_beta&quot;) Now we have our stanvars object, we are ready to fit the first model (i.e., the model for which \\(\\omega = .75\\)). fit10.1 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, prior(beta(my_alpha, my_beta), class = Intercept), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, stanvars = stanvars, control = list(adapt_delta = .999), file = &quot;fits/fit10.01&quot;) Note how we fed our stanvars object into the stanvars function. Anyway, let’s inspect the chains. plot(fit10.1) They look great. Now we glance at the model summary. print(fit10.1) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: trial_data (Number of observations: 9) ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 1; ## total post-warmup samples = 40000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.69 0.10 0.48 0.86 1.00 8007 9306 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Next we’ll follow Kruschke and extract the posterior samples, saving them as theta. theta &lt;- posterior_samples(fit10.1) head(theta) ## b_Intercept lp__ ## 1 0.7263424 -4.691665 ## 2 0.7626307 -4.815941 ## 3 0.7222605 -4.686314 ## 4 0.8012281 -5.125448 ## 5 0.7373272 -4.714354 ## 6 0.6857306 -4.707360 The fixef() function will return the posterior summaries for the model intercept (i.e., \\(\\theta\\)). We can then index and save the desired summaries. fixef(fit10.1) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.691113 0.098457 0.4820692 0.863507 (mean_theta &lt;- fixef(fit10.1)[1]) ## [1] 0.691113 (sd_theta &lt;- fixef(fit10.1)[2]) ## [1] 0.098457 Now we’ll convert them to the \\(\\alpha\\) and \\(\\beta\\) parameters, a_post and b_post, respectively. a_post &lt;- mean_theta * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1) b_post &lt;- (1 - mean_theta) * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1) Recall we’ve already defined several values. n &lt;- 9 z &lt;- 6 omega &lt;- .75 kappa &lt;- 12 The reason we’re saving all these values is we’re aiming to compute \\(p(D)\\), the probability of the data (i.e., the marginal likelihood), given the model. But our intermediary step will be computing its reciprocal, \\(\\frac{1}{p(D)}\\). Here we’ll express Kruschke’s oneOverPD as a function, one_over_pd(). one_over_pd &lt;- function(theta) { mean(dbeta(theta, a_post, b_post ) / (theta^z * (1 - theta)^(n - z) * dbeta(theta, omega * (kappa - 2) + 1, (1 - omega) * (kappa - 2) + 1 ))) } We’re ready to use one_over_pd() to help compute \\(p(D)\\). theta %&gt;% summarise(pd = 1 / one_over_pd(theta = b_Intercept)) ## pd ## 1 0.002338466 That matches up nicely with Kruschke’s value! Let’s rinse, wash, and repeat for \\(\\omega = .25\\). First, we’ll need to redefine omega and our stanvars. omega &lt;- .25 stanvars &lt;- stanvar( omega * (kappa - 2) + 1, name = &quot;my_alpha&quot;) + stanvar((1 - omega) * (kappa - 2) + 1, name = &quot;my_beta&quot;) Fit the model. fit10.2 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, prior(beta(my_alpha, my_beta), class = Intercept), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, stanvars = stanvars, control = list(adapt_delta = .999), file = &quot;fits/fit10.02&quot;) We’ll do the rest in bulk. theta &lt;- posterior_samples(fit10.2) mean_theta &lt;- fixef(fit10.2)[1] sd_theta &lt;- fixef(fit10.2)[2] a_post &lt;- mean_theta * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1) b_post &lt;- (1 - mean_theta) * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1) theta %&gt;% summarise(pd = 1 / one_over_pd(theta = b_Intercept)) ## pd ## 1 0.0004992476 Boom! 10.3.2 Hierarchical MCMC computation of relative model probability is not available in brms: We’ll cover information criteria instead. I’m not aware of a way to specify a model “in which the top-level parameter is the index across models” in brms (p. 278). If you know of a way, share your code. However, we do have options. We can compare and weight models using information criteria, about which you can learn more here. In brms, the LOO and WAIC are two primary information criteria available. You can compute them for a given model with the loo() and waic() functions, respectively. Here’s a quick example of how to use the waic() function. waic(fit10.1) ## ## Computed from 40000 by 9 log-likelihood matrix ## ## Estimate SE ## elpd_waic -6.2 1.3 ## p_waic 0.5 0.1 ## waic 12.5 2.7 We’ll explain that output in a bit. Before we do, you should know the current recommended workflow for information criteria with brms models is to use the add_criterion() function, which will allow us to compute information-criterion-related output and save it to our brms fit objects. Here’s how to do that with both our fits. fit10.1 &lt;- add_criterion(fit10.1, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit10.2 &lt;- add_criterion(fit10.2, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) You can extract the same WAIC output for fit10.1 we saw above by executing fit10.1$criteria$waic. Here we look at the LOO summary for fit10.2, instead. fit10.2$criteria$loo ## ## Computed from 40000 by 9 log-likelihood matrix ## ## Estimate SE ## elpd_loo -7.1 0.3 ## p_loo 0.5 0.0 ## looic 14.1 0.6 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. You get a wealth of output, more of which can be seen by executing str(fit10.1$criteria$loo). First, notice the message “All Pareto k estimates are good (k &lt; 0.5).” Pareto \\(k\\) values can be used for diagnostics. Each case in the data gets its own \\(k\\) value and we like it when those \\(k\\)s are low. The makers of the loo package get worried when \\(k\\) values exceed 0.7 and, as a result, we will get warning messages when they do. Happily, we have no such warning messages in this example. In the main section, we get estimates for the expected log predictive density (elpd_loo), the estimated effective number of parameters (p_loo), and the Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO; looic). Each estimate comes with a standard error (i.e., SE). Like other information criteria, the LOO values aren’t of interest in and of themselves. However, the estimate of one model’s LOO relative to that of another is of great interest. We generally prefer models with lower information criteria. With the loo_compare() function, we can compute a formal difference score between two models. loo_compare(fit10.1, fit10.2, criterion = &quot;loo&quot;) ## elpd_diff se_diff ## fit10.1 0.0 0.0 ## fit10.2 -0.8 1.7 The loo_compare() output rank orders the models such that the best fitting model appears on top. All models receive a difference score relative to the best model. Here the best fitting model is fit10.1 and since the LOO for fit10.1 minus itself is zero, the values in the top row are all zero. Each difference score also comes with a standard error. In this case, even though fit10.1 has the lower estimates, the standard error is twice the magnitude of the difference score. So the LOO difference score puts the two models on similar footing. You can do a similar analysis with the WAIC estimates. In addition to difference-score comparisons, you can also use the LOO or WAIC for AIC-type model weighting. In brms, you do this with the model_weights() function. (mw &lt;- model_weights(fit10.1, fit10.2)) ## fit10.1 fit10.2 ## 0.830191 0.169809 I don’t know that I’d call these weights probabilities, but they do sum to one. In this case, the analysis suggests we put about five times more weight to fit10.1 relative to fit10.2. mw[1] / mw[2] ## fit10.1 ## 4.888969 With brms::model_weights(), we have a variety of weighting schemes avaliable to us. Since we didn’t specify any in the weights argument, we used the default &quot;stacking&quot;, which is–perhaps confusingly given the name–the stacking method according to the paper by Yao et al. (2018) Vehtari has written about the paper on Gelman’s blog, too. But anyway, the point is that different weighting schemes might not produce the same results. For example, here’s the result from weighting using the WAIC. model_weights(fit10.1, fit10.2, weights = &quot;waic&quot;) ## fit10.1 fit10.2 ## 0.6967995 0.3032005 The results are similar, for sure. But they’re not the same. The stacking method via the brms default weights = &quot;stacking&quot; is the current preferred method by the folks on the Stan team (e.g., the authors of the above linked paper). For more on stacking and other weighting schemes, see Vehtari and Gabry’s (2019) vignette, Bayesian Stacking and Pseudo-BMA weights using the loo package, or Vehtari’s modelselection_tutorial GitHub repository. But don’t worry. We will have more opportunities to practice with information criteria, model weights, and such later in this project. 10.3.2.1 Using [No need to use] pseudo-priors to reduce autocorrelation. Since we didn’t use Kruschke’s method from the last subsection, we don’t have the same worry about autocorrelation. For example, here are the autocorrelation plots for fit10.1. library(bayesplot) mcmc_acf(posterior_samples(fit10.1, add_chain = T), pars = &quot;b_Intercept&quot;, lags = 35) Our autocorrelations were a little high for HMC, but nowhere near pathological. The results for fit10.2 were similar. As you might imagine from the moderate autocorrelations, the \\(N_{eff}/N\\) ratio for b_Intercept wasn’t great. neff_ratio(fit10.1)[1] %&gt;% mcmc_neff() + yaxis_text(hjust = 0) But we specified a lot of post-warmup iterations, so we’re still in good shape. Plus, the \\(\\hat R\\) was fine. rhat(fit10.1)[1] ## b_Intercept ## 1.000613 10.3.3 Models with different “noise” distributions in JAGS brms. Probability distribution[s are] sometimes [called “noise”] distribution[s] because [they describe] the random variability of the data values around the underlying trend. In more general applications, different models can have different noise distributions. For example, one model might describe the data as log-normal distributed, while another model might describe the data as gamma distributed. (p. 288) If there are more than one plausible noise distributions for our data, we might want to compare the models. Kruschke then gave us a general trick in the form of this JAGS code: data { C &lt;- 10000 # JAGS does not warn if too small! for (i in 1:N) { ones[i] &lt;- 1 } } model { for (i in 1:N) { spy1[i] &lt;- pdf1(y[i], parameters1) / C # where pdf1 is a formula spy2[i] &lt;- pdf2(y[i], parameters2) / C # where pdf2 is a formula spy[i] &lt;- equals(m,1) * spy1[i] + equals(m, 2) * spy2[i] ones[i] ~ dbern(spy[i]) } parameters1 ~ dprior1... parameters2 ~ dprior2... m ~ dcat(mPriorProb[]) mPriorProb[1] &lt;- .5 mPriorProb[2] &lt;- .5 } I’m not aware that we can do this within the Stan/brms framework. If I’m in error and you know how, please share your code. However, we do have options. In anticipation of Chapter 16, let’s consider Gaussian-like data with thick tails. We might generate some like this: # how many draws would you like? n &lt;- 1e3 set.seed(10) (d &lt;- tibble(y = rt(n, df = 7))) ## # A tibble: 1,000 x 1 ## y ## &lt;dbl&gt; ## 1 0.0214 ## 2 -0.987 ## 3 0.646 ## 4 -0.237 ## 5 0.977 ## 6 -0.200 ## 7 0.781 ## 8 -1.09 ## 9 1.83 ## 10 -0.682 ## # … with 990 more rows The resulting data look like this. d %&gt;% ggplot(aes(x = y)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) As you’d expect with a small-\\(\\nu\\) Student’s \\(t\\), some of our values are far from the central clump. If you don’t recall, Student’s \\(t\\)-distribution has three parameters: \\(\\nu\\), \\(\\mu\\), and \\(\\sigma\\). The Gaussian is a special case of Student’s \\(t\\) for which \\(\\nu = \\infty\\). As \\(\\nu\\) gets small, the distribution allocates more mass in the tails. From a Gaussian perspective, the small-\\(\\nu\\) Student’s \\(t\\) expects more outliers–though it’s a little odd calling them outliers from a small-\\(\\nu\\) Student’s \\(t\\) perspective. Let’s see how well the Gaussian versus the Student’s \\(t\\) likelihoods handle the data. Here we’ll use fairly liberal priors. fit10.3 &lt;- brm(data = d, family = gaussian, y ~ 1, prior = c(prior(normal(0, 5), class = Intercept), prior(normal(0, 5), class = sigma)), # by default, this has a lower bound of 0 chains = 4, cores = 4, seed = 10, file = &quot;fits/fit10.03&quot;) fit10.4 &lt;- brm(data = d, family = student, y ~ 1, prior = c(prior(normal(0, 5), class = Intercept), prior(normal(0, 5), class = sigma), prior(gamma(2, 0.1), class = nu)), # this is the brms default prior for nu chains = 4, cores = 4, seed = 10, file = &quot;fits/fit10.04&quot;) In case you were curious, here’s what that default gamma(2, 0.1) prior on nu looks like. tibble(x = seq(from = 0, to = 110, by = 1)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = dgamma(x, 2, 0.1))) + geom_ribbon(size = 0, fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(italic(p)(nu))) + coord_cartesian(xlim = c(0, 100)) + theme(panel.grid = element_blank()) That prior puts most of the probability mass below 50, but the right tail gently fades off into the triple digits, allowing for the possibility of larger estimates. We can use the posterior_summary() function to get a compact look at the model summaries. posterior_summary(fit10.3) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -0.03 0.04 -0.11 0.05 ## sigma 1.25 0.03 1.20 1.31 ## lp__ -1646.97 0.98 -1649.49 -1646.02 posterior_summary(fit10.4) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -0.01 0.04 -0.08 0.06 ## sigma 0.98 0.04 0.90 1.05 ## nu 5.76 1.02 4.12 8.06 ## lp__ -1590.50 1.26 -1593.76 -1589.07 Now we can compare the two approaches using information criteria. For kicks, we’ll use the WAIC. fit10.3 &lt;- add_criterion(fit10.3, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit10.4 &lt;- add_criterion(fit10.4, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(fit10.3, fit10.4, criterion = &quot;waic&quot;) ## elpd_diff se_diff ## fit10.4 0.0 0.0 ## fit10.3 -60.3 40.1 Based on the WAIC difference, we have some support for preferring the Student’s \\(t\\), but do notice how wide that SE was. We can also compare the models using model weights. Here we’ll use the default weighting scheme. model_weights(fit10.3, fit10.4) ## fit10.3 fit10.4 ## 0.03221231 0.96778769 Virtually all of the stacking weight was placed on the Student’s-\\(t\\) model, fit10.4. Remember what that \\(p(\\nu)\\) looked like? Here’s our posterior distribution for \\(\\nu\\). posterior_samples(fit10.4) %&gt;% ggplot(aes(x = nu)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 20)) + labs(subtitle = expression(&quot;Recall that for the Gaussian, &quot;*nu==infinity.), x = expression(italic(p)(nu*&quot;|&quot;*italic(D)))) + theme(panel.grid = element_blank()) Even though our prior for \\(\\nu\\) was relatively weak, the posterior ended up concentrated on values in the middle-single-digit range. Recall the data-generating value was 7. We can also compare the models using posterior-predictive checks. There are a variety of ways we might do this, but the most convenient way is with brms::pp_check(), which is itself a wrapper for the family of ppc functions from the bayesplot package. pp_check(fit10.3) pp_check(fit10.4) The default pp_check() setting allows us to compare the density of the data \\(y\\) (i.e., the dark blue) with 10 density’s simulated from the posterior \\(y_\\text{rep}\\) (i.e., the light blue). We prefer models that produce \\(y_\\text{rep}\\) distributions resembling \\(y\\). Though the results from both models were similar, the simulated distributions from fit10.4 mimicked the original data a little more convincingly. To learn more about this approach to posterior predictive checks, check out Gabry’s (2019) vignette, Graphical posterior predictive checks using the bayesplot package. 10.4 Prediction: Model averaging In many applications of model comparison, the analyst wants to identify the best model and then base predictions of future data on that single best model, denoted with index \\(b\\). In this case, predictions of future \\(\\hat{y}\\) are based exclusively on the likelihood function \\(p_b(\\hat{y} | \\theta_b, m = b)\\) and the posterior distribution \\(p_b(\\theta_b | D, m = b)\\) of the winning model: \\[p_b(\\hat y | D, m = b) = \\int \\text d \\theta_b p_b (\\hat{y} | \\theta_b, m = b) p_b(\\theta_b | D, m = b)\\] But the full model of the data is actually the complete hierarchical structure that spans all the models being compared, as indicated in Figure 10.1 (p. 267). Therefore, if the hierarchical structure really expresses our prior beliefs, then the most complete prediction of future data takes into account all the models, weighted by their posterior credibilities. In other words, we take a weighted average across the models, with the weights being the posterior probabilities of the models. Instead of conditionalizing on the winning model, we have \\[\\begin{align*} p (\\hat y | D) &amp; = \\sum_m p (\\hat y | D, m) p (m | D) \\\\ &amp; = \\sum_m \\int \\text d \\theta_m p_m (\\hat{y} | \\theta_m, m) p_m(\\theta_m | D, m) p (m | D) \\end{align*}\\] This is called model averaging. (p. 289) Okay, while the concept of model averaging is of great interest, we aren’t going to be able to follow this approach to it within the Stan/brms paradigm. This, recall, is because our paradigm doesn’t allow for a hierarchical organization of models in the same way JAGS does. However, we can still play the model averaging game with extensions of our model weighting paradigm, above. Before we get into the details, recall that there were two models of mints that created the coin, with one mint being tail-biased with mode \\(\\omega = 0.25\\) and one mint being head-biased with mode \\(\\omega = 0.75\\) The two subpanels in the lower-right [of Figure 10.3] illustrate the posterior distributions on \\(\\omega\\) within each model, \\(p(\\theta | D, \\omega = 0.25)\\) and \\(p(\\theta | D, \\omega = 0.75)\\) The winning model was \\(\\omega = 0.75\\), and therefore the predicted value of future data, based on the winning model alone, would use \\(p(\\theta | D, \\omega = 0.75)\\). (p. 289) Here’s the histogram for \\(p(\\theta | D, \\omega = 0.75)\\), which we generate from our fit10.1. library(tidybayes) posterior_samples(fit10.1) %&gt;% ggplot(aes(x = b_Intercept, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;The posterior for the probability, given fit10.1&quot;, x = expression(italic(p)(theta*&quot;|&quot;*italic(D)*&quot;, &quot;*omega==.75))) + xlim(0, 1) + theme(panel.grid = element_blank()) But the overall model included \\(\\omega = 0.75\\), and if we use the overall model, then the predicted value of future data should be based on the complete posterior summed across values of \\(\\omega\\). The complete posterior distribution [is] \\(p(\\theta | D)\\) (p. 289). The cool thing about the model weighting stuff we learned about earlier is that you can use those model weights to average across models. Again, we’re not weighting the models by posterior probabilities the way Kruschke discussed in text. However, the spirit is similar. We can use the brms::pp_average() function to make posterior predictive prediction with mixtures of the models, weighted by our chosen weighting scheme. Here, we’ll go with the default stacking weights. nd &lt;- tibble(y = 1) pp_a &lt;- pp_average(fit10.1, fit10.2, newdata = nd, # this line is not necessary, but you should see how to choose weighing methods weights = &quot;stacking&quot;, method = &quot;fitted&quot;, summary = F) %&gt;% as_tibble() %&gt;% set_names(&quot;theta&quot;) # what does this produce? head(pp_a) ## # A tibble: 6 x 1 ## theta ## &lt;dbl&gt; ## 1 0.730 ## 2 0.594 ## 3 0.736 ## 4 0.853 ## 5 0.839 ## 6 0.744 We can plot our model-averaged \\(\\theta\\) with a little help from good old tidybayes::stat_histintervalh(). pp_a %&gt;% ggplot(aes(x = theta, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .4, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;The posterior for the probability, given the\\nweighted combination of fit10.1 and fit10.2&quot;, x = expression(italic(p)(theta*&quot;|&quot;*italic(D)))) + coord_cartesian(xlim = c(0, 1)) + theme(panel.grid = element_blank()) As Kruschke concluded, “you can see the contribution of \\(p(\\theta | D, \\omega = 0.25)\\) as the extended leftward tail” (p. 289). Interestingly enough, that looks a lot like the density we made with grid approximation in Figure 10.3, doesn’t it? 10.5 Model complexity naturally accounted for A complex model (usually) has an inherent advantage over a simpler model because the complex model can find some combination of its parameter values that match the data better than the simpler model. There are so many more parameter options in the complex model that one of those options is likely to fit the data better than any of the fewer options in the simpler model. The problem is that data are contaminated by random noise, and we do not want to always choose the more complex model merely because it can better fit noise. Without some way of accounting for model complexity, the presence of noise in data will tend to favor the complex model. Bayesian model comparison compensates for model complexity by the fact that each model must have a prior distribution over its parameters, and more complex models must dilute their prior distributions over larger parameter spaces than simpler models. Thus, even if a complex model has some particular combination of parameter values that fit the data well, the prior probability of that particular combination must be small because the prior is spread thinly over the broad parameter space. (pp. 289–290) Now our two models are: \\(p(\\theta | D, \\kappa = 2000)\\) (i.e., the “must-be-fair” model) and \\(p(\\theta | D, \\kappa = 2)\\) (i.e., the “anything’s-possible” model). They look like this. # how granular to you want the theta sequence? n &lt;- 1e3 # simulate the data tibble(omega = .5, kappa = c(1000, 2), model = c(&quot;The must-be-fair model&quot;, &quot;The anything&#39;s-possible model&quot;)) %&gt;% expand(nesting(omega, kappa, model), theta = seq(from = 0, to = 1, length.out = n)) %&gt;% mutate(density = dbeta(theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% # plot ggplot(aes(x = theta, ymin = 0, ymax = density)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Note that in this case, their y-axes are on the same scale.&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) + facet_wrap(~model) Here’s how you might compute the \\(\\alpha\\) and \\(\\beta\\) values for the corresponding beta distributions. tibble(omega = .5, kappa = c(1000, 2), model = c(&quot;The must-be-fair model&quot;, &quot;The anything&#39;s-possible model&quot;)) %&gt;% mutate(alpha = omega * (kappa - 2) + 1, beta = (1 - omega) * (kappa - 2) + 1) ## # A tibble: 2 x 5 ## omega kappa model alpha beta ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5 1000 The must-be-fair model 500 500 ## 2 0.5 2 The anything&#39;s-possible model 1 1 With those in hand, we can use our p_d() function to compute the Bayes factor based on flipping a coin \\(N = 20\\) times and observing \\(z = 15\\) heads. # the data summaries z &lt;- 15 n &lt;- 20 p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1) ## [1] 0.3229023 Let’s try again, this time supposing we observe \\(z = 15\\) heads out of \\(N = 20\\) coin flips. z &lt;- 11 p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1) ## [1] 3.337148 The anything’s-possible model loses because it pays the price of having a small prior probability on the values of \\(\\theta\\) near the data proportion, while the must-be-fair model has large prior probability on \\(\\theta\\) values sufficiently near the data proportion to be credible. Thus, in Bayesian model comparison, a simpler model can win if the data are consistent with it, even if the complex model fits just as well. The complex model pays the price of having small prior probability on parameter values that describe simple data. (p. 291) 10.5.1 Caveats regarding nested model comparison. A frequently encountered special case of comparing models of different complexity occurs when one model is “nested” within the other. Consider a model that implements all the meaningful parameters we can contemplate for the particular application. We call that the full model. We might consider various restrictions of those parameters, such as setting some of them to zero, or forcing some to be equal to each other. A model with such a restriction is said to be nested within the full model. (p. 291) Kruschke didn’t walk out the examples in this section. But for the sake of practice, let’s work through the first one. “Recall the hierarchical model of baseball batting abilities” from Chapter 9 (p. 291). Let’s reload those data. my_data &lt;- read_csv(&quot;data.R/BattingAverage.csv&quot;) glimpse(my_data) ## Rows: 948 ## Columns: 6 ## $ Player &lt;chr&gt; &quot;Fernando Abad&quot;, &quot;Bobby Abreu&quot;, &quot;Tony Abreu&quot;, &quot;Dustin Ac… ## $ PriPos &lt;chr&gt; &quot;Pitcher&quot;, &quot;Left Field&quot;, &quot;2nd Base&quot;, &quot;2nd Base&quot;, &quot;1st Ba… ## $ Hits &lt;dbl&gt; 1, 53, 18, 137, 21, 0, 0, 2, 150, 167, 0, 128, 66, 3, 1,… ## $ AtBats &lt;dbl&gt; 7, 219, 70, 607, 86, 1, 1, 20, 549, 576, 1, 525, 275, 12… ## $ PlayerNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1… ## $ PriPosNumber &lt;dbl&gt; 1, 7, 4, 4, 3, 1, 1, 3, 3, 4, 1, 5, 4, 2, 7, 4, 6, 8, 9,… “The full model has a distinct modal batting ability, \\(\\omega_c\\) , for each of the nine fielding positions. The full model also has distinct concentration parameters for each of the nine positions” (p. 291). Let’s fit that model again. fit9.2 &lt;- brm(data = my_data, family = binomial(link = logit), Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = .99), seed = 9, file = &quot;fits/fit09.02&quot;) Next we’ll consider a restricted version of fit9.2 “in which all infielders (first base, second base, etc.) are grouped together versus all outfielders (right field, center field, and left field). In this restricted model, we are forcing the modal batting abilities of all the outfielders to be the same, that is, \\(\\omega_\\text{left field} = \\omega_\\text{center field} = \\omega_\\text{right field}\\)” (p. 291). To fit that model, we’ll need to make a new variable PriPos_small which is identical to its parent variable PriPos except that it collapses those three positions into our new category Outfield. my_data &lt;- my_data %&gt;% mutate(PriPos_small = if_else(PriPos %in% c(&quot;Center Field&quot;, &quot;Left Field&quot;, &quot;Right Field&quot;), &quot;Outfield&quot;, PriPos)) Now use update() to fit the restricted model. fit10.5 &lt;- update(fit9.2, newdata = my_data, formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos_small) + (1 | PriPos_small:Player), iter = 3500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = .99), seed = 10, file = &quot;fits/fit10.05&quot;) Unlike with what Kruschke alluded to in the prose, here we’ll compare the two models with the WAIC. fit9.2 &lt;- add_criterion(fit9.2, criterion = &quot;waic&quot;) fit10.5 &lt;- add_criterion(fit10.5, criterion = &quot;waic&quot;) loo_compare(fit9.2, fit10.5, criterion = &quot;waic&quot;) ## elpd_diff se_diff ## fit9.2 0.0 0.0 ## fit10.5 -0.3 1.2 Based on the WAIC difference score, they’re near equivalent. Now let’s see how their WAIC weights shake out. model_weights(fit9.2, fit10.5, weights = &quot;waic&quot;) %&gt;% round(2) ## fit9.2 fit10.5 ## 0.58 0.42 In this case, just a little more of the weight went to the full model, fit9.2. The overall pattern between the WAIC difference and the WAIC weights was uncertainty. Make sure to use good substantive reasoning when comparing models. 10.6 Extreme sensitivity to the prior distribution In many realistic applications of Bayesian model comparison, the theoretical emphasis is on the difference between the models’ likelihood functions. For example, one theory predicts planetary motions based on elliptical orbits around the sun, and another theory predicts planetary motions based on circular cycles and epicycles around the earth. The two models involve very different parameters. In these sorts of models, the form of the prior distribution on the parameters is not a focus, and is often an afterthought. But, when doing Bayesian model comparison, the form of the prior is crucial because the Bayes factor integrates the likelihood function weighted by the prior distribution. (p. 292) However, “the sensitivity of Bayes factors to prior distributions is well known in the literature (e.g., Kass &amp; Raftery, 1995; Liu &amp; Aitkin, 2008; Vanpaemel, 2010),” and furthermore, when comparing Bayesian models using the methods Kruschke outlined in this chapter of the text, “different forms of vague priors can yield very different Bayes factors” (p. 293). In the two BFs to follow, we compare the must-be-fair model and the anything’s-possible models from 10.5 to new data: \\(z = 65, N = 100\\). z &lt;- 65 n &lt;- 100 p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1) ## [1] 0.125287 The resulting 0.13 favored the anything’s-possible model. Another way to express the anything’s-possible model is with the Haldane prior, which sets the two parameters within the beta distribution to be a) equivalent and b) quite small (i.e., 0.01 in this case). p_d(z, n, a = 500, b = 500) / p_d(z, n, a = .01, b = .01) ## [1] 5.728066 Now we flipped to favoring the must-be-fair model. You might be asking, Wait, kind of distribution did that Haldane prior produce? Here we compare it to the \\(\\operatorname{beta} (1, 1)\\). # how granular to you want the theta sequence? length &lt;- 1e3 # simulate the data tibble(alpha = c(1, .01), beta = c(1, .01), model = factor(c(&quot;Uninformative prior, beta(1, 1)&quot;, &quot;Haldane prior, beta(0.01, 0.01)&quot;), levels = c(&quot;Uninformative prior, beta(1, 1)&quot;, &quot;Haldane prior, beta(0.01, 0.01)&quot;))) %&gt;% expand(nesting(alpha, beta, model), theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(density = dbeta(theta, shape1 = alpha, shape2 = beta)) %&gt;% # plot ggplot(aes(x = theta, ymin = 0, ymax = density)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;We have two anything’s-possible models.&quot;, subtitle = &quot;These y-axes are on the same scale.&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) + facet_wrap(~model) Before we can complete the analyses of this subsection, we’ll need to define our version of Kruschke’s HDIofICDF function(), hdi_of_icdf(). Like we’ve done in previous chapters, here we mildly reformat the function. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } And here we’ll make a custom variant to be more useful within the context of map2(). hdi_of_qbeta &lt;- function(shape1, shape2) { hdi_of_icdf(name = qbeta, shape1 = shape1, shape2 = shape2) %&gt;% data.frame() %&gt;% mutate(level = c(&quot;ll&quot;, &quot;ul&quot;)) %&gt;% spread(key = level, value = &quot;.&quot;) } Recall that when we combine a \\(\\operatorname{beta} (\\theta | \\alpha, \\beta)\\) prior with the results of a Bernoulli likelihood, we get a posterior defined by \\(\\operatorname{beta} (\\theta | z + \\alpha, N - z + \\beta)\\). d &lt;- tibble(model = c(&quot;Uniform&quot;, &quot;Haldane&quot;), prior_a = c(1, .01), prior_b = c(1, .01)) %&gt;% mutate(posterior_a = z + prior_a, posterior_b = n - z + prior_b) d ## # A tibble: 2 x 5 ## model prior_a prior_b posterior_a posterior_b ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Uniform 1 1 66 36 ## 2 Haldane 0.01 0.01 65.0 35.0 Now we’ll use our custom hdi_of_qbeta() to compute the HDIs. ( d &lt;- d %&gt;% mutate(levels = map2(posterior_a, posterior_b, hdi_of_qbeta)) %&gt;% unnest(levels) ) ## # A tibble: 2 x 7 ## model prior_a prior_b posterior_a posterior_b ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Uniform 1 1 66 36 0.554 0.738 ## 2 Haldane 0.01 0.01 65.0 35.0 0.556 0.742 Let’s compare those HDIs in a plot. d %&gt;% ggplot(aes(x = ll, xend = ul, y = model, yend = model)) + geom_segment(size = .75) + scale_x_continuous(expression(theta), limits = c(0, 1)) + labs(subtitle = &quot;Those two sets of HDIs are quite similar.\\nIt almost seems silly their respective BFs\\nare so different.&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) “The HDIs are virtually identical. In particular, for either prior, the posterior distribution rules out \\(\\theta = 0.5\\), which is to say that the must-be-fair hypothesis is not among the credible values” (p. 294). 10.6.1 Priors of different models should be equally informed. “We have established that seemingly innocuous changes in the vagueness of a vague prior can dramatically change a model’s marginal likelihood, and hence its Bayes factor in comparison with other models. What can be done to ameliorate the problem” (p. 294)? Kruschke posed one method might be taking a small representative portion of the data in hand and use them to make an empirically-based prior for the remaining set of data. From our previous example, “suppose that the 10% subset has 6 heads in 10 flips, so the remaining 90% of the data has \\(z = 65 − 6\\) and \\(N = 100 − 10\\)” (p. 294). Here are the new Bayes factors based on that method. z &lt;- 65 - 6 n &lt;- 100 - 10 # Peaked vs Uniform p_d(z, n, a = 500 + 6, b = 500 + 10 - 6) / p_d(z, n, a = 1 + 6, b = 1 + 10 - 6) ## [1] 0.05570509 # Peaked vs Haldane p_d(z, n, a = 500 + 6, b = 500 + 10 - 6) / p_d(z, n, a = .01 + 6, b = .01 + 10 - 6) ## [1] 0.05748123 Now the two Bayes Factors are nearly the same. It’s not in the text, but let’s compare these three models using brms, information criteria, model weights, model averaging, and posterior predictive checks. First, we’ll save the \\(z\\) and \\(N\\) information as a tibble with a series of 0s and 1s. z &lt;- 65 n &lt;- 100 trial_data &lt;- tibble(y = rep(0:1, times = c(n - z, z))) glimpse(trial_data) ## Rows: 100 ## Columns: 1 ## $ y &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… Next, fit the three models with brms::brm(). fit10.6 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, prior(beta(500, 500), class = Intercept), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, file = &quot;fits/fit10.06&quot;) fit10.7 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, # Uniform prior(beta(1, 1), class = Intercept), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, file = &quot;fits/fit10.07&quot;) fit10.8 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, # Haldane prior(beta(0.01, 0.01), class = Intercept), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, file = &quot;fits/fit10.08&quot;) Compare the models by the LOO. fit10.6 &lt;- add_criterion(fit10.6, criterion = &quot;loo&quot;) fit10.7 &lt;- add_criterion(fit10.7, criterion = &quot;loo&quot;) fit10.8 &lt;- add_criterion(fit10.8, criterion = &quot;loo&quot;) loo_compare(fit10.6, fit10.7, fit10.8) ## elpd_diff se_diff ## fit10.7 0.0 0.0 ## fit10.8 0.0 0.1 ## fit10.6 -2.9 2.7 Based on the LOO comparisons, none of the three models was a clear favorite. Although both versions of the anything’s-possible model (i.e., fit10.7 and fit10.8) had lower numeric estimates than the must-be-fair model (i.e., fit10.6), the standard errors on the difference scores were the same magnitude as the difference estimates themselves. As for comparing the two variants of the anything’s-possible model directly, their LOO estimates were almost indistinguishable. Now let’s see what happens when we compute their model weights. Here we’ll contrast the LOO weights with the stacking weights. mw &lt;- model_weights(fit10.6, fit10.7, fit10.8, weights = &quot;stacking&quot;) mw %&gt;% round(digits = 2) ## fit10.6 fit10.7 fit10.8 ## 0.12 0.33 0.55 model_weights(fit10.6, fit10.7, fit10.8, weights = &quot;loo&quot;) %&gt;% round(digits = 2) ## fit10.6 fit10.7 fit10.8 ## 0.03 0.49 0.48 Here the evidence varied by the specific weight. Across both, the model with the Haldane prior (fit10.8) did arguably the best. But the model with the uniform prior (fit10.9) was clearly in the running. Overall, the evidence for one versus another was weak. Like we did earlier with fit10.1 and fit10.2, we can use the pp_average() function to compute the stacking weighted posterior for \\(\\theta\\). pp_average(fit10.6, fit10.7, fit10.8, newdata = nd, weights = mw, method = &quot;fitted&quot;, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = V1, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;The posterior for the probability, given the weighted\\ncombination of fit10.6, fit10.7, and fit10.8&quot;, x = expression(italic(p)(theta*&quot;|&quot;*italic(D)))) + coord_cartesian(xlim = c(0, 1)) + theme(panel.grid = element_blank()) Did you notice the weights = mw argument, there? From the pp_average.brmsfit section of the brms reference manual (Bürkner, 2020g, p. 155), we read “weights may also be be a numeric vector of pre-specified weights.” Since we saved the results of model_weights() as an object mw, we were able to capitalize on that feature. If you leave out that argument, you’ll have to wait a bit for brms to compute those weights again from scratch. Just for the sake of practice, we can also compare the models with separate posterior predictive checks using pp_check(). p1 &lt;- pp_check(fit10.6, type = &quot;bars&quot;, nsamples = 1e3) + ggtitle(&quot;fit10.6&quot;, subtitle = expression(&quot;beta&quot;*(500*&quot;, &quot;*500))) p2 &lt;- pp_check(fit10.7, type = &quot;bars&quot;, nsamples = 1e3) + ggtitle(&quot;fit10.7&quot;, subtitle = expression(&quot;beta&quot;*(1*&quot;, &quot;*1))) p3 &lt;- pp_check(fit10.8, type = &quot;bars&quot;, nsamples = 1e3) + ggtitle(&quot;fit10.8&quot;, subtitle = expression(&quot;beta&quot;*(0.01*&quot;, &quot;*0.01))) ((p1 + p2 + p3) &amp; scale_x_continuous(breaks = 0:1) &amp; ylim(0, 80) &amp; theme_grey() &amp; theme(panel.grid = element_blank())) + plot_layout(guides = &#39;collect&#39;) Instead of the default 10, this time we used 1,000 posterior simulations from each fit, which we summarized with dot and error bars. This method did a great job showing how little fit10.6 learned from the data. Another nice thing about this method is it reveals how similar the results are between fit10.7 and fit10.8, the two alternate versions of the anything’s-possible model. Also, did you notice how we used ylim(0, 80) when combining the plots with patchwork? Holding the scale of the axes constant makes it easier to compare results across plots. 10.7 Bonus: There’s danger ahead If you’re new to model comparison with Bayes factors, information criteria, model stacking and so on, you should know these methods are still subject to spirited debate amongst scholars. For a recent example, see Gronau and Wagenmakers’ (2019a) Limitations of Bayesian leave-one-out cross-validation for model selection, which criticized the LOO. Their paper was commented on by D. J. Navarro (2019), Chandramouli &amp; Shiffrin (2019), and Vehtari, Simpson, et al. (2019). You can find Gronau and Wagenmakers’ (2019b) rejoinder here. And if you love those hot scholarly twitter discussions, these topics seem to spawn one every few months or so (e.g., here). Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.3.9000 bayesplot_1.7.1 brms_2.12.0 ## [4] Rcpp_1.0.4.6 patchwork_1.0.0 forcats_0.5.0 ## [7] stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 ## [10] readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ## [13] ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.4.1 rstudioapi_0.11 farver_2.0.3 ## [10] rstan_2.19.3 svUnit_1.0.3 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 ## [16] xml2_1.3.1 bridgesampling_1.0-0 knitr_1.28 ## [19] shinythemes_1.1.2 jsonlite_1.6.1 broom_0.5.5 ## [22] dbplyr_1.4.2 shiny_1.4.0.2 compiler_3.6.3 ## [25] httr_1.4.1 backports_1.1.6 assertthat_0.2.1 ## [28] Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 ## [31] later_1.0.0 prettyunits_1.1.1 htmltools_0.4.0 ## [34] tools_3.6.3 igraph_1.2.5 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 ## [40] cellranger_1.1.0 vctrs_0.3.0 nlme_3.1-144 ## [43] crosstalk_1.1.0.1 xfun_0.13 ps_1.3.3 ## [46] rvest_0.3.5 mime_0.9 miniUI_0.1.1.1 ## [49] lifecycle_0.2.0 gtools_3.8.2 zoo_1.8-7 ## [52] scales_1.1.1 colourpicker_1.0 hms_0.5.3 ## [55] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 ## [58] inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [61] gridExtra_2.3 StanHeaders_2.21.0-1 loo_2.2.0 ## [64] stringi_1.4.6 highr_0.8 dygraphs_1.1.1.6 ## [67] pkgbuild_1.0.8 rlang_0.4.6 pkgconfig_2.0.3 ## [70] matrixStats_0.56.0 HDInterval_0.2.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [76] labeling_0.3 tidyselect_1.0.0 processx_3.4.2 ## [79] plyr_1.8.6 magrittr_1.5 bookdown_0.18 ## [82] R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [85] pillar_1.4.4 haven_2.2.0 withr_2.2.0 ## [88] xts_0.12-0 abind_1.4-5 modelr_0.1.6 ## [91] crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 ## [94] rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 ## [97] callr_3.4.3 threejs_0.3.3 reprex_0.3.0 ## [100] digest_0.6.25 xtable_1.8-4 httpuv_1.5.2 ## [103] stats4_3.6.3 munsell_0.5.0 viridisLite_0.3.0 ## [106] shinyjs_1.1 References "],
["null-hypothesis-significance-testing.html", "11 Null Hypothesis Significance Testing 11.1 Paved with good intentions 11.2 Prior knowledge 11.3 Confidence interval and highest density interval 11.4 Multiple comparisons 11.5 What a sampling distribution is good for Session info", " 11 Null Hypothesis Significance Testing It’s worth repeating a couple paragraphs from page 298 (Kruschke, 2015, emphasis in the original): The logic of conventional NHST goes like this. Suppose the coin is fair (i.e., \\(\\theta\\) = 0.50). Then, when we flip the coin, we expect that about half the flips should come up heads. If the actual number of heads is far greater or fewer than half the flips, then we should reject the hypothesis that the coin is fair. To make this reasoning precise, we need to figure out the exact probabilities of all possible outcomes, which in turn can be used to figure out the probability of getting an outcome as extreme as (or more extreme than) the actually observed outcome. This probability, of getting an outcome from the null hypothesis that is as extreme as (or more extreme than) the actual outcome, is called a “\\(p\\) value.” If the \\(p\\) value is very small, say less than 5%, then we decide to reject the null hypothesis. Notice that this reasoning depends on defining a space of all possible outcomes from the null hypothesis, because we have to compute the probabilities of each outcome relative to the space of all possible outcomes. The space of all possible outcomes is based on how we intend to collect data. For example, was the intention to flip the coin exactly \\(N\\) times? In that case, the space of possible outcomes contains all sequences of exactly \\(N\\) flips. Was the intention to flip until the \\(z\\)th head appeared? In that case, the space of possible outcomes contains all sequences for which the \\(z\\)th head appears on the last flip. Was the intention to flip for a fixed duration? In that case, the space of possible outcomes contains all combinations of \\(N\\) and \\(z\\) that could be obtained in that fixed duration. Thus, a more explicit definition of a \\(p\\) value is the probability of getting a sample outcome from the hypothesized population that is as extreme as or more extreme than the actual outcome when using the intended sampling and testing procedures. 11.1 Paved with good intentions This is a little silly, but I wanted to challenge myself to randomly generate a series of 24 H and T characters for which there were 7 Hs. The base R sample() function gets us part of the way there. sample(c(&quot;H&quot;, &quot;T&quot;), size = 24, replace = T) ## [1] &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; ## [20] &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; I wanted the solution to be reproducible, which required I find the appropriate seed for set.seed(). To do that, I made a custom h_counter() function into which I could input an arbitrary seed value and retrieve the count of H. I then fed a sequence of integers into h_counter() and filtered the output to find which seed produces the desirable outcome. library(tidyverse) h_counter &lt;- function(seed) { set.seed(seed) coins &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), size = 24, replace = T) length(which(coins == &quot;H&quot;)) } coins &lt;- tibble(seed = 1:200) %&gt;% mutate(n_heads = map_dbl(seed, h_counter)) coins %&gt;% filter(n_heads == 7) ## # A tibble: 2 x 2 ## seed n_heads ## &lt;int&gt; &lt;dbl&gt; ## 1 115 7 ## 2 143 7 Looks like set.seed(115) will work. set.seed(115) sample(c(&quot;H&quot;, &quot;T&quot;), size = 24, replace = T) ## [1] &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; ## [20] &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; The sequence isn’t in the exact order as the data from page 300, but they do have the crucial ratio of heads to tails. set.seed(115) tibble(flips = sample(c(&quot;H&quot;, &quot;T&quot;), size = 24, replace = T)) %&gt;% ggplot(aes(x = flips)) + geom_bar() + scale_y_continuous(breaks = c(0, 7, 17)) + theme(panel.grid.major.x = element_blank(), panel.grid.minor.y = element_blank()) 11.1.1 Definition of \\(p\\) value. In summary, the likelihood function defines the probability for a single measurement, and the intended sampling process defines the cloud of possible sample outcomes. The null hypothesis is the likelihood function with its specific value for parameter \\(\\theta\\), and the cloud of possible samples is defined by the stopping and testing intentions, denoted \\(I\\). Each imaginary sample generated from the null hypothesis is summarized by a descriptive statistic, denoted \\(D_{\\theta, I}\\). In the case of a sample of coin flips, the descriptive summary statistic is \\(z / N\\) , the proportion of heads in the sample. Now, imagine generating infinitely many samples from the null hypothesis using stopping and testing intention \\(I\\) ; this creates a cloud of possible summary values \\(D_{\\theta, I}\\), each of which has a particular probability. The probability distribution over the cloud of possibilities is the sampling distribution: \\(p (D_{\\theta, I} | \\theta, I )\\). To compute the \\(p\\) value, we want to know how much of that cloud is as extreme as, or more extreme than, the actually observed outcome. To define “extremeness” we must determine the typical value of \\(D_{\\theta, I}\\), which is usually defined as the expected value, \\(E [D_{\\theta, I}]\\) (recall Equations 4.5 and 4.6). This typical value is the center of the cloud of possibilities. An outcome is more “extreme” when it is farther away from the central tendency. The \\(p\\) value of the actual outcome is the probability of getting a hypothetical outcome that is as or more extreme. Formally, we can express this as \\[p \\text{ value} = p (D_{\\theta, I} \\succcurlyeq D_\\text{actual} | \\theta, I)\\] where “\\(\\succcurlyeq\\)” in this context means “as extreme as or more extreme than, relative to the expected value from the hypothesis.” Most introductory applied statistics textbooks suppress the sampling intention \\(I\\) from the definition, but precedents for making the sampling intention explicit can be found in Wagenmakers (2007, Online Supplement A)) and additional references cited therein. (p. 301) 11.1.2 With intention to fix \\(N\\). In this subsection, “the space of possible outcomes is restricted to combinations of \\(z\\) and \\(N\\) for which \\(N\\) is fixed at \\(N = 24\\)” (p. 302). What is the probability of getting a particular number of heads when N is fixed? The answer is provided by the binomial probability distribution, which states that the probability of getting z heads out of N flips is \\[ p(z | N, \\theta) = \\begin{pmatrix} N \\\\ z \\end{pmatrix} \\theta^z (1 - \\theta)^{N - z}\\] where the notation \\(\\begin{pmatrix} N \\\\ z \\end{pmatrix}\\) [is a shorthand notation defined in more detail in the text. It has to do with factorials and, getting more to the point,] the number of ways of allocating \\(z\\) heads among \\(N\\) flips, without duplicate counting of equivalent allocations, is \\(N !/[(N − z)!z!]\\). This factor is also called the number of ways of choosing \\(z\\) items from \\(N\\) possibilities, or “\\(N\\) choose \\(z\\)” for short, and is denoted \\(\\begin{pmatrix} N \\\\ z \\end{pmatrix}\\). Thus, the overall probability of getting \\(z\\) heads in \\(N\\) flips is the probability of any particular sequence of \\(z\\) heads in \\(N\\) flips times the number of ways of choosing \\(z\\) slots from among the \\(N\\) possible flips. (p. 303, emphasis in the original) To do factorials in R, use the factorial() function. E.g., we can use the formula \\(N! / [(N − z)!z!]\\) like so: n &lt;- 24 z &lt;- 7 factorial(n) / (factorial(n - z) * factorial(z)) ## [1] 346104 That value, recall, is a count, “the number of ways of allocating \\(z\\) heads among \\(N\\) flips, without duplicate counting of equivalent allocations” (p. 303). That formula’s a little cumbersome to work with. We can make our programming lives easier by wrapping it into a function. n_choose_z &lt;- function(n, z) { factorial(n) / (factorial(n - z) * factorial(z)) } Now we can employ our n_choose_z() function to help make the data we’ll use for Figure 11.3.b. Here are the data. flips &lt;- tibble(z = 0:24) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n) head(flips, n = 10) ## # A tibble: 10 x 4 ## z n_choose_z `p(z/N)` `Sample Proportion z/N` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0.0000000596 0 ## 2 1 24.0 0.00000143 0.0417 ## 3 2 276 0.0000165 0.0833 ## 4 3 2024 0.000121 0.125 ## 5 4 10626 0.000633 0.167 ## 6 5 42504 0.00253 0.208 ## 7 6 134596 0.00802 0.25 ## 8 7 346104 0.0206 0.292 ## 9 8 735471 0.0438 0.333 ## 10 9 1307504 0.0779 0.375 Now here’s the histogram of that sampling distribution. flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(z/N)`, fill = z &lt;= 7, color = z &lt;= 7)) + geom_col(width = .025) + scale_fill_viridis_d(option = &quot;B&quot;, end = .6) + scale_color_viridis_d(option = &quot;B&quot;, end = .6) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) We can get the one-sided \\(p\\)-value with a quick filter() and summarise(). flips %&gt;% filter(z &lt;= 7) %&gt;% summarise(p_value = sum(`p(z/N)`)) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.0320 Here’s Figure 11.3.a. tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = .5) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col() + coord_cartesian(ylim = c(0, 1)) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.5&quot;)) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) As Kruschke wrote on page 304, “It is important to understand that the sampling distribution is a probability distribution over samples of data, and is not a probability distribution over parameter values.” Here is the probability “of getting exactly \\(z = 7\\) heads in \\(N = 24\\) flips” (p. 304, emphasis in the original): flips %&gt;% filter(z == 7) ## # A tibble: 1 x 4 ## z n_choose_z `p(z/N)` `Sample Proportion z/N` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 346104 0.0206 0.292 It was already sitting there in our p(z/N) column. Here’s the conclusion for our particular case. The actual observation was \\(z/N = 7/24\\). The one-tailed probability is \\(p = 0.032\\), which was computed from Equation 11.4, and is shown in Figure 11.3. Because the \\(p\\) value is not less than 2.5%, we do not reject the null hypothesis that \\(\\theta = 0.5\\). In NHST parlance, we would say that the result “has failed to reach significance.” This does not mean we accept the null hypothesis; we merely suspend judgment regarding rejection of this particular hypothesis. Notice that we have not determined any degree of belief in the hypothesis that \\(\\theta = 0.5\\). The hypothesis might be true or might be false; we suspend judgment. (p. 305, emphasis in the original) 11.1.3 With intention to fix \\(z\\). In this subsection, “\\(z\\) is fixed in advance and \\(N\\) is the random variable. We don’t talk about the probability of getting \\(z\\) heads out of \\(N\\) flips, we instead talk about the probability of taking \\(N\\) flips to get \\(z\\) heads” (p. 306). This time we’re interested in What is the probability of taking \\(N\\) flips to get \\(z\\) heads? To answer this question, consider this: We know that the \\(N\\)th flip is the \\(z\\)th head, because that is what caused flipping to stop. Therefore the previous \\(N - 1\\) flips had \\(z - 1\\) heads in some random sequence. The probability of getting \\(z - 1\\) heads in \\(N - 1\\) flips is \\(\\begin{pmatrix} N-1 \\\\ z-1 \\end{pmatrix} \\theta^{z-1} (1 - \\theta)^{N - z}\\). The probability that the last flip comes up heads is \\(\\theta\\). Therefore, the probability that it takes \\(N\\) flips to get \\(z\\) heads is \\[\\begin{align*} p(N | z, \\theta) &amp; = \\begin{pmatrix} N-1 \\\\ z-1 \\end{pmatrix} \\theta^{z-1} (1 - \\theta)^{N - z} \\cdot \\theta \\\\ &amp; = \\begin{pmatrix} N-1 \\\\ z-1 \\end{pmatrix} \\theta^z (1 - \\theta)^{N - z} \\\\ &amp; = \\frac{z}{N} \\begin{pmatrix} N \\\\ z \\end{pmatrix} \\theta^z (1 - \\theta)^{N - z} \\end{align*}\\] (This distribution is sometimes called the “negative binomial” but that term sometimes refers to other formulations and can be confusing, so I will not use it here.) This is a sampling distribution, like the binomial distribution, because it specifies the relative probabilities of all the possible data outcomes for the hypothesized fixed value of \\(\\theta\\) and the intended stopping rule. (p. 306, emphasis added) With that formula in hand, here’s how to generate the data for Figure 11.4.b. theta &lt;- .5 # we have to stop somewhere. where should we stop? highest_n &lt;- 100 flips &lt;- tibble(n = 7:highest_n) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n, `p(N|z,theta)` = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) To keep things simple, we just went up to \\(N = 100\\). At the bottom of page 306, Kruschke described the probability “spikes” for various values of \\(N\\) when \\(z/N = 7/7\\), \\(z/N = 7/8\\), and \\(z/N = 7/9\\). We have those spike values in the Sample Proportion z/N column of our flips data. Here are those first three spikes. flips %&gt;% head(n = 3) ## # A tibble: 3 x 5 ## n n_choose_z `p(z/N)` `Sample Proportion z/N` `p(N|z,theta)` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 1 4.95e-12 1 0.00781 ## 2 8 8 3.96e-11 0.875 0.0273 ## 3 9 36 1.78e-10 0.778 0.0547 Those values correspond to the rightmost vertical lines in our Figure 11.4.b, below. flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`, fill = n &gt;= 24, color = n &gt;= 24)) + geom_col(width = .005) + scale_fill_viridis_d(option = &quot;B&quot;, end = .6) + scale_color_viridis_d(option = &quot;B&quot;, end = .6) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Our Figure 11.4.a is the same as Figure 11.3.a, above. I won’t repeat it, here. We got the formula for that last variable, p(N|z,theta), from formula 11.6 on page 306. You’ll note how Kruschke continued to refer to it as \\(p(z|N)\\) in his Figure 11.4. It’s entirely opaque, to me, how \\(p(z|N) = p(N|z, \\theta)\\). I’m just going with it. Here’s the \\(p\\)-value. flips %&gt;% filter(n &gt;= 24) %&gt;% summarise(p_value = sum(`p(N|z,theta)`) %&gt;% round(digits = 3)) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.017 If you experiment a bit with the highest_n value from above, you’ll see that the exact value for the \\(p\\)-value is dependent on what \\(N\\) you go up to. 11.1.4 With intention to fix duration. In this subsection, neither \\(N\\) nor \\(z\\) is fixed… The key to analyzing this scenario is specifying how various combinations of \\(z\\) and \\(N\\) can arise when sampling for a fixed duration. There is no single, uniquely “correct” specification, because there are many different real-world constraints on sampling through time. But one approach is to think of the sample size \\(N\\) as a random value. (p. 308). Here’s a glance at the Poisson distribution for which \\(\\lambda = 24\\). The mean is colored orange. tibble(x = 1:50) %&gt;% mutate(y = dpois(x = x, lambda = 24)) %&gt;% ggplot(aes(x = x, y = y, fill = x == 24, color = x == 24)) + geom_col(width = .5) + scale_fill_viridis_d(option = &quot;B&quot;, end = .75) + scale_color_viridis_d(option = &quot;B&quot;, end = .75) + scale_y_continuous(NULL, breaks = NULL) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) In the note for Figure 11.5, Kruschke explained the “sample sizes are drawn randomly from a Poisson distribution with mean \\(\\lambda\\)”. Earlier in the prose he explained “\\(\\lambda\\) was set to 24 merely to match \\(N\\) and make the example most comparable to the preceding examples” (p. 309). To do such a simulation, one must choose how many draws to take from \\(\\operatorname{Poisson}(24)\\). Here’s an example where we take just one. set.seed(11) n_iter &lt;- 1 flips &lt;- tibble(iter = 1:n_iter, n = rpois(n_iter, lambda = 24)) %&gt;% mutate(z = map(n, ~seq(from = 0, to = ., by = 1))) %&gt;% unnest(z) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n) flips ## # A tibble: 22 x 6 ## iter n z n_choose_z `p(z/N)` `Sample Proportion z/N` ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 21 0 1 0.000000477 0 ## 2 1 21 1 21 0.0000100 0.0476 ## 3 1 21 2 210 0.000100 0.0952 ## 4 1 21 3 1330 0.000634 0.143 ## 5 1 21 4 5985 0.00285 0.190 ## 6 1 21 5 20349 0.00970 0.238 ## 7 1 21 6 54264 0.0259 0.286 ## 8 1 21 7 116280 0.0554 0.333 ## 9 1 21 8 203490 0.0970 0.381 ## 10 1 21 9 293930 0.140 0.429 ## # … with 12 more rows As indicate in our n column, by chance we drew a 21. We then computed the same values for all possible values of z, ranging from 0 to 21. But this doesn’t make for a very interesting plot, nor does it make for the same kind of plot Kruschke made in Figure 11.5.b. flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(z/N)`, fill = z &lt;= 7, color = z &lt;= 7)) + geom_col(width = .01) + scale_fill_viridis_d(option = &quot;B&quot;, end = .6) + scale_color_viridis_d(option = &quot;B&quot;, end = .6) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Instead we have to take many draws to take from \\(\\operatorname{Poisson}(24)\\). Here’s what it looks like when we take 10,000. n_iter &lt;- 10000 set.seed(11) flips &lt;- tibble(iter = 1:n_iter, n = rpois(n_iter, lambda = 24)) %&gt;% mutate(z = map(n, ~seq(from = 0, to = ., by = 1))) %&gt;% unnest(z) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n) flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(z/N)`, fill = z &lt;= 7, color = z &lt;= 7)) + geom_col(width = .003, size = 1/15) + scale_fill_viridis_d(option = &quot;B&quot;, end = .6) + scale_color_viridis_d(option = &quot;B&quot;, end = .6) + coord_cartesian(ylim = c(0, 0.03)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) I played around with the simulation a bit, and this is about as good as I’ve gotten. If you have a solution that more faithfully reproduces what Kruschke did, please share your code in my GitHub issue #16. Here’s my attempt at the \\(p\\) value. flips %&gt;% filter(z &lt;= 7) %&gt;% summarise(p_value = sum(`p(z/N)`)) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.000218 It’s unclear, to me, why it’s so much lower than the one Kruschke reported in the text. 11.1.5 With intention to make multiple tests. I’m not sure how to do the simulation for this section. This, for example, doesn’t get the job done. flips &lt;- crossing(z = 0:24, coin = letters[1:2]) %&gt;% group_by(coin) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n) head(flips, n = 10) ## # A tibble: 10 x 5 ## # Groups: coin [2] ## z coin n_choose_z `p(z/N)` `Sample Proportion z/N` ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 a 1 0.0000000596 0 ## 2 0 b 1 0.0000000596 0 ## 3 1 a 24.0 0.00000143 0.0417 ## 4 1 b 24.0 0.00000143 0.0417 ## 5 2 a 276 0.0000165 0.0833 ## 6 2 b 276 0.0000165 0.0833 ## 7 3 a 2024 0.000121 0.125 ## 8 3 b 2024 0.000121 0.125 ## 9 4 a 10626 0.000633 0.167 ## 10 4 b 10626 0.000633 0.167 The result is a failed attempt at Figure 11.6: flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(z/N)`, fill = z &lt;= 7, color = z &lt;= 7)) + geom_col(width = .01) + scale_fill_viridis_d(option = &quot;B&quot;, end = .6) + scale_color_viridis_d(option = &quot;B&quot;, end = .6) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) If you know how to do the simulation properly, please share your code in my GitHub issue #17. 11.1.6 Soul searching. Within the context of NHST, the solution is to establish the true intention of the researcher. This is the approach taken explicitly when applying corrections for multiple tests. The analyst determines what the truly intended tests are, and determines whether those testing intentions were honestly conceived a priori or post hoc (i.e., motivated only after seeing the data), and then computes the appropriate \\(p\\) value. The same approach should be taken for stopping rules: The data analyst should determine what the truly intended stopping rule was, and then compute the appropriate \\(p\\) value. Unfortunately, determining the true intentions can be difficult. Therefore, perhaps researchers who use \\(p\\) values to make decisions should be required to publicly pre-register their intended stopping rule and tests, before collecting the data. (p. 314, emphasis in the original) 11.1.7 Bayesian analysis. Happily for us, “the Bayesian interpretation of data does not depend on the covert sampling and testing intentions of the data collector” (p. 314). 11.2 Prior knowledge Nothing for us, here. 11.2.1 NHST analysis. More nothing. 11.2.2 Bayesian analysis. If you recall from Chapter 6, we need a function to compute the Bernoulli likelihood. bernoulli_likelihood &lt;- function(theta, data) { n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } There are a handful of steps before we can use our bernoulli_likelihood() function to make the plot data. All these are repeats from Chapter 6. # we need these to compute the likelihood n &lt;- 24 z &lt;- 7 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) # (i.e., data) d_nail &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %&gt;% # (i.e., theta) mutate(Prior = dbeta(x = theta, shape1 = 2, shape2 = 20)) %&gt;% mutate(Likelihood = bernoulli_likelihood(theta = theta, # (i.e., p(D | theta)) data = trial_data)) %&gt;% mutate(evidence = sum(Likelihood * Prior)) %&gt;% # (i.e., p(D)) mutate(Posterior = Likelihood * Prior / evidence) # (i.e., p(theta | D)) glimpse(d_nail) ## Rows: 1,000 ## Columns: 5 ## $ theta &lt;dbl&gt; 0.000000000, 0.001001001, 0.002002002, 0.003003003, 0.0040… ## $ Prior &lt;dbl&gt; 0.0000000, 0.4124961, 0.8094267, 1.1912097, 1.5582533, 1.9… ## $ Likelihood &lt;dbl&gt; 0.000000e+00, 9.900280e-22, 1.245822e-19, 2.092598e-18, 1.… ## $ evidence &lt;dbl&gt; 5.260882e-05, 5.260882e-05, 5.260882e-05, 5.260882e-05, 5.… ## $ Posterior &lt;dbl&gt; 0.000000e+00, 7.762627e-18, 1.916792e-15, 4.738222e-14, 4.… Here’s the left column of Figure 11.7. p1 &lt;- d_nail %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Prior), fill = &quot;grey50&quot;) + scale_y_continuous(breaks = NULL) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*2*&quot;, &quot;*20))) p2 &lt;- d_nail %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Likelihood), fill = &quot;grey50&quot;) + scale_y_continuous(breaks = NULL) + labs(title = &quot;Likelihood (Bernoulli)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d_nail %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Posterior), fill = &quot;grey50&quot;) + scale_y_continuous(breaks = NULL) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*9*&quot;, &quot;*37))) library(patchwork) (p1 / p2 / p3) &amp; theme(panel.grid = element_blank()) If we’d like the 95% HDIs, we’ll need to redifine the hdi_of_icdf() function. hdi_of_icdf &lt;- function(name = qbeta, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Note that this time we made the qbeta() function the default setting for the name argument. Here are the HDIs for the prior and posterior, above. hdi_of_icdf(shape1 = 2, shape2 = 20) ## [1] 0.002600585 0.208030932 hdi_of_icdf(shape1 = 2 + z, shape2 = 20 + (n - z)) ## [1] 0.08839668 0.31043265 To get the left column of Figure 11.7, we have to update the data with our new prior, \\(\\operatorname{beta} (11, 11)\\). # here are the data based on our updated beta(11, 11) prior d_coin &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(Prior = dbeta(x = theta, shape1 = 11, shape2 = 11)) %&gt;% mutate(Likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% mutate(Posterior = Likelihood * Prior / sum(Likelihood * Prior)) # The updated plots: p1 &lt;- d_coin %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Prior), fill = &quot;grey50&quot;) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*11*&quot;, &quot;*11))) p2 &lt;- d_coin %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Likelihood), fill = &quot;grey50&quot;) + labs(title = &quot;Likelihood (Bernoulli)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d_coin %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Posterior), fill = &quot;grey50&quot;) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(dbeta(theta*&quot;|&quot;*18*&quot;, &quot;*28))) (p1 / p2 / p3) &amp; scale_y_continuous(breaks = NULL) &amp; theme(panel.grid = element_blank()) Here are the corresponding HDIs for \\(\\operatorname{beta} (11, 11)\\) and \\(\\operatorname{beta} (11 + 7, 11 + 24 - 7)\\). hdi_of_icdf(shape1 = 11, shape2 = 11) ## [1] 0.2978068 0.7021932 hdi_of_icdf(shape1 = 11 + z, shape2 = 11 + (n - z)) ## [1] 0.2539378 0.5312685 11.2.2.1 Priors are overt and relevant. In this subsection’s opening paragraph, Kruschke opined: Prior beliefs are overt, explicitly debated, and founded on publicly accessible previous research. A Bayesian analyst might have personal priors that differ from what most people think, but if the analysis is supposed to convince an audience, then the analysis must use priors that the audience finds palatable. It is the job of the Bayesian analyst to make cogent arguments for the particular prior that is used. (p. 317) 11.3 Confidence interval and highest density interval This section defines CIs and provides examples. It shows that, while CIs ameliorate some of the problems of \\(p\\) values, ultimately CIs suffer the same problems as \\(p\\) values because CIs are defined in terms of \\(p\\) values. Bayesian posterior distributions, on the other hand, provide the needed information. (p. 318) 11.3.1 CI depends on intention. The primary goal of NHST is determining whether a particular “null” value of a parameter can be rejected. One can also ask what range of parameter values would not be rejected. This range of nonrejectable parameter values is called the CI. (There are different ways of defining an NHST CI; this one is conceptually the most general and coherent with NHST precepts.) The 95% CI consists of all values of \\(\\theta\\) that would not be rejected by a (two-tailed) significance test that allows 5% false alarms. (p. 318, emphasis in the original) Here’s the upper- and lower-left panels of Figure 11.8. p1 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .126, .126)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col() + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.126&quot;)) p2 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .511, .511)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col() + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.511&quot;)) (p1 / p2) &amp; coord_cartesian(ylim = c(0, 1)) &amp; theme(title = element_text(size = 10), axis.ticks.x = element_blank(), panel.grid = element_blank()) Here are the corresponding upper- and lower-right panels. p1 &lt;- tibble(z = 0:24, y = dbinom(0:24, size = 24, prob = .126)) %&gt;% ggplot(aes(x = z/25, y = y, fill = z &gt;= 7)) + geom_col(width = .025) p2 &lt;- tibble(z = 0:24, y = dbinom(0:24, size = 24, prob = .511)) %&gt;% ggplot(aes(x = z / 24, y = y, fill = z &lt;= 7)) + geom_col(width = .025) (p1 / p2) &amp; labs(title = &quot;Implied Sampling Distribution&quot;, x = &quot;Sample Proportion z/N&quot;, y = &quot;p(z/N)&quot;) &amp; scale_fill_viridis_d(option = &quot;B&quot;, end = .6) &amp; theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) We’ll continue to develop our patchwork skills by plotting Figure 11.9 all at once. But before the big reveal, we’ll make the subplots in two phases. Here are the two on the left. p1 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .126, .126)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col() + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.126&quot;)) + coord_cartesian(ylim = c(0, 1)) + theme(title = element_text(size = 10), axis.ticks.x = element_blank(), panel.grid = element_blank()) p2 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .484, .484)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col() + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.484&quot;)) + coord_cartesian(ylim = c(0, 1)) + theme(title = element_text(size = 10), axis.ticks.x = element_blank(), panel.grid = element_blank()) Now make the two on the right. theta &lt;- .126 flips &lt;- tibble(n = 7:100) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n, `p(N|z,theta)` = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) p3 &lt;- flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`, fill = n &lt;= 24, color = n &lt;= 24)) + geom_col(width = .005) + scale_fill_viridis_d(option = &quot;B&quot;, end = .6) + scale_color_viridis_d(option = &quot;B&quot;, end = .6) + scale_x_continuous(breaks = 0:5 * 0.2, limits = c(0, 1)) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) theta &lt;- .484 flips &lt;- tibble(n = 7:100) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n, `p(N|z,theta)` = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) p4 &lt;- flips %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`, fill = n &gt;= 24, color = n &gt;= 24)) + geom_col(width = .005) + scale_fill_viridis_d(option = &quot;B&quot;, end = .6) + scale_color_viridis_d(option = &quot;B&quot;, end = .6) + scale_x_continuous(breaks = 0:5 * 0.2, limits = c(0, 1)) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Here we arrange all four of the Figure 11.9 subplots. (p1 + p3 + p2 + p4) + plot_layout(widths = c(2, 4)) We’ll follow the same procedure for Figure 11.10. Here are the subplots on the left. p1 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .135, .135)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col() + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.135&quot;)) + coord_cartesian(ylim = c(0, 1)) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) p2 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .497, .497)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col() + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta==&quot;.497&quot;)) + coord_cartesian(ylim = c(0, 1)) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) Like with Figure 11.5.b, my attempts for the right panels of Figure 11.10 just aren’t quite right. If you understand where I’m going wrong with the simulation, please share your code in my GitHub issue #18. z_maker &lt;- function(i) { set.seed(i) n &lt;- rpois(n = 1, lambda = 24) seq(from = 0, to = n, by = 1) } theta &lt;- .135 p3 &lt;- tibble(seed = 1:100) %&gt;% mutate(z = map(seed, z_maker)) %&gt;% unnest(z) %&gt;% group_by(seed) %&gt;% mutate(n = n()) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n, `p(N|z,theta)` = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`, fill = `Sample Proportion z/N` &gt;= 7 / 24)) + geom_col(width = .004) + scale_fill_viridis_d(option = &quot;B&quot;, end = .6) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) theta &lt;- .497 p4 &lt;- tibble(seed = 1:100) %&gt;% mutate(z = map(seed, z_maker)) %&gt;% unnest(z) %&gt;% group_by(seed) %&gt;% mutate(n = n()) %&gt;% mutate(n_choose_z = n_choose_z(n, z)) %&gt;% mutate(`p(z/N)` = n_choose_z / sum(n_choose_z), `Sample Proportion z/N` = z / n, `p(N|z,theta)` = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) %&gt;% ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`, fill = `Sample Proportion z/N` &lt;= 7 / 24)) + geom_col(width = .004) + scale_fill_viridis_d(option = &quot;B&quot;, end = .6) + ggtitle(&quot;Implied Sampling Distribution&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) (p1 + p3 + p2 + p4) + plot_layout(widths = c(2, 4)) Let’s leave failure behind. Here’s the two left panels for Figure 11.11. p1 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .11, .11)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col() + coord_cartesian(ylim = c(0, 1)) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta[1]==&quot;.11;&quot;*~theta[2]==&quot;.11&quot;)) p2 &lt;- tibble(y = factor(c(&quot;tail&quot;, &quot;head&quot;), levels = c(&quot;tail&quot;, &quot;head&quot;)), `p(y)` = c(1 - .539, .539)) %&gt;% ggplot(aes(x = y, y = `p(y)`)) + geom_col() + coord_cartesian(ylim = c(0, 1)) + labs(title = &quot;Hypothetical Population&quot;, subtitle = expression(theta[1]==&quot;.539;&quot;*~theta[2]==&quot;.539&quot;)) (p1 / p2) &amp; coord_cartesian(ylim = c(0, 1)) &amp; theme(title = element_text(size = 10), axis.ticks.x = element_blank(), panel.grid = element_blank()) Much like with Figure 11.6, I don’t understand how to do the simulation properly for the right panels of Figure 11.11. If you’ve got it, please share your code in my GitHub issue #19. 11.3.1.1 CI is not a distribution. A CI is merely two end points. A common misconception of a confidence interval is that it indicates some sort of probability distribution over values of \\(\\theta\\). It is very tempting to think that values of \\(\\theta\\) in the middle of a CI should be more believable than values of \\(\\theta\\) at or beyond the limits of the CI. … Methods for imposing a distribution upon a CI seem to be motivated by a natural Bayesian intuition: Parameter values that are consistent with the data should be more credible than parameter values that are not consistent with the data (subject to prior credibility). If we were confined to frequentist methods, then the various proposals outlined above would be expressions of that intuition. But we are not confined to frequentist methods. Instead, we can express our natural Bayesian intuitions in fully Bayesian formalisms. (pp. 323–324) 11.3.2 Bayesian HDI. “The 95% HDI consists of those values of \\(\\theta\\) that have at least some minimal level of posterior credibility, such that the total probability of all such \\(\\theta\\) values is 95%” (p. 324). Once again, here’s how to analytically compute the 95% HDIs for our example of \\(z = 7, N = 24\\) and the prior of \\(\\operatorname{beta} (11, 11)\\). hdi_of_icdf(shape1 = 11 + z, shape2 = 11 + (n - z)) ## [1] 0.2539378 0.5312685 11.4 Multiple comparisons It’s worth quoting Kruschke at length: When comparing multiple conditions, a key goal in NHST is to keep the overall false alarm rate down to a desired maximum such as 5%. Abiding by this constraint depends on the number of comparisons that are to be made, which in turn depends on the intentions of the experimenter. In a Bayesian analysis, however, there is just one posterior distribu- tion over the parameters that describe the conditions. That posterior distribution is unaffected by the intentions of the experimenter, and the posterior distribution can be examined from multiple perspectives however is suggested by insight and curiosity. (p. 325) 11.4.1 NHST correction for experiment wise error. In NHST, we have to take into account all comparisons we intend for the whole experiment. Suppose we set a criterion for rejecting the null such that each decision has a “per-comparison” (PC) false alarm rate of \\(\\alpha_\\text{PC}\\), e.g., 5%. Our goal is to determine the overall false alarm rate when we conduct several comparisons. To get there, we do a little algebra. First, suppose the null hypothesis is true, which means that the groups are identical, and we get apparent differences in the samples by chance alone. This means that we get a false alarm on a proportion \\(\\alpha_\\text{PC}\\) of replications of a comparison test. Therefore, we do not get a false alarm on the complementary proportion \\(1 - \\alpha_\\text{PC}\\) of replications. If we run c independent comparison tests, then the probability of not getting a false alarm on any of the tests is \\((1 - \\alpha_\\text{PC})^c\\). Consequently, the probability of getting at least one false alarm is \\(1 - (1 - \\alpha_\\text{PC})^c\\). We call that probability of getting at least one false alarm, across all the comparisons in the experiment, the “experimentwise” false alarm rate, denoted \\(\\alpha_\\text{EW}\\). (pp. 325–326, emphasis in the original) Here’s what this looks like in action. alpha_pc &lt;- .05 c &lt;- 36 # the probability of not getting a false alarm on any of the tests (1 - alpha_pc)^c ## [1] 0.1577792 # the probability of getting at least one false alarm is 1 - (1 - alpha_pc)^c ## [1] 0.8422208 For kicks and giggles, it might be interesting to plot this. tibble(c = 1:100) %&gt;% mutate(p1 = (1 - alpha_pc)^c, p2 = 1 - (1 - alpha_pc)^c) %&gt;% gather(key, probability, -c) %&gt;% ggplot(aes(x = c, y = probability, color = key)) + geom_line(size = 1.1) + geom_text(data = tibble( c = c(85, 75, 70), probability = c(.08, .9, .82), label = c(&quot;no false alarms&quot;, &quot;at least one false alarm&quot;, &quot;(i.e., experimentwise false alarm rate)&quot;), key = c(&quot;p1&quot;, &quot;p2&quot;, &quot;p2&quot;) ), aes(label = label)) + scale_color_viridis_d(option = &quot;D&quot;, end = .4) + xlab(&quot;the number of independent tests, c&quot;) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) One way to keep the experimentwise false alarm rate down to 5% is by reducing the permitted false alarm rate for the individual comparisons, i.e., setting a more stringent criterion for rejecting the null hypothesis in individual comparisons. One often-used re- setting is the Bonferonni correction, which sets \\(\\alpha_\\text{PC} = \\alpha_\\text{EW}^\\text{desired} / c\\). Here’s how to apply the Bonferonni correction to our example if the desired false-alarm rate is .05. alpha_pc &lt;- .05 c &lt;- 36 # the Bonferonni correction alpha_pc / c ## [1] 0.001388889 Again, it might be useful to plot the consequence of Bonferonni’s correction on \\(\\alpha\\) for different levels of \\(c\\). tibble(c = 1:100) %&gt;% mutate(a_ew = alpha_pc^c) %&gt;% ggplot(aes(x = c, y = a_ew)) + geom_line(size = 1.1) + xlab(&quot;the number of independent tests, c&quot;) + theme(panel.grid = element_blank()) A little shocking, isn’t it? If you put it on a log scale, you’ll see the relationship is linear. tibble(c = 1:100) %&gt;% mutate(a_ew = alpha_pc^c) %&gt;% ggplot(aes(x = c, y = a_ew)) + geom_line(size = 1.1) + scale_y_log10() + xlab(&quot;the number of independent tests, c&quot;) + theme(panel.grid = element_blank()) But just look at how low the values on the y-axis get. 11.4.2 Just one Bayesian posterior no matter how you look at it. In a Bayesian analysis, the interpretation of the data is not influenced by the experimenter’s stopping and testing intentions (assuming that those intentions do not affect the data). A Bayesian analysis yields a posterior distribution over the parameters of the model. The posterior distribution is the complete implication of the data. The posterior distribution can be examined in as many different ways as the analyst deems interesting; various comparisons of groups are merely different perspectives on the posterior distribution. (p. 328) 11.4.3 How Bayesian analysis mitigates false alarms. From page 329: “How, then, does a Bayesian analysis address the problem of false alarms? By incorporating prior knowledge into the structure of the model.” In addition, we use heirarchical models when possible (e.g., Gelman et al., 2012). 11.5 What a sampling distribution is good for “Sampling distributions tell us the probability of imaginary outcomes given a parameter value and an intention, \\(p(D_{\\theta, I}|\\theta, I)\\), instead of the probability of parameter values given the actual data, \\((\\theta|D_\\text{actual})\\).” 11.5.1 Planning an experiment. Gelman touched on these sensibilities in a recent blog post. 11.5.2 Exploring model predictions (posterior predictive check). There’s no shortage of ppc talk on Gelman’s blog (e.g., here or here or here). Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.0.0 forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 ## [5] purrr_0.3.4 readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ## [9] ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.0.0 xfun_0.13 haven_2.2.0 lattice_0.20-38 ## [5] colorspace_1.4-1 vctrs_0.3.0 generics_0.0.2 viridisLite_0.3.0 ## [9] htmltools_0.4.0 yaml_2.2.1 utf8_1.1.4 rlang_0.4.6 ## [13] pillar_1.4.4 glue_1.4.0 withr_2.2.0 DBI_1.1.0 ## [17] dbplyr_1.4.2 modelr_0.1.6 readxl_1.3.1 lifecycle_0.2.0 ## [21] munsell_0.5.0 gtable_0.3.0 cellranger_1.1.0 rvest_0.3.5 ## [25] evaluate_0.14 labeling_0.3 knitr_1.28 fansi_0.4.1 ## [29] broom_0.5.5 Rcpp_1.0.4.6 scales_1.1.1 backports_1.1.6 ## [33] jsonlite_1.6.1 farver_2.0.3 fs_1.4.1 hms_0.5.3 ## [37] digest_0.6.25 stringi_1.4.6 bookdown_0.18 grid_3.6.3 ## [41] cli_2.0.2 tools_3.6.3 magrittr_1.5 crayon_1.3.4 ## [45] pkgconfig_2.0.3 ellipsis_0.3.0 xml2_1.3.1 reprex_0.3.0 ## [49] lubridate_1.7.8 assertthat_0.2.1 rmarkdown_2.1 httr_1.4.1 ## [53] rstudioapi_0.11 R6_2.4.1 nlme_3.1-144 compiler_3.6.3 References "],
["bayesian-approaches-to-testing-a-point-null-hypothesis.html", "12 Bayesian Approaches to Testing a Point (“Null”) Hypothesis 12.1 The estimation approach 12.2 The model-comparison approach 12.3 Relations of parameter estimation and model comparison 12.4 Estimation and model comparison? Session info", " 12 Bayesian Approaches to Testing a Point (“Null”) Hypothesis Suppose that you have collected some data, and now you want to answer the question, Is there a non-zero effect or not? Is the coin fair or not? Is there better-than-chance accuracy or not? Is there a difference between groups or not? In the previous chapter, [Kruschke] argued that answering this type of question via null hypothesis significance testing (NHST) has deep problems. This chapter describes Bayesian approaches to the question. (Kruschke, 2015, p 335) 12.1 The estimation approach Throughout this book, we have used Bayesian inference to derive a posterior distribution over a parameter of interest, such as the bias \\(\\theta\\) of a coin. We can then use the posterior distribution to discern the credible values of the parameter. If the null value is far from the credible values, then we reject the null value as not credible. But if all the credible values are virtually equivalent to the null value, then we can accept the null value. (p. 336) 12.1.1 Region of practical equivalence. Kruschke began: “A region of practical equivalence (ROPE) indicates a small range of parameter values that are considered to be practically equivalent to the null value for purposes of the particular application” (p. 336, emphasis in the original) Here’s a plot of Kruschke’s initial coin flip ROPE. library(tidyverse) tibble(xmin = .45, xmax = .55) %&gt;% ggplot() + geom_rect(aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = &quot;white&quot;) + annotate(geom = &quot;text&quot;, x = .5, y = .5, label = &quot;ROPE&quot;, color = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Kruschke&#39;s coin flip ROPE&quot;, x = expression(theta)) + coord_cartesian(0:1) + theme(panel.grid.minor = element_blank()) In the first example (p. 336), we have \\(z = 325\\) heads out of \\(N = 500\\) coin flips. To visualize the analysis, we’ll need the Bernoulli likelihood. bernoulli_likelihood &lt;- function(theta, data) { n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } Now we’ll follow the typical steps to combine the prior, which is flat in this case, and the likelihood to get the posterior. # the data summaries n &lt;- 500 z &lt;- 325 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) # (i.e., data) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %&gt;% # (i.e., theta) # recall Beta(1, 1) is flat mutate(prior = dbeta(theta, shape1 = 1, shape2 = 1), # (i.e., p(theta)) likelihood = bernoulli_likelihood(theta = theta, # (i.e., p(D | theta)) data = trial_data)) %&gt;% mutate(posterior = likelihood * prior / sum(prior * likelihood)) # (i.e., p(theta | D)) glimpse(d) ## Rows: 1,000 ## Columns: 4 ## $ theta &lt;dbl&gt; 0.000000000, 0.001001001, 0.002002002, 0.003003003, 0.004004004, 0.005005005, … ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ posterior &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … Now we can plot the results. ggplot(data = d) + geom_rect(xmin = .45, xmax = .55, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + geom_ribbon(aes(x = theta, ymin = 0, ymax = posterior), fill = &quot;grey67&quot;) + annotate(geom = &quot;text&quot;, x = .5, y = .01, label = &quot;ROPE&quot;, color = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Nope, that density ain&#39;t in that ROPE.&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) With the formula by \\(\\operatorname{beta} (\\theta | z + \\alpha, N - z + \\beta)\\), we can analytically compute the Beta parameters for the posterior. (alpha &lt;- z + 1) ## [1] 326 (beta &lt;- n - z + 1) ## [1] 176 With the hdi_of_icdf() function, we’ll compute the HDIs. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Compute those HDIs and save them as h. ( h &lt;- hdi_of_icdf(name = qbeta, shape1 = alpha, shape2 = beta) ) ## [1] 0.6075644 0.6909070 Now let’s remake the plot from above, this time with the analytically-derived HDI values. tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %&gt;% ggplot() + geom_rect(xmin = .45, xmax = .55, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + geom_ribbon(aes(x = theta, ymin = 0, ymax = dbeta(theta, shape1 = alpha, shape2 = beta)), fill = &quot;grey67&quot;, size = 0) + geom_segment(x = h[1], xend = h[2], y = 0, yend = 0, size = 3/4) + annotate(geom = &quot;text&quot;, x = .5, y = 17.5, label = &quot;ROPE&quot;, color = &quot;grey67&quot;) + annotate(geom = &quot;text&quot;, x = .65, y = 4, label = &quot;95%\\nHDI&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;That `hdi_of_icdf()` function really came through, for us.&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) In his second example (p. 337), Kruschke considered \\(z = 490\\) heads out of \\(N = 1000\\) flips. # we need these to compute the likelihood n &lt;- 1000 z &lt;- 490 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %&gt;% # (i.e., theta) mutate(prior = dbeta(theta, shape1 = 1, shape2 = 1), # (i.e., p(theta)) likelihood = bernoulli_likelihood(theta = theta, # (i.e., p(D | theta)) data = trial_data)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) %&gt;% # (i.e., p(theta | D)) ggplot() + geom_rect(xmin = .45, xmax = .55, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + geom_ribbon(aes(x = theta, ymin = 0, ymax = posterior), fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;This posterior sits right within the ROPE.&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) Here are the new HDIs. hdi_of_icdf(name = qbeta, shape1 = z + 1, shape2 = n - z + 1) ## [1] 0.4590949 0.5209562 Further down the section, Kruschke offered some perspective on the ROPE approach. The ROPE limits, by definition, cannot be uniquely “correct,” but instead are established by practical aims, bearing in mind that wider ROPEs yield more decisions to accept the ROPEd value and fewer decision to reject the ROPEd value. In many situations, the exact limit of the ROPE can be left indeterminate or tacit, so that the audience of the analysis can use whatever ROPE is appropriate at the time, as competing theories and measuring devices evolve. When the HDI is far from the ROPEd value, the exact ROPE is inconsequential because the ROPEd value would be rejected for any reasonable ROPE. When the HDI is very narrow and overlaps the target value, the HDI might again fall within any reasonable ROPE, again rendering the exact ROPE inconsequential. When, however, the HDI is only moderately narrow and near the target value, the analysis can report how much of the posterior falls within a ROPE as a function of different ROPE widths… It is important to be clear that any discrete decision about rejecting or accepting a null value does not exhaustively capture our knowledge about the parameter value. Our knowledge about the parameter value is described by the full posterior distribution. When making a binary decision, we have merely compressed all that rich detail into a single bit of information. The broader goal of Bayesian analysis is conveying an informative summary of the posterior, and where the value of interest falls within that posterior. Reporting the limits of an HDI region is more informative than reporting the declaration of a reject/accept decision. By reporting the HDI and other summary information about the posterior, different readers can apply different ROPEs to decide for themselves whether a parameter is practically equivalent to a null value. The decision procedure is separate from the Bayesian inference. The Bayesian part of the analysis is deriving the posterior distribution. The decision procedure uses the posterior distribution, but does not itself use Bayes’ rule. (pp. 338–339, emphasis in the original) Full disclosure: I’m not a fan of the ROPE method. Though we’re following along with the text and covering it, here, I will deemphasize it in later sections. Kruschke then went on to compare the ROPE with frequentist equivalence tests. This is a part of the literature I have not waded into, yet. It appears psychologist Daniël Lakens and colleagues gave written a bit in the topic, recently. Interested readers might start with Lakens et al. (2020), Lakens et al. (2018), or Lakens &amp; Delacre (2018). 12.1.2 Some examples. Kruschke referenced an analysis from way back in Chapter 9. We’ll need to re-fit the model. First we import data. my_data &lt;- read_csv(&quot;data.R/BattingAverage.csv&quot;) glimpse(my_data) ## Rows: 948 ## Columns: 6 ## $ Player &lt;chr&gt; &quot;Fernando Abad&quot;, &quot;Bobby Abreu&quot;, &quot;Tony Abreu&quot;, &quot;Dustin Ackley&quot;, &quot;Matt Adams&quot;,… ## $ PriPos &lt;chr&gt; &quot;Pitcher&quot;, &quot;Left Field&quot;, &quot;2nd Base&quot;, &quot;2nd Base&quot;, &quot;1st Base&quot;, &quot;Pitcher&quot;, &quot;Pit… ## $ Hits &lt;dbl&gt; 1, 53, 18, 137, 21, 0, 0, 2, 150, 167, 0, 128, 66, 3, 1, 81, 180, 36, 150, 0… ## $ AtBats &lt;dbl&gt; 7, 219, 70, 607, 86, 1, 1, 20, 549, 576, 1, 525, 275, 12, 8, 384, 629, 158, … ## $ PlayerNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2… ## $ PriPosNumber &lt;dbl&gt; 1, 7, 4, 4, 3, 1, 1, 3, 3, 4, 1, 5, 4, 2, 7, 4, 6, 8, 9, 1, 2, 5, 1, 1, 7, 2… Let’s load brms and, while we’re at it, tidybayes. library(brms) library(tidybayes) Fit the model and retain its original name, fit9.2. fit9.2 &lt;- brm(data = my_data, family = binomial(link = logit), Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = .99), seed = 9, file = &quot;fits/fit09.02&quot;) Let’s use coef() to pull the relevant posterior draws. c &lt;- coef(fit9.2, summary = F)$PriPos %&gt;% as_tibble() str(c) ## tibble [9,000 × 9] (S3: tbl_df/tbl/data.frame) ## $ 1st Base.Intercept : num [1:9000] -1.12 -1.03 -1.01 -1.04 -1.09 ... ## $ 2nd Base.Intercept : num [1:9000] -1.059 -1.072 -1.067 -0.997 -1.169 ... ## $ 3rd Base.Intercept : num [1:9000] -1.01 -1.05 -1.04 -1.06 -1.07 ... ## $ Catcher.Intercept : num [1:9000] -1.17 -1.09 -1.14 -1.14 -1.15 ... ## $ Center Field.Intercept: num [1:9000] -1.05 -1.07 -1.06 -1.06 -1.03 ... ## $ Left Field.Intercept : num [1:9000] -1.08 -1.11 -1.06 -1.06 -1.12 ... ## $ Pitcher.Intercept : num [1:9000] -1.86 -1.87 -1.97 -1.94 -1.88 ... ## $ Right Field.Intercept : num [1:9000] -1.03 -1.05 -1.08 -1.09 -1.01 ... ## $ Shortstop.Intercept : num [1:9000] -1.16 -1.1 -1.14 -1.1 -1.12 ... As we pointed out in Chapter 9, keep in mind that coef() returns the values in the logit scale when used for logistic regression models. So we’ll have to use brms::inv_logit_scaled() to convert the estimates to the probability metric. We can make the difference distributions after we’ve converted the estimates. c_small &lt;- c %&gt;% mutate_all(inv_logit_scaled) %&gt;% transmute(`Pitcher - Catcher` = Pitcher.Intercept - Catcher.Intercept, `Catcher - 1st Base` = Catcher.Intercept - `1st Base.Intercept`) head(c_small) ## # A tibble: 6 x 2 ## `Pitcher - Catcher` `Catcher - 1st Base` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.102 -0.00921 ## 2 -0.117 -0.0131 ## 3 -0.120 -0.0256 ## 4 -0.116 -0.0201 ## 5 -0.109 -0.0110 ## 6 -0.106 -0.0161 After a little wrangling, we’ll be ready to re-plot the relevant parts of Figure 9.14. c_small %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;Pitcher - Catcher&quot;, &quot;Catcher - 1st Base&quot;))) %&gt;% ggplot(aes(x = value)) + geom_rect(xmin = -0.05, xmax = 0.05, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + stat_histintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 20, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;The ROPE ranges from −0.05 to +0.05&quot;, x = expression(theta)) + coord_cartesian(c(-.125, .125)) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + facet_wrap(~name, scales = &quot;free&quot;) In order to re-plot part of Figure 9.15, we’ll need to employ fitted() to snatch the player-specific posteriors. # this will make life easier. just go with it name_list &lt;- c(&quot;ShinSoo Choo&quot;, &quot;Ichiro Suzuki&quot;) # we&#39;ll define the data we&#39;d like to feed into `fitted()`, here nd &lt;- my_data %&gt;% filter(Player %in% c(name_list)) %&gt;% # these last two lines aren&#39;t typically necessary, but they allow us to # arrange the rows in the same order we find the names in Figures 9.15 and 9/16 mutate(Player = factor(Player, levels = c(name_list))) %&gt;% arrange(Player) f &lt;- fitted(fit9.2, newdata = nd, scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% mutate_all(inv_logit_scaled) %&gt;% set_names(name_list) %&gt;% # in this last section, we make our difference distributions mutate(`ShinSoo Choo - Ichiro Suzuki` = `ShinSoo Choo` - `Ichiro Suzuki`) glimpse(f) ## Rows: 9,000 ## Columns: 3 ## $ `ShinSoo Choo` &lt;dbl&gt; 0.2836534, 0.3041822, 0.2661348, 0.2970897, 0.2443223, 0.2… ## $ `Ichiro Suzuki` &lt;dbl&gt; 0.2631869, 0.2891847, 0.2999139, 0.2617316, 0.2958017, 0.2… ## $ `ShinSoo Choo - Ichiro Suzuki` &lt;dbl&gt; 0.020466515, 0.014997563, -0.033779074, 0.035358090, -0.05… Now we’re ready to go. f %&gt;% ggplot() + geom_rect(xmin = -0.05, xmax = 0.05, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + stat_histintervalh(aes(x = `ShinSoo Choo - Ichiro Suzuki`, y = 0), point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .4, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;ShinSoo Choo - Ichiro Suzuki&quot;, x = expression(theta)) + coord_cartesian(c(-.125, .125)) + theme(panel.grid = element_blank()) 12.1.3 Differences of correlated parameters. Kruschke didn’t explicate where he got the data for Figure 12.1. If we’re willing to presume a multivariate normal distribution, we can get close using the MASS::mvrnorm() function. You can get the basic steps from Sven Hohenstein’s answer to this stats.stacheschange question. # first we&#39;ll make a correlation matrix # a correlation of .9 seems about right correlation_matrix &lt;- matrix(c(1, .9, .9, 1), nrow = 2, ncol = 2) # next we&#39;ll specify the means and standard deviations mu &lt;- c(.58, .42) sd &lt;- c(.1, .1) # now we&#39;ll use the correlation matrix and standard deviations to make a covariance matrix covariance_matrix &lt;- sd %*% t(sd) * correlation_matrix # after setting our seed, we&#39;re ready to simulate set.seed(12) d &lt;- MASS::mvrnorm(n = 1000, mu = mu, Sigma = covariance_matrix) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;theta[&quot;, 1:2, &quot;]&quot;)) Now it only takes some light wrangling to prepare the data to make the three histograms in the left panel of Figure 12.1. d %&gt;% mutate(`theta[1]-theta[2]` = `theta[1]` - `theta[2]`) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, labeller = label_parsed) Here’s the scatter plot, showing the correlation. I think we got pretty close! d %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[2]`)) + geom_abline(color = &quot;white&quot;) + geom_point(size = 1/2, color = &quot;grey50&quot;, alpha = 1/4) + coord_equal(xlim = c(0, 1), ylim = c(0, 1)) + labs(x = expression(theta[1]), y = expression(theta[2])) + theme(panel.grid = element_blank()) To make the plots in the right panel of Figure 12.1, we just need to convert the correlation from .9 to -.9. # this time we&#39;ll make the correlations -.9 correlation_matrix &lt;- matrix(c(1, -.9, -.9, 1), nrow = 2, ncol = 2) # we&#39;ll have to redo the covariance matrix covariance_matrix &lt;- sd %*% t(sd) * correlation_matrix # here&#39;s the updated data set.seed(1) d &lt;- MASS::mvrnorm(n = 1000, mu = mu, Sigma = covariance_matrix) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;theta[&quot;, 1:2, &quot;]&quot;)) Here are our right-panel Figure 12.1 histograms. d %&gt;% mutate(`theta[1]-theta[2]` = `theta[1]` - `theta[2]`) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 20, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, labeller = label_parsed) Behold the second scatter plot. d %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[2]`)) + geom_abline(color = &quot;white&quot;) + geom_point(size = 1/2, color = &quot;grey50&quot;, alpha = 1/4) + coord_equal(xlim = c(0, 1), ylim = c(0, 1)) + labs(x = expression(theta[1]), y = expression(theta[2])) + theme(panel.grid = element_blank()) In summary, the marginal distributions of two parameters do not indicate the relationship between the parameter values. The joint distribution of the two parameters might have positive or negative correlation (or even a non-linear dependency), and therefore the difference of the parameter values should be explicitly examined. (pp. 341–342) 12.1.4 Why HDI and not equal-tailed interval? Though Kruschke told us Figure 12.2 was of a gamma distribution, he didn’t tell us the parameters for that particular gamma. After playing around for a bit, it appeared dgamma(x, 2, .2) worked pretty well. tibble(x = seq(from = 0, to = 40, by = .1)) %&gt;% ggplot(aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dgamma(x, 2, .2)), fill = &quot;grey67&quot;) + coord_cartesian(xlim = c(0, 35)) + theme(panel.grid = element_blank()) If you want to get the quantile-based intervals (i.e., the ETIs), you can plug in the desired quantiles into the qgamma() function. (ex &lt;- qgamma(c(.025, .975), shape = 2, rate = .2)) ## [1] 1.211046 27.858217 To analytically derive the gamma HDIs, we just use the good old hdi_of_icdf() function. ( hx &lt;- hdi_of_icdf(name = qgamma, shape = 2, rate = .2) ) ## [1] 0.2118165 23.8258411 Next you need to determine how high up to go on the y-axis. For the quantile-based intervals, the ETIs, you can use dgamma(). The trick is pump the output of qgamma() into dgamma(). ( ey &lt;- qgamma(c(.025, .975), shape = 2, rate = .2) %&gt;% dgamma(shape = 2, rate = .2) ) ## [1] 0.038021620 0.004239155 We follow the same basic principle to get the \\(y\\)-axis values for the HDIs. ( hy &lt;- hdi_of_icdf(name = qgamma, shape = 2, rate = .2) %&gt;% dgamma(shape = 2, rate = .2) ) ## [1] 0.008121227 0.008121233 Now we’ve computed all those values, we can collect them into a tibble with the necessary coordinates to make the ETI and HDI lines in our plot. ( lines &lt;- tibble(interval = rep(c(&quot;eti&quot;, &quot;hdi&quot;), each = 4), x = c(ex, hx) %&gt;% rep(., each = 2), y = c(ey[1], 0, 0, ey[2], 0, hy, 0)) ) ## # A tibble: 8 x 3 ## interval x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 eti 1.21 0.0380 ## 2 eti 1.21 0 ## 3 eti 27.9 0 ## 4 eti 27.9 0.00424 ## 5 hdi 0.212 0 ## 6 hdi 0.212 0.00812 ## 7 hdi 23.8 0.00812 ## 8 hdi 23.8 0 We’re finally ready to plot a more complete version of Figure 12.2. tibble(x = seq(from = 0, to = 40, by = .1)) %&gt;% ggplot(aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dgamma(x, 2, .2)), fill = &quot;grey67&quot;) + geom_path(data = lines, aes(y = y, color = interval), size = 1) + geom_text(data = tibble( x = c(15, 12), y = c(.004, .012), label = c(&quot;95% ETI&quot;, &quot;95% HDI&quot;), interval = c(&quot;eti&quot;, &quot;hdi&quot;) ), aes(y = y, color = interval, label = label)) + scale_color_manual(values = c(&quot;black&quot;, &quot;white&quot;)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 35)) + xlab(&quot;Density Value&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) To repeat, ETIs are the only types of intervals available directly by the brms package. When using the default print() or summary() output for a brm() model, the 95% ETIs are displayed in the ‘l-95% CI’ and ‘u-95% CI’ columns. print(fit9.2) ## Family: binomial ## Links: mu = logit ## Formula: Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player) ## Data: my_data (Number of observations: 948) ## Samples: 3 chains, each with iter = 3500; warmup = 500; thin = 1; ## total post-warmup samples = 9000 ## ## Group-Level Effects: ## ~PriPos (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.32 0.10 0.19 0.59 1.00 2660 4283 ## ## ~PriPos:Player (Number of levels: 948) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.14 0.01 0.12 0.15 1.00 3695 5911 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.17 0.11 -1.40 -0.94 1.00 1507 2262 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). In the output of most other brms functions, the 95% ETIs appear in the Q2.5 and Q97.5 columns. Take fitted(), for example. fitted(fit9.2, newdata = nd, scale = &quot;linear&quot;) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] -0.9712888 0.07647876 -1.123799 -0.8193551 ## [2,] -0.9676824 0.07618627 -1.118517 -0.8174031 But as we just did, above, you can always use the convenience functions from the tidybayes package (e.g., mean_hdi()) to get HDIs from a brms fit. fitted(fit9.2, newdata = nd, scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% gather() %&gt;% group_by(key) %&gt;% mean_hdi(value) ## # A tibble: 2 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 V1 -0.971 -1.13 -0.830 0.95 mean hdi ## 2 V2 -0.968 -1.11 -0.813 0.95 mean hdi As you may have gathered, Kruschke clearly prefers using HDIs over ETIs. His preference isn’t without controversy. If you’d like to explore the topic further in the form of saucy twitter banter, the inimitable Dan Simpson has just the thread for you. Here’s the link to the corresponding thread in the Stan discourse forum. 12.2 The model-comparison approach Recall that the motivating issue for this chapter is the question, Is the null value of a parameter credible? The previous section answered the question in terms of parameter estimation. In that approach, we started with a possibly informed prior distribution and examined the posterior distribution. In this section we take a different approach. Some researchers prefer instead to pose the question in terms of model comparison. In this framing of the question, the focus is not on estimating the magnitude of the parameter. Instead, the focus is on deciding which of two hypothetical prior distributions is least incredible. One prior expresses the hypothesis that the parameter value is exactly the null value. The alternative prior expresses the hypothesis that the parameter could be any value, according to some form of broad distribution. (p. 344) 12.2.1 Is a coin fair or not? Some (e.g., Lee &amp; Webb, 2005; Zhu &amp; Lu, 2004) have argued the Haldane prior is superior to the uniform \\(\\operatorname{beta} (1, 1)\\) when choosing an uninformative prior for \\(\\theta\\). The Haldane, recall, is \\(\\operatorname{beta} (\\epsilon, \\epsilon)\\), where \\(\\epsilon\\) is some small value approaching zero (e.g., 0.01). We’ll use our typical steps with the grid aproximation to compute the data for the left column of Figure 12.3 (i.e., the column based on the Haldane prior). # we need these to compute the likelihood n &lt;- 24 z &lt;- 7 epsilon &lt;- .01 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(prior = dbeta(x = theta, shape1 = epsilon, shape2 = epsilon), likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% # we have to slice off the first and last values because they go to infinity on the prior, # which creats problems when computing the denominator `sum(likelihood * prior)` (i.e., p(D)) slice(2:999) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) head(d) ## # A tibble: 6 x 4 ## theta prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00100 4.67 9.90e-22 1.61e-15 ## 2 0.00200 2.35 1.25e-19 1.02e-13 ## 3 0.00300 1.58 2.09e-18 1.15e-12 ## 4 0.00400 1.19 1.54e-17 6.39e-12 ## 5 0.00501 0.952 7.22e-17 2.40e-11 ## 6 0.00601 0.796 2.54e-16 7.07e-11 Here’s the left column of Figure 12.3. p1 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = prior), fill = &quot;grey50&quot;) + annotate(geom = &quot;text&quot;, x = .1, y = 4, label = expression(epsilon == 0.01), size = 3.5) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(&quot;beta&quot;*(theta*&quot;|&quot;*epsilon*&quot;, &quot;*epsilon))) p2 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = likelihood), fill = &quot;grey50&quot;) + labs(title = &quot;Likelihood (Bernoulli)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = posterior), fill = &quot;grey50&quot;) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(&quot;beta&quot;*(theta*&quot;|&quot;*7.01*&quot;, &quot;*17.01))) library(patchwork) (p1 / p2 / p3) &amp; scale_y_continuous(breaks = NULL) &amp; theme(panel.grid = element_blank()) We can calculate the beta parameters for the posterior using the formula \\(\\operatorname{beta} (\\theta | z + \\alpha, N - z + \\beta)\\). # alpha z + epsilon ## [1] 7.01 # beta n - z + epsilon ## [1] 17.01 We need updated data for the right column, based on the \\(\\operatorname{Beta} (2, 4)\\) prior. alpha &lt;- 2 beta &lt;- 4 d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(prior = dbeta(x = theta, shape1 = alpha, shape2 = beta), likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% # no need to `slice(2:999)` this time mutate(posterior = likelihood * prior / sum(likelihood * prior)) head(d) ## # A tibble: 6 x 4 ## theta prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0. 0. ## 2 0.00100 0.0200 9.90e-22 8.91e-20 ## 3 0.00200 0.0398 1.25e-19 2.24e-17 ## 4 0.00300 0.0595 2.09e-18 5.62e-16 ## 5 0.00400 0.0791 1.54e-17 5.50e-15 ## 6 0.00501 0.0986 7.22e-17 3.21e-14 Now here’s the right column of Figure 12.3. p1 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = prior), fill = &quot;grey50&quot;) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(&quot;beta&quot;*(theta*&quot;|&quot;*2*&quot;, &quot;*4))) p2 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = likelihood), fill = &quot;grey50&quot;) + labs(title = &quot;Likelihood (Bernoulli)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = posterior), fill = &quot;grey50&quot;) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(&quot;beta&quot;*(theta*&quot;|&quot;*9*&quot;, &quot;*21))) (p1 / p2 / p3) &amp; scale_y_continuous(breaks = NULL) &amp; theme(panel.grid = element_blank()) Here are those beta parameters for that posterior. # alpha z + alpha ## [1] 9 # beta n - z + beta ## [1] 21 Following the formula for the null hypothesis, \\[p(z, N|M_\\text{null}) = \\theta_\\text{null}^z(1 - \\theta_\\text{null})^{(N - z)},\\] we can compute the probability of the data given the null hypothesis. theta &lt;- .5 (p_d_null &lt;- theta ^ z * (1 - theta) ^ (n - z)) ## [1] 5.960464e-08 The formula for the marginal likelihood for the alternative hypothesis \\(M_\\text{alt}\\) is \\[p(z, N| M_\\text{alt}) = \\frac{\\operatorname{beta} (z + \\alpha_\\text{alt}, N - z + \\beta_\\text{alt})}{\\operatorname{beta} (\\alpha_\\text{alt}, \\beta_\\text{alt})}.\\] We can make our own p_d() function to compute the probability of the data given alternative hypotheses. Here we’ll simplify the function a bit to extract z and n out of the environment. p_d &lt;- function(a, b) { beta(z + a, n - z + b) / beta(a, b) } With p_d_null and our p_d() function in hand, we can reproduce and extend the results in Kruschke’s Equation 12.4. options(scipen = 999) tibble(shape1 = c(2, 1, .1, .01, .001, .0001, .00001), shape2 = c(4, 1, .1, .01, .001, .0001, .00001)) %&gt;% mutate(p_d = p_d(a = shape1, b = shape2), p_d_null = p_d_null) %&gt;% mutate(bf = p_d / p_d_null) %&gt;% # this just reduces the amount of significant digits in the output mutate_all(round, digits = 6) ## # A tibble: 7 x 5 ## shape1 shape2 p_d p_d_null bf ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 4 0 0 3.72 ## 2 1 1 0 0 1.94 ## 3 0.1 0.1 0 0 0.421 ## 4 0.01 0.01 0 0 0.0481 ## 5 0.001 0.001 0 0 0.00488 ## 6 0.0001 0.0001 0 0 0.000489 ## 7 0.00001 0.00001 0 0 0.000049 options(scipen = 0) Did you notice our use of options(scipen)? With the first line, we turned off scientific notation in the print output. We turned scientific notation back on with the second line. But back to the text, for now, notice that when the alternative prior is uniform, with \\(a_\\text{alt} = b_\\text{alt} = 1.000\\), the Bayes’ factor shows a (small) preference for the alternative hypothesis, but when the alternative prior approximates the Haldane, the Bayes’ factor shows a strong preference for the null hypothesis. As the alternative prior gets closer to the Haldane limit, the Bayes’ factor changes by orders of magnitude. Thus, as we have seen before (e.g. Section 10.6, p. 292), the Bayes’ factor is very sensitive to the choice of prior distribution. (p. 345, emphasis added) On page 346, Kruschke showed some of the 95% HDIs for the marginal distributions of the various \\(M_\\text{alt}\\)s. We could compute those one at a time with hdi_of_icdf(). But why not work in bulk? Like we did in Chapter 10, let’s make a custom variant hdi_of_qbeta(), which will be more useful within the context of map2(). hdi_of_qbeta &lt;- function(shape1, shape2) { hdi_of_icdf(name = qbeta, shape1 = shape1, shape2 = shape2) %&gt;% data.frame() %&gt;% mutate(level = c(&quot;ll&quot;, &quot;ul&quot;)) %&gt;% spread(key = level, value = &quot;.&quot;) } Compute the HDIs. tibble(shape1 = z + c(2, 1, .1, .01, .001, .0001, .00001), shape2 = n - z + c(4, 1, .1, .01, .001, .0001, .00001)) %&gt;% mutate(h = map2(shape1, shape2, hdi_of_qbeta)) %&gt;% unnest(h) %&gt;% mutate_at(vars(ends_with(&quot;l&quot;)), .funs = ~round(., digits = 4)) ## # A tibble: 7 x 4 ## shape1 shape2 ll ul ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 21 0.145 0.462 ## 2 8 18 0.141 0.483 ## 3 7.1 17.1 0.124 0.472 ## 4 7.01 17.0 0.122 0.471 ## 5 7.00 17.0 0.122 0.471 ## 6 7.00 17.0 0.122 0.471 ## 7 7.00 17.0 0.122 0.471 As Kruschke mused, if we consider the posterior distribution instead of the Bayes’ factor, we see that the posterior distribution on \\(\\theta\\) within the alternative model is only slightly affected by the prior… In all cases, the 95% HDI excludes the null value, although a wide ROPE might overlap the HDI. Thus, the explicit estimation of the bias parameter robustly indicates that the null value should be rejected, but perhaps only marginally. This contrasts with the Bayes’ factor, model-comparison approach, which rejected the null or accepted the null depending on the alternative prior. Further, of the Bayes’ factors in Equation 12.4, which is most appropriate? If your analysis is driven by the urge for a default, uninformed alternative prior, then the prior that best approximates the Haldane is most appropriate. Following from that, we should strongly prefer the null hypothesis to the Haldane alternative. While this is mathematically correct, it is meaningless for an applied setting because the Haldane alternative represents nothing remotely resembling a credible alternative hypothesis. The Haldane prior sets prior probabilities of virtually zero at all values of \\(\\theta\\) except \\(\\theta = 0\\) and \\(\\theta = 1\\). There are very few applied settings where such a U-shaped prior represents a genuinely meaningful theory. (p. 346). 12.2.2 Bayes’ factor can accept null with poor precision. Here are the steps to make the left column of Figure 12.4 (i.e., the column based on very weak data and the Haldane prior). # we need these to compute the likelihood n &lt;- 2 z &lt;- 1 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(prior = dbeta(x = theta, shape1 = epsilon, shape2 = epsilon), likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% # like before, we have to slice off the first and last values because they go to infinity on the # prior, which creats problems when computing the denominator `sum(likelihood * prior)` (i.e., p(D)) slice(2:999) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) p1 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = prior), fill = &quot;grey50&quot;) + annotate(geom = &quot;text&quot;, x = .1, y = 4, label = expression(epsilon == 0.01), size = 3.5) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(&quot;beta&quot;*(theta*&quot;|&quot;*epsilon*&quot;, &quot;*epsilon))) p2 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = likelihood), fill = &quot;grey50&quot;) + labs(title = &quot;Likelihood (Bernoulli)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = posterior), fill = &quot;grey50&quot;) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(&quot;beta&quot;*(theta*&quot;|&quot;*7.01*&quot;, &quot;*17.01))) (p1 / p2 / p3) &amp; scale_y_continuous(breaks = NULL) &amp; theme(panel.grid = element_blank()) That is one flat posterior! Here are the shape parameters and the HDIs. (alpha &lt;- z + epsilon) ## [1] 1.01 (beta &lt;- n - z + epsilon) ## [1] 1.01 hdi_of_icdf(name = qbeta, shape1 = alpha, shape2 = beta) %&gt;% round(digits = 3) ## [1] 0.026 0.974 How do we compute the BF? theta &lt;- .5 a &lt;- epsilon b &lt;- epsilon # pD_{null} pD_{alternative} (theta ^ z * (1 - theta) ^ (n - z)) / (beta(z + a, n - z + b) / beta(a, b)) ## [1] 51 Just like in the text, “the Bayes’ factor is 51.0 in favor of the null hypothesis” (p. 347)! Here are the steps to make the right column of Figure 12.4, which is based on stronger data and a flat \\(\\operatorname{beta} (1, 1)\\) prior. # we need these to compute the likelihood n &lt;- 14 z &lt;- 7 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %&gt;% mutate(prior = dbeta(x = theta, shape1 = alpha, shape2 = beta), likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% # no need to `slice(2:999)` this time mutate(posterior = likelihood * prior / sum(likelihood * prior)) p1 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = prior), fill = &quot;grey50&quot;) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(&quot;beta&quot;*(theta*&quot;|&quot;*1*&quot;, &quot;*1))) p2 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = likelihood), fill = &quot;grey50&quot;) + labs(title = &quot;Likelihood (Bernoulli)&quot;, x = expression(theta), y = expression(p(D*&quot;|&quot;*theta))) p3 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = posterior), fill = &quot;grey50&quot;) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(&quot;beta&quot;*(theta*&quot;|&quot;*8*&quot;, &quot;*8))) (p1 / p2 / p3) &amp; scale_y_continuous(breaks = NULL) &amp; theme(panel.grid = element_blank()) Here are the updated shape parameters and the HDIs. (alpha &lt;- z + epsilon) ## [1] 7.01 (beta &lt;- n - z + epsilon) ## [1] 7.01 hdi_of_icdf(name = qbeta, shape1 = alpha, shape2 = beta) %&gt;% round(digits = 3) ## [1] 0.252 0.748 Those HDIs are still pretty wide, but much less so than before. Let’s compute the BF. a &lt;- 1 b &lt;- 1 # pD_{null} pD_{alternative} (theta ^ z * (1 - theta) ^ (n - z)) / (beta(z + a, n - z + b) / beta(a, b)) ## [1] 3.14209 A BF of 3.14 in favor of the null is lackluster evidence. And happily so given the breadth of the HDIs. Kruschke discussed how we’d need \\(z = 1200\\) and \\(N = 2400\\) before the posterior HDIs would fit within a narrow ROPE like .48 and .52. Here’s what that would look like based on the priors from Figure 12.4. z &lt;- 1200 n &lt;- 2400 alpha &lt;- z + epsilon beta &lt;- n - z + epsilon d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) # the Haldane-based plot p1 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_rect(xmin = .48, xmax = .52, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + geom_ribbon(aes(ymin = 0, ymax = dbeta(theta, shape1 = alpha, shape2 = beta)), fill = &quot;grey67&quot;, size = 0) + annotate(geom = &quot;text&quot;, x = .01, y = 35, label = expression(epsilon == 0.01), size = 3.5) + labs(title = &quot;This posterior used the Haldane prior.&quot;, x = expression(theta)) # redefine the Beta parameters alpha &lt;- z + 1 beta &lt;- n - z + 1 # the Beta(1, 1)-based plot p2 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_rect(xmin = .48, xmax = .52, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + geom_ribbon(aes(ymin = 0, ymax = dbeta(theta, shape1 = alpha, shape2 = beta)), fill = &quot;grey67&quot;, size = 0) + labs(title = &quot;This time we used the flat beta (1, 1).&quot;, x = expression(theta)) (p1 / p2) &amp; scale_y_continuous(NULL, breaks = NULL) &amp; theme(panel.grid = element_blank()) There is no way around this inconvenient statistical reality: high precision demands a large sample size (and a measurement device with minimal possible noise). But when we are trying to accept a specific value of \\(\\theta\\), is seems logically appropriate that we should have a reasonably precise estimate indicating that specific value. (p. 348) 12.2.3 Are different groups equal or not? Researchers often want to ask the question, Are the groups different or not? As a concrete example, suppose we conduct an experiment about the effect of background music on the ability to remember. As a simple test of memory, each person tries to memorize the same list of 20 words (such as “chair,” “shark,” “radio,” etc.). They see each word for a specific time, and then, after a brief retention interval, recall as many words as they can. (p. 348) If you look in Kruschke’s OneOddGroupModelComp2E.R file, you can get his simulation code. Here we’ve dramatically simplified it. This attempt does not exactly reproduce what his script did, but it gets it in spirit. # For each subject, specify the condition s/he was in, # the number of trials s/he experienced, and the number correct. n_g &lt;- 20 # number of subjects per group n_t &lt;- 20 # number of trials per subject set.seed(12) my_data &lt;- tibble(condition = factor(c(&quot;Das Kruschke&quot;, &quot;Mozart&quot;, &quot;Bach&quot;, &quot;Beethoven&quot;), levels = c(&quot;Das Kruschke&quot;, &quot;Mozart&quot;, &quot;Bach&quot;, &quot;Beethoven&quot;)), group_means = c(.40, .50, .51, .52)) %&gt;% expand(nesting(condition, group_means), row = 1:20) %&gt;% mutate(id = 1:80, n_g = n_g, n_t = n_t) %&gt;% mutate(n_recalled = rbinom(n_g, n_t, group_means)) head(my_data) ## # A tibble: 6 x 7 ## condition group_means row id n_g n_t n_recalled ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Das Kruschke 0.4 1 1 20 20 5 ## 2 Das Kruschke 0.4 2 2 20 20 10 ## 3 Das Kruschke 0.4 3 3 20 20 11 ## 4 Das Kruschke 0.4 4 4 20 20 7 ## 5 Das Kruschke 0.4 5 5 20 20 6 ## 6 Das Kruschke 0.4 6 6 20 20 4 Here are the means for n_recalled, by condition. my_data %&gt;% group_by(condition) %&gt;% summarise(mean_n_recalled = mean(n_recalled)) ## # A tibble: 4 x 2 ## condition mean_n_recalled ## &lt;fct&gt; &lt;dbl&gt; ## 1 Das Kruschke 7.05 ## 2 Mozart 10.2 ## 3 Bach 10.1 ## 4 Beethoven 10.0 12.2.3.1 Model specification in JAGS brms. Recall that although brms does accommodate models based on the Bernoulli likelihood, it doesn’t do so when the data are aggregated. With our aggregate Bernoulli data, we’ll have to use the conventional binomial likelihood, instead. We’ll compute two models. Our full model will be \\[\\begin{align*} \\text{n_recalled}_{ij} &amp; \\sim \\operatorname{Binomial}(n = 20, \\theta_{j}), \\text{where} \\\\ \\operatorname{logit}(\\theta_j) &amp; = \\beta_{0_j}. \\end{align*}\\] In our equation, \\(\\beta_{0_j}\\) is the group-specific intercept within the logistic regression model. We’ll use the \\(N(0, 1.5)\\) prior for the intercept. Though it appears strongly regularizing in the log-odds space, it’s quite flat on the \\(\\theta\\) space. If we wanted to be more conservative in the \\(\\theta\\) space, we might use something more like \\(N(0, 1)\\). fit12.1 &lt;- brm(data = my_data, family = binomial, n_recalled | trials(20) ~ 0 + condition, prior(normal(0, 1.5), class = b), iter = 3000, warmup = 1000, cores = 4, chains = 4, seed = 12, file = &quot;fits/fit12.01&quot;) Here’s the summary for the full model. print(fit12.1) ## Family: binomial ## Links: mu = logit ## Formula: n_recalled | trials(20) ~ 0 + condition ## Data: my_data (Number of observations: 80) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## conditionDasKruschke -0.61 0.10 -0.81 -0.40 1.00 9087 5690 ## conditionMozart 0.05 0.10 -0.15 0.25 1.00 8868 5862 ## conditionBach 0.02 0.10 -0.18 0.21 1.00 10817 6290 ## conditionBeethoven 0.01 0.10 -0.19 0.21 1.00 9440 6789 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Do keep in mind that our results will differ from Kruschke’s because of two factors. First, we simulated slightly different data. In the limit, I suspect our data simulation approaches would converge. But we’re far from the limit. Second, we used a different likelihood to model the data, which resulted in slightly different priors. Yet even with those substantial limitations, our results are pretty close. To make the top portion of Figure 12.5, we’ll need to extract the condition-specific parameters. For that, we’ll employ fixef() and then wrangle a bit. post &lt;- fixef(fit12.1, summary = F) %&gt;% as_tibble() %&gt;% transmute(theta_1 = conditionDasKruschke, theta_2 = conditionMozart, theta_3 = conditionBach, theta_4 = conditionBeethoven) %&gt;% mutate_all(inv_logit_scaled) %&gt;% transmute(`theta[1]-theta[2]` = theta_1 - theta_2, `theta[1]-theta[3]` = theta_1 - theta_3, `theta[1]-theta[4]` = theta_1 - theta_4, `theta[2]-theta[3]` = theta_2 - theta_3, `theta[2]-theta[4]` = theta_2 - theta_4, `theta[3]-theta[4]` = theta_3 - theta_4) glimpse(post) ## Rows: 8,000 ## Columns: 6 ## $ `theta[1]-theta[2]` &lt;dbl&gt; -0.21841777, -0.21252353, -0.18084253, -0.15656343, -0.16462971, -0.1… ## $ `theta[1]-theta[3]` &lt;dbl&gt; -0.12890281, -0.14023258, -0.15852190, -0.10961604, -0.11042587, -0.1… ## $ `theta[1]-theta[4]` &lt;dbl&gt; -0.13939800, -0.15931747, -0.13867352, -0.15648033, -0.18536375, -0.1… ## $ `theta[2]-theta[3]` &lt;dbl&gt; 0.089514959, 0.072290945, 0.022320636, 0.046947384, 0.054203847, -0.0… ## $ `theta[2]-theta[4]` &lt;dbl&gt; 7.901977e-02, 5.320606e-02, 4.216901e-02, 8.309462e-05, -2.073404e-02… ## $ `theta[3]-theta[4]` &lt;dbl&gt; -0.0104951839, -0.0190848859, 0.0198483751, -0.0468642893, -0.0749378… Now we have the wrangled data, we’re ready to gather() them and plot the top of Figure 12.5. post %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-.25, .25)) + theme(panel.grid = element_blank()) + facet_wrap(~key, labeller = label_parsed) Also, do note we’re working with the \\(\\theta\\) parameters in our aggregated binomial models, rather than \\(\\omega\\)s. Here’s how you’d get the eact mode and HDI summaries. post %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 6 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 theta[1]-theta[2] -0.162 -0.228 -0.092 0.95 mode hdi ## 2 theta[1]-theta[3] -0.146 -0.216 -0.084 0.95 mode hdi ## 3 theta[1]-theta[4] -0.154 -0.216 -0.081 0.95 mode hdi ## 4 theta[2]-theta[3] 0.009 -0.059 0.078 0.95 mode hdi ## 5 theta[2]-theta[4] 0.009 -0.058 0.079 0.95 mode hdi ## 6 theta[3]-theta[4] 0.005 -0.067 0.072 0.95 mode hdi If we wanted to know what proportion of the difference distributions were greater than 0, we could do something like this. post %&gt;% gather() %&gt;% group_by(key) %&gt;% summarise(p = mean(value &gt; 0)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 6 x 2 ## key p ## &lt;chr&gt; &lt;dbl&gt; ## 1 theta[1]-theta[2] 0 ## 2 theta[1]-theta[3] 0 ## 3 theta[1]-theta[4] 0 ## 4 theta[2]-theta[3] 0.594 ## 5 theta[2]-theta[4] 0.618 ## 6 theta[3]-theta[4] 0.528 I got this idea from the great Tristan Mahr, who pointed out that conditional tests like value &gt; 0 compute a vector of TRUE and FALSE values. By nesting that within mean(), you end up with the proportion of those values that are TRUE. With our Stan/brms method, we don’t have an analogue to the lower portion of Figure 12.5 because we are not fitting the full and restricted models within a single run. Thus, there’s no plot to show the chains traversing from \\(M_\\text{full}\\) to \\(M_\\text{restricted}\\). Rather, our fit12.1 was just of \\(M_\\text{full}\\). Now we’ll fit \\(M_\\text{restricted}\\), which we’ll save as fit12.2. fit12.2 &lt;- brm(data = my_data, family = binomial, n_recalled | trials(20) ~ 1, prior(normal(0, 1.5), class = Intercept), iter = 3000, warmup = 1000, cores = 4, chains = 4, seed = 12, file = &quot;fits/fit12.02&quot;) Here we’ll compare the two models with the LOO. fit12.1 &lt;- add_criterion(fit12.1, criterion = &quot;loo&quot;) fit12.2 &lt;- add_criterion(fit12.2, criterion = &quot;loo&quot;) loo_compare(fit12.1, fit12.2) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.1 0.0 0.0 -172.5 4.4 3.3 0.4 344.9 8.8 ## fit12.2 -11.9 5.5 -184.4 6.9 1.1 0.2 368.7 13.8 The LOO comparison suggests fit12.1, the full model with the condition-specific intercepts, is an improvement over the restricted one-intercept-only model. We can also compare the models with their weights via the model_weights() function. Here we’ll use weights = &quot;loo&quot; criterion. model_weights(fit12.1, fit12.2, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit12.1 fit12.2 ## 1 0 Recall that within a given comparison, the weights sum to 1, with better fitting models tending closer to 1 than the other(s). In this case, almost all the weight went to the \\(M_\\text{full}\\), fit12.1. 12.2.3.2 Bonus: Hypothesis testing in brms. Disclaimer: I am not a fan of hypothesis texting within the Bayesian framework. Outside of pedagogical material, I do not use these methods. However, it’d seem negligent not to at least mention the convenience function designed for that purpose in brms: the hypothesis() function. From the hypothesis.brmsfit section in the brms reference manual (Bürkner, 2020g, p. 96) we read: Among others, hypothesis computes an evidence ratio (Evid.Ratio) for each hypothesis. For a one-sided hypothesis, this is just the posterior probability (Post.Prob) under the hypothesis against its alternative. That is, when the hypothesis is of the form a &gt; b, the evidence ratio is the ratio of the posterior probability of a &gt; b and the posterior probability of a &lt; b. In this example, values greater than one indicate that the evidence in favor of a &gt; b is larger than evidence in favor of a &lt; b. For an two-sided (point) hypothesis, the evidence ratio is a Bayes factor between the hypothesis and its alternative computed via the Savage-Dickey density ratio method. That is the posterior density at the point of interest divided by the prior density at that point. Values greater than one indicate that evidence in favor of the point hypothesis has increased after seeing the data. In order to calculate this Bayes factor, all parameters related to the hypothesis must have proper priors and argument sample_prior of function brm must be set to &quot;yes&quot;. Otherwise Evid.Ratio (and Post.Prob) will be NA. Please note that, for technical reasons, we cannot sample from priors of certain parameters classes. Most notably, these include overall intercept parameters (prior class &quot;Intercept&quot;) as well as group-level coefficients. When interpreting Bayes factors, make sure that your priors are reasonable and carefully chosen, as the result will depend heavily on the priors. In particular, avoid using default priors. Following the a &lt; b format, let’s say we wanted to test the hypothesis \\(\\theta_\\text{Das Kruschke} &lt; \\theta_\\text{Bach}\\), based on fit12.1. If we convert the relevant parameters from the log-odds metric to the probability scale with inv_logit_scaled(), we can specify that hypothesis as a string and place it into the hypothesis() function. hypothesis(fit12.1, &quot;inv_logit_scaled(conditionDasKruschke) &lt; inv_logit_scaled(conditionBach)&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star ## 1 (inv_logit_scaled... &lt; 0 -0.15 0.03 -0.21 -0.1 Inf 1 * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. In the Estimate through CI.Upper columns, we got the typical brms summary statistics for model parameters. Those CIs, recall, are ETIs rather than HDIs. To interpret the rest, we read further from the brms reference manual that the Evid.Ratio may sometimes be 0 or Inf implying very small or large evidence, respectively, in favor of the tested hypothesis. For one-sided hypotheses pairs, this basically means that all posterior samples are on the same side of the value dividing the two hypotheses. In that sense, instead of 0 or Inf, you may rather read it as Evid.Ratio smaller 1 / S or greater S, respectively, where S denotes the number of posterior samples used in the computations. The argument alpha specifies the size of the credible interval (i.e., Bayesian confidence interval). For instance, if we tested a two-sided hypothesis and set alpha = 0.05 (5%) an, the credible interval will contain 1 -alpha = 0.95 (95%) of the posterior values. Hence, alpha*100% of the posterior values will lie outside of the credible interval. Although this allows testing of hypotheses in a similar manner as in the frequentist null-hypothesis testing framework, we strongly argue against using arbitrary cutoffs (e.g., p &lt; .05) to determine the ‘existence’ of an effect. In this case, the entire posterior distribution (i.e., all the iterations of the chains) was below zero and we ended up with an Evid.Ratio = Inf. Our Bayes factor blew up. If we’d like to test a point null hypothesis, we might reformat the equation to \\(\\theta_\\text{Das Kruschke} = \\theta_\\text{Bach}\\). hypothesis(fit12.1, &quot;inv_logit_scaled(conditionDasKruschke) = inv_logit_scaled(conditionBach)&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star ## 1 (inv_logit_scaled... = 0 -0.15 0.03 -0.22 -0.08 NA NA * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. Here we no longer get summary information in the Evid.Ratio and Post.Prob columns. But we do get that posterior summary information and we also get that little * symbol in the Star column, which was based on the brms default alpha = 0.05. Let’s see what happens when we test a different kind of directional hypothesis, \\(\\theta_\\text{Motzart} - \\theta_\\text{Bach} &gt; 0\\). hypothesis(fit12.1, &quot;inv_logit_scaled(conditionMozart) - inv_logit_scaled(conditionBach) &gt; 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star ## 1 (inv_logit_scaled... &gt; 0 0.01 0.03 -0.05 0.07 1.46 0.59 ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. Here we get an underwhelming BF of 1.46. The posterior probability that hypothesis versus its logical alternative is .59. Notice we no longer have a * in the Star column. We won’t be using the hypothesis() function much, in the project. But if you’re interested, there are other trick ways to make good use of it. To learn more, check out Vourre’s handy blog post, How to calculate contrasts from a fitted brms model. 12.3 Relations of parameter estimation and model comparison Back to the text, Kruschke wrapped up this section by explaining the model comparison focuses on the null value and whether its local probability increases from prior to posterior. The parameter estimation considers the entire posterior distribution, including the uncertainty (i.e., HDI) of the parameter estimate relative to the ROPE. The derivation of the Bayes’ factor by considering the null value in parameter estimation is known as the Savage-Dickey method. A lucid explanation is provided by Wagenmakers, Lodewyckx, Kuriyal, and Grasman (2010), who also provide some historical references and applications to MCMC analysis of hierarchical models. (pp. 353–354) Hey, we just read about that Savage-Dickey method when learning about the brms::hypothesis() function! 12.4 Estimation and model comparison? I’ll leave you to decide. Here’s Kruschke to close us out: “As mentioned above, neither method for null value assessment (parameter estimation or model comparison) is uniquely ‘correct.’ The two approaches merely pose the question of the null value in different ways” (p. 354) Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.0.0 tidybayes_2.0.3.9000 brms_2.12.0 Rcpp_1.0.4.6 ## [5] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 ## [9] readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ggplot2_3.3.0 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 ## [9] farver_2.0.3 rstan_2.19.3 svUnit_1.0.3 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 ## [17] bridgesampling_1.0-0 knitr_1.28 shinythemes_1.1.2 bayesplot_1.7.1 ## [21] jsonlite_1.6.1 broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 ## [25] compiler_3.6.3 httr_1.4.1 backports_1.1.6 assertthat_0.2.1 ## [29] Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 later_1.0.0 ## [33] htmltools_0.4.0 prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 ## [41] cellranger_1.1.0 vctrs_0.3.0 nlme_3.1-144 crosstalk_1.1.0.1 ## [45] xfun_0.13 ps_1.3.3 rvest_0.3.5 mime_0.9 ## [49] miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 MASS_7.3-51.5 ## [53] zoo_1.8-7 scales_1.1.1 colourpicker_1.0 hms_0.5.3 ## [57] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 loo_2.2.0 ## [65] StanHeaders_2.21.0-1 stringi_1.4.6 dygraphs_1.1.1.6 pkgbuild_1.0.8 ## [69] rlang_0.4.6 pkgconfig_2.0.3 matrixStats_0.56.0 HDInterval_0.2.0 ## [73] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [77] labeling_0.3 tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 ## [81] magrittr_1.5 bookdown_0.18 R6_2.4.1 generics_0.0.2 ## [85] DBI_1.1.0 pillar_1.4.4 haven_2.2.0 withr_2.2.0 ## [89] xts_0.12-0 abind_1.4-5 modelr_0.1.6 crayon_1.3.4 ## [93] arrayhelpers_1.1-0 utf8_1.1.4 rmarkdown_2.1 grid_3.6.3 ## [97] readxl_1.3.1 callr_3.4.3 threejs_0.3.3 reprex_0.3.0 ## [101] digest_0.6.25 xtable_1.8-4 httpuv_1.5.2 stats4_3.6.3 ## [105] munsell_0.5.0 shinyjs_1.1 References "],
["goals-power-and-sample-size.html", "13 Goals, Power, and Sample Size 13.1 The will to power 13.2 Computing power and sample size 13.3 Sequential testing and the goal of precision 13.4 Discussion Session info", " 13 Goals, Power, and Sample Size Researchers collect data in order to achieve a goal. Sometimes the goal is to show that a suspected underlying state of the world is credible; other times the goal is to achieve a minimal degree of precision on whatever trends are observed. Whatever the goal, it can only be probabilistically achieved, as opposed to definitely achieved, because data are replete with random noise that can obscure the underlying state of the world. Statistical power is the probability of achieving the goal of a planned empirical study, if a suspected underlying state of the world is true. (Kruschke, 2015, p. 359, emphasis in the original) 13.1 The will to power “In this section, [Kruschke laid out a] framework for research and data analysis [that might lead] to a more precise definition of power and how to compute it” (p. 360). 13.1.1 Goals and obstacles. The three research goals Kruschke dealt with in this chapter were: to reject a null value for a parameter, to confirm the legitimacy of a particular parameter value, and to estimate a parameter with reasonable precision. All these could, of course, be extended to contexts involving multiple parameters and all of these were dealt with using 95% HDIs. 13.1.2 Power. Because of random noise, the goal of a study can be achieved only probabilistically. The probability of achieving the goal, given the hypothetical state of the world and the sampling plan, is called the power of the planned research. In traditional null hypothesis significance testing (NHST), power has only one goal (rejecting the null hypothesis), and there is one conventional sampling plan (stop at predetermined sample size) and the hypothesis is only a single specific value of the parameter. In traditional statistics, that is the definition of power. That definition is generalized in this book to include other goals, other sampling plans, and hypotheses that involve an entire distribution on parameters. (p. 361, emphasis in the original) Three primary methods to increase power are: reducing measurement error, increasing the effect size, and increasing the sample size. Kruschke then laid out a five-step procedure to compute power within a Bayesian workflow. Use theory/prior information to specify hypothetical distributions for all parameter values in the model. Use those distributions to generate synthetic data according to the planned sampling method. Fit the proposed model–including the relevant priors–with the synthetic data. Use the posterior to determine whether we attained the research goal. Repeat the procedure many times (i.e., using different set.seed() values) to get a distribution of results. 13.1.3 Sample size. The best that a large sample can do is exactly reflect the data-generating distribution. If the data-generating distribution has considerable mass straddling the null value, then the best we can do is get estimates that include and straddle the null value. As a simple example, suppose that we think that a coin may be biased, and the data-generating hypothesis entertains four possible values of \\(\\theta\\) , with \\(p (\\theta = 0.5) = 25 \\%\\), \\(p (\\theta = 0.6) = 25 \\%\\), \\(p (\\theta = 0.7) = 25 \\%\\), and \\(p (\\theta = 0.8) = 25 \\%\\). Because \\(25 \\%\\) of the simulated data come from a fair coin, the maximum probability of excluding \\(\\theta = 0.5\\), even with a huge sample, is \\(75 \\%\\). Therefore, when planning the sample size for an experiment, it is crucial to decide what a realistic goal is. If there are good reasons to posit a highly certain data-generating hypothesis, perhaps because of extensive previous results, then a viable goal may be to exclude a null value. On the other hand, if the data-generating hypothesis is somewhat vague, then a more reasonable goal is to attain a desired degree of precision in the posterior. (p. 364, emphasis in the original) 13.1.4 Other expressions of goals. I’m going to skip over these. In the remainder of the chapter, it will be assumed that the goal of the research is estimation of the parameter values, starting with a viable prior. The resulting posterior distribution is then used to assess whether the goal was achieved. (p. 366) 13.2 Computing power and sample size As our first worked-out example, consider the simplest case: Data from a single coin. Perhaps we are polling a population and we want to precisely estimate the preferences for candidates A or B. Perhaps we want to know if a drug has more than a 50% cure rate. (p. 366) 13.2.1 When the goal is to exclude a null value. Usually it is more intuitively accessible to get prior data, or to think of idealized prior data, than to directly specify a distribution over parameter values. For example, based on knowledge about the application domain, we might have 2000 actual or idealized flips of the coin for which the result showed 65% heads. Therefore we’ll describe the data-generating hypothesis as a beta distribution with a mode of 0.65 and concentration based on 2000 flips after a uniform “proto-prior”: \\(\\operatorname{beta} (\\theta | 0.65 \\cdot (2000 - 2) + 1, (1 - 0.65) \\cdot (2000 - 2) + 1)\\). We might express that in code like this. library(tidyverse) kappa &lt;- 2000 omega &lt;- .65 tibble(theta = seq(from = 0, to = 1, by = .001)) %&gt;% mutate(prior = dbeta(theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(size = 0, fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Behold our prior. It&#39;s rather peaked.&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) If we wanted to take some random draws from that prior, say 5, we’d do something like this. n &lt;- 5 set.seed(13) rbeta(n, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) ## [1] 0.6430548 0.6532279 0.6250891 0.6475884 0.6351476 Now let’s just take one draw and call it bias. n &lt;- 1 set.seed(13) bias &lt;- rbeta(n, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) print(bias) ## [1] 0.6430548 Do note that whereas Kruschke based his discussion on a bias of 0.638, we’re moving forward with our randomly-drawn 0.643. Anyways, now we simulate flipping a coin with that bias \\(N\\) times. The simulated data have \\(z\\) heads and \\(N − z\\) tails. The proportion of heads, \\(z/N\\), will tend to be around [0.643], but will be higher or lower because of randomness in the flips. (p. 367) # pick some large number n &lt;- 1e3 set.seed(13) tibble(flips = rbernoulli(n = n, p = bias)) %&gt;% summarise(n = n(), z = sum(flips)) %&gt;% mutate(`proportion of heads` = z / n) ## # A tibble: 1 x 3 ## n z `proportion of heads` ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1000 652 0.652 And indeed our samples did tend around \\(\\theta =.643\\). Had we increased our number of draws by an order of magnitude or two, it our proportion of heads would have been even closer to the true data-generating value. Though he presented Table 13.1 in this section, Kruschke walked out how he came to those values in the following sections. We’ll get to them in just a bit. 13.2.2 Formal solution and implementation in R. I’ve been playing around with this a bit. If you look closely at the code block on page 369, you’ll see that Kruschke’s minNforHDIpower() function requires the HDIofICDF() function from his DBDA2E-utilities.R file, which we usually recast as hdi_of_icdf(). hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Just to warm up, consider a beta distribution for which \\(\\omega = .5\\) and \\(\\kappa = 2000\\). Here are the 95% HDIs. omega &lt;- .5 hdi_of_icdf(name = qbeta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) ## [1] 0.4780947 0.5219053 Those look a whole lot like the ROPE values Kruschke specified in his example at the bottom of page 370. But we’re getting ahead of ourselves. Now that we have our hdi_of_icdf() function, we’re ready to define our version of minNforHDIpower(), which I’m calling min_n_for_hdi_power(). min_n_for_hdi_power &lt;- function(gen_prior_mode, gen_prior_n, hdi_max_width = NULL, null_value = NULL, rope = c(max(0, null_value - 0.02), min(1, null_value + 0.02)), desired_power = 0.8, aud_prior_mode = 0.5, aud_prior_n = 2, hdi_mass = 0.95, init_samp_size = 20, verbose = TRUE) { # Check for argument consistency: if (!xor(is.null(hdi_max_width), is.null(null_value))) { stop(&quot;One and only one of `hdi_max_width` and `null_value` must be specified.&quot;) } # Convert prior mode and N to a, b parameters of beta distribution: gen_prior_a &lt;- gen_prior_mode * (gen_prior_n - 2) + 1 gen_prior_b &lt;- (1.0 - gen_prior_mode) * (gen_prior_n - 2) + 1 aud_prior_a &lt;- aud_prior_mode * (aud_prior_n - 2) + 1 aud_prior_b &lt;- (1.0 - aud_prior_mode) * (aud_prior_n - 2) + 1 # Initialize loop for incrementing `sample_size`: sample_size &lt;- init_samp_size not_powerful_enough = TRUE # Increment `sample_size` until desired power is achieved: while(not_powerful_enough) { z_vec &lt;- 0:sample_size # vector of all possible z values for N flips. # Compute probability of each z value for data-generating prior: p_z_vec &lt;- exp(lchoose(sample_size, z_vec) + lbeta(z_vec + gen_prior_a, sample_size - z_vec + gen_prior_b) - lbeta(gen_prior_a, gen_prior_b)) # For each z value, compute posterior HDI: # `hdi_matrix` will hold HDI limits for each z: hdi_matrix &lt;- matrix(0, nrow = length(z_vec), ncol = 2) for (z_id_x in 1:length(z_vec)) { z &lt;- z_vec[z_id_x] hdi_matrix[z_id_x, ] &lt;- hdi_of_icdf(qbeta, shape1 = z + aud_prior_a, shape2 = sample_size - z + aud_prior_b, width = hdi_mass) } # Compute HDI widths: hdi_width &lt;- hdi_matrix[, 2] - hdi_matrix[, 1] # Sum the probabilities of outcomes with satisfactory HDI widths: if (!is.null(hdi_max_width)) { power_hdi &lt;- sum(p_z_vec[hdi_width &lt; hdi_max_width]) } # Sum the probabilities of outcomes with HDI excluding `rope`: if (!is.null(null_value)) { power_hdi &lt;- sum(p_z_vec[hdi_matrix[, 1] &gt; rope[2] | hdi_matrix[, 2] &lt; rope[1]]) } if (verbose) { cat(&quot; For sample size = &quot;, sample_size, &quot;, power = &quot;, power_hdi, &quot;\\n&quot;, sep = &quot;&quot;); flush.console() } if (power_hdi &gt; desired_power) { # If desired power is attained, not_powerful_enough = FALSE } else { sample_size &lt;- sample_size + 1 # set flag to stop, # otherwise # increment the sample size. } } # End while( not_powerful_enough ). # Return the sample size that achieved the desired power: return(sample_size) } Other than altering Kruschke’s formatting a little bit, the only meaningful change I made to the code was removing the line that checked for the HDIofICD() function and then source()ed it, if necessary. Following along with Kruschke on page 370, here’s an example for which \\(\\omega_\\text{data generating} = .75\\), \\(\\kappa = 2000\\), the ROPE is \\([.48, .52]\\), and the desired power is the conventional .8. min_n_for_hdi_power(gen_prior_mode = .75, gen_prior_n = 2000, hdi_max_width = NULL, null_value = .5, rope = c(.48, .52), desired_power = .8, aud_prior_mode = .5, aud_prior_n = 2, hdi_mass = .95, init_samp_size = 20, verbose = TRUE) ## For sample size = 20, power = 0.6159196 ## For sample size = 21, power = 0.5655352 ## For sample size = 22, power = 0.6976802 ## For sample size = 23, power = 0.6521637 ## For sample size = 24, power = 0.606033 ## For sample size = 25, power = 0.7245362 ## For sample size = 26, power = 0.6832871 ## For sample size = 27, power = 0.7836981 ## For sample size = 28, power = 0.7479021 ## For sample size = 29, power = 0.7103786 ## For sample size = 30, power = 0.8009259 ## [1] 30 Just like in the text, the necessary \\(N = 30\\). Unlike in the text, I increased the value of init_samp_size from 5 to 20 to keep the output a reasonable length. To clarify what we just did, in that function call, the data-generating distribution has a mode of 0.75 and concentration of 2000, which means that the hypothesized world is pretty certain that coins have a bias of 0.75. The goal is to exclude a null value of 0.5 with a ROPE from 0.48 to 0.52. The desired power [is] 80%. The audience prior is uniform. When the function is executed, it displays the power for increasing values of sample size, until stopping at \\(N = 30\\). (p. 370) If it’s unclear why the “audience prior is uniform”, consider this. kappa &lt;- 2 omega &lt;- .5 tibble(theta = seq(from = 0, to = 1, by = .01)) %&gt;% mutate(prior = dbeta(theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(size = 0, fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(ylim = c(0, 1.25)) + labs(title = &quot;Behold the uniform audience prior.&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) If you work out the algebra with omega and kappa, you’ll see this is a \\(\\operatorname{beta} (1, 1)\\). Thus, aud_prior_n is \\(\\kappa\\) and aud_prior_mode is \\(\\omega\\). Here we’ll wrap our min_n_for_hdi_power() function into a simple sim_power() function for use with purrr::map2(). sim_power &lt;- function(mode, power) { min_n_for_hdi_power(gen_prior_mode = mode, gen_prior_n = 2000, hdi_max_width = NULL, null_value = .5, rope = c(.48, .52), desired_power = power, aud_prior_mode = .5, aud_prior_n = 2, hdi_mass = .95, init_samp_size = 1, verbose = TRUE) } Here we use the two functions to compute the values in Table 13.1 on page 367. sim &lt;- crossing(mode = seq(from = .6, to = .85, by = .05), power = c(.7, .8, .9)) %&gt;% mutate(results = map2_dbl(mode, power, sim_power)) The results look like this. print(sim) ## # A tibble: 18 x 3 ## mode power results ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.6 0.7 238 ## 2 0.6 0.8 309 ## 3 0.6 0.9 430 ## 4 0.65 0.7 83 ## 5 0.65 0.8 109 ## 6 0.65 0.9 150 ## 7 0.7 0.7 40 ## 8 0.7 0.8 52 ## 9 0.7 0.9 74 ## 10 0.75 0.7 25 ## 11 0.75 0.8 30 ## 12 0.75 0.9 43 ## 13 0.8 0.7 16 ## 14 0.8 0.8 19 ## 15 0.8 0.9 27 ## 16 0.85 0.7 7 ## 17 0.85 0.8 14 ## 18 0.85 0.9 16 It takes just a tiny bit of wrangling to reproduce Table 13.1. sim %&gt;% pivot_wider(names_from = mode, values_from = results) %&gt;% knitr::kable() power 0.6 0.65 0.7 0.75 0.8 0.85 0.7 238 83 40 25 16 7 0.8 309 109 52 30 19 14 0.9 430 150 74 43 27 16 13.2.3 When the goal is precision. Recall that if we have \\(\\operatorname{beta} (a, b)\\) prior for \\(\\theta\\) of the Bernoulli likelihood function, then the analytic solution for the posterior is \\(\\operatorname{beta} (\\theta | z + a, N – z + b)\\). In our first example, \\(z = 6\\) out of \\(N = 10\\) randomly selected voters preferred candidate A and we started with a flat \\(\\operatorname{beta} (\\theta | 1, 1)\\) prior. We can check that our posterior is indeed \\(\\operatorname{beta} (7, 5)\\) by working through the algebra. z &lt;- 6 n &lt;- 10 # posterior alpha z + 1 ## [1] 7 # posterior beta n - z + 1 ## [1] 5 Here’s how we compute the 95% HDIs. ( h &lt;- hdi_of_icdf(name = qbeta, shape1 = 7, shape2 = 5) ) ## [1] 0.3182322 0.8414276 The \\(\\operatorname{beta} (7, 5)\\) distribution looks like this. tibble(theta = seq(from = 0, to = 1, by = .01)) %&gt;% mutate(density = dbeta(theta, shape1 = 7, shape2 = 5)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = density)) + geom_ribbon(size = 0, fill = &quot;grey67&quot;) + geom_segment(x = h[1], xend = h[2], y = 0, yend = 0, size = 1) + annotate(geom = &quot;text&quot;, x = .6, y = 1/3, label = &quot;95% HDI&quot;) + scale_x_continuous(NULL, breaks = c(0, h[1], z / n, h[2], 1) %&gt;% round(2)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(&quot;beta&quot;*(7*&quot;, &quot;*5))) + theme(panel.grid = element_blank()) “It turns out, in this case, that we can never have a sample size large enough to achieve the goal of 80% of the HDIs falling above \\(\\theta = 0.5\\). To see why,” keep reading in the text (p. 371). Happily, there is a more useful goal, however. Instead of trying to reject a particular value of \\(\\theta\\), we set as our goal a desired degree of precision in the posterior estimate. For example, our goal might be that the 95% HDI has width less than 0.2, at least 80% of the time. (p. 371) If you look back up at our min_n_for_hdi_power() defining code, above, you’ll see that “One and only one of hdi_max_width and null_value must be specified.” So if we want to determine the necessary \\(N\\) for an 95% HDI width of less than .2, we need to set hdi_max_width = .2 and null_value = NULL. min_n_for_hdi_power(gen_prior_mode = .75, gen_prior_n = 10, hdi_max_width = .2, # look here null_value = NULL, rope = NULL, desired_power = .8, aud_prior_mode = .5, aud_prior_n = 2, hdi_mass = .95, init_samp_size = 75, verbose = TRUE) ## For sample size = 75, power = 0.5089359 ## For sample size = 76, power = 0.5337822 ## For sample size = 77, power = 0.5235513 ## For sample size = 78, power = 0.5474934 ## For sample size = 79, power = 0.5706373 ## For sample size = 80, power = 0.5929882 ## For sample size = 81, power = 0.6145578 ## For sample size = 82, power = 0.6353626 ## For sample size = 83, power = 0.6554231 ## For sample size = 84, power = 0.6747629 ## For sample size = 85, power = 0.6934076 ## For sample size = 86, power = 0.7113842 ## For sample size = 87, power = 0.7287209 ## For sample size = 88, power = 0.7716517 ## For sample size = 89, power = 0.787177 ## For sample size = 90, power = 0.8266938 ## [1] 90 Just like in the last section, here I set init_samp_size to a higher value than in the text in order to keep the output reasonably short. To reproduce the results in Table 13.2, we’ll need to adjust the min_n_for_hdi_power() parameters within our sim_power() function. sim_power &lt;- function(mode, power) { min_n_for_hdi_power(gen_prior_mode = mode, gen_prior_n = 10, hdi_max_width = .2, null_value = NULL, rope = NULL, desired_power = power, aud_prior_mode = .5, aud_prior_n = 2, hdi_mass = .95, init_samp_size = 50, verbose = TRUE) } sim &lt;- crossing(mode = seq(from = .6, to = .85, by = .05), power = c(.7, .8, .9)) %&gt;% mutate(results = map2_dbl(mode, power, sim_power)) Let’s make that table. sim %&gt;% pivot_wider(names_from = mode, values_from = results) %&gt;% knitr::kable() power 0.6 0.65 0.7 0.75 0.8 0.85 0.7 91 90 88 86 81 75 0.8 92 92 91 90 87 82 0.9 93 93 93 92 91 89 What did that audience prior look like? kappa &lt;- 2 omega &lt;- .5 tibble(theta = seq(from = 0, to = 1, by = .1)) %&gt;% mutate(density = dbeta(theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = density)) + geom_ribbon(size = 0, fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, 1.25)) + labs(title = &quot;Behold the uniform audience prior.&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) Here are what the beta distributions based on the sim look like. sim %&gt;% rename(n = results) %&gt;% expand(nesting(mode, power, n), theta = seq(from = 0, to = 1, by = .01)) %&gt;% mutate(density = dbeta(theta, shape1 = mode * (n - 2) + 1, shape2 = (1 - mode) * (n - 2) + 1), mode = str_c(&quot;omega == &quot;, mode)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = density)) + geom_vline(xintercept = .5, color = &quot;white&quot;) + geom_ribbon(size = 0, fill = &quot;grey67&quot;) + scale_x_continuous(expression(theta), labels = c(&quot;0&quot;, &quot;&quot;, &quot;.5&quot;, &quot;&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;The power and mode values are in the rows and columns, respectively.&quot;) + theme(panel.grid = element_blank()) + facet_grid(power ~ mode, labeller = label_parsed) Toward the end of the section, Kruschke mentioned the required sample size shoots up if our desired HDI width is 0.1. Here’s the simulation. sim_power &lt;- function(mode, power) { min_n_for_hdi_power(gen_prior_mode = mode, gen_prior_n = 10, hdi_max_width = .1, null_value = NULL, rope = NULL, desired_power = power, aud_prior_mode = .5, aud_prior_n = 2, hdi_mass = .95, init_samp_size = 300, # save some time and up this parameter verbose = TRUE) } sim &lt;- crossing(mode = seq(from = .6, to = .85, by = .05), power = c(.7, .8, .9)) %&gt;% mutate(results = map2_dbl(mode, power, sim_power)) Display the results in a table like before. sim %&gt;% pivot_wider(names_from = mode, values_from = results) %&gt;% knitr::kable() power 0.6 0.65 0.7 0.75 0.8 0.85 0.7 373 370 364 352 332 303 0.8 378 376 373 367 354 334 0.9 380 380 379 378 373 363 13.2.4 Monte Carlo approximation of power. The previous sections illustrated the ideas of power and sample size for a simple case in which the power could be computed by mathematical derivation. [If your field is like mine, this will not be the norm for your research projects.] In this section, we approximate the power by Monte Carlo simulation. The R script for this simple case serves as a template for more realistic applications. The R script is named Jags-Ydich-Xnom1subj-MbernBeta-Power.R, which is the name for the JAGS program for dichotomous data from a single “subject” suffixed with the word “Power.” As you read through the script, presented below, remember that you can find information about any general R command by using the help function in R, as explained in Section 3.3.1 (p. 39). (p. 372) The code in Kruschke’s Jags-Ydich-Xnom1subj-MbernBeta-Power.R file also makes use of the content in his Jags-Ydich-Xnom1subj-MbernBeta.R file. As is often the case, the code in both is JAGS and base-R centric. We’ll be taking a different approach. I’ll walk you through. First, let’s fire up brms. library(brms) This won’t be of much concern for some of the complex models we’ll be fitting in later chapters. But for simple models like this, a lot of the time you spend waiting for brms::brm() to return your posterior and its summary has to do with compilation time. The issue of compilation goes into technical details I just don’t have the will to go through right now. But if we can avoid or minimize compilation time, it’ll be a boon for our power simulations. As it turns out, we can. The first time we fit our desired model, we have to compile. But once we have that initial fit object in hand, we can reuse it with the update() function, which will allow us to avoid further compilation. So that’s what we’re going to do, here. We’re going to fit the model once and save it. # how many rows of 0s and 1s should we have in the data? n &lt;- 74 # should the values in the data be of single draws (i.e., 1), or summaries? size &lt;- 1 # what is the population mode for theta we&#39;d like to base this all on? omega &lt;- .7 # fit that joint fit13.1 &lt;- brm(data = tibble(y = rbinom(n, size, omega)), family = bernoulli(link = identity), y ~ 1, prior(beta(1, 1), class = Intercept), warmup = 1000, iter = 3000, chains = 4, cores = 1, seed = 13, control = list(adapt_delta = .9), file = &quot;fits/fit13.01&quot;) You may (or not) recall that at the end of Chapter 3, we covered how to time an operation in R. When you’re setting up a Monte Carlo power study, it can be important to use those time-tracking skills to get a sense of how long it takes to fit your models. While I was setting this model up, I experimented with keeping the default cores = 1 or setting my typical cores = 4. As it turns out, with a very simple model like this, cores = 1 was a little faster. If you’re fitting one model, that’s no big deal. But in a situation where you’re fitting 100 or 1000, you’ll want to make sure you’re fitting them as efficiently as possible. But anyway, our practice will be to keep all the specifications in fit constant across the simulations. So choose them wisely. If you look deep into the bowels of the Jags-Ydich-Xnom1subj-MbernBeta.R file, you’ll see Kruschke used the flat \\(\\operatorname{beta}(1, 1)\\) prior, which is where our prior(beta(1, 1), class = Intercept) code came from. This is the audience prior. We aren’t particularly concerned about the data we simulated with the data = tibble(y = rbinom(n, size, omega)) line. The main thing is that they follow the same basic structure our subsequent data will. To make sure we’re not setting ourselves up to fail, we might make sure the chains look okay. plot(fit13.1) Looks like a dream. Let’s move forward and run the simulation proper. In his script file, Kruschke suggested we simulate with large \\(N\\)s like 1000 or so. Since this is just an example, I’m gonna cut that to 100. # how many simulations would you like? n_sim &lt;- 100 # specify omega and kappa of the hypothetical parameter distribution omega &lt;- .7 kappa &lt;- 2000 # make it reproducible set.seed(13) sim1 &lt;- # define some of the parameters tibble(n = n, size = size, theta = rbeta(n_sim, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% # simulate the data mutate(data = pmap(list(n, size, theta), rbinom)) %&gt;% # fit the models on the simulated data mutate(fit = map(data, ~update(fit13.1, newdata = list(y = .)))) What have we done? you might ask. head(sim1) ## # A tibble: 6 x 5 ## n size theta data fit ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 74 1 0.693 &lt;int [74]&gt; &lt;brmsfit&gt; ## 2 74 1 0.703 &lt;int [74]&gt; &lt;brmsfit&gt; ## 3 74 1 0.676 &lt;int [74]&gt; &lt;brmsfit&gt; ## 4 74 1 0.698 &lt;int [74]&gt; &lt;brmsfit&gt; ## 5 74 1 0.686 &lt;int [74]&gt; &lt;brmsfit&gt; ## 6 74 1 0.695 &lt;int [74]&gt; &lt;brmsfit&gt; The theta column contains the draws from the hypothesized parameter distribution, which we’ve indicated is hovering tightly around .7. The data column is nested in that within each row, we’ve saved an entire \\(N = 74\\) row tibble. Most importantly, the fit column contains the brms::brm() objects for each of our 100 simulations. See that last mutate() line, above? That’s where those came from. Within the purrr::map() function, we fed our simulated data sets, one row at a time, into the update() function via the newdata argument. Because we used update() based on our initial fit, we avoided subsequent compilation times and just sampled like a boss. Before we move on, I should give some credit. The foundations of this workflow come from Wickham’s talk, Managing many models with R. I got some additional help on twitter from Phil Straforelli. We still have some work to do. Next, we’ll want to make a custom function that will make it easy to compute the intercept HDIs for each of our fits. library(tidybayes) get_hdi &lt;- function(fit) { fit %&gt;% posterior_samples() %&gt;% mode_hdi(b_Intercept) %&gt;% select(.lower:.upper) } # how does it work? get_hdi(fit13.1) ## .lower .upper ## 1 0.6218496 0.8236595 Now we’ll apply that function to our fits tibble to pull those simulated HDIs. Then we’ll program in the markers for the ROPE and HDI width criteria, perform logical tests to see whether they were passes within each of the 100 simulations, and summarize the tests. sim1 %&gt;% # get those HDIs and `unnest()` mutate(hdi = map(fit, get_hdi)) %&gt;% unnest(hdi) %&gt;% # define our test criteria mutate(rope_ll = .48, rope_ul = .52, hdi_width = .2) %&gt;% mutate(pass_rope = .lower &gt; rope_ul | .upper &lt; rope_ll, pass_width = (.upper - .lower) &lt; hdi_width) %&gt;% # summarize those joints summarise(power_rope = mean(pass_rope), power_width = mean(pass_width)) ## # A tibble: 1 x 2 ## power_rope power_width ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.92 0.35 Those are our power estimates. To compute their HDIS, just increase them by a factor of 100 and plug them into the formulas within the shape arguments in hdi_of_icdf(). # HDIs for the ROPE power estimate hdi_of_icdf(name = qbeta, shape1 = 1 + 92, shape2 = 1 + n_sim - 92) %&gt;% round(digits = 2) ## [1] 0.86 0.96 # HDIs for the width power estimate hdi_of_icdf(name = qbeta, shape1 = 1 + 35, shape2 = 1 + n_sim - 35) %&gt;% round(digits = 2) ## [1] 0.26 0.45 Following the middle of page 375, we’ll want to do the whole thing again with \\(\\kappa = 10\\) and \\(N = 91\\). Before we run the next simulation, notice how our first approach had us saving the model fits within our sim1 object. When the models are simple and based on small data and when you’re only simulating 100 times, this isn’t a huge deal. But saving 1000+ brms::brm() fit objects of hierarchical models will bog you down. So for our next simulation, we’ll only save the HDIs from our get_hdi() function. # how many rows of 0s and 1s should we have in the data? n &lt;- 91 # how many simulations would you like? n_sim &lt;- 100 # specify omega and kappa of the hypothetical parameter distribution omega &lt;- .7 kappa &lt;- 10 # make it reproducible set.seed(13) # simulate sim2 &lt;- tibble(n = n, size = size, theta = rbeta(n_sim, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% mutate(data = pmap(list(n, size, theta), rbinom)) %&gt;% mutate(hdi = map(data, ~update(fit13.1, newdata = list(y = .)) %&gt;% get_hdi())) Since we saved the HDI estimates in the hdi column, we can just unnest() then and summarize our power results. sim2 %&gt;% unnest(hdi) %&gt;% mutate(rope_ll = .48, rope_ul = .52, hdi_width = .2) %&gt;% mutate(pass_rope = .lower &gt; rope_ul | .upper &lt; rope_ll, pass_width = (.upper - .lower) &lt; hdi_width) %&gt;% summarise(power_rope = mean(pass_rope), power_width = mean(pass_width)) ## # A tibble: 1 x 2 ## power_rope power_width ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.71 0.91 Compute the corresponding HDIs. # HDIs for the ROPE power estimate hdi_of_icdf(name = qbeta, shape1 = 1 + 71, shape2 = 1 + n_sim - 71) %&gt;% round(digits = 2) ## [1] 0.62 0.79 # HDIs for the width power estimate hdi_of_icdf(name = qbeta, shape1 = 1 + 91, shape2 = 1 + n_sim - 91) %&gt;% round(digits = 2) ## [1] 0.84 0.96 13.2.5 Power from idealized or actual data. In practice, it is often more intuitive to specify actual or idealized data that express the hypothesis, than it is to specify top-level parameter properties. The idea is that we start with the actual or idealized data and then use Bayes’ rule to generate the corresponding distribution on parameter values. (p. 376, emphasis in the original) Here are the idealized parameters Kruschke outlied on pages 376–377. # specify idealized hypothesis: ideal_group_mean &lt;- 0.65 ideal_group_sd &lt;- 0.07 ideal_n_subj &lt;- 100 # more subjects =&gt; higher confidence in hypothesis ideal_n_trl_per_subj &lt;- 100 # more trials =&gt; higher confidence in hypothesis These parameters are for binomial data. To parameterize \\(\\theta\\) in terms of a mean and standard deviation, we need to define the beta_ab_from_mean_sd() function. beta_ab_from_mean_sd &lt;- function(mean, sd) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) kappa &lt;- mean * (1 - mean) / sd^2 - 1 if (kappa &lt;= 0) stop(&quot;invalid combination of mean and sd&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } Now generate data consistent with these values using a tidyverse-style workflow. b &lt;- beta_ab_from_mean_sd(ideal_group_mean, ideal_group_sd) # make the results reproducible set.seed(13) d &lt;- # make a subject index and generate random theta values for idealized subjects tibble(s = 1:ideal_n_subj, theta = rbeta(ideal_n_subj, b$a, b$b)) %&gt;% # transform the theta values to exactly match idealized mean and SD mutate(theta_transformed = ((theta - mean(theta)) / sd(theta)) * ideal_group_sd + ideal_group_mean) %&gt;% # `theta_transformed` must be between 0 and 1 mutate(theta_transformed = ifelse(theta_transformed &gt;= 0.999, 0.999, ifelse(theta_transformed &lt;= 0.001, 0.001, theta_transformed))) %&gt;% # generate idealized data very close to thetas mutate(z = round(theta_transformed * ideal_n_trl_per_subj)) %&gt;% # create vector of 0&#39;s and 1&#39;s matching the z values generated above mutate(y = map(z, ~c(rep(1, .), rep(0, ideal_n_trl_per_subj - .)))) %&gt;% unnest(y) Our main variables are s and y. You can think of the rest as showing our work. Here’s a peek. str(d) ## tibble [10,000 × 5] (S3: tbl_df/tbl/data.frame) ## $ s : int [1:10000] 1 1 1 1 1 1 1 1 1 1 ... ## $ theta : num [1:10000] 0.604 0.604 0.604 0.604 0.604 ... ## $ theta_transformed: num [1:10000] 0.601 0.601 0.601 0.601 0.601 ... ## $ z : num [1:10000] 60 60 60 60 60 60 60 60 60 60 ... ## $ y : num [1:10000] 1 1 1 1 1 1 1 1 1 1 ... We are going to follow the same procedure we did when we originally fit the model to the therapeutic touch data in Chapter 9. Instead of reproducing the model Kruschke presented in his scripts, we are going to fit a hierarchical logistic regression model. fit13.2 &lt;- brm(data = d, family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 13, file = &quot;fits/fit13.02&quot;) Unlike in the text, we had no need for thinning our chains. The effective number of samples was fine. print(fit13.2) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + (1 | s) ## Data: d (Number of observations: 10000) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~s (Number of levels: 100) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.23 0.03 0.17 0.29 1.00 1604 2206 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.63 0.03 0.57 0.69 1.00 4212 2778 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s a look at our two main parameters, our version of the top panels of Figure 13.3. posterior_samples(fit13.2) %&gt;% pivot_longer(b_Intercept:sd_s__Intercept) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Remember, these are in the log-odds metric.&quot;, x = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) Now we have a distribution of parameter values consistent with our idealized hypothesis, but we did not have to figure out the top-level constants in the model. We merely specified the idealized tendencies in the data and expressed our confidence by its amount… So we now have a large set of representative parameter values for conducting a power analysis. (pp. 378–379) With brms, you can sample from those model-implied parameter values with the fitted() function. By default, it will return values in the probability metric for our logistic model. Here we’ll specify a group-level (i.e., s) value that was not in the data. We’ll feed that new value into the newdata argument and set allow_new_levels = T. We’ll also set summary = F, which will return actual probability values rather than a summary. set.seed(13) f &lt;- fitted(fit13.2, newdata = tibble(s = 0), allow_new_levels = T, summary = F) %&gt;% data.frame() %&gt;% set_names(&quot;theta&quot;) str(f) ## &#39;data.frame&#39;: 4000 obs. of 1 variable: ## $ theta: num 0.672 0.564 0.657 0.643 0.552 ... Here’s what that looks like. f %&gt;% ggplot(aes(x = theta, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 20, slab_size = .2, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Behold our \\&quot;distribution of parameter values consistent\\nwith our idealized hypothesis.\\&quot;&quot;, x = expression(theta)) + xlim(0, 1) + theme(panel.grid = element_blank()) We can make a custom function to sample from \\(\\theta\\). We might call it sample_theta(). sample_theta &lt;- function(seed, n_subj) { set.seed(seed) bind_cols(s = 1:n_subj, sample_n(f, size = n_subj, replace = T)) } # take it for a spin sample_theta(seed = 13, n_subj = 5) ## # A tibble: 5 x 2 ## s theta ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.658 ## 2 2 0.615 ## 3 3 0.693 ## 4 4 0.554 ## 5 5 0.684 Now let’s say I wanted to use our little sample_theta() function to sample \\(\\theta\\) values for three people s and then use those \\(\\theta\\) values to sample three draws from the corresponding Bernoulli distribution. We might do that like this. sample_theta(seed = 13, n_subj = 3) %&gt;% mutate(y = map(theta, rbinom, n = 3, size = 1)) %&gt;% unnest(y) ## # A tibble: 9 x 3 ## s theta y ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 0.658 1 ## 2 1 0.658 0 ## 3 1 0.658 1 ## 4 2 0.615 1 ## 5 2 0.615 0 ## 6 2 0.615 0 ## 7 3 0.693 1 ## 8 3 0.693 1 ## 9 3 0.693 0 Notice how after we sampled from \\(\\theta\\), we still needed to take two more steps to simulate the desired data. So perhaps a better approach would be to wrap all those steps into one function and call it something like sample_data(). sample_data &lt;- function(seed, n_subj, n_trial) { set.seed(seed) bind_cols(s = 1:n_subj, sample_n(f, size = n_subj, replace = T)) %&gt;% mutate(y = map(theta, rbinom, n = n_trial, size = 1)) %&gt;% unnest(y) } # test it out sample_data(seed = 13, n_subj = 3, n_trial = 3) ## # A tibble: 9 x 3 ## s theta y ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 0.658 1 ## 2 1 0.658 0 ## 3 1 0.658 1 ## 4 2 0.615 1 ## 5 2 0.615 0 ## 6 2 0.615 0 ## 7 3 0.693 1 ## 8 3 0.693 1 ## 9 3 0.693 0 Here’s how we’d use our sample_data() function to make several data sets within the context of a nested tibble. tibble(seed = 1:4) %&gt;% mutate(data = map(seed, sample_data, n_subj = 14, n_trial = 47)) ## # A tibble: 4 x 2 ## seed data ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;tibble [658 × 3]&gt; ## 2 2 &lt;tibble [658 × 3]&gt; ## 3 3 &lt;tibble [658 × 3]&gt; ## 4 4 &lt;tibble [658 × 3]&gt; With this data type, Kruschke indicated he ran the power analysis twice, using different selections of subjects and trials. In both cases there [was] a total of 658 trials, but in the first case there [were] 14 subjects with 47 trials per subject, and in the second case there [were] seven subjects with 94 trials per subject. (p. 381) Before running the simulations in full, we fit the model once and save that fit to iteratively reuse with update(). # how many subjects should we have? n_subj &lt;- 14 # how many trials should we have? n &lt;- 47 # fit that joint fit13.3 &lt;- brm(data = sample_theta(seed = 13, n_subj = 14) %&gt;% mutate(y = map(theta, rbinom, n = n, size = 1)) %&gt;% unnest(y), family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 13, control = list(adapt_delta = .9), file = &quot;fits/fit13.03&quot;) Check real quick to make sure the fit turned out okay. print(fit13.3) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + (1 | s) ## Data: sample_theta(seed = 13, n_subj = 14) %&gt;% mutate(y (Number of observations: 658) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~s (Number of levels: 14) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.16 0.11 0.01 0.42 1.00 1427 1381 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.73 0.10 0.55 0.93 1.00 3403 2876 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Looks fine. Our new model simulation carries with it some new goals. In this example, [Kruschke] considered goals for achieving precision and exceeding a ROPE around the null value, at both the group level and individual level. For the group level, the goals are for the 95% HDI on the group mode, \\(\\omega\\), to fall above the ROPE around the null value, and for the width of the HDI to be less than 0.2. For the individual level, the goals are for at least one of the \\(\\theta_s\\)s 95% HDIs to exceed the ROPE with none that fall below the ROPE, and for all the \\(\\theta_s\\)s 95% HDIs to have widths less than 0.2. (pp. 379–380) Now since we used an aggregated binomial model, we don’t have a population-level \\(\\omega\\) parameter. Rather, we have a population \\(\\theta\\). So like before, our first goal is for the population \\(\\theta\\) to fall above the range \\([.48, .52]\\). The second corresponding width goal is also like before; we want \\(\\theta\\) to have a width of less than 0.2. But since our aggregated binomial model parameterized \\(\\theta\\) in the log-odds metric, we’ll have to update our get_hdi() function, which we’ll strategically rename get_theta_hdi(). get_theta_hdi &lt;- function(fit) { fit %&gt;% posterior_samples() %&gt;% transmute(theta = inv_logit_scaled(b_Intercept)) %&gt;% mode_hdi() %&gt;% select(.lower:.upper) } # how does it work? get_theta_hdi(fit13.3) ## .lower .upper ## 1 0.6315379 0.7144742 As for the individual-level goals, the two Kruschke outlined in the text apply to our model in a straightforward way. But we will need one more custom function designed to pull the \\(\\theta_s\\)s for the \\(\\theta_s\\)s. Let’s call this one get_theta_s_hdi(). get_theta_s_hdi &lt;- function(fit) { n_col &lt;- coef(fit, summary = F)$s[, , &quot;Intercept&quot;] %&gt;% ncol() coef(fit, summary = F)$s[, , &quot;Intercept&quot;] %&gt;% data.frame() %&gt;% set_names(1:n_col) %&gt;% mutate_all(inv_logit_scaled) %&gt;% gather(s, value) %&gt;% mutate(s = as.numeric(s)) %&gt;% group_by(s) %&gt;% mode_hdi(value) %&gt;% select(s, .lower:.upper) %&gt;% rename(.lower_s = .lower, .upper_s = .upper) } # how does it work? get_theta_s_hdi(fit13.3) ## # A tibble: 14 x 3 ## s .lower_s .upper_s ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.622 0.776 ## 2 2 0.599 0.750 ## 3 3 0.606 0.753 ## 4 4 0.510 0.715 ## 5 5 0.606 0.756 ## 6 6 0.610 0.765 ## 7 7 0.591 0.741 ## 8 8 0.611 0.754 ## 9 9 0.577 0.730 ## 10 10 0.605 0.753 ## 11 11 0.603 0.754 ## 12 12 0.584 0.739 ## 13 13 0.617 0.766 ## 14 14 0.614 0.757 With sim2, we avoided saving our model brms::brm() fit objects by using map(data, ~update(fit1, newdata = list(y = .)) %&gt;% get_hdi()). That is, within the purrr::map() function, we first used update() to update the fit to the new data and then pumped that directly into get_hdi(), which simply returned our intervals. Though slick, this approach won’t work here because we want to pump our updated model fit into two functions, both get_theta_hdi() and get_theta_s_hdi(). Our work-around will be to make a custom function that updates the fit, saves it as an object, inserts that fit object into both get_theta_hdi() and get_theta_s_hdi(), binds their results together, and the only returns the intervals. We’ll call this function fit_then_hdis(). fit_then_hdis &lt;- function(data, seed) { fit &lt;- update(fit13.3, newdata = data, seed = seed) cbind(get_theta_hdi(fit), get_theta_s_hdi(fit)) } Now we’re ready to simulate. # how many subjects should we have? n_subj &lt;- 14 # how many trials should we have? n_trial &lt;- 47 # how many simulations would you like? n_sim &lt;- 100 sim3 &lt;- tibble(seed = 1:n_sim) %&gt;% mutate(data = map(seed, sample_data, n_subj = n_subj, n_trial = n_trial)) %&gt;% mutate(hdi = map2(data, seed, fit_then_hdis)) If we hold these by the criteria of each \\(\\text{HDI}_{\\theta_s} &gt; \\text{ROPE}\\) and all to have widths less than 0.2, It looks like our initial data-generating fit13.3 is in the ballpark. Here are the results for the full power analysis, sim3. sim3 &lt;- sim3 %&gt;% unnest(hdi) %&gt;% # here we determine whether we passed at the group level mutate(pass_rope_theta = .lower &gt; .52 | .upper &lt; .48, pass_width_theta = (.upper - .lower) &lt; .2) %&gt;% # the s-level thetas require two steps. # first, we&#39;ll outline the three criteria mutate(exceed_rope_theta_s = .lower_s &gt; .52, below_rope_theta_s = .upper_s &lt; .48, narrow_width_theta_s = (.upper_s - .lower_s) &lt; .2) %&gt;% # second, we&#39;ll evaluate those criteria by group group_by(seed) %&gt;% mutate(pass_rope_theta_s = sum(exceed_rope_theta_s) &gt; 0 &amp; sum(below_rope_theta_s) == 0, pass_width_theta_s = mean(narrow_width_theta_s) == 1) %&gt;% ungroup() head(sim3) head(sim3) ## # A tibble: 6 x 14 ## seed data .lower .upper s .lower_s .upper_s pass_rope_theta ## &lt;int&gt; &lt;lis&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 1 &lt;tib… 0.617 0.709 1 0.616 0.784 TRUE ## 2 1 &lt;tib… 0.617 0.709 2 0.519 0.707 TRUE ## 3 1 &lt;tib… 0.617 0.709 3 0.605 0.767 TRUE ## 4 1 &lt;tib… 0.617 0.709 4 0.604 0.768 TRUE ## 5 1 &lt;tib… 0.617 0.709 5 0.593 0.752 TRUE ## 6 1 &lt;tib… 0.617 0.709 6 0.621 0.795 TRUE ## # … with 6 more variables: pass_width_theta &lt;lgl&gt;, exceed_rope_theta_s &lt;lgl&gt;, ## # below_rope_theta_s &lt;lgl&gt;, narrow_width_theta_s &lt;lgl&gt;, ## # pass_rope_theta_s &lt;lgl&gt;, pass_width_theta_s &lt;lgl&gt; Summarize the results. sim3 %&gt;% summarise(power_rope_theta = mean(pass_rope_theta), power_width_theta = mean(pass_width_theta)) ## # A tibble: 1 x 2 ## power_rope_theta power_width_theta ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 sim3 %&gt;% summarise(power_rope_theta_s = mean(pass_rope_theta_s), power_width_theta_s = mean(pass_width_theta_s)) ## # A tibble: 1 x 2 ## power_rope_theta_s power_width_theta_s ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.472 The power estimates for power_rope_theta, power_width_theta, and power_rope_theta_s were all the same, 1. Only the estimate for power_width_theta_s was unique. Here are the two sets of HDIs for the power estimate values. hdi_of_icdf(name = qbeta, shape1 = 1 + 100, shape2 = 1 + n_sim - 100) %&gt;% round(digits = 2) ## [1] 0.97 1.00 hdi_of_icdf(name = qbeta, shape1 = 1 + 47, shape2 = 1 + n_sim - 47) %&gt;% round(digits = 2) ## [1] 0.37 0.57 Hopefully it isn’t a surprise our values differ from those in the text. We (a) used a different model and (b) used fewer simulation iterations. But I trust you get the overall idea. Like in the text, let’s do the simulation again. # how many subjects should we have? n_subj &lt;- 7 # how many trials should we have? n_trial &lt;- 94 # how many simulations would you like? n_sim &lt;- 100 sim4 &lt;- tibble(seed = 1:n_sim) %&gt;% mutate(data = map(seed, sample_data, n_subj = n_subj, n_trial = n_trial)) %&gt;% mutate(hdi = map2(data, seed, fit_then_hdis)) Wrangle before summarizing. sim4 &lt;- sim4 %&gt;% unnest(hdi) %&gt;% mutate(pass_rope_theta = .lower &gt; .52 | .upper &lt; .48, pass_width_theta = (.upper - .lower) &lt; .2) %&gt;% mutate(exceed_rope_theta_s = .lower_s &gt; .52, below_rope_theta_s = .upper_s &lt; .48, narrow_width_theta_s = (.upper_s - .lower_s) &lt; .2) %&gt;% group_by(seed) %&gt;% mutate(pass_rope_theta_s = sum(exceed_rope_theta_s) &gt; 0 &amp; sum(below_rope_theta_s) == 0, pass_width_theta_s = mean(narrow_width_theta_s) == 1) %&gt;% ungroup() Summarize the results. sim4 %&gt;% summarise(power_rope_theta = mean(pass_rope_theta), power_width_theta = mean(pass_width_theta)) ## # A tibble: 1 x 2 ## power_rope_theta power_width_theta ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.990 0.940 sim4 %&gt;% summarise(power_rope_theta_s = mean(pass_rope_theta_s), power_width_theta_s = mean(pass_width_theta_s)) ## # A tibble: 1 x 2 ## power_rope_theta_s power_width_theta_s ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.930 Now compute the HDIs for power_rope_theta and power_width_theta. hdi_of_icdf(name = qbeta, shape1 = 1 + 99, shape2 = 1 + n_sim - 99) %&gt;% round(digits = 2) ## [1] 0.95 1.00 hdi_of_icdf(name = qbeta, shape1 = 1 + 94, shape2 = 1 + n_sim - 94) %&gt;% round(digits = 2) ## [1] 0.88 0.98 Second, we now compute the HDIs for power_rope_theta_s and power_width_theta_s. hdi_of_icdf(name = qbeta, shape1 = 1 + 1, shape2 = 1 + n_sim - 1) %&gt;% round(digits = 2) ## [1] 0.00 0.05 hdi_of_icdf(name = qbeta, shape1 = 1 + 93, shape2 = 1 + n_sim - 93) %&gt;% round(digits = 2) ## [1] 0.87 0.97 The results from our simulations contrast with those in the text. Though the results are similar with respect to \\(\\theta_s\\), they are markedly different with regards to our \\(\\theta\\) versus the text’s \\(\\omega\\). But Kruschke’s point is still sound: This example illustrates a general trend in hierarchical estimates. If you want high precision at the individual level, you need lots of data within individuals. If you want high precision at the group level, you need lots of individuals (without necessarily lots of data per individual, but more is better). (p. 382) ideal_group_mean &lt;- 0.65 ideal_group_sd &lt;- 0.07 ideal_n_subj &lt;- 10 # instead of 100 ideal_n_trl_per_subj &lt;- 10 # instead of 100 b &lt;- beta_ab_from_mean_sd(ideal_group_mean, ideal_group_sd) set.seed(13) d &lt;- tibble(s = 1:ideal_n_subj, theta = rbeta(ideal_n_subj, b$a, b$b)) %&gt;% mutate(theta_transformed = ((theta - mean(theta)) / sd(theta)) * ideal_group_sd + ideal_group_mean) %&gt;% mutate(theta_transformed = ifelse(theta_transformed &gt;= 0.999, 0.999, ifelse(theta_transformed &lt;= 0.001, 0.001, theta_transformed))) %&gt;% mutate(z = round(theta_transformed * ideal_n_trl_per_subj)) %&gt;% mutate(y = map(z, ~c(rep(1, .), rep(0, ideal_n_trl_per_subj - .)))) %&gt;% unnest(y) head(d) ## # A tibble: 6 x 5 ## s theta theta_transformed z y ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.604 0.659 7 1 ## 2 1 0.604 0.659 7 1 ## 3 1 0.604 0.659 7 1 ## 4 1 0.604 0.659 7 1 ## 5 1 0.604 0.659 7 1 ## 6 1 0.604 0.659 7 1 Fit the \\(\\theta\\)-generating model. fit13.4 &lt;- update(fit13.2, newdata = d, cores = 4, seed = 13, file = &quot;fits/fit13.04&quot;) Check to make sure things look alright. print(fit13.4) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + (1 | s) ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~s (Number of levels: 10) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.23 0.19 0.01 0.72 1.00 2262 1562 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.62 0.23 0.17 1.07 1.00 3776 2600 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s a look at our two main parameters, our version of the bottom panels of Figure 13.3. posterior_samples(fit13.4) %&gt;% pivot_longer(b_Intercept:sd_s__Intercept) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Remember, these are in the log-odds metric.&quot;, x = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) Now redefine our fitted() object, f, which gets pumped into the sample_data() function. set.seed(13) f &lt;- fitted(fit13.4, newdata = tibble(s = 0), allow_new_levels = T, summary = F) %&gt;% data.frame() %&gt;% set_names(&quot;theta&quot;) Here’s what our updated distribution of \\(\\theta\\) values looks like. f %&gt;% ggplot(aes(x = theta, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Behold our \\&quot;distribution of parameter values consistent\\nwith our idealized hypothesis.\\&quot;&quot;, x = expression(theta)) + coord_cartesian(xlim = c(0, 1)) + theme(panel.grid = element_blank()) Note the distribution is wider than the previous one. Anyway, now we’re good to go. Here’s our version of the first power analysis. # how many subjects should we have? n_subj &lt;- 14 # how many trials should we have? n_trial &lt;- 47 # how many simulations would you like? n_sim &lt;- 100 sim5 &lt;- tibble(seed = 1:n_sim) %&gt;% mutate(data = map(seed, sample_data, n_subj = n_subj, n_trial = n_trial)) %&gt;% mutate(hdi = map2(data, seed, fit_then_hdis)) Wrangle before summarizing. sim5 &lt;- sim5 %&gt;% unnest(hdi) %&gt;% mutate(pass_rope_theta = .lower &gt; .52 | .upper &lt; .48, pass_width_theta = (.upper - .lower) &lt; .2) %&gt;% mutate(exceed_rope_theta_s = .lower_s &gt; .52, below_rope_theta_s = .upper_s &lt; .48, narrow_width_theta_s = (.upper_s - .lower_s) &lt; .2) %&gt;% group_by(seed) %&gt;% mutate(pass_rope_theta_s = sum(exceed_rope_theta_s) &gt; 0 &amp; sum(below_rope_theta_s) == 0, pass_width_theta_s = mean(narrow_width_theta_s) == 1) %&gt;% ungroup() Summarize the results. sim5 %&gt;% summarise(power_rope_theta = mean(pass_rope_theta), power_width_theta = mean(pass_width_theta)) ## # A tibble: 1 x 2 ## power_rope_theta power_width_theta ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 sim5 %&gt;% summarise(power_rope_theta_s = mean(pass_rope_theta_s), power_width_theta_s = mean(pass_width_theta_s)) ## # A tibble: 1 x 2 ## power_rope_theta_s power_width_theta_s ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.27 Here are the two interval types. hdi_of_icdf(name = qbeta, shape1 = 1 + 100, shape2 = 1 + n_sim - 100) %&gt;% round(digits = 2) ## [1] 0.97 1.00 hdi_of_icdf(name = qbeta, shape1 = 1 + 27, shape2 = 1 + n_sim - 27) %&gt;% round(digits = 2) ## [1] 0.19 0.36 The classical definition of power in NHST assumes a specific value for the parameters without any uncertainty. The classical approach can compute power for different specific parameter values, but the approach does not weigh the different values by their credibility. One consequence is that for the classical approach, retrospective power is extremely uncertain, rendering it virtually useless, because the estimated powers at the two ends of the confidence interval are close to the baseline false alarm rate and 100% (Gerard, Smith, &amp; Weerakkody, 2009; Nakagawa &amp; Foster, 2004; O’Keefe, 2007; Steidl, Hayes, &amp; Schauber, 1997; Sun, Pan, &amp; Wang, 2011; L. Thomas, 1997). (p. 383) 13.3 Sequential testing and the goal of precision In classical power analysis, it is assumed that the goal is to reject the null hypothesis. For many researchers, the sine qua non of research is to reject the null hypothesis. The practice of NHST is so deeply institutionalized in scientific journals that it is difficult to get research findings published without showing “significant” results, in the sense of \\(p &lt; 0.05\\). As a consequence, many researchers will monitor data as they are being collected and stop collecting data only when \\(p &lt; 0.05\\) (conditionalizing on the current sample size) or when their patience runs out. This practice seems intuitively not to be problematic because the data collected after testing previous data are not affected by the previously collected data. For example, if I flip a coin repeatedly, the probability of heads on the next flip is not affected by whether or not I happened to check whether \\(p &lt; 0.05\\) on the previous flip. Unfortunately, that intuition about independence across flips only tells part of story. What’s missing is the realization that the stopping procedure biases which data are sampled, because the procedure stops only when extreme values happen to be randomly sampled… The remainder of this section shows examples of sequential testing with different decision criteria. We consider decisions by \\(p\\) values, BFs, HDIs with ROPEs, and precision. We will see that decisions by \\(p\\) values not only lead to 100% false alarms (with infinite patience), but also lead to biased estimates that are more extreme than the true value. The two Bayesian methods both can decide to accept the null hypothesis, and therefore do not lead to 100% false alarms, but both do produce biased estimates because they stop when extreme values are sampled. Stopping when precision is achieved produces accurate estimates. (pp. 383–385, emphasis in the original) 13.3.1 Examples ofsequential tests. To start our sequence of simulated coin flips, we’ll set the total number of trials we’d like, n_trial, specify our bias, and set out seed. Like Kruschke did in the text we’ll do bias &lt;- .5, first. n_trial &lt;- 700 bias &lt;- .5 set.seed(13) coin.5 &lt;- tibble(n = 1:n_trial, flip = rbinom(n = n_trial, size = 1, prob = bias)) %&gt;% mutate(z = cumsum(flip)) head(coin.5) ## # A tibble: 6 x 3 ## n flip z ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 ## 2 2 0 1 ## 3 3 0 1 ## 4 4 0 1 ## 5 5 1 2 ## 6 6 0 2 Here’s a little custom function that will fit frequentist logistic regression models for each combination of n and z and then extract the associated \\(p\\)-value. fit_glm &lt;- function(n, z) { d &lt;- tibble(y = rep(1:0, times = c(z, n - z))) glm(data = d, y ~ 1, family = binomial(link = &quot;logit&quot;)) %&gt;% broom::tidy() %&gt;% select(p.value) %&gt;% pull() } # here&#39;s how it works fit_glm(n = 5, z = 2) ## [1] 0.6569235 Use fit_glm() to compute the \\(p\\)-values. coin.5 &lt;- coin.5 %&gt;% mutate(p = map2_dbl(n, z, fit_glm)) head(coin.5) ## # A tibble: 6 x 4 ## n flip z p ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1 1.00 ## 2 2 0 1 1 ## 3 3 0 1 0.571 ## 4 4 0 1 0.341 ## 5 5 1 2 0.657 ## 6 6 0 2 0.423 For the Bayes factors, Kruschke indicated these were computed based on Equation 12.3 from page 344. That equation followed the form \\[ \\frac{p(z, N | M_\\text{alt})}{p(z, N | M_\\text{null})} = \\frac{B (z + a_\\text{alt}, N - z + b_\\text{alt}) / B (a_\\text{alt}, b_\\text{alt})}{\\theta_\\text{null}^z (1 - \\theta_\\text{null})^{(N - z)}}. \\] To ground ourselves a bit, here’s some of the content from that page that followed the equation. For a default alternative prior, the beta distribution is supposed to be uninformed, according to particular mathematical criteria. Intuition might suggest that a uniform distribution suits this requirement, that is, \\(\\operatorname{beta} (\\theta | 1, 1)\\). Instead, some argue that the most appropriate uninformed beta distribution is \\(\\operatorname{beta} (\\theta | \\epsilon, \\epsilon)\\), where \\(\\epsilon\\) is a small number approaching zero (p. 344) The \\(\\operatorname{beta} (\\epsilon, \\epsilon)\\), recall, is the Haldane prior. Often times, \\(\\epsilon = 0.01\\). That will be our approach here, too. Let’s make another custom function. log_bf &lt;- function(n, z, theta) { # define epsilon for the Haldane prior e &lt;- 0.01 # compute p(d | H_0) p_d_null &lt;- theta ^ z * (1 - theta) ^ (n - z) # compute p(d | H_1) p_d_alt &lt;- beta(z + e, n - z + e) / beta(e, e) # compute BF bf &lt;- p_d_alt / p_d_null # take the log log(bf) } Here’s how it works. log_bf(n = 6, z = 2, theta = bias) ## [1] -4.152328 Now we’ll use it in bulk. coin.5 &lt;- coin.5 %&gt;% mutate(log_bf = map2_dbl(n, z, log_bf, theta = .5)) head(coin.5) ## # A tibble: 6 x 5 ## n flip z p log_bf ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1.00 0 ## 2 2 0 1 1 -3.93 ## 3 3 0 1 0.571 -3.93 ## 4 4 0 1 0.341 -3.65 ## 5 5 1 2 0.657 -4.33 ## 6 6 0 2 0.423 -4.15 To compute the HDIs for each iteration, we’ll want to use the hdi_of_qbeta() function from Chapters 10 and 12. hdi_of_qbeta &lt;- function(shape1, shape2) { hdi_of_icdf(name = qbeta, shape1 = shape1, shape2 = shape2) %&gt;% data.frame() %&gt;% mutate(level = c(&quot;ll&quot;, &quot;ul&quot;)) %&gt;% spread(key = level, value = &quot;.&quot;) } Here’s how it works. hdi_of_qbeta(3, 3) ## ll ul ## 1 0.1466328 0.8533672 Put it to use. coin.5 &lt;- coin.5 %&gt;% mutate(hdi = map2(n, z, ~hdi_of_qbeta(.y + 1, .x - .y + 1))) %&gt;% unnest(hdi) %&gt;% mutate(width = ul - ll) head(coin.5) ## # A tibble: 6 x 8 ## n flip z p log_bf ll ul width ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1.00 0 0.224 1.00 0.776 ## 2 2 0 1 1 -3.93 0.0943 0.906 0.811 ## 3 3 0 1 0.571 -3.93 0.0438 0.772 0.729 ## 4 4 0 1 0.341 -3.65 0.0260 0.670 0.644 ## 5 5 1 2 0.657 -4.33 0.105 0.761 0.656 ## 6 6 0 2 0.423 -4.15 0.0805 0.685 0.604 We’re finally ready to define the five subplots for our version of Figure 13.4. p1 &lt;- coin.5 %&gt;% ggplot(aes(x = n, y = z / n)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_line() + geom_point(size = 2/3) + scale_x_continuous(NULL, breaks = 0:7 * 100) + coord_cartesian(ylim = c(0, 1)) p2 &lt;- coin.5 %&gt;% ggplot(aes(x = n, y = p)) + geom_hline(yintercept = .05, color = &quot;white&quot;) + geom_line() + geom_point(size = 2/3) + scale_x_continuous(NULL, breaks = 0:7 * 100) + scale_y_continuous(expression(italic(p)*&quot;-value&quot;), limits = c(0, 1)) p3 &lt;- coin.5 %&gt;% ggplot(aes(x = n, y = log_bf)) + geom_hline(yintercept = -1.1, color = &quot;white&quot;) + geom_line() + geom_point(aes(color = log_bf &lt; -1.1 | log_bf &gt; 1.1), alpha = 1/2, size = 2/3, show.legend = F) + annotate(geom = &quot;text&quot;, x = 60, y = -1.5, label = &quot;accept the null&quot;, color = &quot;grey50&quot;) + scale_color_viridis_d(option = &quot;D&quot;, end = .75) + scale_x_continuous(NULL, breaks = 0:7 * 100) + ylab(&quot;log(BF)&quot;) p4 &lt;- coin.5 %&gt;% ggplot(aes(x = n)) + geom_hline(yintercept = c(.45, .55), color = &quot;white&quot;) + geom_linerange(aes(ymin = ll, ymax = ul, color = ll &gt; .45 &amp; ul &lt; .55), alpha = 1/2, show.legend = F) + scale_color_viridis_d(option = &quot;D&quot;, end = .75) + scale_x_continuous(NULL, breaks = 0:7 * 100) + ylab(&quot;95% HDI&quot;) + coord_cartesian(ylim = c(0, 1)) p5 &lt;- coin.5 %&gt;% ggplot(aes(x = n, y = width)) + geom_hline(yintercept = .08, color = &quot;white&quot;) + geom_line() + geom_point(aes(color = width &lt; .08), alpha = 1/2, size = 2/3, show.legend = F) + scale_color_viridis_d(option = &quot;D&quot;, end = .75) + scale_x_continuous(NULL, breaks = 0:7 * 100) + ylab(&quot;HDI width&quot;) + coord_cartesian(ylim = c(0, 1)) With syntax from the patchwork package, we’ll arrange them one atop another. library(patchwork) (p1 / p2 / p3 / p4 / p5) &amp; theme(panel.grid = element_blank()) Now let’s compute data of the same form, but based on \\(\\theta = .65\\). This time we’ll do all the data wrangling steps in one code block. n_trial &lt;- 700 bias &lt;- .65 set.seed(13) coin.65 &lt;- # n, flip, and z tibble(n = 1:n_trial, flip = rbinom(n = n_trial, size = 1, prob = bias)) %&gt;% mutate(z = cumsum(flip)) %&gt;% # p-values mutate(p = map2_dbl(n, z, fit_glm)) %&gt;% # log(BF) mutate(log_bf = map2_dbl(n, z, log_bf, theta = .5)) %&gt;% # HDIs mutate(hdi = map2(n, z, ~hdi_of_qbeta(.y + 1, .x - .y + 1))) %&gt;% unnest(hdi) %&gt;% # HDI width mutate(width = ul - ll) head(coin.65) ## # A tibble: 6 x 8 ## n flip z p log_bf ll ul width ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 1.00 2.22e-16 0.00000000317 0.776 0.776 ## 2 2 1 1 1 -3.93e+ 0 0.0943 0.906 0.811 ## 3 3 1 2 0.571 -3.93e+ 0 0.228 0.956 0.729 ## 4 4 1 3 0.341 -3.65e+ 0 0.330 0.974 0.644 ## 5 5 0 3 0.657 -4.33e+ 0 0.239 0.895 0.656 ## 6 6 1 4 0.423 -4.15e+ 0 0.315 0.919 0.604 Here is the code for our version of Figure 13.5. p1 &lt;- coin.65 %&gt;% ggplot(aes(x = n, y = z / n)) + geom_hline(yintercept = .65, color = &quot;white&quot;) + geom_line() + geom_point(size = 2/3) + coord_cartesian(ylim = c(0, 1)) p2 &lt;- coin.65 %&gt;% ggplot(aes(x = n, y = p)) + geom_hline(yintercept = .05, color = &quot;white&quot;) + geom_line() + geom_point(aes(color = p &lt; .05), alpha = 1/2, size = 2/3, show.legend = F) + scale_color_viridis_d(option = &quot;D&quot;, end = .75) + scale_y_continuous(expression(italic(p)*&quot;-value&quot;), limits = c(0, 1)) p3 &lt;- coin.65 %&gt;% ggplot(aes(x = n, y = log_bf)) + geom_hline(yintercept = c(-1.1, 1.1), color = &quot;white&quot;) + geom_line() + geom_point(aes(color = log_bf &lt; -1.1 | log_bf &gt; 1.1), alpha = 1/2, size = 2/3, show.legend = F) + annotate(geom = &quot;text&quot;, x = 60, y = c(-8, 28), label = c(&quot;accept the null&quot;, &quot;reject the null&quot;), color = &quot;grey50&quot;) + scale_color_viridis_d(option = &quot;D&quot;, end = .75) + scale_y_continuous(&quot;log(BF)&quot;, limits = c(-10, 30)) p4 &lt;- coin.65 %&gt;% ggplot(aes(x = n)) + geom_hline(yintercept = c(.45, .55), color = &quot;white&quot;) + geom_linerange(aes(ymin = ll, ymax = ul, color = ll &gt; .55 | ul &lt; .45), alpha = 1/2, show.legend = F) + scale_color_viridis_d(option = &quot;D&quot;, end = .75) + scale_y_continuous(&quot;95% HDI&quot;, limits = c(0, 1)) p5 &lt;- coin.65 %&gt;% ggplot(aes(x = n, y = width)) + geom_hline(yintercept = .08, color = &quot;white&quot;) + geom_line() + geom_point(aes(color = width &lt; .08), alpha = 1/2, size = 2/3, show.legend = F) + scale_color_viridis_d(option = &quot;D&quot;, end = .75) + scale_y_continuous(&quot;HDI width&quot;, limits = c(0, 1)) (p1 / p2 / p3 / p4 / p5) &amp; scale_x_continuous(NULL, breaks = 0:7 * 100) &amp; theme(panel.grid = element_blank()) 13.3.2 Average behavior of sequential tests. This section is still in the works. In short, I’m not quite sure how to pull off the simulations. If you’ve got the chops, please share your code in my GitHub issue #20. 13.4 Discussion 13.4.1 Power and multiple comparisons. In NHST, the overall p value for any particular test is increased when the test is considered in the space of all other intended tests… Bayesian power analysis is not affected by intending multiple tests. In Bayesian analysis, the decision is based on the posterior distribution, which is determined by the data in hand, whether actual or simulated, and not by what other tests are intended. In Bayesian analysis, the probability of achieving a goal, that is the power, is determined only by the data-generating process (which includes the stopping rule) and not by the cloud of counterfactual samples (which includes other tests). (p. 393) For more thoughts on the multiple comparisons issue, check out this post from Gelman’s blog. 13.4.2 Power: prospective, retrospective, and replication. There are different types of power analysis, depending on the source of the hypothetical distribution over parameter values and the prior used for analyzing the simulated data. The most typical and useful type is prospective power analysis. In prospective power analysis, research is being planned for which there has not yet been any data collected. The hypothetical distribution over parameter values comes from either theory or idealized data or actual data from related research. (p. 393, emphasis in the original) Prospective is probably the type that comes to mind with you think of “power analyses.” On the other hand, retrospective power analysis refers to a situation in which we have already collected data from a research project, and we want to determine the power of the research we conducted. In this case, we can use the posterior distribution, derived from the actual data, as the representative parameter values for generating new simulated data. (This is tantamount to a posterior predictive check.) In other words, at a step in the posterior MCMC chain, the parameter values are used to generate simulated data. The simulated data are then analyzed with the same Bayesian model as the actual data, and the posterior from the simulated data is examined for whether or not the goals are achieved. (p. 393, emphasis in the original) Though researchers sometimes use retrospective power analyses and are sometimes asked to perform them during the peer-review process, they are generally looked down upon within methodological circles. A recent controversy arose on the issue within the surgical literature. To dip your toes into the topic, check out this post by Reaction Watch or this post by Zad Chow or these two (here, here) posts from Gelman’s blog. Finally, suppose that we have already collected some data, and we want to know the probability that we would achieve our goal if we exactly replicated the experiment. In other words, if we were simply to collect a new batch of data, what is the probability that we would achieve our goal in the replicated study, also taking into account the results of the first set of data? This is the replication power. As with retrospective power analysis, we use the actual posterior derived from the first sample of data as the data generator. But for analysis of the simulated data, we again use the actual posterior from first sample of data, because that is the best-informed prior for the follow-up experiment. An easy way to execute this analysis by MCMC is as follows: Use the actual set of data with a skeptical- audience prior to generate representative parameter values and representative simulated data. Then, concatenate the original data with the novel simulated data and update the original skeptical-audience prior with the enlarged data set. This technique is tantamount to using the posterior of the original data set as the prior for the novel simulated data. (p. 394, emphasis in the original) 13.4.3 Power analysis requires verisimilitude of simulated data. Power analysis is only useful when the simulated data imitate actual data. We generate simulated data from a descriptive model that has uncertainty in its parameter values, but we assume that the model is a reasonably good description of the actual data. If the model is instead a poor description of the actual data, then the simulated data do not imitate actual data, and inferences from the simulated data are not very meaningful. It is advisable, therefore, to check that the simulated data accurately reflect the actual data. (p. 394) 13.4.4 The importance of planning. Conducting a power analysis in advance of collecting data is very important and valuable. Often in real research, a fascinating theory and clever experimental manipulation imply a subtle effect. It can come as a shock to the researcher when power analysis reveals that detecting the subtle effect would take many hundreds of subjects! But the shock of power analysis is far less than the pain of actually running dozens of subjects and finding highly uncertain estimates of the sought-after effect. (p. 395) For more handy uses of power analyses, keep reading in the text. For more practice with simulation approaches to Bayesian power analyses with brms, check out my blog series on the topic. You might start with the first post, Bayesian power analysis: Part I. Prepare to reject \\(H_0\\) with simulation. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.0.0 tidybayes_2.0.3.9000 brms_2.12.0 ## [4] Rcpp_1.0.4.6 forcats_0.5.0 stringr_1.4.0 ## [7] dplyr_0.8.5 purrr_0.3.4 readr_1.3.1 ## [10] tidyr_1.0.2 tibble_3.0.1 ggplot2_3.3.0 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.4.1 rstudioapi_0.11 farver_2.0.3 ## [10] rstan_2.19.3 svUnit_1.0.3 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 ## [16] xml2_1.3.1 bridgesampling_1.0-0 knitr_1.28 ## [19] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 ## [22] broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 ## [25] compiler_3.6.3 httr_1.4.1 backports_1.1.6 ## [28] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] cli_2.0.2 later_1.0.0 htmltools_0.4.0 ## [34] prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.4.0 ## [40] reshape2_1.4.4 cellranger_1.1.0 vctrs_0.3.0 ## [43] nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ## [46] ps_1.3.3 rvest_0.3.5 mime_0.9 ## [49] miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 ## [52] zoo_1.8-7 scales_1.1.1 colourpicker_1.0 ## [55] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 ## [58] parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 StanHeaders_2.21.0-1 ## [64] loo_2.2.0 stringi_1.4.6 highr_0.8 ## [67] dygraphs_1.1.1.6 pkgbuild_1.0.8 rlang_0.4.6 ## [70] pkgconfig_2.0.3 matrixStats_0.56.0 HDInterval_0.2.0 ## [73] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [76] htmlwidgets_1.5.1 labeling_0.3 tidyselect_1.0.0 ## [79] processx_3.4.2 plyr_1.8.6 magrittr_1.5 ## [82] bookdown_0.18 R6_2.4.1 generics_0.0.2 ## [85] DBI_1.1.0 pillar_1.4.4 haven_2.2.0 ## [88] withr_2.2.0 xts_0.12-0 abind_1.4-5 ## [91] modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 ## [94] utf8_1.1.4 rmarkdown_2.1 grid_3.6.3 ## [97] readxl_1.3.1 callr_3.4.3 threejs_0.3.3 ## [100] reprex_0.3.0 digest_0.6.25 xtable_1.8-4 ## [103] httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 ## [106] viridisLite_0.3.0 shinyjs_1.1 References "],
["stan.html", "14 Stan 14.1 HMC sampling 14.2 Installing Stan 14.3 A Complete example 14.4 Specify models top-down in Stan 14.5 Limitations and extras Session info", " 14 Stan Stan is the name of a software package that creates representative samples of parameter values from a posterior distribution for complex hierarchical models, analogous to JAGS… According to the Stan reference manual, Stan is named after Stanislaw Ulam (1909–1984), who was a pioneer of Monte Carlo methods. (Stan is not named after the slang term referring to an overenthusiastic or psychotic fanatic, formed by a combination of the words “stalker” and “fan.”) The name of the software package has also been unpacked as the acronym, Sampling Through Adaptive Neighborhoods (Gelman et al., 2013, p. 307), but it is usually written as Stan not STAN. Stan uses a different method than JAGS for generating Monte Carlo steps. The method is called Hamiltonian Monte Carlo (HMC). HMC can be more effective than the various samplers in JAGS and BUGS, especially for large complex models. Moreover, Stan operates with compiled C++ and allows greater programming flexibility, which again is especially useful for unusual or complex models. For large data sets or complex models, Stan can provide solutions when JAGS (or BUGS) takes too long or fails. (pp. 399–400, emphasis in the original) To learn more about Stan from the Stan team themselves, check out the main website https://mc-stan.org/. If you like to dive deep, bookmark the Stan user’s guide (Stan Development Team, 2020c) and the Stan reference manual (Stan Development Team, 2020b). We won’t be using Stan directly in this project. I prefer working with it indirectly through the interface of Bürkner’s brms package instead. If you haven’t already, bookmark the brms GitHub repository, CRAN page, and reference manual (Bürkner, 2020g). You can also view Bürkner’s talk from the useR! International R User 2017 Conference, brms: Bayesian multilevel models using Stan. Here’s how Bürkner described brms in its GitHub repo: The brms package provides an interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan, which is a C++ package for performing full Bayesian inference (see http://mc-stan.org/). The formula syntax is very similar to that of the package lme4 to provide a familiar and simple interface for performing regression analyses. A wide range of response distributions are supported, allowing users to fit – among others – linear, robust linear, count data, survival, response times, ordinal, zero-inflated, and even self-defined mixture models all in a multilevel context. Further modeling options include non-linear and smooth terms, auto-correlation structures, censored data, missing value imputation, and quite a few more. In addition, all parameters of the response distribution can be predicted in order to perform distributional regression. Multivariate models (i.e., models with multiple response variables) can be fit, as well. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. Model fit can easily be assessed and compared with posterior predictive checks, cross-validation, and Bayes factors. (emphasis in the original) 14.1 HMC sampling “Stan generates random representative samples from a posterior distribution by using a variation of the Metropolis algorithm called HMC” (p. 400). I’m not going to walk through the the details of HMC sampling, at this time. In addition to Kruschke’s explanation, you might check out McElreath’s lecture on HMC from January, 2019 or one of these lectures (here, here, or here) by Michael Betancourt. I’m also not sufficiently up on the math required to properly make the figures in this section. But we can at least get the ball rolling. library(tidyverse) library(patchwork) Here’s the primary data for the two upper left panels for Figure 14.1. d &lt;- tibble(theta = seq(from = -4, to = 4, by = 0.1)) %&gt;% mutate(density = dnorm(theta, mean = 0, sd = 1)) %&gt;% mutate(`-log(density)` = -log(density)) head(d) ## # A tibble: 6 x 3 ## theta density `-log(density)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -4 0.000134 8.92 ## 2 -3.9 0.000199 8.52 ## 3 -3.8 0.000292 8.14 ## 4 -3.7 0.000425 7.76 ## 5 -3.6 0.000612 7.40 ## 6 -3.5 0.000873 7.04 We need a couple more tibbles for the annotation. position &lt;- tibble(theta = -0.5, density = 0, `-log(density)` = 1.5) text &lt;- tibble(theta = -0.5, density = 0.2, `-log(density)` = 2.75, label1 = &quot;current position&quot;, label2 = &quot;random\\ninitial momentum&quot;) Plot. p1 &lt;- d %&gt;% ggplot(aes(x = theta, y = density)) + geom_line(size = 2, color = &quot;grey67&quot;) + geom_point(data = position, size = 4) + geom_text(data = text, aes(label = label1)) + geom_segment(x = -0.5, xend = -0.5, y = 0.16, yend = 0.04, arrow = arrow(length = unit(0.2, &quot;cm&quot;)), size = 1/4, color = &quot;grey50&quot;) + ggtitle(&quot;Posterior Distrib.&quot;) + coord_cartesian(xlim = c(-3, 3)) p2 &lt;- d %&gt;% ggplot(aes(x = theta, y = `-log(density)`)) + geom_line(size = 2, color = &quot;grey67&quot;) + geom_point(data = position, size = 4) + geom_text(data = text, aes(label = label2)) + geom_segment(x = -1.1, xend = 0.1, y = 1.5, yend = 1.5, arrow = arrow(length = unit(0.275, &quot;cm&quot;), ends = &quot;both&quot;)) + ggtitle(&quot;Negative Log Posterior (&#39;Potential&#39;)&quot;) + coord_cartesian(xlim = c(-3, 3), ylim = c(0, 5)) (p1 / p2) &amp; scale_x_continuous(breaks = -3:3) &amp; theme(panel.grid = element_blank()) Because I’m not sure how to make the dots and trajectories depicted in the third row, I also won’t be able to make proper histograms for the bottom rows. This will go for Figures 14.2 and 14.3, too. If you know how to reproduce them properly, please share your code in my GitHub issue #21. Let’s let Kruschke close this section out: Mathematical theories that accurately describe the dynamics of mechanical systems have been worked out by physicists. The formulation here, in terms of kinetic and potential energy, is named after William Rowan Hamilton (1805–1865). HMC was described in the physics literature by Duane et al. (1987) (who called it “hybrid” Monte Carlo), and HMC was applied to statistical problems by Neal (1994). A brief mathematical overview of HMC is presented by (MacKay, 2003, Chapter 30). A more thorough mathematical review of HMC is provided by (Neal, 2011). Details of how HMC is implemented in Stan can be found in the Stan reference manual and in the book by Gelman et al. (2013). (pp. 405–406) 14.2 Installing Stan You can learn about installing Stan at https://mc-stan.org/users/interfaces/. We, of course, have already been working with Stan via brms. Bürkner has some nice information on how to install brms in the FAQ section of the brms GitHub repository. To install the latest official release from CRAN, execute install.packages(&quot;brms&quot;). If you’d like to install the current developmental version, you can execute the following. if (!requireNamespace(&quot;remotes&quot;)) { install.packages(&quot;remotes&quot;) } remotes::install_github(&quot;paul-buerkner/brms&quot;) As Kruschke advised, it’s a good idea to “be sure that your versions of R and RStudio are up to date” (p. 407) when installing brms and/or Stan. 14.3 A Complete example If you’d like to learn how to fit models in Stan itself, you might consult the updated versions of the Stan User’s Guide and Stan Reference Manual, which you can find at https://mc-stan.org/users/documentation/. You might also check out the Stan Case Studies and other tutorials listed by the Stan team. We will continue using Stan via brms. The model Kruschke walked through in this section followed the form \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Bernoulli} (\\theta) \\\\ \\theta &amp; \\sim \\operatorname{beta} (1, 1), \\end{align*}\\] where \\(\\theta\\) is the probability \\(y = 1\\). Kruschke showed how to simulate the data at the top of page 409. Here’s our tidyverse version. n &lt;- 50 z &lt;- 10 my_data &lt;- tibble(y = rep(1:0, times = c(z, n - z))) glimpse(my_data) ## Rows: 50 ## Columns: 1 ## $ y &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… Time to fire up brms. library(brms) In the absence of predictors, you might think of this as an intercept-only model. You can fit the simple intercept-only Bernoulli model with brms::brm() like this: fit14.1 &lt;- brm(data = my_data, family = bernoulli(link = identity), y ~ 1, prior(beta(1, 1), class = Intercept), iter = 1000, warmup = 200, chains = 3, cores = 3, seed = 14, file = &quot;fits/fit14.01&quot;) As Kruschke wrote, iter is the total number of steps per chain, including warmup steps in each chain. Thinning merely marks some steps as not to be used; thinning does not increase the number of steps taken. Thus, the total number of steps that Stan takes is chains·iter. Of those steps, the ones actually used as representative have a total count of chains·(iter−warmup)/thin. Therefore, if you know the desired total steps you want to keep, and you know the warm-up, chains, and thinning, then you can compute that the necessary iter equals the desired total multiplied by thin/chains+warmup. We did not specify the initial values of the chains in the example above, instead letting Stan randomly initialize the chains by default. The chains can be initialized by the user with the argument init, analogous to JAGS. (p. 409) Unlike what Kruschek showed on page 409, we did not use the thin argument, above, and will generally avoid thinning in this project. You just don’t tend to need to thin your chains when using Stan. I do, however, tend to use the seed argument. Because computers use pseudorandom number generators to take random draws, I prefer to make my random draws reproducible by setting my seed. Others have argued against this. You do you. Kruschke mentioned trace plots and model summaries. Here’s our trace plot. plot(fit14.1) Here’s the summary. print(fit14.1) ## Warning: There were 3 divergent transitions after warmup. Increasing adapt_delta ## above 0.8 may help. See http://mc-stan.org/misc/warnings.html#divergent- ## transitions-after-warmup ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: my_data (Number of observations: 50) ## Samples: 3 chains, each with iter = 1000; warmup = 200; thin = 1; ## total post-warmup samples = 2400 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.21 0.05 0.11 0.32 1.00 911 1203 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Did you notice that divergent transitions warning at the top? I don’t believe Kruschke discussed those in the text. For an good introduction, check out the Divergent Transitions section in the Stan reference manual. In short, divergent transitions indicate your HMC chains had some difficulty exploring the parameter space. As indicated in the warning, increasing the adapt_delta parameter to a value above 0.8, but below 1, can help. Other times increasing the number of warmup iterations or using better priors will help, too. Sometimes divergent transitions indicate you might need to seriously reconsider your model. When you’re stumped, the Stan Forums can be a good place to look for help. You can even find a subsection dedicated to brms users, there. Anyway, we will solve our divergent-transitions problem by setting adapt_delta = 0.99 and using a better prior. If you recall, \\(\\operatorname{beta} (1, 1)\\) is flat. Why not use a weakly-regularizing \\(\\operatorname{beta} (2, 2)\\), instead? Before fitting the model, we can use the tidybayes::stat_dist_halfeyeh() function to visualize those two priors. library(tidybayes) tibble(fit = str_c(&quot;fit14.&quot;, 1:2), alpha = 1:2, beta = 1:2, x = .5, y = c(1.33, 2.33), label = c(&quot;beta (1, 1)&quot;, &quot;beta (2, 2)&quot;)) %&gt;% ggplot() + stat_dist_halfeyeh(aes(y = fit, dist = &quot;beta&quot;, arg1 = alpha, arg2 = beta), .width = c(.5, .95)) + geom_text(aes(x = x, y = y, label = label), color = &quot;grey94&quot;, size = 5) + labs(title = &quot;Plot your priors with\\ntidybayes::stat_dist_halfeyeh()&quot;, x = expression(italic(p)(theta)), y = NULL) + theme(panel.grid = element_blank()) Fit the updated model. fit14.2 &lt;- brm(data = my_data, family = bernoulli(link = identity), y ~ 1, prior(beta(2, 2), class = Intercept), iter = 1000, warmup = 200, chains = 3, cores = 3, control = list(adapt_delta = 0.99), seed = 14, file = &quot;fits/fit14.02&quot;) print(fit14.2) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: my_data (Number of observations: 50) ## Samples: 3 chains, each with iter = 1000; warmup = 200; thin = 1; ## total post-warmup samples = 2400 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.22 0.05 0.12 0.33 1.00 545 536 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). See? All better. 14.3.1 Reusing the compiled model. “Because model compilation can take a while in Stan, it is convenient to store the DSO of a successfully compiled model and use it repeatedly for different data sets” (p. 410). This true for our brms paradigm, too. To reuse a compiled brm() model, we typically use the update() function. To demonstrate, we’ll first want some new data. Here we’ll increase our z value to 20. z &lt;- 20 my_data &lt;- tibble(y = rep(1:0, times = c(z, n - z))) glimpse(my_data) ## Rows: 50 ## Columns: 1 ## $ y &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0… For the first and most important argument, you need to tell update() what fit you’re reusing. We’ll use fit14.2. You also need to tell update() about your new data with the newdata argument. Because the model formula and priors are the same as before, we don’t need to use those arguments, here. fit14.3 &lt;- update(fit14.2, newdata = my_data, iter = 1000, warmup = 200, chains = 3, cores = 3, control = list(adapt_delta = 0.99), seed = 14, file = &quot;fits/fit14.03&quot;) Here’s the summary using the fixef() function. fixef(fit14.3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.4047471 0.06471263 0.2789483 0.532756 14.3.2 General structure of Stan model specification. “The general structure of model specifications in Stan consist of six blocks” (p. 410). We don’t need to worry about this when using brms. Just use brm() or update(). But if you’re curious about what the underlying Stan code is for your brms models, index the model fit with $model. fit14.3$model ## // generated with brms 2.11.5 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // number of observations ## int Y[N]; // response variable ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## } ## parameters { ## real Intercept; // temporary intercept for centered predictors ## } ## transformed parameters { ## } ## model { ## // initialize linear predictor term ## vector[N] mu = Intercept + rep_vector(0, N); ## // priors including all constants ## target += beta_lpdf(Intercept | 2, 2); ## // likelihood including all constants ## if (!prior_only) { ## target += bernoulli_lpmf(Y | mu); ## } ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = Intercept; ## } 14.3.3 Think log probability to think like Stan. The material in this subsection is outside of the scope of this project. 14.3.4 Sampling the prior in Stan. “There are several reasons why we might want to examine a sample from the prior distribution of a model” (p. 413). Happily, we can do this with brms with the sample_prior argument. By default, it is set to &quot;no&quot; and does not take prior samples. If you instead set sample_prior = &quot;yes&quot; or sample_prior = TRUE, samples are drawn solely from the prior. But do note that for technical reasons, brms will not sample from the prior of the default intercept. We can bypass that difficulty by using the ... ~ 0 + Intercept + ... syntax, which makes the prior for the intercept of class = b. Here’s how to do that with an updated version of fit14.3. fit14.4 &lt;- brm(data = my_data, family = bernoulli(link = identity), y ~ 0 + Intercept, prior(beta(2, 2), class = b), iter = 1000, warmup = 200, chains = 3, cores = 3, control = list(adapt_delta = 0.99), sample_prior = &quot;yes&quot;, seed = 14, file = &quot;fits/fit14.04&quot;) Now we can gather the prior samples with the prior_samples() function. prior_samples(fit14.4) %&gt;% head() ## b ## 1 0.5169509 ## 2 0.1240944 ## 3 0.5525183 ## 4 0.3014115 ## 5 0.1425645 ## 6 0.7825860 Here’s a look at the prior distribution. prior_samples(fit14.4) %&gt;% ggplot(aes(x = b, y = 0)) + stat_halfeyeh(.width = c(.5, .95)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(&quot;Beta&quot;*(2*&quot;, &quot;*2)), x = expression(italic(p)(theta))) + theme(panel.grid = element_blank()) 14.3.5 Simplified scripts for frequently used analyses. This is not our approach when using brms. Throughout the chapters of this project, we will learn to make skillful use of the brms::brm() function to fit all our models. Once in a while we’ll take a shortcut and reuse a precompiled fit with update(). 14.4 Specify models top-down in Stan For humans, descriptive models begin, conceptually, with the data that are to be described. We first know the measurement scale of the data and their structure. Then we conceive of a likelihood function for the data. The likelihood function has meaningful parameters, which we might want to re-express in terms of other data (called covariates, predictors, or regressors). Then we build a meaningful hierarchical prior on the parameters. Finally, at the top level, we specify constants that express our prior knowledge, which might be vague or noncommittal. (p. 414) If you look at how I typically organize the arguments within brms::brm(), you’ll see this is generally the case there, too. Take another look at the code for fit14.2: fit14.2 &lt;- brm(data = my_data, family = bernoulli(link = identity), y ~ 1, prior(beta(2, 2), class = Intercept), iter = 1000, warmup = 200, chains = 3, cores = 3, control = list(adapt_delta = 0.99), seed = 14, file = &quot;fits/fit14.02&quot;) The first line within brm() defined the data. The second line defined the likelihood function and its link function. We haven’t talked much about link functions, yet, but that will start in Chapter 15. Likelihoods contain parameters and our third line within brm() defined the equation we wanted to use to predict/describe our parameter of interest, \\(\\theta\\). We defined our sole prior in the fourth line. The remaining arguments contain the unsexy technical specifications, such as how many MCMC chains we’d like to use and into what folder we’d like to save our fit as an external file. You do not need to arrange brm() arguments this way. For other arrangements, take a look at the examples in the brms reference manual or in some of Bürkner’s vignettes, such as his Estimating multivariate models with brms (2020d). However you go about fitting your models with brm(), I mainly recommend you find a general style and stick with it. Standardizing your approach will make your code more readable for others and yourself. 14.5 Limitations and extras At the time of this writing, one of the main limitations of Stan is that it does not allow discrete (i.e., categorical) parameters. The reason for this limitation is that Stan has HMC as its foundational sampling method, and HMC requires computing the gradient (i.e., derivative) of the posterior distribution with respect to the parameters. Of course, gradients are undefined for discrete parameters. (p. 415) To my knowledge this is still the case, which means brms has this limitation, too. As wildly powerful as it is, brms it not as flexible as working directly with Stan. However, Bürkner and others are constantly expanding its capabilities. Probably the best places keep track of the new and evolving features of brms are the issues and news sections in its GitHub repo, https://github.com/paul-buerkner/brms. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.3.9000 brms_2.12.0 Rcpp_1.0.4.6 ## [4] patchwork_1.0.0 forcats_0.5.0 stringr_1.4.0 ## [7] dplyr_0.8.5 purrr_0.3.4 readr_1.3.1 ## [10] tidyr_1.0.2 tibble_3.0.1 ggplot2_3.3.0 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.4.1 rstudioapi_0.11 farver_2.0.3 ## [10] rstan_2.19.3 svUnit_1.0.3 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 ## [16] xml2_1.3.1 bridgesampling_1.0-0 knitr_1.28 ## [19] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 ## [22] broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 ## [25] compiler_3.6.3 httr_1.4.1 backports_1.1.6 ## [28] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] cli_2.0.2 later_1.0.0 htmltools_0.4.0 ## [34] prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.4.0 ## [40] reshape2_1.4.4 cellranger_1.1.0 vctrs_0.3.0 ## [43] nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ## [46] ps_1.3.3 rvest_0.3.5 mime_0.9 ## [49] miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 ## [52] zoo_1.8-7 scales_1.1.1 colourpicker_1.0 ## [55] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 ## [58] parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 StanHeaders_2.21.0-1 ## [64] loo_2.2.0 stringi_1.4.6 dygraphs_1.1.1.6 ## [67] pkgbuild_1.0.8 rlang_0.4.6 pkgconfig_2.0.3 ## [70] matrixStats_0.56.0 evaluate_0.14 lattice_0.20-38 ## [73] rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 ## [76] tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 ## [79] magrittr_1.5 bookdown_0.18 R6_2.4.1 ## [82] generics_0.0.2 DBI_1.1.0 pillar_1.4.4 ## [85] haven_2.2.0 withr_2.2.0 xts_0.12-0 ## [88] abind_1.4-5 modelr_0.1.6 crayon_1.3.4 ## [91] arrayhelpers_1.1-0 utf8_1.1.4 rmarkdown_2.1 ## [94] grid_3.6.3 readxl_1.3.1 callr_3.4.3 ## [97] threejs_0.3.3 reprex_0.3.0 digest_0.6.25 ## [100] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.3 ## [103] munsell_0.5.0 shinyjs_1.1 References "],
["overview-of-the-generalized-linear-model.html", "15 Overview of the Generalized Linear Model 15.1 Types of variables 15.2 Linear combination of predictors 15.3 Linking from combined predictors to noisy predicted data 15.4 Formal expression of the GLM Session info", " 15 Overview of the Generalized Linear Model Along with Kruschke’s text, in this part if the project we’re moving away from simple Bernoulli coin flipping examples to more complicated analyses of the type we’d actually see in applied data analysis. As Kruschke explained, we’ll be using a versatile family of models known as the generalized linear model (GLM; Nelder &amp; Wedderburn, 1972, 1972). This family of models comprises the traditional “off the shelf” analyses such as \\(t\\) tests, analysis of variance (ANOVA), multiple linear regression, logistic regression, log-linear models, etc. (Kruschke, 2015, p. 420) 15.1 Types of variables “To understand the GLM and its many specific cases, we must build up a variety of component concepts regarding relationships between variables and how variables are measured in the first place (p. 420).” 15.1.1 Predictor and predicted variables. It’s worth repeating the second paragraph of this subsection in its entirety. The key mathematical difference between predictor and predicted variables is that the likelihood function expresses the probability of values of the predicted variable as a function of values of the predictor variable. The likelihood function does not describe the probabilities of values of the predictor variable. The value of the predictor variable comes from outside the system being modeled, whereas the value of the predicted variable depends on the value of the predictor variable. (p. 420) This is one of those fine points that can be easy to miss when you’re struggling through the examples in this book or chest-deep in the murky waters of your own real-world data problem. But write this down on a sticky note and put it in your sock drawer or something. There are good reasons to fret about the distributional properties of your predictor variables–rules of thumb about the likelihood aren’t among them. 15.1.2 Scale types: metric, ordinal, nominal, and count. I don’t know that I’m interested in detailing the content of this section. But it’s worth while considering what Kruschke wrote in its close. Why we care: We care about the scale type because the likelihood function must specify a probability distribution on the appropriate scale. If the scale has two nominal values, then a Bernoulli likelihood function may be appropriate. If the scale is metric, then a normal distribution may be appropriate as a probability distribution to describe the data. Whenever we are choosing a model for data, we must answer the question, What kind of scale are we dealing with? (p. 423, emphasis in the original) 15.2 Linear combination of predictors “The core of the GLM is expressing the combined influence of predictors as their weighted sum. The following sections build this idea by scaffolding from the simplest intuitive cases” (p. 423). 15.2.1 Linear function of a single metric predictor. “A linear function is the generic, ‘vanilla,’ off-the-shelf dependency that is used in statistical models” (p. 424). Its basic form is \\[y = \\beta_0 + \\beta_1 x,\\] where \\(y\\) is the variable being predicted and \\(x\\) is the predictor. \\(\\beta_0\\) is the intercept (i.e., the expected value when \\(x\\) is zero) and \\(\\beta_1\\) is the expected increase in \\(y\\) after a one-unit increase in \\(x\\). We’ll fire up the tidyverse to make the left panel of Figure 15.1. library(tidyverse) tibble(x = -3:7) %&gt;% mutate(y_1 = -5 + 2 * x, y_2 = 10 + 2 * x) %&gt;% ggplot(aes(x = x)) + geom_vline(xintercept = 0, color = &quot;white&quot;, linetype = 2) + geom_hline(yintercept = 0, color = &quot;white&quot;, linetype = 2) + geom_line(aes(y = y_1)) + geom_line(aes(y = y_2)) + geom_text(data = tibble( x = 2.25, y = c(-5, 10), label = c(&quot;y = -5 + 2x&quot;, &quot;y = 10 + 2x&quot;) ), aes(y = y, label = label), size = 4.5) + labs(title = &quot;Different Intercepts&quot;, y = &quot;y&quot;) + coord_cartesian(xlim = c(-2, 6), ylim = c(-10, 25)) + theme(panel.grid = element_blank()) Did you notice how we followed the form of the basic linear function when we defined the values for y_1 and y_2? It’s that simple! Here’s the right panel. tibble(x = -3:7) %&gt;% mutate(y_1 = 10 + -0.5 * x, y_2 = 10 + 2 * x) %&gt;% ggplot(aes(x = x)) + geom_vline(xintercept = 0, color = &quot;white&quot;, linetype = 2) + geom_hline(yintercept = 0, color = &quot;white&quot;, linetype = 2) + geom_line(aes(y = y_1)) + geom_line(aes(y = y_2)) + geom_text(data = tibble( x = 4, y = c(11, 13.75), label = c(&quot;y = 10 + -0.5x&quot;, &quot;y = 10 + 2x&quot;) ), aes(y = y, label = label), size = 4.5) + labs(title = &quot;Different Slopes&quot;, y = &quot;y&quot;) + coord_cartesian(xlim = c(-2, 6), ylim = c(-10, 25)) + theme(panel.grid = element_blank()) Summary of why we care. The likelihood function includes the form of the dependency of \\(y\\) on \\(x\\). When \\(y\\) and \\(x\\) are metric variables, the simplest form of dependency, both mathematically and intuitively, is one that preserves proportionality. The mathematical expression of this relation is a so-called linear function. The usual mathematical expression of a line is the \\(y\\)-intercept form, but sometimes a more intuitive expression is the \\(x\\) threshold form. Linear functions form the core of the GLM. (pp. 424–425, emphasis in the original) 15.2.2 Additive combination of metric predictors. The linear combination of \\(K\\) predictors has the general form \\[\\begin{align*} y &amp; = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_K x_K \\\\ &amp; = \\beta_0 + \\sum_{k = 1}^K \\beta_k x_k. \\end{align*}\\] In the special case where \\(K = 0\\), you have an intercept-only model, \\[y = \\beta_0,\\] in which \\(\\beta_0\\) simply models the mean of \\(y\\). We won’t be able to reproduce the wireframe plots of Figure 15.2, exactly. But we can use some geom_raster() tricks from back in Chapter 10 to express the third y dimension as fill gradients. crossing(x1 = seq(from = 0, to = 10, by = .5), x2 = seq(from = 0, to = 10, by = .5)) %&gt;% mutate(`y[1] == 0 + 1 * x[1] + 0 * x[2]` = 0 + 1 * x1 + 0 * x2, `y[2] == 0 + 0 * x[1] + 2 * x[2]` = 0 + 0 * x1 + 2 * x2, `y[3] == 0 + 1 * x[1] + 2 * x[2]` = 0 + 1 * x1 + 2 * x2, `y[4] == 10 + 1 * x[1] + 2 * x[2]` = 10 + 1 * x1 + 2 * x2) %&gt;% pivot_longer(-c(x1, x2), names_to = &quot;key&quot;, values_to = &quot;y&quot;) %&gt;% ggplot(aes(x = x1, y = x2, fill = y)) + geom_raster() + scale_fill_viridis_c(option = &quot;D&quot;) + scale_x_continuous(expression(x[1]), expand = c(0, 0), breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(expression(x[2]), expand = c(0, 0), breaks = seq(from = 0, to = 10, by = 2)) + coord_equal() + theme(panel.grid = element_blank()) + facet_wrap(~key, labeller = label_parsed) Here we’ve captured some of Kruschke’s grid aesthetic by keeping the by argument within seq() somewhat coarse and omitting the interpolate = T argument from geom_raster(). If you’d prefer smoother fill transitions for the y-values, set by = .1 and interpolate = T. 15.2.3 Nonadditive interaction of metric predictors. As it turns out, “the combined influence of two predictors does not have to be additive” (p. 427). Let’s explore what that can look like with our version of Figure 15.3. crossing(x1 = seq(from = 0, to = 10, by = .5), x2 = seq(from = 0, to = 10, by = .5)) %&gt;% mutate(`y[1] == 0 + 0 * x[1] + 0 * x[2] + 0.2 * x[1] * x[2]` = 0 + 0 * x1 + 0 * x2 + 0.2 * x1 * x2, `y[2] == 0 + 1 * x[1] + 1 * x[2] + 0 * x[1] * x[2]` = 0 + 1 * x1 + 1 * x2 + 0 * x1 * x2, `y[3] == 0 + 1 * x[1] + 1 * x[2] + -0.3 * x[1] * x[2]` = 0 + 1 * x1 + 1 * x2 + -0.3 * x1 * x2, `y[4] == 0 + -1 * x[1] + 1 * x[2] + 0.2 * x[1] * x[2]` = 0 + -1 * x1 + 1 * x2 + 0.2 * x1 * x2) %&gt;% pivot_longer(-c(x1, x2), names_to = &quot;key&quot;, values_to = &quot;y&quot;) %&gt;% ggplot(aes(x = x1, y = x2, fill = y)) + geom_raster() + scale_fill_viridis_c(option = &quot;D&quot;) + scale_x_continuous(expression(x[1]), expand = c(0, 0), breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(expression(x[2]), expand = c(0, 0), breaks = seq(from = 0, to = 10, by = 2)) + coord_equal() + theme(panel.grid = element_blank()) + facet_wrap(~key, labeller = label_parsed) Did you notice all those * operators in the mutate() code? We’re no longer just additive. We’re square in multiplicative town! There is a subtlety in the use of the term “linear” that can sometimes cause confusion in this context. The interactions shown in Figure 15.3 are not linear on the two predictors \\(x_1\\) and \\(x_2\\). But if the product of the two predictors, \\(x_1 x_2\\), is thought of as a third predictor, then the model is linear on the three predictors, because the predicted value of \\(y\\) is a weighted additive combination of the three predictors. This reconceptualization can be useful for implementing nonlinear interactions in software for linear models, but we will not be making that semantic leap to a third predictor, and instead we will think of a nonadditive combination of two predictors. A nonadditive interaction of predictors does not have to be multiplicative. Other types of interaction are possible. (p. 428, emphasis in the original) 15.2.4 Nominal predictors. 15.2.4.1 Linear model for a single nominal predictor. Instead of representing the value of the nominal predictor by a single scalar value \\(x\\), we will represent the nominal predictor by a vector \\(\\vec{x} = \\langle x_{[1]},...,x_{[J]} \\rangle\\) where \\(J\\) is the number of categories that the predictor has… We will denote the baseline value of the prediction as \\(\\beta_0\\). The deflection for the \\(j\\)th level of the predictor is denoted \\(\\beta_{[j]}\\). Then the predicted value is \\[\\begin{align*} y &amp; = \\beta_0 + \\beta_{[1]} x_{[1]} + \\dots + \\beta_{[J]} x_{[J]} \\\\ &amp; = \\beta_0 + \\vec{\\beta} \\cdot \\vec{x} \\end{align*}\\] where the notation \\(\\vec{\\beta} \\cdot \\vec{x}\\) is sometimes called the ‘dot product’ of the vectors. (p. 429) 15.2.4.2 Additive combination of nominal predictors. When you have two additive nominal predictors, the model follows the form \\[\\begin{align*} y &amp; = \\beta_0 + \\vec \\beta_1 \\vec x_1 + \\vec \\beta_2 \\vec x_2 \\\\ &amp; = \\beta_0 + \\sum_{j} \\beta_{1[j]} x_{1[j]} + \\sum_{k} \\beta_{2[k]} x_{2[k]}, \\end{align*}\\] given the constraints \\[\\sum_{j} \\beta_{1[j]} = 0 \\;\\;\\; \\text{and} \\;\\;\\; \\sum_{j} \\beta_{2[k]} = 0.\\] Both panels in Figure 15.4 are going to require three separate data objects. Here’s the code for the top panel. arrows &lt;- tibble(x = c(0.1, 1.1, 2.1), y = c(1, 1.69, 1.69), yend = c(1.69, 1.69 + .07, 1.69 - .07)) text &lt;- tibble(x = c(0.44, 1.46, 2.51), y = c(1.68, 1.753, 1.625), label = c(&quot;beta[0] == 1.69&quot;, &quot;beta[&#39;[1]&#39;] == 0.07&quot;, &quot;beta[&#39;[2]&#39;] == -0.07&quot;)) tibble(x = 1:2, y = c(1.69 + .07, 1.69 - .07)) %&gt;% # plot! ggplot(aes(x = x, y = y)) + geom_hline(yintercept = 1.69, color = &quot;white&quot;, linetype = 2) + geom_col(width = .075, fill = &quot;grey67&quot;) + geom_segment(data = arrows, aes(xend = x, yend = yend), arrow = arrow(length = unit(0.2, &quot;cm&quot;)), size = 1/3, color = &quot;grey25&quot;) + geom_text(data = text, aes(label = label), size = 3.5, parse = T) + scale_x_continuous(breaks = 1:2, labels = c(&quot;&lt;1,0&gt;&quot;, &quot;&lt;0,1&gt;&quot;)) + coord_cartesian(xlim = c(0, 3), ylim = c(1.5, 1.75)) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) Here’s the code for the bottom panel. arrows &lt;- tibble(x = c(0.1, 1.1, 2.1, 3.1, 4.1, 5.1), y = rep(c(50, 101), times = c(1, 5)), yend = c(101, 101 + 4, 101 - 3, 101 - 2, 101 + 6, 101 - 5)) text &lt;- tibble(x = c(0.41, 1.36, 2.41, 3.4, 4.35, 5.4), y = c(100.5, 104.5, 98.5, 99.5, 106.5, 96.5), label = c(&quot;beta[0] == 101&quot;, &quot;beta[&#39;[1]&#39;] == 4&quot;, &quot;beta[&#39;[2]&#39;] == -3&quot;, &quot;beta[&#39;[3]&#39;] == -2&quot;, &quot;beta[&#39;[4]&#39;] == 6&quot;, &quot;beta[&#39;[5]&#39;] == -5&quot;)) tibble(x = 1:5, y = c(101 + 4, 101 - 3, 101 - 2, 101 + 6, 101 - 5)) %&gt;% # the plot ggplot(aes(x = x, y = y)) + geom_hline(yintercept = 101, color = &quot;white&quot;, linetype = 2) + geom_col(width = .075, fill = &quot;grey67&quot;) + geom_segment(data = arrows, aes(xend = x, yend = yend), arrow = arrow(length = unit(0.2, &quot;cm&quot;)), size = 1/3, color = &quot;grey25&quot;) + geom_text(data = text, aes(label = label), size = 3.5, parse = T) + scale_x_continuous(breaks = 1:5, labels = c(&quot;&lt;1,0,0,0,0&gt;&quot;, &quot;&lt;0,1,0,0,0&gt;&quot;, &quot;&lt;0,0,1,0,0&gt;&quot;, &quot;&lt;0,0,0,1,0&gt;&quot;, &quot;&lt;0,0,0,0,1&gt;&quot;)) + coord_cartesian(xlim = c(0, 5.5), ylim = c(90, 106.5)) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank()) Before we make our versions of the two panels in Figure 15.5, we should note that the values in the prose are at odds with the values implied in the figure. For simplicity, our versions of Figure 15.5 will match up with the values in the original figure, not the prose. For a look at what the figure might have looked like had it been based on the values in the prose, check out Kruschke’s Corrigenda. Here’s the code for the left panel of Figure 15.5. d &lt;- tibble(x_1 = rep(c(&quot; &lt;1,0&gt;&quot;, &quot;&lt;0,1&gt; &quot;), times = 4), x_2 = c(rep(0:2, each = 2), -0.25, 0.25), y = c(8, 10, 3, 5, 4, 6, 8.5, 10.5), type = rep(c(&quot;number&quot;, &quot;text&quot;), times = c(6, 2))) %&gt;% mutate(x_1 = factor(x_1, levels = c(&quot; &lt;1,0&gt;&quot;, &quot;&lt;0,1&gt; &quot;))) d %&gt;% filter(type == &quot;number&quot;) %&gt;% ggplot(aes(x = x_2, y = y, fill = x_1)) + geom_col(position = &quot;dodge&quot;) + geom_text(data = d %&gt;% filter(type == &quot;text&quot;), aes(label = x_1, color = x_1)) + scale_fill_viridis_d(NULL, option = &quot;C&quot;, end = .6) + scale_color_viridis_d(NULL, option = &quot;C&quot;, end = .6) + scale_x_continuous(breaks = 0:2, labels = c(&quot;&lt;1,0,0&gt;&quot;, &quot;&lt;0,1,0&gt;&quot;, &quot;&lt;0,0,1&gt;&quot;)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) + ggtitle(&quot;Additive (no interaction)&quot;) + theme(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;, panel.grid = element_blank()) Did you notice our filter() trick in the code? 15.2.4.3 Nonadditive interaction of nominal predictors. We need new notation to formalize the nonadditive influence of a combination of nominal values. Just as \\(\\vec x_1\\) refers to the value of predictor 1, and \\(\\vec x_2\\) refers to the value of predictor 2, the notation \\(\\vec x_{1 \\times 2}\\) will refer to a particular combination of values of predictors 1 and 2. If there are \\(J\\) levels of predictor 1 and \\(K\\) levels of predictor 2, then there are \\(J \\times K\\) combinations of the two predictors. To indicate a particular combination of levels from predictors 1 and 2, the corresponding component of \\(\\vec x_{1 \\times 2}\\) is set to 1 while all other components are set to 0. A nonadditive interaction of predictors is formally represented by including a term for the influence of combinations of predictors, beyond the additive influences, as follows: \\(y = \\beta_0 + \\vec \\beta_1 \\cdot \\vec x_1 + \\vec \\beta_2 \\cdot \\vec x_2 + \\vec \\beta_{1 \\times 2} \\cdot \\vec x_{1 \\times 2}\\). (pp. 432–433, emphasis in the original) Now we’re in nonadditive interaction land, here’s the code for the right panel of Figure 15.5. d &lt;- tibble(x_1 = rep(c(&quot; &lt;1,0&gt;&quot;, &quot;&lt;0,1&gt; &quot;), times = 4), x_2 = c(rep(0:2, each = 2), -0.25, 0.25), y = c(8, 10, 5, 3, 3, 7, 8.5, 10.5), type = rep(c(&quot;number&quot;, &quot;text&quot;), times = c(6, 2))) %&gt;% mutate(x_1 = factor(x_1, levels = c(&quot; &lt;1,0&gt;&quot;, &quot;&lt;0,1&gt; &quot;))) d %&gt;% filter(type == &quot;number&quot;) %&gt;% ggplot(aes(x = x_2, y = y, fill = x_1)) + geom_col(position = &quot;dodge&quot;) + geom_text(data = d %&gt;% filter(type == &quot;text&quot;), aes(label = x_1, color = x_1)) + scale_fill_viridis_d(NULL, option = &quot;C&quot;, end = .6) + scale_color_viridis_d(NULL, option = &quot;C&quot;, end = .6) + scale_x_continuous(breaks = 0:2, labels = c(&quot;&lt;1,0,0&gt;&quot;, &quot;&lt;0,1,0&gt;&quot;, &quot;&lt;0,0,1&gt;&quot;)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) + ggtitle(&quot;Non-Additive Interaction&quot;) + theme(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;, panel.grid = element_blank()) “The main point to understand now is that the term ‘interaction’ refers to a nonadditive influence of the predictors on the predicted, regardless of whether the predictors are measured on a nominal scale or a metric scale” (p. 434, emphasis in the original). 15.3 Linking from combined predictors to noisy predicted data 15.3.1 From predictors to predicted central tendency. After the predictor variables are combined, they need to be mapped to the predicted variable. This mathematical mapping is called the (inverse) link function, and is denoted by \\(f()\\) in the following equation: \\[y = f(\\operatorname{lin}(x))\\] Until now, we have been assuming that the link function is merely the identity function, \\(f(\\operatorname{lin}(x)) = \\operatorname{lin}(x)\\). (p. 436, emphasis in the original) Yet as we’ll see, many models use links other than the identity function. 15.3.1.1 The logistic function. The logistic link function follows the form \\[y = \\operatorname{logistic}(x) = \\frac{1}{ \\big (1 + \\exp (-x) \\big )}.\\] We can write the logistic function for a univariable metric predictor as \\[y = \\operatorname{logistic}(x; \\beta_0, \\beta_1) = \\frac{1}{\\big (1 + \\exp (-(\\beta_0 + \\beta_1)) \\big )}.\\] And if we prefer to parameterize it in terms of gain \\(\\gamma\\) and threshold \\(\\theta\\), it’d be \\[y = \\operatorname{logistic}(x; \\gamma, \\theta) = \\frac{1}{ \\big (1 + \\exp (-\\gamma (x - \\theta)) \\big )}.\\] We can make the sexy logistic curves of Figure 15.6 with stat_function(), into which we’ll plug our very own custom make_logistic() function. Here’s the left panel. make_logistic &lt;- function(x, gamma, theta) { 1 / (1 + exp(-gamma * (x - theta))) } # we&#39;ll need this for the annotation text &lt;- tibble(x = c(-1, 3), y = c(.9, .1), label = c(&#39;list(gamma == 0.5, theta == -1)&#39;, &#39;list(gamma == 0.5, theta == 3)&#39;)) tibble(x = c(-11, 11)) %&gt;% ggplot(aes(x = x)) + geom_vline(xintercept = c(-1, 3), color = &quot;white&quot;, linetype = 2) + geom_hline(yintercept = .5, color = &quot;white&quot;, linetype = 2) + stat_function(fun = make_logistic, args = list(gamma = .5, theta = -1), size = 1, color = &quot;grey50&quot;) + stat_function(fun = make_logistic, args = list(gamma = .5, theta = 3), size = 1, color = &quot;grey50&quot;) + geom_text(data = text, aes(y = y, label = label), size = 3.5, parse = T) + coord_cartesian(xlim = c(-10, 10)) + ggtitle(&quot;Different Thresholds&quot;) + theme(panel.grid = element_blank()) For kicks, we’ll take a different approach for the right panel. Instead of pumping values through stat_function() within our plot code, we’ll use our make_logistic() function within mutate() before beginning the plot code. # define the annotation values text &lt;- tibble(x = c(2, -2), y = c(.92, .4), label = c(&#39;list(gamma == 2, theta == 4)&#39;, &#39;list(gamma == 0.2, theta == 4)&#39;)) # make the data crossing(gamma = c(2, .2), x = seq(from = -11, to = 11, by = .2)) %&gt;% mutate(y = make_logistic(x, gamma, theta = 4)) %&gt;% # plot! ggplot(aes(x = x, y = y)) + geom_vline(xintercept = 4, color = &quot;white&quot;, linetype = 2) + geom_hline(yintercept = .5, color = &quot;white&quot;, linetype = 2) + geom_line(aes(group = gamma), size = 1, color = &quot;grey50&quot;) + geom_text(data = text, aes(label = label), size = 3.5, parse = T) + coord_cartesian(xlim = c(-10, 10)) + ggtitle(&quot;Different Gains&quot;) + theme(panel.grid = element_blank()) I don’t know that one plotting approach is better than the other. It’s good to have options. To make our two-dimensional version of the wireframe plots of Figure 15.7, we’ll first want to define the logistic() function. logistic &lt;- function(x) { 1 / (1 + exp(-x)) } Now we’ll just extend the same method we used for Figures 15.2 and 15.3. crossing(x1 = seq(from = -6, to = 6, by = .5), x2 = seq(from = -6, to = 6, by = .5)) %&gt;% mutate(`y[1] == logistic(1 * ( 0 * x[1] + 1 * x[2] - 0))` = logistic(1 * ( 0 * x1 + 1 * x2 - 0)), `y[2] == logistic(1 * ( 0.71 * x[1] + 0.71 * x[2] - 0))` = logistic(1 * ( 0.71 * x1 + 0.71 * x2 - 0)), `y[3] == logistic(2 * ( 0 * x[1] + 1 * x[2] - -3))` = logistic(2 * ( 0 * x1 + 1 * x2 - -3)), `y[4] == logistic(2 * (-0.71 * x[1] + 0.71 * x[2] - 3))` = logistic(2 * (-0.71 * x1 + 0.71 * x2 - 3))) %&gt;% gather(key, y, -x1, -x2) %&gt;% ggplot(aes(x = x1, y = x2, fill = y)) + geom_raster() + scale_fill_viridis_c(option = &quot;D&quot;, limits = c(0, 1)) + scale_x_continuous(expression(x[1]), expand = c(0, 0), breaks = seq(from = -6, to = 6, by = 2)) + scale_y_continuous(expression(x[2]), expand = c(0, 0), breaks = seq(from = -6, to = 6, by = 2), position = &quot;right&quot;) + coord_equal() + theme(legend.position = &quot;left&quot;, panel.grid = element_blank(), strip.text = element_text(size = 8)) + facet_wrap(~key, labeller = label_parsed) “The threshold determines the position of the logistical cliff. In other words, the threshold determines the x values for which \\(y = 0.5\\)… The gain determines the steepness of the logistical cliff” (pp. 437–439, emphasis in the original). 15.3.1.2 The cumulative normal function. The cumulative normal is denoted \\(\\Phi (x; \\mu, \\sigma)\\), where x is a real number and where \\(\\mu\\) and \\(\\sigma\\) are parameter values, called the mean and standard deviation of the normal distribution. The parameter \\(\\mu\\) governs the point at which the cumulative normal, \\(\\Phi(x)\\), equals 0.5. In other words, \\(\\mu\\) plays the same role as the threshold in the logistic. The parameter \\(\\sigma\\) governs the steepness of the cumulative normal function at \\(x = \\mu\\), but inversely, such that a smaller value of \\(\\sigma\\) corresponds to a steeper cumulative normal. (p. 440, emphasis in the original) Here we plot the standard normal density in the top panel of Figure 15.8. tibble(x = seq(from = -4, to = 4, by = .1)) %&gt;% ggplot(aes(x = x)) + geom_ribbon(data = . %&gt;% filter(x &lt;=1), aes(ymin = 0, ymax = dnorm(x, mean = 0, sd = 1)), fill = &quot;grey67&quot;) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1), size = 1, color = &quot;grey50&quot;) + coord_cartesian(xlim = c(-3, 3)) + labs(title = &quot;Normal Density&quot;, y = &quot;p(x)&quot;) + theme(panel.grid = element_blank()) Did you notice our data = . %&gt;% filter(x &lt;=1) trick in geom_ribbon()? Anyway, here’s the analogous cumulative normal function depicted in the bottom panel of Figure 15.8. tibble(x = seq(from = -4, to = 4, by = .1)) %&gt;% ggplot(aes(x = x)) + stat_function(fun = pnorm, args = list(mean = 0, sd = 1), size = 1, color = &quot;grey50&quot;) + geom_segment(aes(x = 1, xend = 1, y = 0, yend = pnorm(1, 0, 1)), arrow = arrow(length = unit(0.2, &quot;cm&quot;)), size = 1/4, color = &quot;grey25&quot;) + coord_cartesian(xlim = c(-3, 3)) + labs(title = &quot;Cumulative Normal&quot;, y = expression(Phi(x))) + theme(panel.grid = element_blank()) Terminology: The inverse of the cumulative normal is called the probit function. (“Probit” stands for “probability unit”; Bliss, 1934). The probit function maps a value \\(p\\), for \\(0.0 \\leq p \\leq 1.0\\), onto the infinite real line, and a graph of the probit function looks very much like the logit function. (p. 439, emphasis in the original) 15.3.2 From predicted central tendency to noisy data. In the real world, there is always variation in \\(y\\) that we cannot predict from \\(x\\). This unpredictable “noise” in \\(y\\) might be deterministically caused by sundry factors we have neither measured nor controlled, or the noise might be caused by inherent non-determinism in \\(y\\). It does not matter either way because in practice the best we can do is predict the probability that \\(y\\) will have any particular value, dependent upon \\(x\\)… To make this notion of probabilistic tendency precise, we need to specify a probability distribution for \\(y\\) that depends on \\(f (\\operatorname{lin} (x))\\). To keep the notation tractable, first define \\(\\mu = f (\\operatorname{lin} (x))\\). The value \\(\\mu\\) represents the central tendency of the predicted \\(y\\) values, which might or might not be the mean. With this notation, we then denote the probability distribution of \\(y\\) as some to-be-specified probability density function, abbreviated as “pdf”: \\(y \\sim \\operatorname{pdf} \\big ( \\mu, [\\text{scale, shape, etc.}] \\big )\\) As indicated by the bracketed terms after \\(\\mu\\), the pdf might have various additional parameters that control the distribution’s scale (i.e., standard deviation), shape, etc. The form of the pdf depends on the measurement scale of the predicted variable. (pp. 440–441, emphasis in the original) The top panel of Figure 15.9 is tricky. One way to make those multiple densities tipped on their sides is with ggridges::geom_ridgeline() followed by coord_flip(). However, explore a different approach that’ll come in handy in many of the plots we’ll be making in later chapters. The method requires we make a data set with the necessary coordinates for the side-tipped densities. Let’s walk through that step slowly. curves &lt;- # define the 3 x-values we want the Gaussians to originate from tibble(x = seq(from = -7.5, to = 7.5, length.out = 4)) %&gt;% # use the formula 10 + 2x to compute the expected y-value for x mutate(y_mean = 10 + (2 * x)) %&gt;% # based on a Gaussian with `mean = y_mean` and `sd = 2`, compute the 99.5% intervals mutate(ll = qnorm(.0025, mean = y_mean, sd = 2), ul = qnorm(.9975, mean = y_mean, sd = 2)) %&gt;% # now use those interval bounds to make a sequence of y-values mutate(y = map2(ll, ul, seq, length.out = 100)) %&gt;% # since that operation returned a nested column, we need to `unnest()` unnest(y) %&gt;% # compute the density values mutate(density = map2_dbl(y, y_mean, dnorm, sd = 2)) %&gt;% # now rescale the density values to be wider. # since we want these to be our x-values, we&#39;ll # just redefine the x column with these results mutate(x = x - density * 2 / max(density)) str(curves) ## tibble [400 × 6] (S3: tbl_df/tbl/data.frame) ## $ x : num [1:400] -7.54 -7.55 -7.55 -7.56 -7.57 ... ## $ y_mean : num [1:400] -5 -5 -5 -5 -5 -5 -5 -5 -5 -5 ... ## $ ll : num [1:400] -10.6 -10.6 -10.6 -10.6 -10.6 ... ## $ ul : num [1:400] 0.614 0.614 0.614 0.614 0.614 ... ## $ y : num [1:400] -10.6 -10.5 -10.4 -10.3 -10.2 ... ## $ density: num [1:400] 0.00388 0.00454 0.0053 0.00617 0.00715 ... In case it’s not clear, we’ll be plotting with the x and y columns. Think of the other columns as showing our work. But now we’ve got those curves data, we’re ready to simulate the points and plot. # how many points would you like? n_samples &lt;- 750 # generate the points tibble(x = runif(n = n_samples, -10, 10)) %&gt;% mutate(y = rnorm(n = n_samples, mean = 10 + 2 * x, sd = 2)) %&gt;% # plot! ggplot(aes(x = x, y = y)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point(size = 1/5) + geom_abline(intercept = 10, slope = 2, size = 1, color = &quot;blue1&quot;) + geom_path(data = curves, aes(group = y_mean), color = &quot;blue&quot;, size = 3/4) + labs(title = &quot;Normal PDF around Linear Function&quot;, y = &quot;y&quot;) + coord_cartesian(ylim = c(-10, 30)) + theme(panel.grid = element_blank()) We’ll revisit this method in Chapter 17. The wireframe plots at the bottom of Figure 15.9 and in Figure 15.10 are outside of our ggplot2 purview. 15.4 Formal expression of the GLM We can write the GLM as \\[\\begin{align*} y &amp; \\sim \\operatorname{pdf} \\big (\\mu, [\\text{parameters}] \\big ), \\text{where} \\\\ \\mu &amp; = f \\big (\\operatorname{lin}(x), [\\text{parameters}] \\big ). \\end{align*}\\] As has been previously explained, the predictors \\(x\\) are combined in the linear function \\(\\operatorname{lin}(x)\\), and the function \\(f\\) in [the first equation] is called the inverse link function. The data, \\(y\\), are distributed around the central tendency \\(\\mu\\) according to the probability density function labeled “pdf.” (p. 444) 15.4.1 Cases of the GLM. When a client brings an application to a [statistical] consultant, one of the first things the consultant does is find out from the client which data are supposed to be predictors and which data are supposed to be predicted, and the measurement scales of the data… When you are considering how to analyze data, your first task is to be your own consultant and find out which data are predictors, which are predicted, and what measurement scales they are. (pp. 445–446) Soak this last bit in. It’s gold. I’ve found it to be true in my exchanges with other researchers and with my own data problems, too. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 ## [5] readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ggplot2_3.3.0 ## [9] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.0.0 xfun_0.13 haven_2.2.0 lattice_0.20-38 ## [5] colorspace_1.4-1 vctrs_0.3.0 generics_0.0.2 viridisLite_0.3.0 ## [9] htmltools_0.4.0 yaml_2.2.1 rlang_0.4.6 pillar_1.4.4 ## [13] glue_1.4.0 withr_2.2.0 DBI_1.1.0 dbplyr_1.4.2 ## [17] modelr_0.1.6 readxl_1.3.1 lifecycle_0.2.0 munsell_0.5.0 ## [21] gtable_0.3.0 cellranger_1.1.0 rvest_0.3.5 evaluate_0.14 ## [25] labeling_0.3 knitr_1.28 fansi_0.4.1 broom_0.5.5 ## [29] Rcpp_1.0.4.6 scales_1.1.1 backports_1.1.6 jsonlite_1.6.1 ## [33] farver_2.0.3 fs_1.4.1 hms_0.5.3 digest_0.6.25 ## [37] stringi_1.4.6 bookdown_0.18 grid_3.6.3 cli_2.0.2 ## [41] tools_3.6.3 magrittr_1.5 crayon_1.3.4 pkgconfig_2.0.3 ## [45] ellipsis_0.3.0 xml2_1.3.1 reprex_0.3.0 lubridate_1.7.8 ## [49] assertthat_0.2.1 rmarkdown_2.1 httr_1.4.1 rstudioapi_0.11 ## [53] R6_2.4.1 nlme_3.1-144 compiler_3.6.3 References "],
["metric-predicted-variable-on-one-or-two-groups.html", "16 Metric-Predicted Variable on One or Two Groups 16.1 Estimating the mean and standard deviation of a normal distribution 16.2 Outliers and robust estimation: The \\(t\\) distribution 16.3 Two groups 16.4 Other noise distributions and transforming data Session info", " 16 Metric-Predicted Variable on One or Two Groups In the context of the generalized linear model (GLM) introduced in the previous chapter, this chapter’s situation involves the most trivial cases of the linear core of the GLM, as indicated in the left cells of Table 15.1 (p. 434), with a link function that is the identity along with a normal distribution for describing noise in the data, as indicated in the first row of Table 15.2 (p. 443). We will explore options for the prior distribution on parameters of the normal distribution, and methods for Bayesian estimation of the parameters. We will also consider alternative noise distributions for describing data that have outliers. (Kruschke, 2015, pp. 449–450) Although I agree this chapter covers the “most trivial cases of the linear core of the GLM”, Kruschke’s underselling himself a bit, here. In addition to “trivial” Gaussian models, Kruschke went well beyond and introduced robust Student’s \\(t\\) modeling. It’s a testament to Kruschke’s rigorous approach that he did so so early in the text. IMO, we could use more robust Student’s \\(t\\) models in the social sciences. So heed well, friends. 16.1 Estimating the mean and standard deviation of a normal distribution The Gaussian probability density function follows the form \\[p(y | \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\Bigg (-\\frac{1}{2} \\frac{(y - \\mu)^2}{\\sigma^2} \\Bigg ),\\] where the two parameters to estimate are \\(\\mu\\) (i.e., the mean) and \\(\\sigma\\) (i.e., the standard deviation). If you prefer to think in terms of \\(\\sigma^2\\), that’s the variance. In case is wasn’t clear, \\(\\pi\\) is the actual number \\(\\pi\\), not a parameter to be estimated. We’ll divide Figure 16.1 into data and plot steps. I came up with the primary data like so: library(tidyverse) sequence_length &lt;- 100 d &lt;- crossing(y = seq(from = 50, to = 150, length.out = sequence_length), mu = c(87.8, 100, 112), sigma = c(7.35, 12.2, 18.4)) %&gt;% mutate(density = dnorm(y, mean = mu, sd = sigma), mu = factor(mu, labels = str_c(&quot;mu==&quot;, c(87.8, 100, 112))), sigma = factor(sigma, labels = str_c(&quot;sigma==&quot;, c(7.35, 12.2, 18.4)))) head(d) ## # A tibble: 6 x 4 ## y mu sigma density ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 50 mu==87.8 sigma==7.35 9.80e- 8 ## 2 50 mu==87.8 sigma==12.2 2.69e- 4 ## 3 50 mu==87.8 sigma==18.4 2.63e- 3 ## 4 50 mu==100 sigma==7.35 4.85e-12 ## 5 50 mu==100 sigma==12.2 7.37e- 6 ## 6 50 mu==100 sigma==18.4 5.40e- 4 Instead of putting the coordinates for the three data points in our d tibble, I just threw them into their own tibble in the geom_point() function. # here we suppress grid lines for all plots in this chapter theme_set(theme_grey() + theme(panel.grid = element_blank())) # plot! d %&gt;% ggplot(aes(x = y)) + geom_ribbon(aes(ymin = 0, ymax = density), fill = &quot;grey67&quot;) + geom_vline(xintercept = c(85, 100, 115), linetype = 3, color = &quot;grey92&quot;) + geom_point(data = tibble(y = c(85, 100, 115)), aes(y = 0), size = 2) + scale_y_continuous(expression(paste(&quot;p(y|&quot;, mu, &quot;, &quot;, sigma, &quot;)&quot;)), breaks = NULL) + ggtitle(&quot;Competing Gaussian likelihoods given the same data&quot;) + coord_cartesian(xlim = c(60, 140)) + facet_grid(sigma ~ mu, labeller = label_parsed) 16.1.1 Solution by mathematical analysis Heads up on precision. Not much for us, here. But we might reiterate that sometimes we talk about the precision (see page 453), which is the reciprocal of the variance (i.e., \\(\\frac{1}{\\sigma^2}\\)). As we’ll see, the brms package doesn’t use priors parameterized in terms of precision. But JAGS does, which means we’ll need to be able to translate Kruschke’s precision-laden JAGS code into \\(\\sigma\\)-oriented brms code in many of the remaining chapters. Proceed with caution. 16.1.2 Approximation by MCMC in JAGS HMC in brms. Let’s load and glimpse() at the data. my_data &lt;- read_csv(&quot;data.R/TwoGroupIQ.csv&quot;) glimpse(my_data) ## Rows: 120 ## Columns: 2 ## $ Score &lt;dbl&gt; 102, 107, 92, 101, 110, 68, 119, 106, 99, 103, 90, 93, 79, 89, 137, 119, 126, 110, … ## $ Group &lt;chr&gt; &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;,… The data file included values from two groups. my_data %&gt;% distinct(Group) ## # A tibble: 2 x 1 ## Group ## &lt;chr&gt; ## 1 Smart Drug ## 2 Placebo Kruschke clarified that for the following example, “the data are IQ (intelligence quotient) scores from a group of people who have consumed a ‘smart drug’” (p. 456). That means we’ll want to subset the data. my_data &lt;- my_data %&gt;% filter(Group == &quot;Smart Drug&quot;) It’s a good idea to take a look at the data before modeling. my_data %&gt;% ggplot(aes(x = Score)) + geom_density(color = &quot;transparent&quot;, fill = &quot;grey67&quot;) + geom_rug(size = 1/4, alpha = 1/2) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;The ticks show individual data points.&quot;) Here are the mean and \\(SD\\) of the Score data. (mean_y &lt;- mean(my_data$Score)) ## [1] 107.8413 (sd_y &lt;- sd(my_data$Score)) ## [1] 25.4452 Those values will come in handy in just a bit. But first, let’s load brms. library(brms) If we want to pass user-defined values into our brm() prior code, we’ll need to define them first in using brms::stanvar(). stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) It’s been a while since we used the stanvar() function, so we should review. Though we’ve saved that as stanvars, you could name it whatever you want. The main trick is to them tell brms::brm() about your values in a stanvars statement. Kruschke mentioned that the “the conventional noncommittal gamma prior [for the precision] has shape and rate constants that are close to zero, such as Sh = 0.01 and R = 0.01” (p. 456). Here’s what that looks like. tibble(x = seq(from = 0, to = 12, by = .05)) %&gt;% mutate(d = dgamma(x, shape = 0.01, rate = 0.01)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = &quot;grey67&quot;) + geom_vline(xintercept = 1 / sd_y, linetype = 2) + labs(subtitle = &quot;The grey density in the background is the conventional gamma prior for precision.\\nThe dashed vertical line is our precision value.&quot;) + scale_y_continuous(breaks = NULL) + coord_cartesian(xlim = c(0, 10)) The thing is, with brms we typically estimate \\(\\sigma\\) rather than precision. Though gamma is also a feasible prior distribution for \\(\\sigma\\), we won’t use it here. But we won’t be using Kruschke’s uniform prior, either. The Stan team discourages uniform priors for variance parameters, such as our \\(\\sigma\\). I’m not going to get into the details of why, but you’ve got that hyperlink above and the good old Stan user’s guide (Stan Development Team, 2020c) if you’d like to dive deeper. Here we’ll use the half normal. By “half normal,” we mean that the mean is zero and it’s bounded from zero to positive infinity–no negative \\(\\sigma\\) values for us! By the “half normal,” we also mean to suggest that smaller values are more credible than those approaching infinity. When working with unstandardized data, an easy default for a weakly-regularizing half normal is to set the \\(\\sigma\\) hyperparameter (i.e., S) to the standard deviation of the criterion variable (i.e., \\(s_Y\\)). Here’s that that looks like for this example. tibble(x = seq(from = 0, to = 110, by = .1)) %&gt;% mutate(d = dnorm(x, mean = 0, sd = sd_y)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = &quot;grey67&quot;) + geom_vline(xintercept = sd_y, linetype = 2) + scale_y_continuous(breaks = NULL) + labs(subtitle = &quot;The gray density in the background is the half-normal prior for sigma.\\nThe dashed vertical line is our &#39;sd_y&#39; value.&quot;) + coord_cartesian(xlim = c(0, 100)) This prior isn’t quite as non-committal as the conventional gamma prior for precision. It discourages the HMC algorithm from exploring \\(\\sigma\\) values much larger than two or three times the standard deviation in the data themselves. In practice, I’ve found it to have a minimal influence on the posterior. If you’d like to make it even less committal, try setting that \\(\\sigma\\) hyperparameter to some multiple of \\(s_Y\\) like \\(2 \\times s_Y\\) or \\(10 \\times s_Y\\). Compare this to Kruschke’s recommendations for setting a noncommittal uniform prior for \\(\\sigma\\). When using the uniform distribution, \\(\\operatorname{uniform} (L, H)\\), we will set the high value \\(H\\) of the uniform prior on \\(\\sigma\\) to a huge multiple of the standard deviation in the data, and set the low value \\(L\\) to a tiny fraction of the standard deviation in the data. Again, this means that the prior is vague no matter what the scale of the data happens to be. (p. 455) On page 456, Kruschke gave an example of such a uniform prior with the code snip dunif( sdY/1000 , sdY*1000 ). Here’s what that would look like with our data. tibble(x = 0:(sd_y * 1000)) %&gt;% mutate(d = dunif(x, min = sd_y / 1000, max = sd_y * 1000)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = &quot;grey67&quot;) + geom_vline(xintercept = sd_y, linetype = 2) + scale_y_continuous(breaks = NULL) + labs(subtitle = &quot;The gray density in the background is Kruschke&#39;s uniform prior for sigma.\\nThe dashed vertical line is our &#39;sd_y&#39; value.&quot;) + coord_cartesian() That’s really noncommittal. I’ll stick with my half normal. You do you. Kruschke has this to say about the prior for the mean: In this application we seek broad priors relative to typical data, so that the priors have minimal influence on the posterior. One way to discover the constants is by asking an expert in the domain being studied. But in lieu of that, we will use the data themselves to tell us what the typical scale of the data is. We will set \\(M\\) to the mean of the data, and set \\(S\\) to a huge multiple (e.g., 100) of the standard deviation of the data. This way, no matter what the scale of the data is, the prior will be vague. (p. 455) In case you’re not following along closely in the text, we often use the normal distribution for the intercept and slope parameters in a simple regression model. By \\(M\\) and \\(S\\), Kruschke was referring to the \\(\\mu\\) and \\(\\sigma\\) parameters of the normal prior for our intercept. Here’s what that prior looks like in this data example. tibble(x = seq(from = -10000, to = 10000, by = 1)) %&gt;% mutate(d = dnorm(x, mean = mean_y, sd = sd_y * 100)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = &quot;grey67&quot;) + geom_vline(xintercept = mean_y, linetype = 2) + scale_y_continuous(breaks = NULL) + labs(subtitle = &quot;The grey density in the background is the normal prior for mu.\\nThe dashed vertical line is our &#39;mean_y&#39; value.&quot;) Yep, Kruschke’s right. That is one non-committal prior given our data. We could tighten that up by an order of magnitude and still have little influence on the posterior. Anyway, here’s how to put our priors to use with brms. fit16.1 &lt;- brm(data = my_data, family = gaussian, Score ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 16, file = &quot;fits/fit16.01&quot;) To be more explicit, the stanvars = stanvars argument at the bottom of our code is what allowed us to define our intercept prior as normal(mean_y, sd_y * 100) instead of requiring us to type in the parameters as normal(107.8413, 25.4452 * 100). The same basic point goes for our \\(\\sigma\\) prior. Also, notice our prior code for \\(\\sigma\\), prior(normal(0, sd_y), class = sigma). Nowhere in there did we actually say we wanted a half normal as opposed to a typical normal. That’s because the brms default is to set the lower bound for priors of class = sigma to zero. There’s no need for us to fiddle with it. Let’s examine the chains. plot(fit16.1) They look good! The model summary looks sensible, too. print(fit16.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Score ~ 1 ## Data: my_data (Number of observations: 63) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 107.78 3.32 101.05 114.23 1.00 3269 2484 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 25.78 2.33 21.66 30.88 1.00 3315 2518 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Compare those values with mean_y and sd_y. mean_y ## [1] 107.8413 sd_y ## [1] 25.4452 Good times. Let’s extract the posterior draws and save them in a data frame post. post &lt;- posterior_samples(fit16.1, add_chain = T) Here’s the leg work required to make our version of the three histograms in Figure 16.3. # we&#39;ll need this for `stat_pointintervalh()` library(tidybayes) # we&#39;ll use this to mark off the ROPEs as white strips in the background rope &lt;- tibble(key = c(&quot;Mean&quot;, &quot;Standard Deviation&quot;, &quot;Effect Size&quot;), xmin = c(99, 14, -.1), xmax = c(101, 16, .1)) # here are the primary data post %&gt;% transmute(Mean = b_Intercept, `Standard Deviation` = sigma) %&gt;% mutate(`Effect Size` = (Mean - 100) / `Standard Deviation`) %&gt;% gather() %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = &quot;white&quot;) + geom_histogram(aes(x = value), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + stat_pointintervalh(aes(x = value, y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 3) If we wanted those exact 95% HDIs, we’d execute this. post %&gt;% transmute(Mean = b_Intercept, `Standard Deviation` = sigma) %&gt;% mutate(`Effect Size` = (Mean - 100) / `Standard Deviation`) %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) ## # A tibble: 3 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Effect Size 0.299 0.0510 0.566 0.95 mode hdi ## 2 Mean 108. 102. 115. 0.95 mode hdi ## 3 Standard Deviation 25.4 21.3 30.4 0.95 mode hdi For the next part, we should look at the structure of the posterior draws, post. head(post) ## b_Intercept sigma lp__ chain iter ## 1 104.4952 27.43337 -303.1645 1 1001 ## 2 109.1532 23.62915 -302.6664 1 1002 ## 3 105.2486 23.75718 -302.8977 1 1003 ## 4 102.3687 26.10857 -303.7315 1 1004 ## 5 111.5466 24.77262 -303.0047 1 1005 ## 6 109.2983 22.53756 -303.2961 1 1006 By default, head() returned six rows, each one corresponding to the credible parameter values from a given posterior draw. Following our model equation \\(\\text{Score}_i \\sim N(\\mu, \\sigma)\\), we might reformat the first two columns as: Score ~ \\(N\\)(104.495, 27.433) Score ~ \\(N\\)(109.153, 23.629) Score ~ \\(N\\)(105.249, 23.757) Score ~ \\(N\\)(102.369, 26.109) Score ~ \\(N\\)(111.547, 24.773) Score ~ \\(N\\)(109.298, 22.538) Each row of post yields a full model equation with which we might credibly describe the data–or at least as credibly as we can within the limits of the model. We can give voice to a subset of these credible distributions with our version of the upper right panel of Figure 16.3. Before I show that plotting code, it might make sense to slow down on the preparatory data wrangling steps. There are several ways to overlay multiple posterior predictive density lines like those in our upcoming plots. We’ll practice a few over the next few chapters. For the method we’ll use in this chapter, it’s handy to first determine how many you’d like. Here we’ll follow Kruschke and choose 63, which we’ll save as n_lines. # how many credible density lines would you like? n_lines &lt;- 63 Now we’ve got our n_lines value, we’ll use it to subset the rows in post with the slice() function. We’ll then use expand() to include a sequence of Score values to correspond to the formula implied in each of the remaining rows of post. Notice how we also kept the iter index in the game. That will help us with the plot in a bit. But the main event is how we used Score, b_Intercept, and sigma as the input for the arguments in the dnorm(). The output is a column of the corresponding density values. post &lt;- post %&gt;% slice(1:n_lines) %&gt;% expand(nesting(b_Intercept, sigma, iter), Score = seq(from = 40, to = 250, by = 1)) %&gt;% mutate(density = dnorm(x = Score, mean = b_Intercept, sd = sigma)) str(post) ## tibble [13,293 × 5] (S3: tbl_df/tbl/data.frame) ## $ b_Intercept: num [1:13293] 99.3 99.3 99.3 99.3 99.3 ... ## $ sigma : num [1:13293] 26.1 26.1 26.1 26.1 26.1 ... ## $ iter : num [1:13293] 1008 1008 1008 1008 1008 ... ## $ Score : num [1:13293] 40 41 42 43 44 45 46 47 48 49 ... ## $ density : num [1:13293] 0.00115 0.00126 0.00137 0.00149 0.00162 ... Note that after using expand(), we have a rather long data frame. Anyway, we’re ready to plot. post %&gt;% ggplot(aes(x = Score)) + geom_histogram(data = my_data, aes(y = stat(density)), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = 5, boundary = 0) + geom_line(aes(y = density, group = iter), size = 1/4, alpha = 1/3, color = &quot;grey25&quot;) + scale_x_continuous(&quot;y&quot;, limits = c(50, 210)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Data with Post. Pred.&quot;) Note the stat(density) argument in the geom_histogram() function. That’s what rescaled the histogram to the density metric. If you leave that part out, all the density lines will drop to the bottom of the plot. Also, did you see how we used iter to group the density lines within the geom_line() function? That’s why we kept that information. Without that group = iter argument, the resulting lines are a mess. Kruschke pointed out this last plot constitutes a form of posterior-predictive check, by which we check whether the model appears to be a reasonable description of the data. With such a small amount of data, it is difficult to visually assess whether normality is badly violated, but there appears to be a hint that the normal model is straining to accommodate some outliers: The peak of the data protrudes prominently above the normal curves, and there are gaps under the shoulders of the normal curves. (p. 458) We can perform a similar posterior-predictive check with the brms::pp_check() function. By default, it will return 10 simulated density lines. Like we did above, we’ll increase that by setting the nsamples argument to our n_lines value. pp_check(fit16.1, nsamples = n_lines) Before we move on, we should talk a little about effect sizes, which we all but glossed over in our code. Effect size is simply the amount of change induced by the treatment relative to the standard deviation: \\((\\mu - 100) / \\sigma\\). In other words, the effect size is the “standardized” change… A conventionally “small” effect size in psychological research is 0.2 (Cohen, 1988), and the ROPE limits are set at half that size for purposed of illustration. (p. 457, emphasis in the original). Another way to describe this kind of effect size is as a standardized mean difference. In addition to the seminal work by Cohen, you might brush up on effect sizes with Kelley and Preacher’s (2012) On effect size. 16.2 Outliers and robust estimation: The \\(t\\) distribution Here’s the code for our version of Figure 16.4. # wrangle crossing(nu = c(Inf, 4, 2, 1), y = seq(from = -8, to = 8, length.out = 500)) %&gt;% mutate(density = dt(x = y, df = nu)) %&gt;% # this line is unnecessary, but will help with the plot legend mutate(nu = factor(nu, levels = c(&quot;Inf&quot;, &quot;4&quot;, &quot;2&quot;, &quot;1&quot;))) %&gt;% # plot ggplot(aes(x = y, y = density, group = nu, color = nu)) + geom_line() + scale_color_viridis_d(expression(paste(italic(t)[nu])), option = &quot;B&quot;, direction = 1, end = .8) + ylab(&quot;p(y)&quot;) + coord_cartesian(xlim = c(-6, 6)) + theme(legend.background = element_rect(fill = &quot;grey92&quot;), legend.key = element_rect(color = &quot;transparent&quot;, fill = &quot;transparent&quot;), legend.position = c(.92, .75)) Although the \\(t\\) distribution is usually conceived as a sampling distribution for the NHST \\(t\\) test, we will use it instead as a convenient descriptive model of data with outliers… Outliers are simply data values that fall unusually far from a model’s expected value. Real data often contain outliers relative to a normal distribution. Sometimes the anomalous values can be attributed to extraneous influences that can be explicitly identified, in which case the affected data values can be corrected or removed. But usually we have no way of knowing whether a suspected outlying value was caused by an extraneous influence, or is a genuine representation of the target being measured. Instead of deleting suspected outliers from the data according to some arbitrary criterion, we retain all the data but use a noise distribution that is less affected by outliers than is the normal distribution. (p. 459) Here’s Figure 16.5.a. tibble(y = seq(from = -10, to = 20, length.out = 1e3)) %&gt;% ggplot(aes(x = y)) + geom_ribbon(aes(ymin = 0, ymax = dnorm(y, mean = 2.5, sd = 5.73)), color = &quot;transparent&quot;, fill = &quot;grey50&quot;, alpha = 1/2) + geom_ribbon(aes(ymin = 0, ymax = metRology::dt.scaled(y, df = 1.14, mean = .12, sd = 1.47)), color = &quot;transparent&quot;, fill = &quot;grey50&quot;, alpha = 1/2) + geom_vline(xintercept = c(.12, 2.5), color = &quot;grey92&quot;, linetype = 3) + geom_point(data = tibble(y = c(-2:2, 15)), aes(y = 0), size = 2) + labs(title = &quot;Maximum Likelihood Estimates&quot;, y = &quot;p(y)&quot;) + coord_cartesian(xlim = c(-5, 15)) I’m not aware that we have the data for the bottom panel of Figure 16.5. However, we can simulate similar data with the rt.scaled() function from the metRology package (Ellison., 2018). set.seed(145) # simulate the data d &lt;- tibble(y = metRology::rt.scaled(n = 177, df = 2.63, mean = 1.11, sd = 0.15)) # plot tibble(y = seq(from = -3, to = 12, length.out = 1e3)) %&gt;% ggplot(aes(y)) + geom_histogram(data = d, aes(y = stat(density)), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = .1) + geom_line(aes(y = dnorm(y, mean = 1.16, sd = 0.63)), color = &quot;grey33&quot;) + geom_line(aes(y = metRology::dt.scaled(y, df = 2.63, mean = 1.11, sd = 0.15)), color = &quot;grey33&quot;) + scale_x_continuous(breaks = seq(from = -2, to = 10, by = 2)) + labs(title = &quot;Maximum Likelihood Estimates&quot;, y = &quot;p(y)&quot;) + coord_cartesian(xlim = c(-1.5, 10.25)) In case you were curious, this is how I selected the seed for the plot. Run the code yourself to get a sense of how it works. # in the R Notebook code block settings, I used: fig.width = 2, fig.height = 8 t_maker &lt;- function(seed) { set.seed(seed) tibble(y = metRology::rt.scaled(n = 177, df = 2.63, mean = 1.11, sd = 0.15)) %&gt;% summarise(min = min(y), max = max(y)) %&gt;% mutate(spread = max - min) } tibble(seed = 1:200) %&gt;% mutate(t = map(seed, t_maker)) %&gt;% unnest(t) %&gt;% ggplot(aes(x = reorder(seed, spread), ymin = min, ymax = max)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_linerange() + coord_flip() It is important to understand that the scale parameter \\(\\sigma\\) in the \\(t\\) distribution is not the standard deviation of the distribution. (Recall that the standard deviation is the square root of the variance, which is the expected value of the squared deviation from the mean, as defined back in Equation 4.8, p. 86.) The standard deviation is actually larger than \\(\\sigma\\) because of the heavy tails… While this value of the scale parameter is not the standard deviation of the distribution, it does have an intuitive relation to the spread of the data. Just as the range \\(\\pm \\sigma\\) covers the middle 68% of a normal distribution, the range \\(\\pm \\sigma\\) covers the middle 58% of a \\(t\\) distribution when \\(\\nu = 2\\), and the middle 50% when \\(\\nu = 1\\). These areas are illustrated in the left column of Figure 16.6. The right column of Figure 16.6 shows the width under the middle of a \\(t\\) distribution that is needed to span 68.27% of the distribution, which is the area under a normal distribution for \\(\\sigma = \\pm 1\\). (pp. 460–461, emphasis in the original) Speaking of which, here’s the code for the left column for Figure 16.6. # the primary data d &lt;- crossing(y = seq(from = -8, to = 8, length.out = 1e3), nu = c(Inf, 5, 2, 1)) %&gt;% mutate(label = str_c(&quot;nu == &quot;, nu) %&gt;% factor(., levels = c(&quot;nu == Inf&quot;, &quot;nu == 5&quot;, &quot;nu == 2&quot;, &quot;nu == 1&quot;))) # the subplot p1 &lt;- d %&gt;% ggplot(aes(x = y)) + geom_ribbon(aes(ymin = 0, ymax = dt(y, df = nu)), fill = &quot;grey67&quot;) + geom_ribbon(data = . %&gt;% filter(y &gt;= -1 &amp; y &lt;= 1), aes(ymin = 0, ymax = dt(y, df = nu)), fill = &quot;grey33&quot;) + # note how this function has its own data geom_text(data = tibble( y = 0, text = c(&quot;68%&quot;, &quot;64%&quot;, &quot;58%&quot;, &quot;50%&quot;), label = factor(c(&quot;nu == Inf&quot;, &quot;nu == 5&quot;, &quot;nu == 2&quot;, &quot;nu == 1&quot;))), aes(y = .175, label = text), color = &quot;grey92&quot;) + scale_y_continuous(&quot;p(y)&quot;, breaks = c(0, .2, .4)) + labs(subtitle = &quot;Shaded from y = - 1 to y = 1&quot;) + coord_cartesian(xlim = c(-6, 6)) + facet_wrap(~label, ncol = 1, labeller = label_parsed) Here’s the code for the right column. # the primary data d &lt;- tibble(nu = c(Inf, 5, 2, 1), ymin = c(-1.84, -1.32, -1.11, -1)) %&gt;% mutate(ymax = -ymin) %&gt;% expand(nesting(nu, ymin, ymax), y = seq(from = -8, to = 8, length.out = 1e3)) %&gt;% mutate(label = factor(str_c(&quot;nu==&quot;, nu), levels = str_c(&quot;nu==&quot;, c(Inf, 5, 2, 1)))) # the subplot p2 &lt;- d %&gt;% ggplot(aes(x = y)) + geom_ribbon(aes(ymin = 0, ymax = dt(y, df = nu)), fill = &quot;grey67&quot;) + geom_ribbon(data = . %&gt;% # notice our `filter()` argument has changed filter(y &gt;= ymin &amp; y &lt;= ymax), aes(ymin = 0, ymax = dt(y, df = nu)), fill = &quot;grey33&quot;) + annotate(geom = &quot;text&quot;, x = 0, y = .175, label = &quot;68%&quot;, color = &quot;grey92&quot;) + scale_y_continuous(&quot;p(y)&quot;, breaks = c(0, .2, .4)) + labs(subtitle = &quot;Shaded for the middle 68.27%&quot;) + coord_cartesian(xlim = c(-6, 6)) + facet_wrap(~label, ncol = 1, labeller = label_parsed) Now we’ll bind the two ggplots together with the patchwork package to make the full version of Figure 16.6. library(patchwork) p1 + p2 The use of a heavy-tailed distribution is often called robust estimation because the estimated value of the central tendency is stable, that is, “robust,” against outliers. The \\(t\\) distribution is useful as a likelihood function for modeling outliers at the level of observed data. But the \\(t\\) distribution is also useful for modeling outliers at higher levels in a hierarchical prior. We will encounter several applications. (p. 462, emphasis in the original) 16.2.1 Using the \\(t\\) distribution in JAGS brms. It’s easy to use Student’s \\(t\\) in brms. Make sure to specify family = student. By default, brms already sets the lower bound for \\(\\nu\\) to 1. But we do still need to use 1/29. To get a sense, let’s simulate exponential data using the rexp() function. Like Kruschke covered in the text (p. 462), the rexp() function has one parameter, rate, which is the reciprocal of the mean. Here we’ll set the mean to 29. n_draws &lt;- 1e7 mu &lt;- 29 set.seed(16) tibble(y = rexp(n = n_draws, rate = 1 / mu)) %&gt;% mutate(y_at_least_1 = ifelse(y &lt; 1, NA, y)) %&gt;% gather() %&gt;% group_by(key) %&gt;% summarise(mean = mean(value, na.rm = T)) ## # A tibble: 2 x 2 ## key mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 y 29.0 ## 2 y_at_least_1 30.0 The simulation showed that when we define the exponential rate as 1/29 and use the typical boundary at 0, the mean of our samples converges to 29. But when we only consider the samples of 1 or greater, the mean converges to 30. Thus, our exponential(1/29) prior with a boundary at 1 is how we get a shifted exponential distribution when we use it as our prior for \\(\\nu\\) in brms. Just make sure to remember that if you want the mean to be 30, you’ll need to specify the rate of 1/29. Also, Stan will bark if you simply try to set that exponential prior with the code prior(exponential(1/29), class = nu): DIAGNOSTIC(S) FROM PARSER: Info: integer division implicitly rounds to integer. Found int division: 1 / 29 Positive values rounded down, negative values rounded up or down in platform-dependent way. To avoid this, just do the division beforehand and save the results with stanvar(). stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) Here’s the brm() code. Note that we set the prior for our new \\(\\nu\\) parameter by specifying class = nu within the last prior() line. fit16.2 &lt;- brm(data = my_data, family = student, Score ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvars, seed = 16, file = &quot;fits/fit16.02&quot;) We can make the shifted exponential distribution (i.e., Figure 16.7) with simple addition. # how many draws would you like? n_draws &lt;- 1e6 # here are the data d &lt;- tibble(exp = rexp(n_draws, rate = 1/29)) %&gt;% transmute(exp_plus_1 = exp + 1, log_10_exp_plus_1 = log10(exp + 1)) # this is the plot in the top panel p1 &lt;- d %&gt;% ggplot(aes(x = exp_plus_1)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = 5, boundary = 1) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(exponential(lambda==29)~shifted~+1), x = expression(nu)) + coord_cartesian(xlim = c(1, 150)) # the bottom panel plot p2 &lt;- d %&gt;% ggplot(aes(x = log_10_exp_plus_1)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = .1, boundary = 0) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(log10(nu))) + coord_cartesian(xlim = c(0, 2.5)) # bind them together p1 / p2 Here are the scatter plots of Figure 16.8. pairs(fit16.2, off_diag_args = list(size = 1/3, alpha = 1/3)) I’m not aware of an easy way to use log10(nu) instead of nu with brms::pairs(). However, you can get those plots with posterior_samples() and a little wrangling. post &lt;- posterior_samples(fit16.2) post %&gt;% mutate(`log10(nu)` = log10(nu)) %&gt;% select(b_Intercept, sigma, `log10(nu)`) %&gt;% gather(key, value, -`log10(nu)`) %&gt;% ggplot(aes(x = `log10(nu)`, y = value)) + geom_point(color = &quot;grey50&quot;, size = 1/3, alpha = 1/3) + labs(x = expression(log10(nu)), y = NULL) + facet_grid(key~., scales = &quot;free&quot;, switch = &quot;y&quot;) If you want the Pearson’s correlation coefficients, you can use base R cor(). post %&gt;% mutate(`log10(nu)` = log10(nu)) %&gt;% select(b_Intercept, sigma, `log10(nu)`) %&gt;% cor() %&gt;% round(digits = 3) ## b_Intercept sigma log10(nu) ## b_Intercept 1.000 0.003 0.008 ## sigma 0.003 1.000 0.732 ## log10(nu) 0.008 0.732 1.000 The correlations among our parameters are a similar magnitude as those Kruschke presented in the text. Here are four of the panels for Figure 16.9. # we&#39;ll use this to mark off the ROPEs as white strips in the background rope &lt;- tibble(key = c(&quot;Mean&quot;, &quot;Scale&quot;, &quot;Effect Size&quot;), xmin = c(99, 14, -.1), xmax = c(101, 16, .1)) # here are the primary data post %&gt;% transmute(Mean = b_Intercept, Scale = sigma, Normality = log10(nu)) %&gt;% mutate(`Effect Size` = (Mean - 100) / Scale) %&gt;% gather() %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = &quot;white&quot;) + geom_histogram(aes(x = value), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + stat_pointintervalh(aes(x = value, y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 2) For the final panel of Figure 16.9, we’ll make our \\(t\\) lines in much the same way we did, earlier. But last time, we just took the first \\(\\mu\\) and \\(\\sigma\\) values from the first 63 rows of the post tibble. This time we’ll use dplyr::sample_n() to take random draws from the post rows instead. We tell sample_n() how many draws we’d like with the size argument. Since sample_n() tells us which rows it drew from in the form of rownames, we then use the tibble::rownames_to_column() function to save those rownames in an actual column named draw–this is the variable we group the lines by in geom_line(). In addition to the change in our row selection strategy, this time we’ll slightly amend the code within the last mutate() line. Since we’d like to work with the \\(t\\) distribution, we specified metRology::dt.scaled() function instead of dnorm(). # how many credible density lines would you like? n_lines &lt;- 63 # setting the seed makes the results from `sample_n()` reproducible set.seed(16) # wragle post %&gt;% sample_n(size = n_lines) %&gt;% rownames_to_column(var = &quot;draw&quot;) %&gt;% expand(nesting(b_Intercept, sigma, nu, draw), Score = seq(from = 40, to = 250, by = 1)) %&gt;% mutate(density = metRology::dt.scaled(x = Score, df = nu, mean = b_Intercept, sd = sigma)) %&gt;% # plot ggplot(aes(x = Score)) + geom_histogram(data = my_data, aes(y = stat(density)), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = 5, boundary = 0) + geom_line(aes(y = density, group = draw), size = 1/4, alpha = 1/3, color = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Data with Post. Pred.&quot;, x = &quot;y&quot;) + coord_cartesian(xlim = c(50, 210)) Much like Kruschke mused in the text, this plot shows that the posterior predictive \\(t\\) distributions appear to describe the data better than the normal distribution in Figure 16.3, insofar as the data histogram does not poke out at the mode and the gaps under the shoulders are smaller. (p. 464) In case you were wondering, here’s the model summary(). summary(fit16.2) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: Score ~ 1 ## Data: my_data (Number of observations: 63) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 107.21 2.73 101.94 112.58 1.00 2282 2198 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 19.44 3.39 13.15 26.63 1.00 1359 2053 ## nu 8.89 12.82 1.83 41.30 1.00 1528 2063 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It’s easy to miss how \\(\\sigma\\) in the robust estimate is much smaller than in the normal estimate. What we had interpreted as increased standard deviation induced by the smart drug might be better described as increased outliers. Both of these differences, that is, \\(\\mu\\) more tightly estimated and \\(\\sigma\\) smaller in magnitude, are a result of there being outliers in the data. The only way a normal distribution can accommodate the outliers is to use a large value for \\(\\sigma\\). In turn, that leads to “slop” in the estimate of \\(\\mu\\) because there is a wider range of \\(\\mu\\) values that reasonably fit the data when the standard deviation is large. (p. 464) We can use the brms::VarCorr() function to pull the summary statistics for \\(\\sigma\\) from both models. VarCorr(fit16.1)$residual__$sd ## Estimate Est.Error Q2.5 Q97.5 ## 25.77896 2.326489 21.66347 30.88419 VarCorr(fit16.2)$residual__$sd ## Estimate Est.Error Q2.5 Q97.5 ## 19.44218 3.391467 13.1486 26.63353 It is indeed the case that estimate for \\(\\sigma\\) is smaller in the \\(t\\) model. That smaller \\(\\sigma\\) resulted in a more precise estimate for \\(\\mu\\), as can be seen in the ‘Est.Error’ columns from the fixef() output. fixef(fit16.1) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 107.7823 3.317812 101.0451 114.2292 fixef(fit16.2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 107.2099 2.731697 101.9441 112.5784 Here that is in a coefficient plot using tidybayes::stat_intervalh(). bind_rows(posterior_samples(fit16.1) %&gt;% select(b_Intercept), posterior_samples(fit16.2) %&gt;% select(b_Intercept)) %&gt;% mutate(fit = rep(c(&quot;fit16.1&quot;, &quot;fit16.2&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = b_Intercept, y = fit)) + stat_intervalh(point_interval = mode_hdi, .width = c(.5, .8, .95)) + scale_color_grey(&quot;HDI&quot;, start = 2/3, end = 0, labels = c(&quot;95%&quot;, &quot;80%&quot;, &quot;50%&quot;)) + ylab(NULL) + theme(legend.key.size = unit(0.45, &quot;cm&quot;)) 16.2.2 Using the \\(t\\) distribution in Stan. Kruschke expressed concern about high autocorrelations in the chains of his JAGS model. Here are the results of our Stan/brms attempt. post &lt;- posterior_samples(fit16.2, add_chain = T) library(bayesplot) mcmc_acf(post, pars = c(&quot;b_Intercept&quot;, &quot;sigma&quot;, &quot;nu&quot;), lags = 35) For all three parameters, the autocorrelations were near zero by lag 3 or 4. Not bad. The \\(N_{eff}/N\\) ratios are okay. neff_ratio(fit16.2) %&gt;% mcmc_neff() + yaxis_text(hjust = 0) The trace plots look fine. plot(fit16.2) The values for nu are pretty skewed, but hopefully it makes sense to you why that might be the case. Here are the overlaid density plots. mcmc_dens_overlay(post, pars = c(&quot;b_Intercept&quot;, &quot;sigma&quot;, &quot;nu&quot;)) The \\(\\hat R\\) values are right where we like them. rhat(fit16.2) ## b_Intercept sigma nu lp__ ## 1.000481 1.004534 1.001197 1.002847 If you peer into the contents of a brm() fit object (e.g., fit16.2 %&gt;% str()), you’ll discover it contains the Stan code. Here it is for our fit16.2. fit16.2$fit@stanmodel ## S4 class stanmodel &#39;492bc3ca2c99d9c80c80ca16542cd5bb&#39; coded as follows: ## // generated with brms 2.11.5 ## functions { ## ## /* compute the logm1 link ## * Args: ## * p: a positive scalar ## * Returns: ## * a scalar in (-Inf, Inf) ## */ ## real logm1(real y) { ## return log(y - 1); ## } ## /* compute the inverse of the logm1 link ## * Args: ## * y: a scalar in (-Inf, Inf) ## * Returns: ## * a positive scalar ## */ ## real expp1(real y) { ## return exp(y) + 1; ## } ## } ## data { ## int&lt;lower=1&gt; N; // number of observations ## vector[N] Y; // response variable ## int prior_only; // should the likelihood be ignored? ## real mean_y; ## real sd_y; ## real one_over_twentynine; ## } ## transformed data { ## } ## parameters { ## real Intercept; // temporary intercept for centered predictors ## real&lt;lower=0&gt; sigma; // residual SD ## real&lt;lower=1&gt; nu; // degrees of freedom or shape ## } ## transformed parameters { ## } ## model { ## // initialize linear predictor term ## vector[N] mu = Intercept + rep_vector(0, N); ## // priors including all constants ## target += normal_lpdf(Intercept | mean_y, sd_y * 100); ## target += normal_lpdf(sigma | 0, sd_y) ## - 1 * normal_lccdf(0 | 0, sd_y); ## target += exponential_lpdf(nu | one_over_twentynine) ## - 1 * exponential_lccdf(1 | one_over_twentynine); ## // likelihood including all constants ## if (!prior_only) { ## target += student_t_lpdf(Y | nu, mu, sigma); ## } ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = Intercept; ## } ## Note the last line in the parameters block, “real&lt;lower=1&gt; nu; // degrees of freedom or shape.” By default, brms set the lower bound for \\(\\nu\\) to 1. Just for kicks and giggles, the pp_check() offers us a handy way to compare the performance of our Gaussian fit16.2 and our Student’s \\(t\\) fit16.2. If we set type = &quot;ecdf_overlay&quot; within pp_check(), we’ll get the criterion Score displayed as a cumulative distribution function (CDF) rather than a typical density. Then, pp_check() presents CDF’s based on draws from the posterior for comparison. Just like with the default pp_check() plots, we like it when those simulated distributions mimic the one from the original data. # fit16.1 with Gaus set.seed(16) p1 &lt;- pp_check(fit16.1, nsamples = n_lines, type = &quot;ecdf_overlay&quot;) + labs(subtitle = &quot;fit16.1 with `family = gaussian`&quot;) + coord_cartesian(xlim = range(my_data$Score)) + theme(legend.position = &quot;none&quot;) # fit16.2 with Student&#39;s t p2 &lt;- pp_check(fit16.2, nsamples = n_lines, type = &quot;ecdf_overlay&quot;) + labs(subtitle = &quot;fit16.2 with `family = student`&quot;) + coord_cartesian(xlim = range(my_data$Score)) # combine the subplots p1 + p2 It’s subtle, but you might notice that the simulated CDFs from fit16.1 have steeper slopes in the middle when compared to the original data in the dark blue. However, the fit16.2-based simulated CDFs match up more closely with the original data. This suggests an edge for fit16.2. Revisiting our skills from Chapter 10, we might also compare their model weights. model_weights(fit16.1, fit16.2) %&gt;% round(digits = 6) ## fit16.1 fit16.2 ## 0.000004 0.999996 Almost all the stacking weight (see Yao et al., 2018) went to fit16.2, our robust Student’s \\(t\\) model. 16.3 Two groups When there are two groups, we estimate the mean and scale for each group. When using \\(t\\) distributions for robust estimation, we could also estimate the normality of each group separately. But because there usually are relatively few outliers, we will use a single normality parameter to describe both groups, so that the estimate of the normality is more stably estimated. (p. 468) Since we subset the data, earlier, we’ll just reload it to get the full data set. my_data &lt;- read_csv(&quot;data.R/TwoGroupIQ.csv&quot;) This time, we’ll compute mean_y and sd_y from the full data. (mean_y &lt;- mean(my_data$Score)) ## [1] 104.1333 (sd_y &lt;- sd(my_data$Score)) ## [1] 22.43532 stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) Within the brms framework, Bürkner calls it distributional modeling when you model more than the mean. Since we’re now modeling both \\(\\mu\\) and \\(\\sigma\\), we’re fitting a distributional model. When doing so with brms, you typically wrap your formula syntax into the bf() function. It’s also important to know that when modeling \\(\\sigma\\), brms defaults to modeling its log. So we’ll use log(sd_y) in its prior. For more on all this, see Bürkner’s (2020c) vignette, Estimating distributional models with brms. fit16.3 &lt;- brm(data = my_data, family = student, bf(Score ~ 0 + Group, sigma ~ 0 + Group), prior = c(prior(normal(mean_y, sd_y * 100), class = b), prior(normal(0, log(sd_y)), class = b, dpar = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvars, seed = 16, file = &quot;fits/fit16.03&quot;) Did you catch our ~ 0 + Group syntax? That suppressed the usual intercept for our estimates of both \\(\\mu\\) and \\(\\log (\\sigma)\\). Since Group is a categorical variable, that results in brm() fitting separate intercepts for each category. This is our brms analogue to the x[i] syntax Kruschke mentioned on page 468. It’s what allowed us to estimate \\(\\mu_j\\) and \\(\\log (\\sigma_j)\\). Let’s look at the model summary. print(fit16.3) ## Family: student ## Links: mu = identity; sigma = log; nu = identity ## Formula: Score ~ 0 + Group ## sigma ~ 0 + Group ## Data: my_data (Number of observations: 120) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## GroupPlacebo 99.29 1.72 95.96 102.74 1.00 4043 3012 ## GroupSmartDrug 107.11 2.65 102.03 112.41 1.00 5034 2787 ## sigma_GroupPlacebo 2.38 0.16 2.06 2.67 1.00 3632 2706 ## sigma_GroupSmartDrug 2.84 0.15 2.54 3.13 1.00 3238 2740 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## nu 3.56 1.42 1.81 7.12 1.00 2648 2153 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Remember that the \\(\\sigma\\)s are now in the log scale. If you want a quick conversion, you might just exponentiate the point estimates from fixef(). fixef(fit16.3)[3:4, 1] %&gt;% exp() ## sigma_GroupPlacebo sigma_GroupSmartDrug ## 10.75387 17.04000 But please don’t stop there. Get your hands dirty with the full posterior. Speaking of which, if we want to make the histograms in Figure 16.12, we’ll need to first extract the posterior samples. post &lt;- posterior_samples(fit16.3) glimpse(post) ## Rows: 4,000 ## Columns: 6 ## $ b_GroupPlacebo &lt;dbl&gt; 95.05456, 97.65789, 100.74824, 100.35265, 98.35897, 100.39547, 97.… ## $ b_GroupSmartDrug &lt;dbl&gt; 98.87536, 102.64673, 107.50948, 109.26261, 104.65081, 106.44534, 1… ## $ b_sigma_GroupPlacebo &lt;dbl&gt; 2.521396, 2.296794, 2.515365, 2.304293, 2.249436, 2.596969, 2.4919… ## $ b_sigma_GroupSmartDrug &lt;dbl&gt; 2.919710, 3.028132, 2.658009, 3.026779, 2.756936, 3.010234, 2.9466… ## $ nu &lt;dbl&gt; 2.487891, 7.475595, 4.669025, 2.521117, 4.123748, 6.066377, 3.2276… ## $ lp__ &lt;dbl&gt; -557.6583, -554.3776, -551.5875, -551.1064, -550.4912, -550.1686, … Along with transforming the metrics of a few of the parameters, we may as well rename them to match those in the text. post &lt;- post %&gt;% transmute(`Placebo Mean` = b_GroupPlacebo, `Smart Drug Mean` = b_GroupSmartDrug, # we need to transform the next three parameters `Placebo Scale` = b_sigma_GroupPlacebo %&gt;% exp(), `Smart Drug Scale` = b_sigma_GroupSmartDrug %&gt;% exp(), Normality = nu %&gt;% log10()) %&gt;% mutate(`Difference of Means` = `Smart Drug Mean` - `Placebo Mean`, `Difference of Scales` = `Smart Drug Scale` - `Placebo Scale`, `Effect Size` = (`Smart Drug Mean` - `Placebo Mean`) / sqrt((`Smart Drug Scale`^2 + `Placebo Scale`^2) / 2)) glimpse(post) ## Rows: 4,000 ## Columns: 8 ## $ `Placebo Mean` &lt;dbl&gt; 95.05456, 97.65789, 100.74824, 100.35265, 98.35897, 100.39547, 97.… ## $ `Smart Drug Mean` &lt;dbl&gt; 98.87536, 102.64673, 107.50948, 109.26261, 104.65081, 106.44534, 1… ## $ `Placebo Scale` &lt;dbl&gt; 12.445965, 9.942252, 12.371126, 10.017089, 9.482389, 13.422995, 12… ## $ `Smart Drug Scale` &lt;dbl&gt; 18.53591, 20.65861, 14.26785, 20.63068, 15.75150, 20.29214, 19.041… ## $ Normality &lt;dbl&gt; 0.3958314, 0.8736458, 0.6692262, 0.4015930, 0.6152921, 0.7829294, … ## $ `Difference of Means` &lt;dbl&gt; 3.820800, 4.988845, 6.761232, 8.909960, 6.291841, 6.049876, 6.6027… ## $ `Difference of Scales` &lt;dbl&gt; 6.089950, 10.716360, 1.896728, 10.613589, 6.269113, 6.869148, 6.95… ## $ `Effect Size` &lt;dbl&gt; 0.2420162, 0.3077346, 0.5063376, 0.5494289, 0.4839695, 0.3516574, … Now we’re ready for the bulk of Figure 16.12. # we&#39;ll use this to mark off the ROPEs as white strips in the background rope &lt;- tibble(key = factor(c(&quot;Difference of Means&quot;, &quot;Difference of Scales&quot;, &quot;Effect Size&quot;), levels = c(&quot;Placebo Mean&quot;, &quot;Smart Drug Mean&quot;, &quot;Placebo Scale&quot;, &quot;Difference of Means&quot;, &quot;Smart Drug Scale&quot;, &quot;Difference of Scales&quot;, &quot;Normality&quot;, &quot;Effect Size&quot;)), xmin = c(-1, -1, -.1), xmax = c(1, 1, .1)) # here are the primary data post %&gt;% gather() %&gt;% # this isn&#39;t necessary, but it arranged our subplots like those in the text mutate(key = factor(key, levels = c(&quot;Placebo Mean&quot;, &quot;Smart Drug Mean&quot;, &quot;Placebo Scale&quot;, &quot;Difference of Means&quot;, &quot;Smart Drug Scale&quot;, &quot;Difference of Scales&quot;, &quot;Normality&quot;, &quot;Effect Size&quot;))) %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = &quot;white&quot;) + geom_histogram(aes(x = value), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + stat_pointintervalh(aes(x = value, y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 2) Now let’s make the upper two panels in the right column of Figure 16.12. You might note a few things in our wrangling code. First, in the mutate() function, we defined the density values for the two Group conditions one at a time. If you look carefully within those definitions, you’ll see we used 10^Normality for the df argument, rather than just Normality. Why? Remember how a few code blocks up we transformed the original nu column to Normality by placing it within log10()? Well, if you want to undo that, you have to take 10 to the power of Normality. Next, notice that we subset the post tibble with select(). This wasn’t technically necessary, but it made the next line easier. Finally, with the gather() function, we transformed the Placebo and Smart Drug columns into an index column strategically named Group, which matched up with the original my_data data, and a density column, which contained the actual density values for the lines. I know. That’s a lot to take in. If you’re confused, run the lines in the code one at a time to see how they work. But anyway, here’s the final result! # how many credible density lines would you like? n_lines &lt;- 63 # setting the seed makes the results from `sample_n()` reproducible set.seed(16) # wragle post %&gt;% sample_n(size = n_lines) %&gt;% rownames_to_column(var = &quot;draw&quot;) %&gt;% expand(nesting(draw, `Placebo Mean`, `Smart Drug Mean`, `Placebo Scale`, `Smart Drug Scale`, Normality), Score = seq(from = 40, to = 250, by = 1)) %&gt;% mutate(Placebo = metRology::dt.scaled(x = Score, df = 10^Normality, mean = `Placebo Mean`, sd = `Placebo Scale`), `Smart Drug` = metRology::dt.scaled(x = Score, df = 10^Normality, mean = `Smart Drug Mean`, sd = `Smart Drug Scale`)) %&gt;% select(draw, Score:`Smart Drug`) %&gt;% gather(Group, density, -draw, -Score) %&gt;% # plot ggplot(aes(x = Score)) + geom_histogram(data = my_data, aes(y = stat(density)), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = 5, boundary = 0) + geom_line(aes(y = density, group = draw), size = 1/4, alpha = 1/3, color = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Data with Post. Pred.&quot;, x = &quot;y&quot;) + coord_cartesian(xlim = c(50, 210)) + facet_wrap(~Group) 16.3.1 Analysis by NHST. Here’s how we might perform a \\(t\\)-test. t.test(data = my_data, Score ~ Group) ## ## Welch Two Sample t-test ## ## data: Score by Group ## t = -1.958, df = 111.44, p-value = 0.05273 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -15.70602585 0.09366161 ## sample estimates: ## mean in group Placebo mean in group Smart Drug ## 100.0351 107.8413 To help with the comparison, notice how the 95% CIs from the \\(t\\)-test range from -15.7 to 0.1. That indicates that the \\(t\\)-test subtracted the Smart Drug from Placebo. I point this out because the difference scores we computed along with Kruschke for Figure 16.12 did the subtraction in the other direction. But don’t fret. The magnitude of the difference stays the same either way. Only the signs changed. For example: fixef(fit16.3)[&quot;GroupPlacebo&quot;, 1] - fixef(fit16.3)[&quot;GroupSmartDrug&quot;, 1] ## [1] -7.817559 fixef(fit16.3)[&quot;GroupSmartDrug&quot;, 1] - fixef(fit16.3)[&quot;GroupPlacebo&quot;, 1] ## [1] 7.817559 But anyway, in addition to the difference distributions from Figure 16.12, we might also use the brms::hypothesis() function to do something of a Bayesian \\(t\\)-test. hypothesis(fit16.3, &quot;GroupPlacebo - GroupSmartDrug = 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star ## 1 (GroupPlacebo-Gro... = 0 -7.82 3.15 -14.18 -1.66 NA NA * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. See how our 95% ETIs are both negative and how we have that little * in the Star column? If you like the visual approach, you can even feed the hypothesis() code into plot(). hypothesis(fit16.3, &quot;GroupPlacebo - GroupSmartDrug = 0&quot;) %&gt;% plot() Back to the text: The reason that the \\(t\\) test is less sensitive than the Bayesian estimation in this example is that the \\(t\\) test assumes normality and therefore its estimate of the within-group variances is too large when there are outliers. The \\(t\\) test has other problems. Unlike the Bayesian analysis, the \\(t\\) test provides only a test of the equality of means, without a test of the equality of variances. To test equality of variances, we need to run an additional test, namely an \\(F\\) test of the ratio of variances, which would inflate the \\(p\\) values of both tests. Moreover, both tests compute \\(p\\) values based on hypothetical normally distributed data, and the \\(F\\) test is particularly sensitive to violations of this assumption. Therefore it would be better to use resampling methods to compute the \\(p\\) values (and correcting them for multiple tests). (pp. 471–472) Don’t do all that. Use robust Bayes, instead. Just for kicks, we can compare the \\(\\sigma\\)s with hypothesis(), too. hypothesis(fit16.3, &quot;exp(sigma_GroupPlacebo) - exp(sigma_GroupSmartDrug) = 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star ## 1 (exp(sigma_GroupP... = 0 -6.35 2.72 -12.02 -1.23 NA NA * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. See? We don’t need an \\(F\\) test. 16.4 Other noise distributions and transforming data It’s worth repeating a portion of this section. If the initially assumed noise distribution does not match the data distribution, there are two ways to pursue a better description. The preferred way is to use a better noise distribution. The other way is to transform the data to a new scale so that they tolerably match the shape of the assumed noise distribution. In other words, we can either change the shoe to fit the foot, or we can squeeze the foot to fit in the shoe. Changing the shoe is preferable to squeezing the foot. In traditional statistical software, users were stuck with the pre-packaged noise distribution, and had no way to change it, so they transformed their data and squeezed them into the software. This practice can lead to confusion in interpreting the parameters because they are describing the transformed data, not the data on the original scale. In software such as [brms, we can spend less time squeezing our feet into ill-fitting shoes]. (p. 472) We’ll have more practice with the robust Student’s \\(t\\) in some of the following chapters. But if you’d like even more, you might check out my blog post on the topic, Robust Linear Regression with Student’s \\(t\\)-Distribution, and the companion post, Bayesian robust correlations with brms (and why you should love Student’s \\(t\\)). Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bayesplot_1.7.1 patchwork_1.0.0 tidybayes_2.0.3.9000 brms_2.12.0 ## [5] Rcpp_1.0.4.6 forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 ## [9] purrr_0.3.4 readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ## [13] ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 ## [9] farver_2.0.3 rstan_2.19.3 svUnit_1.0.3 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 ## [17] bridgesampling_1.0-0 robustbase_0.93-6 knitr_1.28 shinythemes_1.1.2 ## [21] jsonlite_1.6.1 broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 ## [25] compiler_3.6.3 httr_1.4.1 backports_1.1.6 assertthat_0.2.1 ## [29] Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 later_1.0.0 ## [33] htmltools_0.4.0 prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 ## [41] cellranger_1.1.0 vctrs_0.3.0 nlme_3.1-144 crosstalk_1.1.0.1 ## [45] xfun_0.13 ps_1.3.3 rvest_0.3.5 mime_0.9 ## [49] miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 DEoptimR_1.0-8 ## [53] MASS_7.3-51.5 zoo_1.8-7 scales_1.1.1 colourpicker_1.0 ## [57] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 ## [61] inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 ## [65] loo_2.2.0 StanHeaders_2.21.0-1 stringi_1.4.6 dygraphs_1.1.1.6 ## [69] pkgbuild_1.0.8 rlang_0.4.6 pkgconfig_2.0.3 matrixStats_0.56.0 ## [73] HDInterval_0.2.0 evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [77] htmlwidgets_1.5.1 labeling_0.3 tidyselect_1.0.0 processx_3.4.2 ## [81] plyr_1.8.6 magrittr_1.5 bookdown_0.18 R6_2.4.1 ## [85] generics_0.0.2 DBI_1.1.0 pillar_1.4.4 haven_2.2.0 ## [89] withr_2.2.0 xts_0.12-0 abind_1.4-5 metRology_0.9-28-1 ## [93] modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 ## [97] rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 callr_3.4.3 ## [101] threejs_0.3.3 reprex_0.3.0 digest_0.6.25 xtable_1.8-4 ## [105] numDeriv_2016.8-1.1 httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 ## [109] viridisLite_0.3.0 shinyjs_1.1 References "],
["metric-predicted-variable-with-one-metric-predictor.html", "17 Metric Predicted Variable with One Metric Predictor 17.1 Simple linear regression 17.2 Robust linear regression 17.3 Hierarchical regression on individuals within groups 17.4 Quadratic trend and weighted data 17.5 Procedure and perils for expanding a model Session info", " 17 Metric Predicted Variable with One Metric Predictor We will initially describe the relationship between the predicted variable, \\(y\\) and predictor, \\(x\\), with a simple linear model and normally distributed residual randomness in \\(y\\). This model is often referred to as ‘simple linear regression.’ We will generalize the model in three ways. First, we will give it a noise distribution that accommodates outliers, which is to say that we will replace the normal distribution with a \\(t\\) distribution as we did in the previous chapter. The model will be implemented in [brms]. Next, we will consider differently shaped relations between the predictor and the predicted, such as quadratic trend. Finally, we will consider hierarchical models of situations in which every individual has data that can be described by an individual trend, and we also want to estimate group-level typical trends across individuals. (Kruschke, 2015, p. 478) 17.1 Simple linear regression It wasn’t entirely clear how Kruschke simulated the bimodal data on the right panel of Figure 17.1. I figured an even split of two Gaussians would suffice and just sighted their \\(\\mu\\)s and \\(\\sigma\\)s. library(tidyverse) # how many draws per panel would you like? n_draw &lt;- 1000 set.seed(17) d &lt;- tibble(panel = rep(letters[1:2], each = n_draw), x = c(runif(n = n_draw, -10, 10), rnorm(n = n_draw / 2, -6, 2), rnorm(n = n_draw / 2, 3, 2))) %&gt;% mutate(y = 10 + 2 * x + rnorm(n = n(), 0, 2)) head(d) ## # A tibble: 6 x 3 ## panel x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a -6.90 -4.09 ## 2 a 9.37 28.6 ## 3 a -0.635 11.8 ## 4 a 5.54 23.3 ## 5 a -1.84 10.9 ## 6 a 0.776 10.9 In case you missed it, Kruschke defied the formula for these data in Figure 17.1. It is \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Normal} (\\mu, \\sigma = 2), \\text{where} \\\\ \\mu &amp; = 10 + 2 x_i. \\end{align*}\\] “Note that the model only specifies the dependency of \\(y\\) on \\(x\\). The model does not say anything about what generates \\(x\\), and there is no probability distribution assumed for describing \\(x\\)” (p. 479). Let this sink into your soul. It took a long time, for me. E.g., a lot of people fret over the distributions of their \\(x\\) variables. Now one might should examine them to make sure nothing looks off, such as for data coding mistakes. But if they’re not perfectly or even approximately Gaussian, that isn’t necessarily an issue. The typical linear model makes no presumption about the distribution of the predictors. Often times, the largest issue is whether the \\(x\\) variables are categorical or continuous. Before we make our Figure 17.1, we’ll want to make a separate tibble of the values necessary to plot those sideways Gaussians. Here are the steps. curves &lt;- # define the 3 x-values we want the Gaussians to originate from tibble(x = seq(from = -7.5, to = 7.5, length.out = 4)) %&gt;% # use the formula 10 + 2x to compute the expected y-value for x mutate(y_mean = 10 + (2 * x)) %&gt;% # based on a Gaussian with `mean = y_mean` and `sd = 2`, compute the 99% intervals mutate(ll = qnorm(.005, mean = y_mean, sd = 2), ul = qnorm(.995, mean = y_mean, sd = 2)) %&gt;% # now use those interval bounds to make a sequence of y-values mutate(y = map2(ll, ul, seq, length.out = 100)) %&gt;% # since that operation returned a nested column, we need to `unnest()` unnest(y) %&gt;% # compute the density values mutate(density = map2_dbl(y, y_mean, dnorm, sd = 2)) %&gt;% # now rescale the density values to be wider. # since we want these to be our x-values, we&#39;ll # just redefine the x column with these results mutate(x = x - density * 2 / max(density)) str(curves) ## tibble [400 × 6] (S3: tbl_df/tbl/data.frame) ## $ x : num [1:400] -7.57 -7.58 -7.59 -7.61 -7.62 ... ## $ y_mean : num [1:400] -5 -5 -5 -5 -5 -5 -5 -5 -5 -5 ... ## $ ll : num [1:400] -10.2 -10.2 -10.2 -10.2 -10.2 ... ## $ ul : num [1:400] 0.152 0.152 0.152 0.152 0.152 ... ## $ y : num [1:400] -10.15 -10.05 -9.94 -9.84 -9.74 ... ## $ density: num [1:400] 0.00723 0.00826 0.0094 0.01068 0.01209 ... Now we’re ready to make Figure 17.1. theme_set(theme_grey() + theme(panel.grid = element_blank())) d %&gt;% ggplot(aes(x = x, y = y)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point(size = 1/3, alpha = 1/3) + stat_smooth(method = &quot;lm&quot;, se = F, fullrange = T) + geom_path(data = curves, aes(group = y_mean), color = &quot;blue&quot;, size = 1) + labs(title = &quot;Normal PDF around Linear Function&quot;, subtitle = &quot;We simulated x from a uniform distribution in the left panel and simulated it from a mixture of\\n two Gaussians on the right.&quot;) + coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 30)) + theme(strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(~panel) ## `geom_smooth()` using formula &#39;y ~ x&#39; Concerning causality, the simple linear model makes no claims about causal connections between \\(x\\) and \\(y\\). The simple linear model merely describes a tendency for \\(y\\) values to be linearly related to \\(x\\) values, hence “predictable” from the \\(x\\) values. When describing data with this model, we are starting with a scatter plot of points generated by an unknown process in the real world, and estimating parameter values that would produce a smattering of points that might mimic the real data. Even if the descriptive model mimics the data well (and it might not), the mathematical “process” in the model may have little if anything to do with the real-world process that created the data. Nevertheless, the parameters in the descriptive model are meaningful because they describe tendencies in the data. (p. 479, emphasis added) I emphasized these points because I’ve heard and seen a lot of academics conflate linear regression models with causal models. For sure, it might well be preferable if your regression model was also a causal model. But good old prediction is fine, too. 17.2 Robust linear regression There is no requirement to use a normal distribution for the noise distribution. The normal distribution is traditional because of its relative simplicity in mathematical derivations. But real data may have outliers, and the use of (optionally) heavy-tailed noise distributions is straight forward in contemporary Bayesian software[, like brms]. (pp. 479–480) Here’s Kruschke’s HtWtDataGenerator() code. HtWtDataGenerator &lt;- function(nSubj, rndsd = NULL, maleProb = 0.50) { # Random height, weight generator for males and females. Uses parameters from # Brainard, J. &amp; Burmaster, D. E. (1992). Bivariate distributions for height and # weight of men and women in the United States. Risk Analysis, 12(2), 267-275. # Kruschke, J. K. (2011). Doing Bayesian data analysis: # A Tutorial with R and BUGS. Academic Press / Elsevier. # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition: # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier. # require(MASS) # Specify parameters of multivariate normal (MVN) distributions. # Men: HtMmu &lt;- 69.18 HtMsd &lt;- 2.87 lnWtMmu &lt;- 5.14 lnWtMsd &lt;- 0.17 Mrho &lt;- 0.42 Mmean &lt;- c(HtMmu, lnWtMmu) Msigma &lt;- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd, Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2) # Women cluster 1: HtFmu1 &lt;- 63.11 HtFsd1 &lt;- 2.76 lnWtFmu1 &lt;- 5.06 lnWtFsd1 &lt;- 0.24 Frho1 &lt;- 0.41 prop1 &lt;- 0.46 Fmean1 &lt;- c(HtFmu1, lnWtFmu1) Fsigma1 &lt;- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1, Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2) # Women cluster 2: HtFmu2 &lt;- 64.36 HtFsd2 &lt;- 2.49 lnWtFmu2 &lt;- 4.86 lnWtFsd2 &lt;- 0.14 Frho2 &lt;- 0.44 prop2 &lt;- 1 - prop1 Fmean2 &lt;- c(HtFmu2, lnWtFmu2) Fsigma2 &lt;- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2, Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2) # Randomly generate data values from those MVN distributions. if (!is.null(rndsd)) {set.seed(rndsd)} datamatrix &lt;- matrix(0, nrow = nSubj, ncol = 3) colnames(datamatrix) &lt;- c(&quot;male&quot;, &quot;height&quot;, &quot;weight&quot;) maleval &lt;- 1; femaleval &lt;- 0 # arbitrary coding values for (i in 1:nSubj) { # Flip coin to decide sex sex &lt;- sample(c(maleval, femaleval), size = 1, replace = TRUE, prob = c(maleProb, 1 - maleProb)) if (sex == maleval) {datum = MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)} if (sex == femaleval) { Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2)) if (Fclust == 1) {datum = MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)} if (Fclust == 2) {datum = MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)} } datamatrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1)) } return(datamatrix) } Let’s take this baby for a spin to simulate our data. d &lt;- HtWtDataGenerator(nSubj = 300, rndsd = 17, maleProb = .50) %&gt;% as_tibble() %&gt;% # this will allow us to subset 30 of the values into their own group mutate(subset = rep(0:1, times = c(9, 1)) %&gt;% rep(., 30)) head(d) ## # A tibble: 6 x 4 ## male height weight subset ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0 63.3 167. 0 ## 2 0 62.4 126. 0 ## 3 1 66.4 124 0 ## 4 0 62.9 148. 0 ## 5 1 65.5 151. 0 ## 6 1 71.4 234. 0 Fortunately, we do not have to worry much about analytical derivations because we can let JAGS or Stan generate a high resolution picture of the posterior distribution. Our job, therefore, is to specify sensible priors and to make sure that the MCMC process generates a trustworthy posterior sample that is converged and well mixed. (p. 483) 17.2.1 Robust linear regression in JAGS brms. Presuming a data set with a sole standardized predictor x_z for a sole standardized criterion y_z, the basic brms code corresponding to the JAGS code Kruschke showed on page 483 looks like this. fit &lt;- brm(data = my_data, family = student, y_z ~ 1 + x_z, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;)) Like we discussed in Chapter 16, we won’t be using the uniform prior for \\(\\sigma\\). Since we’re presuming standardized data, a half-unit normal is a fine choice. But do note this is much tighter than Kruschke’s \\(\\operatorname{uniform} (0.001, 1000)\\) and it will have down-the-road consequences for our results versus those in the text. Also, look at how we just pumped the definition of our sole stanvar(1/29, name = &quot;one_over_twentynine&quot;) operation right into the stanvar argument. If we were defining multiple values this way, I’d prefer to save this as an object first and then just pump that object into stanvars. But in this case, it was simple enough to just throw directly into the brm() function. 17.2.1.1 Standardizing the data for MCMC sampling. Kruschke mentioned how standardizing your data before feeding it into JAGS often helps the algorithm operate smoothly. The same basic principle holds for brms and Stan. Stan can often handle unstandardized data just fine. But if you ever run into estimation difficulties, consider standardizing your data and trying again. We’ll make a simple function to standardize the height and weight values. standardize &lt;- function(x) { (x - mean(x)) / sd(x) } d &lt;- d %&gt;% mutate(height_z = standardize(height), weight_z = standardize(weight)) Somewhat analogous to how Kruschke standardized his data within the JAGS code, you could standardize the data within the brm() function. That would look something like this. fit &lt;- brm(data = d %&gt;% # the standardizing occurs in the next two lines mutate(height_z = standardize(height), weight_z = standardize(weight)), family = student, weight_z ~ 1 + height_z) But anyway, let’s open brms. library(brms) We’ll fit the two models at once. fit1 will be of the total data sample. fit2 is of the \\(n = 30\\) subset. fit17.1 &lt;- brm(data = d, family = student, weight_z ~ 1 + height_z, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 17, file = &quot;fits/fit17.01&quot;) fit17.2 &lt;- update(fit17.1, newdata = d %&gt;% filter(subset == 1), chains = 4, cores = 4, seed = 17, file = &quot;fits/fit17.02&quot;) Here are the results. print(fit17.1) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: weight_z ~ 1 + height_z ## Data: d (Number of observations: 300) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.03 0.05 -0.13 0.08 1.00 3803 2730 ## height_z 0.45 0.05 0.35 0.56 1.00 3482 2259 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.85 0.05 0.75 0.94 1.00 3206 2704 ## nu 25.60 21.67 6.31 86.62 1.00 2919 2543 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit17.2) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: weight_z ~ 1 + height_z ## Data: d %&gt;% filter(subset == 1) (Number of observations: 30) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.11 0.12 -0.34 0.12 1.00 4038 2686 ## height_z 0.61 0.11 0.39 0.82 1.00 4173 2589 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.64 0.09 0.48 0.84 1.00 3622 2528 ## nu 39.57 30.75 5.84 119.87 1.00 4129 2872 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Based on Kruschke’s Equation 17.2, we can convert the standardized coefficients back to their original metric using the formulas \\[\\begin{align*} \\beta_0 &amp; = \\zeta_0 \\operatorname{SD}_y + M_y - \\frac{\\zeta_1 M_x \\operatorname{SD}_y}{\\operatorname{SD}_x} \\;\\; \\text{and} \\\\ \\beta_1 &amp; = \\frac{\\zeta_1 \\operatorname{SD}_y}{\\operatorname{SD}_x}, \\end{align*}\\] where \\(\\zeta_0\\) and \\(\\zeta_1\\) denote the intercept and slope for the model of the standardized data, respectively, and that model follows the familiar form \\[z_{\\hat y} = \\zeta_0 + \\zeta_1 z_x.\\] To implement those equations, we’ll first extract the posterior samples. We begin with fit17.1, the model for which \\(N = 300\\). post &lt;- posterior_samples(fit17.1) head(post) ## b_Intercept b_height_z sigma nu lp__ ## 1 -0.008565966 0.4506511 0.9079515 29.13748 -401.0110 ## 2 -0.044204302 0.4281183 0.7970452 31.07005 -402.4087 ## 3 -0.105379516 0.4123081 0.8323686 14.74480 -401.5056 ## 4 -0.153969750 0.3571412 0.8509155 14.65994 -404.8230 ## 5 -0.125288079 0.3651884 0.8242179 18.03175 -403.6745 ## 6 -0.129856036 0.4115427 0.8529961 19.64789 -402.5184 Let’s wrap the consequences of Equation 17.2 into two functions. make_beta_0 &lt;- function(zeta_0, zeta_1, sd_x, sd_y, m_x, m_y) { zeta_0 * sd_y + m_y - zeta_1 * m_x * sd_y / sd_x } make_beta_1 &lt;- function(zeta_1, sd_x, sd_y) { zeta_1 * sd_y / sd_x } After saving a few values, we’re ready to use our custom functions to convert our posteriors for b_Intercept and b_height_z to their natural metric. sd_x &lt;- sd(d$height) sd_y &lt;- sd(d$weight) m_x &lt;- mean(d$height) m_y &lt;- mean(d$weight) post &lt;- post %&gt;% mutate(b_0 = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_height_z, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = b_height_z, sd_x = sd_x, sd_y = sd_y)) glimpse(post) ## Rows: 4,000 ## Columns: 7 ## $ b_Intercept &lt;dbl&gt; -0.0085659663, -0.0442043023, -0.1053795163, -0.1539697497, -0.1252880790, -0… ## $ b_height_z &lt;dbl&gt; 0.4506511, 0.4281183, 0.4123081, 0.3571412, 0.3651884, 0.4115427, 0.4193145, … ## $ sigma &lt;dbl&gt; 0.9079515, 0.7970452, 0.8323686, 0.8509155, 0.8242179, 0.8529961, 0.8232428, … ## $ nu &lt;dbl&gt; 29.137481, 31.070051, 14.744799, 14.659941, 18.031749, 19.647894, 11.043079, … ## $ lp__ &lt;dbl&gt; -401.0110, -402.4087, -401.5056, -404.8230, -403.6745, -402.5184, -400.9171, … ## $ b_0 &lt;dbl&gt; -117.61990, -104.89450, -97.21419, -64.72410, -68.71807, -97.58500, -100.6906… ## $ b_1 &lt;dbl&gt; 4.182206, 3.973094, 3.826369, 3.314400, 3.389081, 3.819266, 3.891391, 4.52945… Now we’re finally ready to make the top panel of Figure 17.4. # how many posterior lines would you like? n_lines &lt;- 100 ggplot(data = d, aes(x = height, y = weight)) + geom_abline(intercept = post[1:n_lines, &quot;b_0&quot;], slope = post[1:n_lines, &quot;b_1&quot;], color = &quot;grey67&quot;, size = 1/4, alpha = .3) + geom_point(alpha = 1/2) + labs(subtitle = eval(substitute(paste(&quot;Data with&quot;, n_lines, &quot;credible regression lines&quot;))), x = &quot;height&quot;, y = &quot;weight&quot;) + coord_cartesian(xlim = c(50, 80), ylim = c(-50, 470)) We’ll want to open the tidybayes package to help make the histograms. library(tidybayes) # we&#39;ll use this to mark off the ROPEs as white strips in the background rope &lt;- tibble(key = &quot;Slope&quot;, xmin = -.5, xmax = .5) # here are the primary data post %&gt;% transmute(Intercept = b_0, Slope = b_1, Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% gather() %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = &quot;white&quot;) + geom_histogram(aes(x = value), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 40) + stat_pointintervalh(aes(x = value, y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 2) Here’s the scatter plot for the slope and intercept. post %&gt;% ggplot(aes(x = b_1, y = b_0)) + geom_point(color = &quot;grey56&quot;, size = 1/3, alpha = 1/3) + labs(x = expression(beta[1]), y = expression(beta[0])) That is one strong correlation! Finally, here’s the scatter plot for \\(\\operatorname{log10}(\\nu)\\) and \\(\\sigma_{\\text{transformed back to its raw metric}}\\). post %&gt;% transmute(Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% ggplot(aes(x = Normality, y = Scale)) + geom_point(color = &quot;grey56&quot;, size = 1/3, alpha = 1/3) + labs(x = expression(log10(nu)), y = expression(sigma)) Let’s back track and make the plots for Figure 17.3 with fit17.2. We’ll need to extract the posterior samples and wrangle, as before. post &lt;- posterior_samples(fit17.2) post &lt;- post %&gt;% mutate(b_0 = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_height_z, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = b_height_z, sd_x = sd_x, sd_y = sd_y)) glimpse(post) ## Rows: 4,000 ## Columns: 7 ## $ b_Intercept &lt;dbl&gt; -0.15966043, -0.05963733, -0.10287799, -0.08226864, 0.08219908, -0.26932327, … ## $ b_height_z &lt;dbl&gt; 0.6156750, 0.4158976, 0.4348758, 0.7235308, 0.6377568, 0.6681059, 0.4613149, … ## $ sigma &lt;dbl&gt; 0.6158242, 0.6801605, 0.6325821, 0.4960271, 0.4937104, 0.6232176, 0.5654167, … ## $ nu &lt;dbl&gt; 29.243356, 31.527094, 20.264225, 23.611639, 12.533737, 11.569457, 33.638157, … ## $ lp__ &lt;dbl&gt; -36.11596, -38.05293, -37.72598, -38.17854, -39.75470, -37.86057, -38.74105, … ## $ b_0 &lt;dbl&gt; -225.04181, -97.85839, -111.10497, -289.17023, -230.36964, -261.29985, -121.4… ## $ b_1 &lt;dbl&gt; 5.713688, 3.859681, 4.035805, 6.714629, 5.918615, 6.200266, 4.281170, 6.41192… Here’s the top panel of Figure 17.3. # how many posterior lines would you like? n_lines &lt;- 100 ggplot(data = d %&gt;% filter(subset == 1), aes(x = height, y = weight)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_abline(intercept = post[1:n_lines, &quot;b_0&quot;], slope = post[1:n_lines, &quot;b_1&quot;], color = &quot;grey67&quot;, size = 1/4, alpha = .3) + geom_point(alpha = 1/2) + scale_y_continuous(breaks = seq(from = -300, to = 200, by = 100)) + labs(subtitle = eval(substitute(paste(&quot;Data with&quot;, n_lines, &quot;credible regression lines&quot;))), x = &quot;height&quot;, y = &quot;weight&quot;) + coord_cartesian(xlim = c(0, 80), ylim = c(-350, 250)) Next we’ll make the histograms. # here are the primary data post %&gt;% transmute(Intercept = b_0, Slope = b_1, Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% gather() %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = &quot;white&quot;) + geom_histogram(aes(x = value), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 40) + stat_pointintervalh(aes(x = value, y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 2) And we’ll finish up with the scatter plots. post %&gt;% ggplot(aes(x = b_1, y = b_0)) + geom_point(color = &quot;grey56&quot;, size = 1/3, alpha = 1/3) + labs(x = expression(beta[1]), y = expression(beta[0])) post %&gt;% transmute(Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% ggplot(aes(x = Normality, y = Scale)) + geom_point(color = &quot;grey56&quot;, size = 1/3, alpha = 1/3) + labs(x = expression(log10(nu)), y = expression(sigma)) 17.2.2 Robust linear regression in Stan. Recall from Section 14.1 (p. 400) that Stan uses Hamiltonian dynamics to find proposed positions in parameter space. The trajectories use the gradient of the posterior distribution to move large distances even in narrow distributions. Thus, HMC by itself, without data standardization, should be able to efficiently generate a representative sample from the posterior distribution. (p. 487) To be clear, we’re going to fit the models with Stan/brms twice. Above, we used the standardized data like Kruschke did with his JAGS code. Now we’re getting ready to follow along with the text and use Stan/brms to fit the models with the unstandardized data. 17.2.2.1 Constants for vague priors. The issues is we want a system where we can readily specify vague priors on our regression models when the data are not standardized. As it turns out, a regression slope can take on a maximum value of \\(\\operatorname{SD}_y / \\operatorname{SD}_x\\) for data that are perfectly correlated. Therefore, the prior on the slope will be given a standard deviation that is large compared to that maximum. The biggest that an intercept could be, for data that are perfectly correlated, is \\(M_x \\operatorname{SD}_y / \\operatorname{SD}_x\\). Therefore, the prior on the intercept will have a standard deviation that is large compared to that maximum. (p. 487) With that in mind, we’ll specify our stanvars as follows: beta_0_sigma &lt;- 10 * abs(m_x * sd_y / sd_x) beta_1_sigma &lt;- 10 * abs(sd_y / sd_x) stanvars &lt;- stanvar(beta_0_sigma, name = &quot;beta_0_sigma&quot;) + stanvar(beta_1_sigma, name = &quot;beta_1_sigma&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) As in Chapter 16, “set the priors to be extremely broad relative to the data” (p. 487). With our stanvars defined, we’re ready to fit fit17.3. fit17.3 &lt;- brm(data = d, family = student, weight ~ 1 + height, prior = c(prior(normal(0, beta_0_sigma), class = Intercept), prior(normal(0, beta_1_sigma), class = b), prior(normal(0, sd_y), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvars, seed = 17, file = &quot;fits/fit17.03&quot;) Here’s the model summary. print(fit17.3) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: weight ~ 1 + height ## Data: d (Number of observations: 300) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -121.10 33.32 -186.82 -54.98 1.00 3348 2858 ## height 4.22 0.50 3.24 5.20 1.00 3369 2816 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 29.08 1.76 25.58 32.36 1.00 2500 1920 ## nu 25.37 21.47 6.20 84.33 1.00 2320 2473 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now compare the histograms for these posterior draws to those we made, above, from those fit17.1. You’ll see they’re quite similar. # here are the primary data posterior_samples(fit17.3) %&gt;% transmute(Intercept = b_Intercept, Slope = b_height, Scale = sigma, Normality = nu %&gt;% log10()) %&gt;% gather() %&gt;% # the plot ggplot() + geom_rect(data = rope, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = &quot;white&quot;) + geom_histogram(aes(x = value), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 40) + stat_pointintervalh(aes(x = value, y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 2) 17.2.3 Stan or JAGS? In this project we only fit the models with brms, which uses Stan under the hood. But since we fit the \\(N = 300\\) model with both standardized and unstandardized data, we can compare their performance. For that, we’ll want bayesplot. library(bayesplot) They had equally impressive autocorrelation plots. mcmc_acf(posterior_samples(fit17.1), pars = c(&quot;b_Intercept&quot;, &quot;b_height_z&quot;, &quot;sigma&quot;, &quot;nu&quot;), lags = 10) mcmc_acf(posterior_samples(fit17.3), pars = c(&quot;b_Intercept&quot;, &quot;b_height&quot;, &quot;sigma&quot;, &quot;nu&quot;), lags = 10) Their \\(N_{eff}/N\\) ratios were pretty similar. Both were reasonable. You’d probably want to run a simulation to contrast them with any rigor. library(patchwork) p1 &lt;- neff_ratio(fit17.1) %&gt;% mcmc_neff() + yaxis_text(hjust = 0) + ggtitle(&quot;fit17.1&quot;) p2 &lt;- neff_ratio(fit17.3) %&gt;% mcmc_neff() + yaxis_text(hjust = 0) + ggtitle(&quot;fit17.3&quot;) p1 / p2 + plot_layout(guide = &quot;collect&quot;) 17.2.4 Interpreting the posterior distribution. Halfway through the prose, Kruschke mentioned how the models provide entire posteriors for the weight of a 50-inch-tall person. brms offers a few ways to do so. In some applications, there is interest in extrapolating or interpolating trends at \\(x\\) values sparsely represented in the current data. For instance, we might want to predict the weight of a person who is 50 inches tall. A feature of Bayesian analysis is that we get an entire distribution of credible predicted values, not only a point estimate. (p. 489) Since this is such a simple model, one way is to work directly with the posterior samples. Here we use the model formula \\(y_i = \\beta_0 + \\beta_1 x_i\\) by adding the transformed intercept b_0 to the product of \\(50\\) and the transformed coefficient for height, b_1. post %&gt;% mutate(weight_at_50 = b_0 + b_1 * 50) %&gt;% ggplot(aes(x = weight_at_50, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;lbs&quot;) Looks pretty wide, doesn’t it? Hopefully this isn’t a surprise. Recall that this post is from fit17.2, the posterior based on the \\(n = 30\\) data. With so few cases, most predictions from that model are uncertain. But also, 50 inches is way out of the bounds of the data the model was based on, so we should be uncertain in this range. Let’s practice a second method. With the brms::fitted() function, we can specify the desired height value into a tibble, which we’ll then feed into the newdata argument. Fitted will then return the model-implied criterion value for that predictor variable. To warm up, we’ll first to it with fit17.3, the model based on the untransformed data. nd &lt;- tibble(height = 50) fitted(fit17.3, newdata = nd) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 90.13275 8.579962 73.14503 107.2745 The code returned a typical brms-style summary of the posterior mean, standard deviation, and 95% percentile-based intervals. The same basic method will work for the standardized models, fit17.1 or fit17.2. But that will take a little more wrangling. First, we’ll need to transform our desired value 50 into its standardized version. nd &lt;- tibble(height_z = (50 - mean(d$height)) / sd(d$height)) When we feed this value into fitted(), it will return the corresponding posterior within the standardized metric. But we want unstandardized, so we’ll need to transform. That’ll be a few-step process. First, to do the transformation properly, we’ll want to work with the poster draws themselves, rather than summary values. So we’ll set summary = F. We’ll then convert the draws into a tibble format. Then we’ll use the transmute() function to do the conversion. In the final step, we’ll use mean_qi() to compute the summary values. fitted(fit17.1, newdata = nd, summary = F) %&gt;% as_tibble() %&gt;% transmute(weight = V1 * sd(d$weight) + mean(d$weight)) %&gt;% mean_qi() ## # A tibble: 1 x 6 ## weight .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 90.5 73.7 107. 0.95 mean qi If you look above, you’ll see the results are within rounding error of those from fit3. 17.3 Hierarchical regression on individuals within groups In the previous applications, the \\(j\\)th individual contributed a single \\(x_j, y_j\\) pair. But suppose instead that every individual, \\(j\\), contributes multiple observations of \\(x_{i|j}, y_{i|j}\\) pairs. (The subscript notation \\(i|j\\) means the \\(i\\)th observation within the \\(j\\)th individual.) With these data, we can estimate a regression curve for every individual. If we also assume that the individuals are mutually representative of a common group, then we can estimate group-level parameters too. (p. 490) Load the fictitious data and take a glimpse(). my_data &lt;- read_csv(&quot;data.R/HierLinRegressData.csv&quot;) glimpse(my_data) ## Rows: 132 ## Columns: 3 ## $ Subj &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 5, 5, 5, 5, … ## $ X &lt;dbl&gt; 60.2, 61.5, 61.7, 62.3, 67.6, 69.2, 53.7, 60.1, 60.5, 62.3, 63.0, 64.0, 64.1, 66.7, … ## $ Y &lt;dbl&gt; 145.6, 157.3, 165.6, 158.8, 196.1, 183.9, 165.0, 166.9, 179.0, 196.2, 192.3, 200.7, … Our goal is to describe each individual with a linear regression, and simultaneously to estimate the typical slope and intercept of the group overall. A key assumption for our analysis is that each individual is representative of the group. Therefore, every individual informs the estimate of the group slope and intercept, which in turn inform the estimates of all the individual slopes and intercepts. Thereby we get sharing of information across individuals, and shrinkage of individual estimates toward the overarching mode. (p. 491) 17.3.1 The model and implementation in JAGS brms. Here we’ll standardize the data and define our stanvars. I should note that standardizing and mean centering, more generally, becomes complicated with multilevel models. Here we’re just standardizing based on the grand mean and grand standard deviation. But there are other ways to standardize, such as within groups. Craig Enders has a good (2013) book chapter that touched on the topic, as well as an earlier (2007) paper with Tofighi. my_data &lt;- my_data %&gt;% mutate(x_z = standardize(X), y_z = standardize(Y)) In my experience, you typically use the (|) syntax when fitting a hierarchical model with thebrm() function. The terms before the | are those varying by group and you tell brm() what the grouping variable is after the |. In the case of multiple group-level parameters–which is the case with this model (i.e., both intercept and the x_z slope–, this syntax also estimates correlations among the group-level parameters. Kruschke’s model doesn’t appear to include such a correlation. Happily, we can use the (||) syntax instead, which omits correlations among the group-level parameters. If you’re curious about the distinction, fit the model both ways and explore the differences in the print() output. For more on the topic, see the Group-level terms subsection of the brmsformula section of the brms reference manual (Bürkner, 2020g). fit17.4 &lt;- brm(data = my_data, family = student, y_z ~ 1 + x_z + (1 + x_z || Subj), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 1), class = sigma), # the next line is new prior(normal(0, 1), class = sd), prior(exponential(one_over_twentynine) + 1, class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 17, file = &quot;fits/fit17.04&quot;) Did you catch that prior(normal(0, 1), class = sd) line in the code? That’s the prior we used for our hierarchical variance parameters, \\(\\sigma_0\\) and \\(\\sigma_1\\). Just like with the scale parameter, \\(\\sigma\\), we used the zero-mean half-normal distribution. By default, brms sets their left boundary to zero, which keeps the HMC algorithm from exploring negative variance values. Anyway, here’s the model summary(). summary(fit17.4) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: y_z ~ 1 + x_z + (1 + x_z || Subj) ## Data: my_data (Number of observations: 132) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~Subj (Number of levels: 25) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.02 0.17 0.73 1.40 1.00 1185 2187 ## sd(x_z) 0.22 0.12 0.02 0.47 1.00 944 1549 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.07 0.22 -0.37 0.51 1.01 1002 1682 ## x_z 0.70 0.10 0.51 0.90 1.00 2316 2489 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.59 0.05 0.49 0.70 1.00 1768 2071 ## nu 38.56 29.91 6.24 118.14 1.00 3792 2083 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 17.3.2 The posterior distribution: Shrinkage and prediction. Keeping in the same spirit of section 17.2.4, we’ll make the plots of Figure 17.5 in two ways. First, we’ll use our make_beta_0() and make_beta_1() functions to transform the model coefficients. post &lt;- posterior_samples(fit17.4) sd_x &lt;- sd(my_data$X) sd_y &lt;- sd(my_data$Y) m_x &lt;- mean(my_data$X) m_y &lt;- mean(my_data$Y) post &lt;- post %&gt;% transmute(b_0 = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_x_z, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = b_x_z, sd_x = sd_x, sd_y = sd_y)) head(post) ## b_0 b_1 ## 1 -43.98003 2.930472 ## 2 -44.70868 2.913752 ## 3 -94.67366 3.671535 ## 4 -39.53594 2.934772 ## 5 -30.23631 2.967107 ## 6 -97.21916 3.936589 Here’s the top panel of Figure 17.4. # how many posterior lines would you like? n_lines &lt;- 250 my_data %&gt;% mutate(Subj = Subj %&gt;% as.character()) %&gt;% ggplot(aes(x = X, y = Y)) + geom_abline(intercept = post[1:n_lines, &quot;b_0&quot;], slope = post[1:n_lines, &quot;b_1&quot;], color = &quot;grey67&quot;, size = 1/4, alpha = .3) + geom_point(aes(color = Subj), alpha = 1/2) + geom_line(aes(group = Subj, color = Subj), size = 1/4) + scale_color_viridis_d() + scale_y_continuous(breaks = seq(from = 50, to = 250, by = 50)) + labs(subtitle = eval(substitute(paste(&quot;Data from all units with&quot;, n_lines, &quot;credible population-level\\nregression lines&quot;)))) + coord_cartesian(xlim = c(40, 95), ylim = c(30, 270)) + theme(legend.position = &quot;none&quot;) Recall how we can use coef() to extract the Subj-specific parameters. But we’ll want posterior draws rather than summaries, which requires summary = F. It’ll take a bit of wrangling to get the output in a tidy format. Once we’re there, the plot code will be fairly simple. c &lt;- # first we&#39;ll wrangle the `coef()` output for the intercept coef(fit17.4, summary = F)$Subj[, , &quot;Intercept&quot;] %&gt;% as_tibble() %&gt;% gather(Subj, Intercept) %&gt;% # add the slope bind_cols( coef(fit17.4, summary = F)$Subj[, , &quot;x_z&quot;] %&gt;% as_tibble() %&gt;% gather(Subj, Slope) %&gt;% select(Slope) ) %&gt;% # now we&#39;re ready to un-standardize the standardized coefficients mutate(b_0 = make_beta_0(zeta_0 = Intercept, zeta_1 = Slope, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = Slope, sd_x = sd_x, sd_y = sd_y)) %&gt;% # we need an iteration index so we might `filter()` the number of lines per case mutate(iter = rep(1:4000, times = 25)) # how many lines would you like? n_lines &lt;- 250 # the plot: my_data %&gt;% ggplot(aes(x = X, y = Y)) + geom_abline(data = c %&gt;% filter(iter &lt;= n_lines), aes(intercept = b_0, slope = b_1), color = &quot;grey67&quot;, size = 1/4, alpha = .3) + geom_point(aes(color = Subj)) + scale_color_viridis_c() + scale_x_continuous(breaks = seq(from = 50, to = 90, by = 20)) + scale_y_continuous(breaks = seq(from = 50, to = 250, by = 100)) + labs(subtitle = &quot;Each unit now has its own bundle of credible regression lines&quot;) + coord_cartesian(xlim = c(45, 90), ylim = c(50, 270)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~Subj %&gt;% factor(., levels = 1:25)) There’s some good pedagogy in that method. But I like having options and in this case fitted() affords a simpler workflow. Here’s the preparatory data wrangling step. # how many posterior lines would you like? n_lines &lt;- 250 nd &lt;- # since we&#39;re working with straight lines, we only need two x-values tibble(x_z = c(-5, 5)) %&gt;% mutate(X = x_z * sd(my_data$X) + mean(my_data$X)) f &lt;- fitted(fit17.4, newdata = nd, # since we only want the fixed effects, we&#39;ll use `re_formula` # to maginalize over the random effects re_formula = Y_z ~ 1 + X_z, summary = F, # here we use `nsamples` to subset right from the get go nsamples = n_lines) %&gt;% as_tibble() %&gt;% gather() %&gt;% # transform the `y_z` values back into the `Y` metric transmute(Y = value * sd(my_data$Y) + mean(my_data$Y)) %&gt;% # now attach the predictor values to the output bind_cols(nd %&gt;% expand(X, iter = 1:n_lines)) head(f) ## # A tibble: 6 x 3 ## Y X iter ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 7.72 31.4 1 ## 2 63.6 31.4 2 ## 3 46.9 31.4 3 ## 4 62.5 31.4 4 ## 5 3.98 31.4 5 ## 6 37.0 31.4 6 For the second time, here’s the top panel of Figure 17.4, this time based off of fitted(). p1 &lt;- my_data %&gt;% mutate(Subj = Subj %&gt;% as.character()) %&gt;% ggplot(aes(x = X, y = Y)) + geom_line(data = f, aes(group = iter), color = &quot;grey67&quot;, size = 1/4, alpha = .3) + geom_point(aes(color = Subj), alpha = 1/2) + geom_line(aes(group = Subj, color = Subj), size = 1/4) + scale_color_viridis_d(option = &quot;C&quot;) + scale_y_continuous(breaks = seq(from = 50, to = 250, by = 50)) + labs(subtitle = eval(substitute(paste(&quot;Data from all units with&quot;, n_lines, &quot;credible population-level\\nregression lines&quot;)))) + coord_cartesian(xlim = c(40, 95), ylim = c(30, 270)) + theme(legend.position = &quot;none&quot;) p1 The whole process is quite similar for the Subj-specific lines. There are two main differences. First, we need to specify which Subj values we’d like to get fitted() points for. That goes into our nd tibble. Second, we omit the re_formula argument. There are other subtleties, like with the contents of the bind_cols() function. But hopefully those are self-evident. # how many posterior lines would you like? n_lines &lt;- 250 nd &lt;- tibble(x_z = c(-5, 5)) %&gt;% mutate(X = x_z * sd(my_data$X) + mean(my_data$X)) %&gt;% expand(nesting(x_z, X), Subj = distinct(my_data, Subj) %&gt;% pull()) f &lt;- fitted(fit17.4, newdata = nd, summary = F, nsamples = n_lines) %&gt;% as_tibble() %&gt;% gather() %&gt;% transmute(Y = value * sd(my_data$Y) + mean(my_data$Y)) %&gt;% bind_cols(nd %&gt;% expand(nesting(X, Subj), iter = 1:n_lines)) head(f) ## # A tibble: 6 x 4 ## Y X Subj iter ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 81.2 31.4 1 1 ## 2 64.0 31.4 1 2 ## 3 73.4 31.4 1 3 ## 4 67.4 31.4 1 4 ## 5 90.9 31.4 1 5 ## 6 64.4 31.4 1 6 And now for the second time, here’s the bottom panel of Figure 17.4, this time based off of fitted(). p2 &lt;- my_data %&gt;% ggplot(aes(x = X, y = Y)) + geom_line(data = f, aes(group = iter), color = &quot;grey67&quot;, size = 1/4, alpha = .3) + geom_point(aes(color = Subj)) + scale_color_viridis_c(option = &quot;C&quot;) + scale_x_continuous(breaks = seq(from = 50, to = 90, by = 20)) + scale_y_continuous(breaks = seq(from = 50, to = 250, by = 100)) + labs(subtitle = &quot;Each unit now has its own bundle of credible regression lines&quot;) + coord_cartesian(xlim = c(45, 90), ylim = c(50, 270)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~Subj %&gt;% factor(., levels = 1:25)) # combine with patchwork p3 &lt;- plot_spacer() p4 &lt;- (p3 | p1 | p3) + plot_layout(widths = c(1, 4, 1)) (p4 / p2) + plot_layout(heights = c(0.6, 1)) Especially if you’re new to these kinds of models, it’s easy to get lost in all that code. And for real–the wrangling required for those plots was no joke. The primary difficulty was that we had to convert standardized solutions to unstandardized solutions, which leads to an important distinction. When we used the first method of working with the posterior_samples() and coef() output, we focused on transforming the model parameters. In contrast, when we used the second method of working with the fitted() output, we focused instead on transforming the model predictions and predictor values. This distinction can be really confusing, at first. Stick with it! There will be times one method is more convenient or intuitive than the other. It’s good to have both methods in your repertoire. 17.4 Quadratic trend and weighted data Quadratic models follow the general form \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2,\\] where \\(\\beta_2\\) is the quadratic term which, when 0, reduces the results to a simple linear model. That’s right; the linear model is a special case of the quadratic. This time the data come from the American Community Survey and Puerto Rico Community Survey. In his footnote #3, Kruschke indicated “Data are from http://www.census.gov/hhes/www/income/data/Fam_Inc_SizeofFam1.xls, retrieved December 11, 2013. Median family income for years 2009-2011.” As to our read_csv() code, note the comment argument. my_data &lt;- read_csv(&quot;data.R/IncomeFamszState3yr.csv&quot;, comment = &quot;#&quot;) glimpse(my_data) ## Rows: 312 ## Columns: 4 ## $ FamilySize &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2, 3… ## $ State &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alaska&quot;, … ## $ MedianIncome &lt;dbl&gt; 48177, 53323, 64899, 59417, 54099, 47655, 73966, 82276, 87726, 87216, 84488,… ## $ SampErr &lt;dbl&gt; 581, 1177, 1170, 2446, 3781, 3561, 1858, 3236, 3722, 6127, 6761, 5754, 590, … Here we’ll standardize all variables but State, our grouping variable. It’d be silly to try to standardize that. my_data &lt;- my_data %&gt;% mutate(family_size_z = standardize(FamilySize), median_income_z = standardize(MedianIncome), se_z = SampErr / (mean(SampErr))) glimpse(my_data) ## Rows: 312 ## Columns: 7 ## $ FamilySize &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 7, 2… ## $ State &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alaska… ## $ MedianIncome &lt;dbl&gt; 48177, 53323, 64899, 59417, 54099, 47655, 73966, 82276, 87726, 87216, 844… ## $ SampErr &lt;dbl&gt; 581, 1177, 1170, 2446, 3781, 3561, 1858, 3236, 3722, 6127, 6761, 5754, 59… ## $ family_size_z &lt;dbl&gt; -1.4615023, -0.8769014, -0.2923005, 0.2923005, 0.8769014, 1.4615023, -1.4… ## $ median_income_z &lt;dbl&gt; -1.26216763, -0.91386251, -0.13034520, -0.50139236, -0.86133924, -1.29749… ## $ se_z &lt;dbl&gt; 0.2242541, 0.4542979, 0.4515961, 0.9441060, 1.4593886, 1.3744731, 0.71715… With brms, there are a couple ways to handle measurement error on a variable (e.g., see Chapter 14 of my ebook, Statistical rethinking with brms, ggplot2, and the tidyverse (Kurz, 2020). Here we’ll use the se() syntax, following the form response | se(se_response, sigma = TRUE). In this form, se stands for standard error, the loose frequentist analogue to the Bayesian posterior \\(SD\\). Unless you’re fitting a meta-analysis on summary information, make sure to specify sigma = TRUE. Without that you’ll have no estimate for \\(\\sigma\\)! For more information on the se() method, go to the brms reference manual and find the Additional response information subsection of the brmsformula section (Bürkner, 2020g, p. 37). fit17.5 &lt;- brm(data = my_data, family = student, median_income_z | se(se_z, sigma = TRUE) ~ 1 + family_size_z + I(family_size_z^2) + (1 + family_size_z + I(family_size_z^2) || State), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 1), class = sigma), prior(normal(0, 1), class = sd), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 17, file = &quot;fits/fit17.05&quot;) Did you notice the I(family_size_z^2) part of the formula? The brms package follows a typical convention in R statistical functions in that if you want to multiply a variable by itself as in a quadratic model, you nest the family_size_z^2 part within the I() function. Take a look at the model summary. print(fit17.5) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: median_income_z | se(se_z, sigma = TRUE) ~ 1 + family_size_z + I(family_size_z^2) + (1 + family_size_z + I(family_size_z^2) || State) ## Data: my_data (Number of observations: 312) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~State (Number of levels: 52) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.75 0.09 0.61 0.94 1.00 1092 2003 ## sd(family_size_z) 0.07 0.04 0.00 0.16 1.00 925 1380 ## sd(Ifamily_size_zE2) 0.05 0.04 0.00 0.13 1.01 678 1250 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.39 0.12 0.16 0.63 1.00 561 1049 ## family_size_z 0.12 0.05 0.02 0.23 1.00 3058 2293 ## Ifamily_size_zE2 -0.44 0.04 -0.52 -0.36 1.00 3508 2803 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.05 0.04 0.00 0.14 1.00 2345 1872 ## nu 68.95 36.56 21.41 160.29 1.00 6386 2542 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Do see that Ifamily_size_zE2 row? That’s the summary of our quadratic term. 17.4.1 Results and interpretation. A new model type requires a different approach to un-standardizing our standardized coefficients. Based on Equation 17.3, we can convert our coefficients using the formulas \\[\\begin{align*} \\beta_0 &amp; = \\zeta_0 \\operatorname{SD}_y + M_y - \\frac{\\zeta_1 M_x \\operatorname{SD}_y}{\\operatorname{SD}_x} + \\frac{\\zeta_2 M^{2}_x \\operatorname{SD}_y}{\\operatorname{SD}^{2}_x}, \\\\ \\beta_1 &amp; = \\frac{\\zeta_1 \\operatorname{SD}_y}{\\operatorname{SD}_x} - \\frac{2 \\zeta_2 M_x \\operatorname{SD}_y}{\\operatorname{SD}^{2}_x}, \\text{and} \\\\ \\beta_2 &amp; = \\frac{\\zeta_2 \\operatorname{SD}_y}{\\operatorname{SD}^{2}_x}. \\end{align*}\\] We’ll make new custom functions to use them. make_beta_0 &lt;- function(zeta_0, zeta_1, zeta_2, sd_x, sd_y, m_x, m_y) { zeta_0 * sd_y + m_y - zeta_1 * m_x * sd_y / sd_x + zeta_2 * m_x^2 * sd_y / sd_x^2 } make_beta_1 &lt;- function(zeta_1, zeta_2, sd_x, sd_y, m_x) { zeta_1 * sd_y / sd_x - 2 * zeta_2 * m_x * sd_y / sd_x^2 } make_beta_2 &lt;- function(zeta_2, sd_x, sd_y) { zeta_2 * sd_y / sd_x^2 } # may as well respecify these, too m_x &lt;- mean(my_data$FamilySize) m_y &lt;- mean(my_data$MedianIncome) sd_x &lt;- sd(my_data$FamilySize) sd_y &lt;- sd(my_data$MedianIncome) Now we’ll extract our posterior samples and make the conversions. post &lt;- posterior_samples(fit17.5) %&gt;% transmute(b_0 = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_family_size_z, zeta_2 = b_Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = b_family_size_z, zeta_2 = b_Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y, m_x = m_x), b_2 = make_beta_2(zeta_2 = b_Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y)) Our geom_abline() approach from before won’t work with curves. We’ll have to resort to geom_line(). With the geom_line() approach, we’ll need many specific values of model-implied MedianIncome across a densely-packed range of FamilySize. We want to use a lot of FamilySize values, like 30 or 50 or so, to make sure the curves look smooth. Below, we’ll use 50 (i.e., length.out = 50). But if it’s still not clear why, try plugging in a lesser value, like 5 or so. You’ll see. # how many posterior lines would you like? n_lines &lt;- 200 set.seed(17) post &lt;- post %&gt;% sample_n(size = n_lines) %&gt;% rownames_to_column(var = &quot;draw&quot;) %&gt;% expand(nesting(draw, b_0, b_1, b_2), FamilySize = seq(from = 1, to = 9, length.out = 50)) %&gt;% mutate(MedianIncome = b_0 + b_1 * FamilySize + b_2 * FamilySize^2) head(post) ## # A tibble: 6 x 6 ## draw b_0 b_1 b_2 FamilySize MedianIncome ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 25517. 19003. -1947. 1 42574. ## 2 1 25517. 19003. -1947. 1.16 44989. ## 3 1 25517. 19003. -1947. 1.33 47300. ## 4 1 25517. 19003. -1947. 1.49 49507. ## 5 1 25517. 19003. -1947. 1.65 51611. ## 6 1 25517. 19003. -1947. 1.82 53611. Now we’re ready to make the top panel of Figure 17.7. my_data %&gt;% ggplot(aes(x = FamilySize, y = MedianIncome)) + geom_line(data = post, aes(group = draw), size = 1/4, alpha = 1/3, color = &quot;grey67&quot;) + geom_line(aes(group = State), alpha = 1/2, size = 1/4) + geom_point(alpha = 1/2, size = 1/2) + scale_color_viridis_c() + scale_x_continuous(&quot;Family size&quot;, breaks = 1:8) + labs(title = &quot;All states&quot;, y = &quot;Median income&quot;) + coord_cartesian(xlim = c(1, 8), ylim = c(0, 150000)) Like before, we’ll extract the group-level coefficients (i.e., those specific to the States) with the coef() function. And also like before, the coef() output will require a little wrangling. c &lt;- # collect the `State`-level intercepts coef(fit17.5, summary = F)$State[, , &quot;Intercept&quot;] %&gt;% as_tibble() %&gt;% gather(State, Intercept) %&gt;% # add the `State`-level `family_size_z` slopes bind_cols( coef(fit17.5, summary = F)$State[, , &quot;family_size_z&quot;] %&gt;% as_tibble() %&gt;% gather(Subj, family_size_z) %&gt;% select(family_size_z) ) %&gt;% # add the `State`-level `Ifamily_size_zE2` slopes bind_cols( coef(fit17.5, summary = F)$State[, , &quot;Ifamily_size_zE2&quot;] %&gt;% as_tibble() %&gt;% gather(Subj, Ifamily_size_zE2) %&gt;% select(Ifamily_size_zE2) ) %&gt;% # let&#39;s go ahead and make the standardized-to-unstandardized conversions, here mutate(b_0 = make_beta_0(zeta_0 = Intercept, zeta_1 = family_size_z, zeta_2 = Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y, m_x = m_x, m_y = m_y), b_1 = make_beta_1(zeta_1 = family_size_z, zeta_2 = Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y, m_x = m_x), b_2 = make_beta_2(zeta_2 = Ifamily_size_zE2, sd_x = sd_x, sd_y = sd_y)) %&gt;% # We just want the first 25 states, from Alabama through Mississippi, so we&#39;ll `filter()` filter(State &lt;= &quot;Mississippi&quot;) str(c) ## tibble [100,000 × 7] (S3: tbl_df/tbl/data.frame) ## $ State : chr [1:100000] &quot;Alabama&quot; &quot;Alabama&quot; &quot;Alabama&quot; &quot;Alabama&quot; ... ## $ Intercept : num [1:100000] -0.1123 -0.181 0.0925 -0.308 -0.434 ... ## $ family_size_z : num [1:100000] -0.0423 0.0359 0.1572 0.1319 0.1026 ... ## $ Ifamily_size_zE2: num [1:100000] -0.487 -0.45 -0.488 -0.377 -0.251 ... ## $ b_0 : num [1:100000] 17051 16740 12164 18649 30718 ... ## $ b_1 : num [1:100000] 21750 20761 23544 18250 12311 ... ## $ b_2 : num [1:100000] -2457 -2272 -2465 -1901 -1270 ... Now we’ll subset by n_lines, expand() by FamilySize, and use the model formula to compute the expected MedianIncome values. # how many posterior lines would you like? n_lines &lt;- 200 set.seed(17) c &lt;- c %&gt;% group_by(State) %&gt;% sample_n(size = n_lines) %&gt;% mutate(draw = 1:n_lines) %&gt;% expand(nesting(draw, State, b_0, b_1, b_2), FamilySize = seq(from = 1, to = 9, length.out = 50)) %&gt;% mutate(MedianIncome = b_0 + b_1 * FamilySize + b_2 * FamilySize^2) head(c) ## # A tibble: 6 x 7 ## # Groups: State [1] ## draw State b_0 b_1 b_2 FamilySize MedianIncome ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Alabama 15959. 21584. -1941. 1 35602. ## 2 1 Alabama 15959. 21584. -1941. 1.16 38440. ## 3 1 Alabama 15959. 21584. -1941. 1.33 41175. ## 4 1 Alabama 15959. 21584. -1941. 1.49 43807. ## 5 1 Alabama 15959. 21584. -1941. 1.65 46335. ## 6 1 Alabama 15959. 21584. -1941. 1.82 48759. Finally, we’re ready for the State-specific miniatures in Figure 17.7. my_data %&gt;% filter(State &lt;= &quot;Mississippi&quot;) %&gt;% ggplot(aes(x = FamilySize, y = MedianIncome)) + geom_line(data = c, aes(group = draw), size = 1/4, alpha = 1/3, color = &quot;grey67&quot;) + geom_point(aes(color = State)) + geom_line(aes(color = State)) + scale_color_viridis_d() + scale_x_continuous(&quot;Family size&quot;, breaks = 1:8) + labs(subtitle = &quot;Each State now has its own bundle of credible regression curves.&quot;, y = &quot;Median income&quot;) + coord_cartesian(xlim = c(1, 8), ylim = c(0, 150000)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~State) Magic! As our model coefficients proliferate, the fitted() approach from above starts to look more and more appetizing. Check it out for yourself. Although “almost all of the posterior distribution [was] below \\(\\nu = 4\\)” in the text (p. 500), the bulk of our \\(\\nu\\) distribution spanned across much larger values. posterior_samples(fit17.5) %&gt;% ggplot(aes(x = nu, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(Our~big~nu), x = NULL) I’m guessing the distinction in our \\(\\nu\\) distribution and that in the text is our use of the se() syntax in the brm() formula. If you have a better explanation, share it. 17.4.2 Further extensions. Kruschke discussed the ease with which users of Bayesian software might specify nonlinear models. Check out Bürkner’s (2020e) vignette, Estimating non-linear models with brms, for more on the topic. Though I haven’t used it, I believe it is also possible to use the \\(t\\) distribution to model group-level variation in brms (see this GitHub discussion for details). 17.5 Procedure and perils for expanding a model Across several chapters, we’ve already dipped our toes into posterior predictive checks. For more on the PPC “double dipping” issue, check out Gelman’s Discussion with Sander Greenland on posterior predictive checks or Simpson’s Touch me, I want to feel your data, which is itself connected to Gabry et al. (2019), Visualization in Bayesian workflow. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.0.0 bayesplot_1.7.1 tidybayes_2.0.3.9000 brms_2.12.0 ## [5] Rcpp_1.0.4.6 forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 ## [9] purrr_0.3.4 readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ## [13] ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 ## [9] farver_2.0.3 rstan_2.19.3 svUnit_1.0.3 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 ## [17] bridgesampling_1.0-0 splines_3.6.3 knitr_1.28 shinythemes_1.1.2 ## [21] jsonlite_1.6.1 broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 ## [25] compiler_3.6.3 httr_1.4.1 backports_1.1.6 assertthat_0.2.1 ## [29] Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 later_1.0.0 ## [33] prettyunits_1.1.1 htmltools_0.4.0 tools_3.6.3 igraph_1.2.5 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 ## [41] cellranger_1.1.0 vctrs_0.3.0 nlme_3.1-144 crosstalk_1.1.0.1 ## [45] xfun_0.13 ps_1.3.3 rvest_0.3.5 mime_0.9 ## [49] miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 MASS_7.3-51.5 ## [53] zoo_1.8-7 scales_1.1.1 colourpicker_1.0 hms_0.5.3 ## [57] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 StanHeaders_2.21.0-1 ## [65] loo_2.2.0 stringi_1.4.6 dygraphs_1.1.1.6 pkgbuild_1.0.8 ## [69] rlang_0.4.6 pkgconfig_2.0.3 matrixStats_0.56.0 HDInterval_0.2.0 ## [73] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [77] labeling_0.3 tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 ## [81] magrittr_1.5 bookdown_0.18 R6_2.4.1 generics_0.0.2 ## [85] DBI_1.1.0 pillar_1.4.4 haven_2.2.0 withr_2.2.0 ## [89] mgcv_1.8-31 xts_0.12-0 abind_1.4-5 modelr_0.1.6 ## [93] crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 rmarkdown_2.1 ## [97] grid_3.6.3 readxl_1.3.1 callr_3.4.3 threejs_0.3.3 ## [101] reprex_0.3.0 digest_0.6.25 xtable_1.8-4 httpuv_1.5.2 ## [105] stats4_3.6.3 munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.1 References "],
["metric-predicted-variable-with-multiple-metric-predictors.html", "18 Metric Predicted Variable with Multiple Metric Predictors 18.1 Multiple linear regression 18.2 Multiplicative interaction of metric predictors 18.3 Shrinkage of regression coefficients 18.4 Variable selection Session info", " 18 Metric Predicted Variable with Multiple Metric Predictors We will consider models in which the predicted variable is an additive combination of predictors, all of which have proportional influence on the prediction. This kind of model is called multiple linear regression. We will also consider nonadditive combinations of predictors, which are called interactions. (Kruschke, 2015, p. 509, emphasis in the original) 18.1 Multiple linear regression Say we have one criterion \\(y\\) and two predictors, \\(x_1\\) and \\(x_2\\). If \\(y \\sim \\operatorname{Normal} (\\mu, \\sigma)\\) and \\(\\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\), then it’s also the case that we can rewrite the formula for \\(y\\) as \\[y \\sim \\operatorname{Normal} (\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2, \\sigma).\\] As Kruschke pointed out, the basic model “assumes homogeneity of variance, which means that at all values of \\(x_1\\) and \\(x_2\\), the variance \\(\\sigma^2\\) of \\(y\\) is the same” (p. 510). If we presume the data for the two \\(x\\) variables are uniformly distributed within 0 and 10, we can make the data for Figure 18.1 like this. library(tidyverse) n &lt;- 300 set.seed(18) d &lt;- tibble(x_1 = runif(n = n, 0, 10), x_2 = runif(n = n, 0, 10)) %&gt;% mutate(y = rnorm(n = n, mean = 10 + x_1 + 2 * x_2)) head(d) ## # A tibble: 6 x 3 ## x_1 x_2 y ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.23 8.62 36.9 ## 2 7.10 1.33 20.3 ## 3 9.66 1.08 19.5 ## 4 0.786 7.09 25.1 ## 5 0.536 6.67 25.6 ## 6 5.75 4.72 25.7 Here are three of the scatter plots in Figure 18.1. theme_set(theme_grey() + theme(panel.grid = element_blank())) d %&gt;% ggplot(aes(x = x_1, y = y)) + geom_point(alpha = 2/3) + coord_cartesian(ylim = c(0, 50)) d %&gt;% ggplot(aes(x = x_2, y = y)) + geom_point(alpha = 2/3) + coord_cartesian(ylim = c(0, 50)) d %&gt;% ggplot(aes(x = x_1, y = x_2)) + geom_vline(xintercept = seq(from = 0, to = 10, by = .5), color = &quot;grey98&quot;) + geom_hline(yintercept = seq(from = 0, to = 10, by = .5), color = &quot;grey98&quot;) + geom_point(alpha = 2/3) As in previous chapters, I’m not aware that ggplot2 allows for three-dimensional wireframe plots of the kind in the upper left panel. If you’d like to make one in base R, have at it. But with respect to the plots in the off-diagonal, I have no idea what Kruschke did with the compressed and diagonal grids. If you have that figured out, please share your code. For Figure 18.2, the \\(x\\) variables look to be multivariate normal with a correlation of about -.95. We can simulate such data with help from the MASS package (Ripley, 2019; Venables &amp; Ripley, 2002). Sven Hohenstein’s answer to this stats.stackexchange.com question provides the steps for simulating the data. First, we’ll need to specify the desired means and standard deviations for our variables. Then we’ll make a correlation matrix with 1s on the diagonal and the desired correlation coefficient, \\(\\rho\\) on the off-diagonal. Since the correlation matrix is symmetric, both off-diagonal positions are the same. Then we convert the correlation matrix to a covariance matrix. mus &lt;- c(5, 5) sds &lt;- c(2, 2) cors &lt;- matrix(c(1, -.95, -.95, 1), ncol = 2) cors ## [,1] [,2] ## [1,] 1.00 -0.95 ## [2,] -0.95 1.00 covs &lt;- sds %*% t(sds) * cors covs ## [,1] [,2] ## [1,] 4.0 -3.8 ## [2,] -3.8 4.0 Now we’ve defined our means, standard deviations, and covariance matrix, we’re ready to simulate the data with the MASS::mvrnorm() function. # how many data points would you like to simulate? n &lt;- 300 set.seed(18.2) d &lt;- MASS::mvrnorm(n = n, mu = mus, Sigma = covs, empirical = T) %&gt;% as_tibble() %&gt;% set_names(&quot;x_1&quot;, &quot;x_2&quot;) %&gt;% mutate(y = rnorm(n = n, mean = 10 + x_1 + 2 * x_2)) Now we have our simulated data in hand, we’re ready for three of the four panels of Figure 18.2. p1 &lt;- d %&gt;% ggplot(aes(x = x_1, y = y)) + geom_point(alpha = 2/3) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 50)) p2 &lt;- d %&gt;% ggplot(aes(x = x_2, y = y)) + geom_point(alpha = 2/3) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 50)) p3 &lt;- d %&gt;% ggplot(aes(x = x_1, y = x_2)) + geom_vline(xintercept = seq(from = 0, to = 10, by = .5), color = &quot;grey98&quot;) + geom_hline(yintercept = seq(from = 0, to = 10, by = .5), color = &quot;grey98&quot;) + geom_point(alpha = 2/3) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) # bind them together with patchwork library(patchwork) plot_spacer() + p1 + p2 + p3 We came pretty close. 18.1.1 The perils of correlated predictors. Figures 18.1 and 18.2 show data generated from the same model. In both figures, \\(\\sigma = 2, \\beta_0 = 10, \\beta_1 = 1, \\text{ and } \\beta_2 = 2\\). All that differs between the two figures is the distribution of the \\(\\langle x_1, x_2 \\rangle\\) values, which is not specified by the model. In Figure 18.1, the \\(\\langle x_1, x_2 \\rangle\\) values are distributed independently. In Figure 18.2, the \\(\\langle x_1, x_2 \\rangle\\) values are negatively correlated: When \\(x_1\\) is small, \\(x_2\\) tends to be large, and when \\(x_1\\) is large, \\(x_2\\) tends to be small. (p. 510) If you look closely at our simulation code from above, you’ll see we have done so, too. Real data often have correlated predictors. For example, consider trying to predict a state’s average high-school SAT score on the basis of the amount of money the state spends per pupil. If you plot only mean SAT against money spent, there is actually a decreasing trend… (p. 513, emphasis in the original) Before we remake Figure 18.3 to examine that decreasing trend, we’ll need to load the data from (Guber, 1999). my_data &lt;- read_csv(&quot;data.R/Guber1999data.csv&quot;) glimpse(my_data) ## Rows: 50 ## Columns: 8 ## $ State &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;… ## $ Spend &lt;dbl&gt; 4.405, 8.963, 4.778, 4.459, 4.992, 5.443, 8.817, 7.030, 5.7… ## $ StuTeaRat &lt;dbl&gt; 17.2, 17.6, 19.3, 17.1, 24.0, 18.4, 14.4, 16.6, 19.1, 16.3,… ## $ Salary &lt;dbl&gt; 31.144, 47.951, 32.175, 28.934, 41.078, 34.571, 50.045, 39.… ## $ PrcntTake &lt;dbl&gt; 8, 47, 27, 6, 45, 29, 81, 68, 48, 65, 57, 15, 13, 58, 5, 9,… ## $ SATV &lt;dbl&gt; 491, 445, 448, 482, 417, 462, 431, 429, 420, 406, 407, 468,… ## $ SATM &lt;dbl&gt; 538, 489, 496, 523, 485, 518, 477, 468, 469, 448, 482, 511,… ## $ SATT &lt;dbl&gt; 1029, 934, 944, 1005, 902, 980, 908, 897, 889, 854, 889, 97… Now we have the data, here are three of the four plots. p1 &lt;- my_data %&gt;% ggplot(aes(x = Spend, y = SATT)) + geom_point(alpha = 2/3) + coord_cartesian(ylim = c(800, 1120)) p2 &lt;- my_data %&gt;% ggplot(aes(x = PrcntTake, y = SATT)) + geom_point(alpha = 2/3) + xlab(&quot;% Take&quot;) + coord_cartesian(ylim = c(800, 1120)) p3 &lt;- my_data %&gt;% ggplot(aes(x = PrcntTake, y = Spend)) + geom_vline(xintercept = seq(from = 5, to = 80, by = 5), color = &quot;grey98&quot;) + geom_hline(yintercept = seq(from = 3.5, to = 10, by = .5), color = &quot;grey98&quot;) + geom_point(alpha = 2/3) + xlab(&quot;% Take&quot;) # bind them together and add a title wrap_elements(grid::textGrob(&#39;No 3D wireframe plots for us&#39;)) + p1 + p2 + p3 + plot_annotation(title = &quot;SATT ~ N(m,sd=31.5), m = 993.8 + −2.9 %Take + 12.3 Spend&quot;) You can learn more about how we added that title to our plot ensemble from Pedersen’s (2020a) vignette, Adding annotation and style, and more about how we added that text in place of a wireframe plot from another of his (2020b) vignettes, Plot assembly. The separate influences of the two predictors could be assessed in this example because the predictors had only mild correlation with each other. There was enough independent variation of the two predictors that their distinct relationships to the outcome variable could be detected. In some situations, however, the predictors are so tightly correlated that their distinct effects are difficult to tease apart. Correlation of predictors causes the estimates of their regression coefficients to trade-off, as we will see when we examine the posterior distribution. (p. 514) 18.1.2 The model and implementation. “As with the model for simple linear regression, the Markov Chain Monte Carlo (MCMC) sampling can be more efficient if the data are mean-centered or standardized” (p. 515). We’ll make a custom function to standardize the criterion and predictor values. standardize &lt;- function(x) { (x - mean(x)) / sd(x) } my_data &lt;- my_data %&gt;% mutate(prcnt_take_z = standardize(PrcntTake), spend_z = standardize(Spend), satt_z = standardize(SATT)) Let’s open brms. library(brms) Now we’re ready to fit the model. As Kruschke pointed out, the priors on the standardized predictors are set with an arbitrary standard deviation of 2.0. This value was chosen because standardized regression coefficients are algebraically constrained to fall between −1 and +1 in least-squares regression, [2] and therefore, the regression coefficients will not exceed those limits by much. A normal distribution with standard deviation of 2.0 is reasonably flat over the range from −1 to +1. (p. 516) With data like this, even a prior(normal(0, 1), class = b) would be only mildly regularizing. This is a good place to emphasize how priors in brms are given classes. If you’d like all parameters within a given class to have the prior, you can just specify one prior argument within that class. For our fit8.1, both parameters of class = b have a normal(0, 2) prior. So we can just include one statement to handle both. Had we wanted different priors for the coefficients for spend_z and prcnt_take_z, we’d need to include two prior() arguments with at least one including a coef argument. fit18.1 &lt;- brm(data = my_data, family = student, satt_z ~ 1 + spend_z + prcnt_take_z, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 18, file = &quot;fits/fit18.01&quot;) Check the model summary. print(fit18.1) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: satt_z ~ 1 + spend_z + prcnt_take_z ## Data: my_data (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.00 0.06 -0.12 0.13 1.00 4222 2585 ## spend_z 0.23 0.08 0.08 0.38 1.00 3599 2990 ## prcnt_take_z -1.03 0.08 -1.18 -0.87 1.00 3490 3160 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.42 0.05 0.32 0.53 1.00 3523 2413 ## nu 33.07 27.90 4.27 107.31 1.00 3398 2360 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). So when we use a multivariable model, increases in spending now appear associated with increases in SAT scores. 18.1.3 The posterior distribution. Based on Equation 18.1, we can convert the standardized coefficients from our multivariable model back to their original metric as follows: \\[\\begin{align*} \\beta_0 &amp; = \\operatorname{SD}_y \\zeta_0 + M_y - \\operatorname{SD}_y \\sum_j \\frac{\\zeta_j M_{x_j}}{\\operatorname{SD}_{x_j}} \\;\\;\\; \\text{and} \\\\ \\beta_j &amp; = \\frac{\\operatorname{SD}_y \\zeta_j}{\\operatorname{SD}_{x_j}}. \\end{align*}\\] To use them, we’ll first extract the posterior samples. post &lt;- posterior_samples(fit18.1) head(post) ## b_Intercept b_spend_z b_prcnt_take_z sigma nu lp__ ## 1 0.03332938 0.2246157 -1.044109 0.4381914 58.891563 -35.40259 ## 2 -0.05051637 0.2270969 -1.076683 0.3759888 13.331876 -35.92027 ## 3 -0.04916057 0.2770627 -1.092591 0.3992656 18.839407 -35.58017 ## 4 0.01043252 0.3638677 -1.141387 0.4167015 5.558774 -37.95336 ## 5 0.02980380 0.3671874 -1.262402 0.4401791 29.336185 -39.44394 ## 6 0.00317025 0.3080994 -1.259354 0.4584194 46.137031 -39.81174 Like we did in Chapter 17, let’s wrap the consequences of Equation 18.1 into two functions. make_beta_0 &lt;- function(zeta_0, zeta_1, zeta_2, sd_x_1, sd_x_2, sd_y, m_x_1, m_x_2, m_y) { sd_y * zeta_0 + m_y - sd_y * ((zeta_1 * m_x_1 / sd_x_1) + (zeta_2 * m_x_2 / sd_x_2)) } make_beta_j &lt;- function(zeta_j, sd_j, sd_y) { sd_y * zeta_j / sd_j } After saving a few values, we’re ready to use our custom functions. sd_x_1 &lt;- sd(my_data$Spend) sd_x_2 &lt;- sd(my_data$PrcntTake) sd_y &lt;- sd(my_data$SATT) m_x_1 &lt;- mean(my_data$Spend) m_x_2 &lt;- mean(my_data$PrcntTake) m_y &lt;- mean(my_data$SATT) post &lt;- post %&gt;% mutate(b_0 = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_y = m_y), b_1 = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), b_2 = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) glimpse(post) ## Rows: 4,000 ## Columns: 9 ## $ b_Intercept &lt;dbl&gt; 0.033329378, -0.050516372, -0.049160566, 0.010432522, … ## $ b_spend_z &lt;dbl&gt; 0.22461570, 0.22709693, 0.27706273, 0.36386773, 0.3671… ## $ b_prcnt_take_z &lt;dbl&gt; -1.0441089, -1.0766835, -1.0925912, -1.1413875, -1.262… ## $ sigma &lt;dbl&gt; 0.4381914, 0.3759888, 0.3992656, 0.4167015, 0.4401791,… ## $ nu &lt;dbl&gt; 58.891563, 13.331876, 18.839407, 5.558774, 29.336185, … ## $ lp__ &lt;dbl&gt; -35.40259, -35.92027, -35.58017, -37.95336, -39.44394,… ## $ b_0 &lt;dbl&gt; 998.4585, 994.5899, 980.0592, 961.1825, 973.4781, 990.… ## $ b_1 &lt;dbl&gt; 12.331810, 12.468035, 15.211248, 19.977000, 20.159255,… ## $ b_2 &lt;dbl&gt; -2.919049, -3.010119, -3.054593, -3.191014, -3.529337,… Here’s the top panel of Figure 18.5. library(tidybayes) # here are the primary data post %&gt;% transmute(Intercept = b_0, Spend = b_1, `Percent Take` = b_2, Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% gather() %&gt;% # the plot ggplot(aes(x = value)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 40) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 3) The slope on spending has a mode of about 13, which suggests that SAT scores rise by about 13 points for every extra $1000 spent per pupil. The slope on percentage taking the exam (PrcntTake) is also credibly non-zero, with a mode around −2.8, which suggests that SAT scores fall by about 2.8 points for every additional 1% of students who take the test. (p. 517) If you want those exact modes and, say, 50% intervals around them, you can just use tidybayes::mode_hdi(). post %&gt;% transmute(Spend = b_1, `Percent Take` = b_2) %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value, .width = .5) ## # A tibble: 2 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Percent Take -2.86 -3.04 -2.74 0.5 mode hdi ## 2 Spend 12.9 10.3 15.9 0.5 mode hdi The brms::bayes_R2() function makes it easy to compute a Bayesian \\(R^2\\). Simply feed a brm() fit object into bayes_R2() and you’ll get back the posterior mean, \\(SD\\), and 95% intervals. bayes_R2(fit18.1) ## Estimate Est.Error Q2.5 Q97.5 ## R2 0.8139321 0.02239821 0.7572305 0.8429055 I’m not going to go into the technical details here, but you should be aware that the Bayeisan \\(R^2\\) returned from the bayes_R2() function is not calculated the same as it is with OLS. If you want to dive in, check out the paper by Gelman et al. (2019), R-squared for Bayesian regression models. Anyway, if you’d like to view the Bayesian \\(R^2\\) distribution rather than just get the summaries, specify summary = F, convert the output to a tibble, and plot as usual. bayes_R2(fit18.1, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = R2, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 20, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(paste(&quot;Bayesian &quot;, italic(R)^2)), x = NULL) + coord_cartesian(xlim = c(.6, 1)) Since the brms::bayes_R2() function is not identical with Kruschke’s method in the text, the results might differ a bit. We can get a sense of the scatter plots with bayesplot::mcmc_pairs(). library(bayesplot) color_scheme_set(&quot;gray&quot;) post %&gt;% transmute(Intercept = b_0, Spend = b_1, `Percent Take` = b_2, Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% mcmc_pairs(off_diag_args = list(size = 1/8, alpha = 1/8)) One way to get the Pearson’s correlation coefficients among the parameters is with psych::lowerCor(). post %&gt;% transmute(Intercept = b_0, Spend = b_1, `Percent Take` = b_2, Scale = sigma * sd_y, Normality = nu %&gt;% log10()) %&gt;% psych::lowerCor(digits = 3) ## Intrcp Spend PrcntT Scale Nrmlty ## Intercept 1.000 ## Spend -0.934 1.000 ## Percent Take 0.349 -0.613 1.000 ## Scale 0.076 -0.083 0.080 1.000 ## Normality 0.099 -0.108 0.089 0.342 1.000 Kruschke finished the subsection with the observation: “Sometimes we are interested in using the linear model to predict \\(y\\) values for \\(x\\) values of interest. It is straight forward to generate a large sample of credible \\(y\\) values for specified \\(x\\) values” (p. 519). Like we practiced with in the last chapter, the simplest way to do so in brms is with the fitted() function. For a quick example, say we wanted to know what the model would predict if we were to have a standard-score increase in spending and a simultaneous standard-score decrease in the percent taking the exam. We’d just specify those values in a tibble and feed that tibble into fitted() along with the model. nd &lt;- tibble(prcnt_take_z = -1, spend_z = 1) fitted(fit18.1, newdata = nd) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 1.262009 0.1547968 0.9596958 1.568147 18.1.4 Redundant predictors. As a simplified example of correlated predictors, think of just two data points: Suppose \\(y = 1 \\text{ for } \\langle x_1, x_2 \\rangle = \\langle 1, 1 \\rangle \\text{ and } y = 2 \\text{ for } \\langle x_1, x_2 \\rangle = \\langle 2, 2 \\rangle\\). The linear model, \\(y = \\beta_1 x_1 + \\beta_2 x_2\\) is supposed to satisfy both data points, and in this case both are satisfied by \\(1 = \\beta_1 + \\beta_2\\). Therefore, many different combinations of \\(\\beta_1\\) and \\(\\beta_2\\) satisfy the data. For example, it could be that \\(\\beta_1 = 2\\) and \\(\\beta_2 = -1\\), or \\(\\beta_1 = 0.5\\) and \\(\\beta_2 = 0.5\\), or \\(\\beta_1 = 0\\) and \\(\\beta_2 = 1\\). In other words, the credible values of \\(\\beta_1\\) and \\(\\beta_2\\) are anticorrelated and trade-off to fit the data. (p. 519) Here are what those data look like. You would not want to fit a regression model with these data. tibble(x_1 = 1:2, x_2 = 1:2, y = 1:2) ## # A tibble: 2 x 3 ## x_1 x_2 y ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 ## 2 2 2 2 We can take percentages and turn them into their inverse re-expressed as a proportion. percent_take &lt;- 37 (100 - percent_take) / 100 ## [1] 0.63 Let’s make a redundant predictor and then standardize() it. my_data &lt;- my_data %&gt;% mutate(prop_not_take = (100 - PrcntTake) / 100) %&gt;% mutate(prop_not_take_z = standardize(prop_not_take)) glimpse(my_data) ## Rows: 50 ## Columns: 13 ## $ State &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Californ… ## $ Spend &lt;dbl&gt; 4.405, 8.963, 4.778, 4.459, 4.992, 5.443, 8.817, 7.03… ## $ StuTeaRat &lt;dbl&gt; 17.2, 17.6, 19.3, 17.1, 24.0, 18.4, 14.4, 16.6, 19.1,… ## $ Salary &lt;dbl&gt; 31.144, 47.951, 32.175, 28.934, 41.078, 34.571, 50.04… ## $ PrcntTake &lt;dbl&gt; 8, 47, 27, 6, 45, 29, 81, 68, 48, 65, 57, 15, 13, 58,… ## $ SATV &lt;dbl&gt; 491, 445, 448, 482, 417, 462, 431, 429, 420, 406, 407… ## $ SATM &lt;dbl&gt; 538, 489, 496, 523, 485, 518, 477, 468, 469, 448, 482… ## $ SATT &lt;dbl&gt; 1029, 934, 944, 1005, 902, 980, 908, 897, 889, 854, 8… ## $ prcnt_take_z &lt;dbl&gt; -1.0178453, 0.4394222, -0.3078945, -1.0925770, 0.3646… ## $ spend_z &lt;dbl&gt; -1.10086058, 2.24370805, -0.82716069, -1.06123647, -0… ## $ satt_z &lt;dbl&gt; 0.8430838, -0.4266207, -0.2929676, 0.5223163, -0.8543… ## $ prop_not_take &lt;dbl&gt; 0.92, 0.53, 0.73, 0.94, 0.55, 0.71, 0.19, 0.32, 0.52,… ## $ prop_not_take_z &lt;dbl&gt; 1.0178453, -0.4394222, 0.3078945, 1.0925770, -0.36469… We’re ready to fit the redundant-predictor model. fit18.2 &lt;- brm(data = my_data, family = student, satt_z ~ 0 + Intercept + spend_z + prcnt_take_z + prop_not_take_z, prior = c(prior(normal(0, 2), class = b, coef = &quot;Intercept&quot;), prior(normal(0, 2), class = b, coef = &quot;spend_z&quot;), prior(normal(0, 2), class = b, coef = &quot;prcnt_take_z&quot;), prior(normal(0, 2), class = b, coef = &quot;prop_not_take_z&quot;), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 18, # this will let us use `prior_samples()` later on sample_prior = &quot;yes&quot;, file = &quot;fits/fit18.02&quot;) You might notice a few things about the brm() code. First, we have used the ~ 0 + Intercept + ... syntax instead of the default syntax for intercepts. In normal situations, we would have been in good shape using the typical ~ 1 + ... syntax for the intercept, especially given our use of standardized data. However, since brms version 2.5.0, using the sample_prior argument to draw samples from the prior distribution will no longer allow us to return samples from the typical brms intercept. Bürkner addressed the issue on the Stan forums. As he pointed out, if you want to get prior samples from an intercept, you’ll have to use the alternative syntax. The other thing to point out is that even though we used the same prior on all the predictors, including the intercept, we still explicitly spelled each out with the coef argument. If we hadn’t been explicit like this, we would only get a single b vector from the prior_samples() function. But since we want separate vectors for each of our predictors, we used the verbose code. If you’re having a difficult time understanding these two points, experiment. Fit the model in a few different ways with either the typical or the alternative intercept syntax and with either the verbose prior code or the simplified prior(normal(0, 2), class = b) code. And after each, execute prior_samples(fit18.2). You’ll see. Let’s move on. Kruschke mentioned high autocorrelations in the prose. Here are the autocorrelation plots for our \\(\\beta\\)s. post &lt;- posterior_samples(fit18.2, add_chain = T) mcmc_acf(post, pars = c(&quot;b_Intercept&quot;, &quot;b_spend_z&quot;, &quot;b_prcnt_take_z&quot;, &quot;b_prop_not_take_z&quot;), lags = 10) Looks like HMC made a big difference. The \\(N_{eff}/N\\) ratios weren’t terrible, either. neff_ratio(fit18.2)[1:6] %&gt;% mcmc_neff() + yaxis_text(hjust = 0) The brms::vcov() function returns a variance/covariance matrix–or a correlation matrix when you set correlation = T–of the population-level parameters (i.e., the fixed effects). It returns the values to a decadent level of precision, so we’ll simplify the output with round(). vcov(fit18.2, correlation = T) %&gt;% round(digits = 3) ## Intercept spend_z prcnt_take_z prop_not_take_z ## Intercept 1.000 0.026 0.028 0.029 ## spend_z 0.026 1.000 0.013 0.048 ## prcnt_take_z 0.028 0.013 1.000 0.998 ## prop_not_take_z 0.029 0.048 0.998 1.000 Notice how much lower our Spend_z correlations are than those Kruschke displayed on page 520 of the text. However, it turns out the correlations among the redundant predictors were still very high. If any of the nondiagonal correlations are high (i.e., close to +1 or close to −1), be careful when interpreting the posterior distribution. Here, we can see that the correlation of PrcntTake and PropNotTake is −1.0, which is an immediate sign of redundant predictors. (p. 520) You can really get a sense of the silliness of the parameters if you plot them. We’ll use geom_halfeyeh() to get a sense of densities and summaries of the \\(\\beta\\)s. post %&gt;% select(b_Intercept:b_prop_not_take_z) %&gt;% gather() %&gt;% # this line isn&#39;t necessary, but it does allow us to arrange the parameters on the y-axis mutate(key = factor(key, levels = c(&quot;b_prop_not_take_z&quot;, &quot;b_prcnt_take_z&quot;, &quot;b_spend_z&quot;, &quot;b_Intercept&quot;))) %&gt;% ggplot(aes(x = value, y = key)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_halfeyeh(point_interval = mode_hdi, .width = .95, # the next two lines are purely aesthetic size = 1.5, normalize = &quot;xy&quot;) + labs(x = NULL, y = NULL) + coord_cartesian(xlim = c(-5, 5)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) Yeah, on the standardized scale those are some ridiculous estimates. Let’s update our make_beta_0() function. make_beta_0 &lt;- function(zeta_0, zeta_1, zeta_2, zeta_3, sd_x_1, sd_x_2, sd_x_3, sd_y, m_x_1, m_x_2, m_x_3, m_y) { sd_y * zeta_0 + m_y - sd_y * ((zeta_1 * m_x_1 / sd_x_1) + (zeta_2 * m_x_2 / sd_x_2) + (zeta_3 * m_x_3 / sd_x_3)) } sd_x_1 &lt;- sd(my_data$Spend) sd_x_2 &lt;- sd(my_data$PrcntTake) sd_x_3 &lt;- sd(my_data$prop_not_take) sd_y &lt;- sd(my_data$SATT) m_x_1 &lt;- mean(my_data$Spend) m_x_2 &lt;- mean(my_data$PrcntTake) m_x_3 &lt;- mean(my_data$prop_not_take) m_y &lt;- mean(my_data$SATT) post &lt;- post %&gt;% transmute(Intercept = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, zeta_3 = b_prop_not_take_z, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_x_3 = sd_x_3, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_x_3 = m_x_3, m_y = m_y), Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), `Proportion not Take` = make_beta_j(zeta_j = b_prop_not_take_z, sd_j = sd_x_3, sd_y = sd_y), Scale = sigma * sd_y, Normality = nu %&gt;% log10()) glimpse(post) ## Rows: 4,000 ## Columns: 6 ## $ Intercept &lt;dbl&gt; 1061.1824, 280.0482, 873.1032, 551.0707, 699.25… ## $ Spend &lt;dbl&gt; 5.454006, 17.469214, 15.816227, 12.931905, 8.68… ## $ `Percent Take` &lt;dbl&gt; -3.1303938, 3.9409313, -1.8844443, 1.4769968, 0… ## $ `Proportion not Take` &lt;dbl&gt; -26.318716, 682.212440, 105.706334, 432.377767,… ## $ Scale &lt;dbl&gt; 30.16235, 32.49213, 26.98000, 36.66645, 28.8521… ## $ Normality &lt;dbl&gt; 1.1409028, 1.3884920, 1.8217587, 1.0168820, 1.6… Now we’ve done the conversions, here are the histograms of Figure 18.6. post %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 3) Their scatter plots are as follows: post %&gt;% mcmc_pairs(off_diag_args = list(size = 1/8, alpha = 1/8)) Since brms::vcov() only computes the Pearson’s correlations among the untransformed population-level intercepts and slopes, it will be of limited use computing the correlations among all our parameters of interest. So here we’ll use psych::lowerCor() to compute the correlations among the full range of our parameters. post %&gt;% psych::lowerCor(digits = 3) ## Intrcp Spend PrcntT PrprnT Scale Nrmlty ## Intercept 1.000 ## Spend -0.100 1.000 ## Percent Take -0.996 0.013 1.000 ## Proportion not Take -0.998 0.048 0.998 1.000 ## Scale 0.002 -0.086 0.007 0.003 1.000 ## Normality 0.033 -0.082 -0.024 -0.030 0.373 1.000 Figure 18.7 is all about the prior predictive distribution. Here we’ll extract the priors with prior_samples() and wrangle all in one step. prior &lt;- prior_samples(fit18.2) %&gt;% transmute(Intercept = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, zeta_3 = b_prop_not_take_z, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_x_3 = sd_x_3, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_x_3 = m_x_3, m_y = m_y), Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), `Proportion not Take` = make_beta_j(zeta_j = b_prop_not_take_z, sd_j = sd_x_3, sd_y = sd_y), Scale = sigma * sd_y, Normality = nu %&gt;% log10()) glimpse(prior) ## Rows: 4,000 ## Columns: 6 ## $ Intercept &lt;dbl&gt; 1462.43927, 664.21947, 1289.22313, 1476.56145, … ## $ Spend &lt;dbl&gt; -38.749588, 13.362327, -49.410718, -93.439239, … ## $ `Percent Take` &lt;dbl&gt; 1.2899620, 6.4475454, 7.1217195, 1.7590988, 2.4… ## $ `Proportion not Take` &lt;dbl&gt; -310.88980, 63.95417, -280.14134, 101.81310, -3… ## $ Scale &lt;dbl&gt; 2.822765, 6.293331, 67.317724, 11.692913, 148.9… ## $ Normality &lt;dbl&gt; 1.5678380, 1.5254245, 1.2846292, 0.6427115, 1.3… Now we’ve wrangled the priors, we’re ready to make the histograms at the top of Figure 18.7. prior %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 3) Since we used the half-Gaussian prior for our \\(\\sigma\\), our Scale histogram looks different from Kruschke’s. Otherwise, everything’s on the up and up. Here are the scatter plots at the bottom of Figure 18.7. prior %&gt;% mcmc_pairs(off_diag_args = list(size = 1/8, alpha = 1/8)) And finally, here are those Pearson’s correlation coefficients for the priors. prior %&gt;% psych::lowerCor(digits = 3) ## Intrcp Spend PrcntT PrprnT Scale Nrmlty ## Intercept 1.000 ## Spend -0.826 1.000 ## Percent Take -0.256 0.003 1.000 ## Proportion not Take -0.486 0.020 0.019 1.000 ## Scale -0.007 0.005 0.024 -0.017 1.000 ## Normality 0.007 -0.006 -0.006 -0.001 -0.001 1.000 At the top of page 523, Kruschke asked us to “notice that the posterior distribution in Figure 18.6 has ranges for the redundant parameters that are only a little smaller than their priors.” With a little wrangling, we can compare the prior/posterior distributions for our redundant parameters more directly. post %&gt;% gather(parameter, posterior) %&gt;% bind_cols( prior %&gt;% gather() %&gt;% transmute(prior = value) ) %&gt;% gather(key, value, -parameter) %&gt;% filter(parameter %in% c(&quot;Percent Take&quot;, &quot;Proportion not Take&quot;)) %&gt;% ggplot(aes(x = value, y = 0, fill = key)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_fill_viridis_d(option = &quot;D&quot;, begin = .35, end = .65) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(legend.position = &quot;none&quot;) + facet_grid(key~parameter, scales = &quot;free&quot;) Kruschke was right. The posterior distributions are only slightly narrower than the priors for those two. With our combination of data and model, we learned virtually nothing beyond the knowledge we encoded in those priors. Kruschke mentioned SEM as a possible solution to multicollinearity. brms isn’t fully capable of SEM, at the moment (see issue #304), but its multivariate syntax (Bürkner, 2020d) does allow for path analysis and IRT models. However, you can currently fit a variety of Bayesian SEMs with the blavaan package (Merkle et al., 2020; Merkle &amp; Rosseel, 2018). I’m not aware of any textbooks highlighting blavaan. If you know of any, please share. 18.1.5 Informative priors, sparse data, and correlated predictors. It’s worth reproducing some of Kruschke’s prose from this subsection. The examples in this book tend to use mildly informed priors (e.g., using information about the rough magnitude and range of the data). But a benefit of Bayesian analysis is the potential for cumulative scientific progress by using priors that have been informed from previous research. Informed priors can be especially useful when the amount of data is small compared to the parameter space. A strongly informed prior essentially reduces the scope of the credible parameter space, so that a small amount of new data implies a narrow zone of credible parameter values. (p. 523) 18.2 Multiplicative interaction of metric predictors From page 526: Formally, interactions can have many different specific functional forms. We will consider multiplicative interaction. This means that the nonadditive interaction is expressed by multiplying the predictors. The predicted value is a weighted combination of the individual predictors and, additionally, the multiplicative product of the predictors. For two metric predictors, regression with multiplicative interaction has these algebraically equivalent expressions: \\[\\begin{align*} \\mu &amp; = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{1 \\times 2} x_1 x_2 \\\\ &amp; = \\beta_0 + \\underbrace{(\\beta_1 + \\beta_{1 \\times 2} x_2)}_{\\text{slope of } x_1} x_1 + \\beta_2 x_2 \\\\ &amp; = \\beta_0 + \\beta_1 x_1 + \\underbrace{(\\beta_2 + \\beta_{1 \\times 2} x_1)}_{\\text{slope of } x_2} x_2. \\end{align*}\\] Figure 18.8 is out of our ggplot2 repertoire. Even without it, we can still appreciate Kruschke’s overall message that “Great care must be taken when interpreting the coefficients of a model that includes interaction terms (Braumoeller, 2004). In particular, low-order terms are especially difficult to interpret when higher-order interactions are present” (p. 526). When in doubt, plot. 18.2.1 An example. Presuming we’re still just modeling \\(\\mu\\) with two predictors, we can express the formula with the interaction term as \\[ \\mu = \\beta_0 + \\beta_1+ \\beta_2 x_2 + \\underbrace{\\beta_{1 \\times 2}}_{\\beta_3} \\underbrace{x_2 x_1 }_{x_3}. \\] With brms, you can specify an interaction with either the x_i*x_j syntax or the x_i:x_j syntax. I typically use x_i:x_j. It’s often the case that you can just make the interaction term right in the model formula. But since we’re fitting the model with standardized predictors and then using Kruschke’s equations to convert the parameters back to the unstandardized metric, it seems easier to make the interaction term in the data, first. my_data &lt;- my_data %&gt;% # make x_3 mutate(interaction = Spend * PrcntTake) %&gt;% mutate(interaction_z = standardize(interaction)) Now we’ll fit the model. fit18.3 &lt;- brm(data = my_data, family = student, satt_z ~ 1 + spend_z + prcnt_take_z + interaction_z, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 18, file = &quot;fits/fit18.03&quot;) Note that even though an interaction term might seem different kind from other regression terms, it’s just another coefficient of class = b as far as the prior() statements are concerned. Anyway, let’s inspect the summary(). summary(fit18.3) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: satt_z ~ 1 + spend_z + prcnt_take_z + interaction_z ## Data: my_data (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.06 -0.12 0.11 1.00 2779 2427 ## spend_z 0.04 0.14 -0.24 0.32 1.00 1484 2038 ## prcnt_take_z -1.48 0.29 -2.03 -0.90 1.00 1336 1985 ## interaction_z 0.59 0.37 -0.12 1.30 1.00 1217 1696 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.41 0.05 0.33 0.52 1.00 2837 2503 ## nu 34.38 28.67 4.52 110.70 1.00 2685 2567 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The correlations among our parameters are about as severe as those in the text. vcov(fit18.3, correlation = T) %&gt;% round(digits = 3) ## Intercept spend_z prcnt_take_z interaction_z ## Intercept 1.000 -0.009 0.003 -0.001 ## spend_z -0.009 1.000 0.713 -0.835 ## prcnt_take_z 0.003 0.713 1.000 -0.962 ## interaction_z -0.001 -0.835 -0.962 1.000 We can see that the interaction variable is strongly correlated with both predictors. Therefore, we know that there will be strong trade-offs among the regression coefficients, and the marginal distributions of single regression coefficients might be much wider than when there was no interaction included. (p. 528) Let’s convert the posterior draws to the unstandardized metric. sd_x_3 &lt;- sd(my_data$interaction) m_x_3 &lt;- mean(my_data$interaction) post &lt;- posterior_samples(fit18.3) %&gt;% transmute(Intercept = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, zeta_3 = b_interaction_z, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_x_3 = sd_x_3, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_x_3 = m_x_3, m_y = m_y), Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), `Spend : Percent Take` = make_beta_j(zeta_j = b_interaction_z, sd_j = sd_x_3, sd_y = sd_y), Scale = sigma * sd_y, Normality = nu %&gt;% log10()) glimpse(post) ## Rows: 4,000 ## Columns: 6 ## $ Intercept &lt;dbl&gt; 1072.273, 1052.266, 1051.125, 1068.692, 1062.7… ## $ Spend &lt;dbl&gt; -1.6619618, 4.1428271, 2.3548164, -1.6373580, … ## $ `Percent Take` &lt;dbl&gt; -4.815437, -4.600243, -4.150411, -4.258956, -4… ## $ `Spend : Percent Take` &lt;dbl&gt; 0.31441969, 0.22608990, 0.21413414, 0.25557705… ## $ Scale &lt;dbl&gt; 28.15976, 32.34262, 29.09982, 29.94091, 29.149… ## $ Normality &lt;dbl&gt; 1.6180165, 1.1979350, 1.5089735, 1.5696519, 1.… Now we’ve done the conversions, here are the histograms of Figure 18.9. post %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 3) “To properly understand the credible slopes on the two predictors, we must consider the credible slopes on each predictor as a function of the value of the other predictor” (p. 528). This is our motivation for the middle panel of Figure 18.9. To make it, we’ll need to expand() our post, wrangle a bit, and plot with geom_pointrange(). # this will come in handy in `expand()` bounds &lt;- range(my_data$PrcntTake) p1 &lt;- # wrangle post %&gt;% expand(nesting(Spend, `Spend : Percent Take`), PrcntTake = seq(from = bounds[1], to = bounds[2], length.out = 20)) %&gt;% mutate(slope = Spend + `Spend : Percent Take` * PrcntTake) %&gt;% group_by(PrcntTake) %&gt;% median_hdi(slope) %&gt;% # plot ggplot(aes(x = PrcntTake, y = slope, ymin = .lower, ymax = .upper)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_pointrange(color = &quot;grey50&quot;) + labs(title = expression(&quot;Slope on spend is &quot;~beta[1]+beta[3]%.%prcnt_take), x = &quot;Value of prcnt_take&quot;, y = &quot;Slope on spend&quot;) We’ll follow the same basic order of operations for the final panel and then bind them together with patchwork. # this will come in handy in `expand()` bounds &lt;- range(my_data$Spend) p2 &lt;- # wrangle post %&gt;% expand(nesting(`Percent Take`, `Spend : Percent Take`), Spend = seq(from = bounds[1], to = bounds[2], length.out = 20)) %&gt;% mutate(slope = `Percent Take` + `Spend : Percent Take` * Spend) %&gt;% group_by(Spend) %&gt;% median_hdi(slope) %&gt;% # plot ggplot(aes(x = Spend, y = slope, ymin = .lower, ymax = .upper)) + geom_pointrange(color = &quot;grey50&quot;) + labs(title = expression(&quot;Slope on prcnt_take is &quot;~beta[2]+beta[3]%.%spend), x = &quot;Value of spend&quot;, y = &quot;Slope on prcnt_take&quot;) p1 / p2 Kruschke outlined all this in the opening paragraphs of page 530. His parting words of this subsection warrant repeating: “if you include an interaction term, you cannot ignore it even if its marginal posterior distribution includes zero” (p. 530). 18.3 Shrinkage of regression coefficients In some research, there are many candidate predictors which we suspect could possibly be informative about the predicted variable. For example, when predicting college GPA, we might include high-school GPA, high-school SAT score, income of student, income of parents, years of education of the parents, spending per pupil at the student’s high school, student IQ, student height, weight, shoe size, hours of sleep per night, distance from home to school, amount of caffeine consumed, hours spent studying, hours spent earning a wage, blood pressure, etc. We can include all the candidate predictors in the model, with a regression coefficient for every predictor. And this is not even considering interactions, which we will ignore for now. With so many candidate predictors of noisy data, there may be some regression coefficients that are spuriously estimated to be non-zero. We would like some protection against accidentally nonzero regression coefficients. (p. 530) That’s what this section is all about. We’ll make our random noise predictors with rnorm(). set.seed(18) my_data &lt;- my_data %&gt;% mutate(x_rand_1 = rnorm(n = n(), 0, 1), x_rand_2 = rnorm(n = n(), 0, 1), x_rand_3 = rnorm(n = n(), 0, 1), x_rand_4 = rnorm(n = n(), 0, 1), x_rand_5 = rnorm(n = n(), 0, 1), x_rand_6 = rnorm(n = n(), 0, 1), x_rand_7 = rnorm(n = n(), 0, 1), x_rand_8 = rnorm(n = n(), 0, 1), x_rand_9 = rnorm(n = n(), 0, 1), x_rand_10 = rnorm(n = n(), 0, 1), x_rand_11 = rnorm(n = n(), 0, 1), x_rand_12 = rnorm(n = n(), 0, 1)) glimpse(my_data) ## Rows: 50 ## Columns: 27 ## $ State &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Californ… ## $ Spend &lt;dbl&gt; 4.405, 8.963, 4.778, 4.459, 4.992, 5.443, 8.817, 7.03… ## $ StuTeaRat &lt;dbl&gt; 17.2, 17.6, 19.3, 17.1, 24.0, 18.4, 14.4, 16.6, 19.1,… ## $ Salary &lt;dbl&gt; 31.144, 47.951, 32.175, 28.934, 41.078, 34.571, 50.04… ## $ PrcntTake &lt;dbl&gt; 8, 47, 27, 6, 45, 29, 81, 68, 48, 65, 57, 15, 13, 58,… ## $ SATV &lt;dbl&gt; 491, 445, 448, 482, 417, 462, 431, 429, 420, 406, 407… ## $ SATM &lt;dbl&gt; 538, 489, 496, 523, 485, 518, 477, 468, 469, 448, 482… ## $ SATT &lt;dbl&gt; 1029, 934, 944, 1005, 902, 980, 908, 897, 889, 854, 8… ## $ prcnt_take_z &lt;dbl&gt; -1.0178453, 0.4394222, -0.3078945, -1.0925770, 0.3646… ## $ spend_z &lt;dbl&gt; -1.10086058, 2.24370805, -0.82716069, -1.06123647, -0… ## $ satt_z &lt;dbl&gt; 0.8430838, -0.4266207, -0.2929676, 0.5223163, -0.8543… ## $ prop_not_take &lt;dbl&gt; 0.92, 0.53, 0.73, 0.94, 0.55, 0.71, 0.19, 0.32, 0.52,… ## $ prop_not_take_z &lt;dbl&gt; 1.0178453, -0.4394222, 0.3078945, 1.0925770, -0.36469… ## $ interaction &lt;dbl&gt; 35.240, 421.261, 129.006, 26.754, 224.640, 157.847, 7… ## $ interaction_z &lt;dbl&gt; -0.94113720, 0.93111798, -0.48635915, -0.98229547, -0… ## $ x_rand_1 &lt;dbl&gt; 0.92645924, 1.82282117, -1.61056690, -0.28510975, -0.… ## $ x_rand_2 &lt;dbl&gt; -0.90258025, -1.13163679, 0.49708131, -0.54771876, -0… ## $ x_rand_3 &lt;dbl&gt; 0.51576102, 0.30710965, 0.66199996, 2.21990655, -2.04… ## $ x_rand_4 &lt;dbl&gt; 1.08730491, -1.23909473, 0.43161390, 1.06733141, -0.7… ## $ x_rand_5 &lt;dbl&gt; -0.23846777, 0.15702031, -1.02132795, 0.75395217, -2.… ## $ x_rand_6 &lt;dbl&gt; 0.06014956, 1.00555800, 1.47981871, -0.82827890, -0.5… ## $ x_rand_7 &lt;dbl&gt; 1.46961709, 0.51790320, -2.33110353, 0.11339996, 1.72… ## $ x_rand_8 &lt;dbl&gt; 0.03463437, -1.48737599, -0.01528284, 0.48480309, 0.2… ## $ x_rand_9 &lt;dbl&gt; -0.4556078, -0.7035475, -0.5001913, -0.6526022, 0.774… ## $ x_rand_10 &lt;dbl&gt; 1.2858586, -0.7474640, -0.3107255, -1.1037468, 0.3313… ## $ x_rand_11 &lt;dbl&gt; 0.17236599, -0.37956084, 0.31982301, 0.29678108, 1.22… ## $ x_rand_12 &lt;dbl&gt; -0.53048519, 0.92465424, 0.66876661, 0.30935146, 1.47… Here’s the naïve model. fit18.4 &lt;- update(fit18.1, newdata = my_data, formula = satt_z ~ 1 + prcnt_take_z + spend_z + x_rand_1 + x_rand_2 + x_rand_3 + x_rand_4 + x_rand_5 + x_rand_6 + x_rand_7 + x_rand_8 + x_rand_9 + x_rand_10 + x_rand_11 + x_rand_12, seed = 18, file = &quot;fits/fit18.04&quot;) Here we’ll examine the posterior with posterior_summary(). posterior_summary(fit18.4) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -0.02 0.07 -0.16 0.10 ## b_prcnt_take_z -1.12 0.09 -1.28 -0.95 ## b_spend_z 0.33 0.09 0.16 0.50 ## b_x_rand_1 0.03 0.06 -0.08 0.15 ## b_x_rand_2 0.02 0.09 -0.16 0.20 ## b_x_rand_3 0.09 0.07 -0.06 0.22 ## b_x_rand_4 -0.09 0.07 -0.22 0.05 ## b_x_rand_5 0.01 0.06 -0.12 0.13 ## b_x_rand_6 -0.02 0.08 -0.17 0.12 ## b_x_rand_7 -0.03 0.07 -0.17 0.11 ## b_x_rand_8 -0.18 0.07 -0.31 -0.04 ## b_x_rand_9 0.13 0.06 0.02 0.25 ## b_x_rand_10 0.00 0.05 -0.10 0.11 ## b_x_rand_11 0.05 0.09 -0.13 0.22 ## b_x_rand_12 -0.08 0.06 -0.21 0.04 ## sigma 0.38 0.07 0.23 0.52 ## nu 25.09 26.21 2.15 95.21 ## lp__ -54.70 3.49 -62.56 -49.11 Before we can make Figure 18.11, we’ll need to update our make_beta_0() function to accommodate this model. make_beta_0 &lt;- function(zeta_0, zeta_1, zeta_2, zeta_3, zeta_4, zeta_5, zeta_6, zeta_7, zeta_8, zeta_9, zeta_10, zeta_11, zeta_12, zeta_13, zeta_14, sd_x_1, sd_x_2, sd_x_3, sd_x_4, sd_x_5, sd_x_6, sd_x_7, sd_x_8, sd_x_9, sd_x_10, sd_x_11, sd_x_12, sd_x_13, sd_x_14, sd_y, m_x_1, m_x_2, m_x_3, m_x_4, m_x_5, m_x_6, m_x_7, m_x_8, m_x_9, m_x_10, m_x_11, m_x_12, m_x_13, m_x_14, m_y) { sd_y * zeta_0 + m_y - sd_y * ((zeta_1 * m_x_1 / sd_x_1) + (zeta_2 * m_x_2 / sd_x_2) + (zeta_3 * m_x_3 / sd_x_3) + (zeta_4 * m_x_4 / sd_x_4) + (zeta_5 * m_x_5 / sd_x_5) + (zeta_6 * m_x_6 / sd_x_6) + (zeta_7 * m_x_7 / sd_x_7) + (zeta_8 * m_x_8 / sd_x_8) + (zeta_9 * m_x_9 / sd_x_9) + (zeta_10 * m_x_10 / sd_x_10) + (zeta_11 * m_x_11 / sd_x_11) + (zeta_12 * m_x_12 / sd_x_12) + (zeta_13 * m_x_13 / sd_x_13) + (zeta_14 * m_x_14 / sd_x_14)) } Sigh, our poor make_beta_0() and make_beta_1() code is getting obscene. I don’t have the energy to think of how to wrap this into a simpler function. Someone probably should. If that ends up as you, do share your code. sd_x_1 &lt;- sd(my_data$Spend) sd_x_2 &lt;- sd(my_data$PrcntTake) sd_x_3 &lt;- sd(my_data$x_rand_1) sd_x_4 &lt;- sd(my_data$x_rand_2) sd_x_5 &lt;- sd(my_data$x_rand_3) sd_x_6 &lt;- sd(my_data$x_rand_4) sd_x_7 &lt;- sd(my_data$x_rand_5) sd_x_8 &lt;- sd(my_data$x_rand_6) sd_x_9 &lt;- sd(my_data$x_rand_7) sd_x_10 &lt;- sd(my_data$x_rand_8) sd_x_11 &lt;- sd(my_data$x_rand_9) sd_x_12 &lt;- sd(my_data$x_rand_10) sd_x_13 &lt;- sd(my_data$x_rand_11) sd_x_14 &lt;- sd(my_data$x_rand_12) sd_y &lt;- sd(my_data$SATT) m_x_1 &lt;- mean(my_data$Spend) m_x_2 &lt;- mean(my_data$PrcntTake) m_x_3 &lt;- mean(my_data$x_rand_1) m_x_4 &lt;- mean(my_data$x_rand_2) m_x_5 &lt;- mean(my_data$x_rand_3) m_x_6 &lt;- mean(my_data$x_rand_4) m_x_7 &lt;- mean(my_data$x_rand_5) m_x_8 &lt;- mean(my_data$x_rand_6) m_x_9 &lt;- mean(my_data$x_rand_7) m_x_10 &lt;- mean(my_data$x_rand_8) m_x_11 &lt;- mean(my_data$x_rand_9) m_x_12 &lt;- mean(my_data$x_rand_10) m_x_13 &lt;- mean(my_data$x_rand_11) m_x_14 &lt;- mean(my_data$x_rand_12) m_y &lt;- mean(my_data$SATT) post &lt;- posterior_samples(fit18.4) %&gt;% transmute(Intercept = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, zeta_3 = b_x_rand_1, zeta_4 = b_x_rand_2, zeta_5 = b_x_rand_3, zeta_6 = b_x_rand_4, zeta_7 = b_x_rand_5, zeta_8 = b_x_rand_6, zeta_9 = b_x_rand_7, zeta_10 = b_x_rand_8, zeta_11 = b_x_rand_9, zeta_12 = b_x_rand_10, zeta_13 = b_x_rand_11, zeta_14 = b_x_rand_12, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_x_3 = sd_x_3, sd_x_4 = sd_x_4, sd_x_5 = sd_x_5, sd_x_6 = sd_x_6, sd_x_7 = sd_x_7, sd_x_8 = sd_x_8, sd_x_9 = sd_x_9, sd_x_10 = sd_x_10, sd_x_11 = sd_x_11, sd_x_12 = sd_x_12, sd_x_13 = sd_x_13, sd_x_14 = sd_x_14, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_x_3 = m_x_3, m_x_4 = m_x_4, m_x_5 = m_x_5, m_x_6 = m_x_6, m_x_7 = m_x_7, m_x_8 = m_x_8, m_x_9 = m_x_9, m_x_10 = m_x_10, m_x_11 = m_x_11, m_x_12 = m_x_12, m_x_13 = m_x_13, m_x_14 = m_x_14, m_y = m_y), Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), x_rand_1 = make_beta_j(zeta_j = b_x_rand_1, sd_j = sd_x_3, sd_y = sd_y), x_rand_2 = make_beta_j(zeta_j = b_x_rand_2, sd_j = sd_x_4, sd_y = sd_y), x_rand_3 = make_beta_j(zeta_j = b_x_rand_3, sd_j = sd_x_5, sd_y = sd_y), x_rand_4 = make_beta_j(zeta_j = b_x_rand_4, sd_j = sd_x_6, sd_y = sd_y), x_rand_5 = make_beta_j(zeta_j = b_x_rand_5, sd_j = sd_x_7, sd_y = sd_y), x_rand_6 = make_beta_j(zeta_j = b_x_rand_6, sd_j = sd_x_8, sd_y = sd_y), x_rand_7 = make_beta_j(zeta_j = b_x_rand_7, sd_j = sd_x_9, sd_y = sd_y), x_rand_8 = make_beta_j(zeta_j = b_x_rand_8, sd_j = sd_x_10, sd_y = sd_y), x_rand_9 = make_beta_j(zeta_j = b_x_rand_9, sd_j = sd_x_11, sd_y = sd_y), x_rand_10 = make_beta_j(zeta_j = b_x_rand_10, sd_j = sd_x_12, sd_y = sd_y), x_rand_11 = make_beta_j(zeta_j = b_x_rand_11, sd_j = sd_x_13, sd_y = sd_y), x_rand_12 = make_beta_j(zeta_j = b_x_rand_12, sd_j = sd_x_14, sd_y = sd_y), Scale = sigma * sd_y, Normality = nu %&gt;% log10()) glimpse(post) ## Rows: 4,000 ## Columns: 17 ## $ Intercept &lt;dbl&gt; 978.2629, 954.1022, 936.3345, 927.7583, 964.4377, 962.… ## $ Spend &lt;dbl&gt; 16.23816, 21.85959, 26.38032, 26.63065, 18.40972, 20.5… ## $ `Percent Take` &lt;dbl&gt; -3.123617, -3.470104, -3.496485, -3.622503, -3.310432,… ## $ x_rand_1 &lt;dbl&gt; 0.37557071, 0.47578746, 2.01992361, -2.49333200, 1.832… ## $ x_rand_2 &lt;dbl&gt; -2.747509054, -0.637063446, 3.112304494, -4.640257872,… ## $ x_rand_3 &lt;dbl&gt; 8.9258844, 1.5485414, 3.0112088, 0.4144604, 11.0116601… ## $ x_rand_4 &lt;dbl&gt; -8.1747088, -9.1809834, -13.4576786, -14.0589291, -6.2… ## $ x_rand_5 &lt;dbl&gt; -0.5797268, 0.1277984, 0.6989087, -2.2552537, -0.35658… ## $ x_rand_6 &lt;dbl&gt; -6.499016480, 2.006864887, 2.694773713, 0.716707338, -… ## $ x_rand_7 &lt;dbl&gt; -7.4236507, 2.4005544, 0.5063171, 0.9188742, 10.253492… ## $ x_rand_8 &lt;dbl&gt; -11.9493748, -12.7780330, -12.2361496, -16.8079619, -1… ## $ x_rand_9 &lt;dbl&gt; 4.132977, 11.727639, 12.163225, 6.259811, 13.047699, 8… ## $ x_rand_10 &lt;dbl&gt; 0.19896636, 4.15941187, 4.02509269, 1.58380409, -2.119… ## $ x_rand_11 &lt;dbl&gt; 9.3111126, -4.5582639, -8.9587840, -9.6417867, -4.1413… ## $ x_rand_12 &lt;dbl&gt; -8.3675915, -2.5825711, -3.9334521, -3.8179492, -4.809… ## $ Scale &lt;dbl&gt; 26.16620, 19.98710, 20.51592, 26.75018, 32.71402, 33.9… ## $ Normality &lt;dbl&gt; 0.7053100, 0.7184234, 0.9159173, 0.4871434, 1.7521047,… Okay, here are the histograms of Figure 18.11. post %&gt;% select(Intercept:x_rand_3, x_rand_10:Normality) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;Intercept&quot;, &quot;Spend&quot;, &quot;Percent Take&quot;, &quot;x_rand_1&quot;, &quot;x_rand_2&quot;, &quot;x_rand_3&quot;, &quot;x_rand_10&quot;, &quot;x_rand_11&quot;, &quot;x_rand_12&quot;, &quot;Scale&quot;, &quot;Normality&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 3) And here’s the final histogram depicting the Bayesian \\(R^2\\). bayes_R2(fit18.4, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = R2, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 20, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(paste(&quot;Bayesian &quot;, italic(R)^2)), x = NULL) + coord_cartesian(xlim = c(.6, 1)) Note that unlike the one Kruschke displayed in the text, our brms::bayes_R2()-based \\(R^2\\) distribution did not exceed the logical right bound of 1. Sometimes when you have this many parameters you’d like to compare, it’s better to display their summaries with an ordered coefficient plot. post %&gt;% select(Spend:x_rand_12) %&gt;% gather() %&gt;% mutate(key = fct_reorder(key, value)) %&gt;% ggplot(aes(x = value, y = key)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + stat_pointintervalh(point_interval = mode_hdi, .width = .95, color = &quot;grey50&quot;) + labs(x = NULL, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid.major.y = element_line(color = &quot;grey98&quot;, linetype = 2)) Now we can see that by chance alone, the coefficients for x_rand_8 and x_rand_9 are clearly distinct from zero. The geom_density_ridges() function from the ggridges package can be informative, too. library(ggridges) post %&gt;% select(Spend:x_rand_12) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = reorder(key, value), group = reorder(key, value))) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_density_ridges(scale = 4, fill = &quot;grey50&quot;, color = &quot;grey50&quot;, alpha = 3/4, size = 1/5) + labs(x = NULL, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) With brms, we can fit something like the model Kruschke displayed in Figure 18.12 with the horseshoe() prior. From the horseshoe section of the brms reference manual: The horseshoe prior is a special shrinkage prior initially proposed by (Carvalho et al., 2009). It is symmetric around zero with fat tails and an infinitely large spike at zero. This makes it ideal for sparse models that have many regression coefficients, although only a minority of them is non-zero. The horseshoe prior can be applied on all population-level effects at once (excluding the intercept) by using set_prior(&quot;horseshoe(1)&quot;). The 1 implies that the student-t prior of the local shrinkage parameters has 1 degrees of freedom. (Bürkner, 2020g, p. 93) Based on the quote, here’s how to fit our horseshoe-prior model. fit18.5 &lt;- update(fit18.4, newdata = my_data, formula = satt_z ~ 1 + prcnt_take_z + spend_z + x_rand_1 + x_rand_2 + x_rand_3 + x_rand_4 + x_rand_5 + x_rand_6 + x_rand_7 + x_rand_8 + x_rand_9 + x_rand_10 + x_rand_11 + x_rand_12, prior = c(prior(normal(0, 2), class = Intercept), prior(horseshoe(1), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), seed = 18, file = &quot;fits/fit18.05&quot;) Check the parameter summary. posterior_summary(fit18.5) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -0.02 0.06 -0.14 0.11 ## b_prcnt_take_z -1.02 0.10 -1.20 -0.82 ## b_spend_z 0.20 0.11 0.00 0.39 ## b_x_rand_1 0.02 0.04 -0.05 0.10 ## b_x_rand_2 0.01 0.04 -0.08 0.11 ## b_x_rand_3 0.02 0.04 -0.06 0.12 ## b_x_rand_4 -0.03 0.05 -0.14 0.04 ## b_x_rand_5 0.00 0.04 -0.07 0.09 ## b_x_rand_6 -0.02 0.04 -0.13 0.05 ## b_x_rand_7 -0.01 0.04 -0.09 0.07 ## b_x_rand_8 -0.09 0.07 -0.24 0.02 ## b_x_rand_9 0.05 0.05 -0.02 0.17 ## b_x_rand_10 0.00 0.03 -0.06 0.07 ## b_x_rand_11 0.01 0.04 -0.07 0.12 ## b_x_rand_12 -0.02 0.04 -0.12 0.05 ## sigma 0.40 0.06 0.27 0.52 ## nu 30.80 28.89 2.86 108.14 ## lp__ -105.49 6.71 -119.76 -93.33 ## hs_c2 1.89 3.56 0.35 7.79 Our make_beta_0() and make_beta_1() code remains obscene. post &lt;- posterior_samples(fit18.5) %&gt;% transmute(Intercept = make_beta_0(zeta_0 = b_Intercept, zeta_1 = b_spend_z, zeta_2 = b_prcnt_take_z, zeta_3 = b_x_rand_1, zeta_4 = b_x_rand_2, zeta_5 = b_x_rand_3, zeta_6 = b_x_rand_4, zeta_7 = b_x_rand_5, zeta_8 = b_x_rand_6, zeta_9 = b_x_rand_7, zeta_10 = b_x_rand_8, zeta_11 = b_x_rand_9, zeta_12 = b_x_rand_10, zeta_13 = b_x_rand_11, zeta_14 = b_x_rand_12, sd_x_1 = sd_x_1, sd_x_2 = sd_x_2, sd_x_3 = sd_x_3, sd_x_4 = sd_x_4, sd_x_5 = sd_x_5, sd_x_6 = sd_x_6, sd_x_7 = sd_x_7, sd_x_8 = sd_x_8, sd_x_9 = sd_x_9, sd_x_10 = sd_x_10, sd_x_11 = sd_x_11, sd_x_12 = sd_x_12, sd_x_13 = sd_x_13, sd_x_14 = sd_x_14, sd_y = sd_y, m_x_1 = m_x_1, m_x_2 = m_x_2, m_x_3 = m_x_3, m_x_4 = m_x_4, m_x_5 = m_x_5, m_x_6 = m_x_6, m_x_7 = m_x_7, m_x_8 = m_x_8, m_x_9 = m_x_9, m_x_10 = m_x_10, m_x_11 = m_x_11, m_x_12 = m_x_12, m_x_13 = m_x_13, m_x_14 = m_x_14, m_y = m_y), Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), x_rand_1 = make_beta_j(zeta_j = b_x_rand_1, sd_j = sd_x_3, sd_y = sd_y), x_rand_2 = make_beta_j(zeta_j = b_x_rand_2, sd_j = sd_x_4, sd_y = sd_y), x_rand_3 = make_beta_j(zeta_j = b_x_rand_3, sd_j = sd_x_5, sd_y = sd_y), x_rand_4 = make_beta_j(zeta_j = b_x_rand_4, sd_j = sd_x_6, sd_y = sd_y), x_rand_5 = make_beta_j(zeta_j = b_x_rand_5, sd_j = sd_x_7, sd_y = sd_y), x_rand_6 = make_beta_j(zeta_j = b_x_rand_6, sd_j = sd_x_8, sd_y = sd_y), x_rand_7 = make_beta_j(zeta_j = b_x_rand_7, sd_j = sd_x_9, sd_y = sd_y), x_rand_8 = make_beta_j(zeta_j = b_x_rand_8, sd_j = sd_x_10, sd_y = sd_y), x_rand_9 = make_beta_j(zeta_j = b_x_rand_9, sd_j = sd_x_11, sd_y = sd_y), x_rand_10 = make_beta_j(zeta_j = b_x_rand_10, sd_j = sd_x_12, sd_y = sd_y), x_rand_11 = make_beta_j(zeta_j = b_x_rand_11, sd_j = sd_x_13, sd_y = sd_y), x_rand_12 = make_beta_j(zeta_j = b_x_rand_12, sd_j = sd_x_14, sd_y = sd_y), Scale = sigma * sd_y, Normality = nu %&gt;% log10()) glimpse(post) ## Rows: 4,000 ## Columns: 17 ## $ Intercept &lt;dbl&gt; 979.2471, 1023.5298, 1005.1208, 1019.9144, 987.8714, 9… ## $ Spend &lt;dbl&gt; 10.433664, 6.800388, 8.659539, 6.694374, 13.332008, 12… ## $ `Percent Take` &lt;dbl&gt; -2.647076, -2.530614, -2.792637, -2.863597, -2.827868,… ## $ x_rand_1 &lt;dbl&gt; -0.122140323, -1.158510122, 4.129300096, 2.557700885, … ## $ x_rand_2 &lt;dbl&gt; -0.08683436, -7.80656229, -0.29081925, -0.43797021, 7.… ## $ x_rand_3 &lt;dbl&gt; -0.52525371, 4.52651239, -1.31939726, -1.97089650, -2.… ## $ x_rand_4 &lt;dbl&gt; -0.14436314, 0.16171124, 1.24412546, 1.66430716, -5.34… ## $ x_rand_5 &lt;dbl&gt; -1.00575083, 7.17913473, -0.49749734, -0.80289945, 1.0… ## $ x_rand_6 &lt;dbl&gt; -7.70462095, -2.23838325, -1.33121990, -2.02961768, -0… ## $ x_rand_7 &lt;dbl&gt; -0.52338825, -0.97967618, -1.66890548, -3.60525742, 3.… ## $ x_rand_8 &lt;dbl&gt; -16.30586010, -4.60620698, 0.95248757, 1.40157515, -0.… ## $ x_rand_9 &lt;dbl&gt; -0.06430865, 14.47257381, 0.02975212, 0.24323983, 5.37… ## $ x_rand_10 &lt;dbl&gt; -0.693127222, 0.282968557, -0.575203802, 0.764815565, … ## $ x_rand_11 &lt;dbl&gt; 0.61800434, -0.14365478, 9.95488153, 13.63182415, -0.5… ## $ x_rand_12 &lt;dbl&gt; 0.004654011, -9.772196168, 0.691943833, 2.679307854, -… ## $ Scale &lt;dbl&gt; 32.13506, 27.57153, 36.89317, 37.52294, 37.52031, 22.6… ## $ Normality &lt;dbl&gt; 1.5010222, 1.2281283, 1.4576176, 1.5595392, 1.4483391,… And here are the majority of the histograms of Figure 18.12. post %&gt;% select(Intercept:x_rand_3, x_rand_10:Normality) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;Intercept&quot;, &quot;Spend&quot;, &quot;Percent Take&quot;, &quot;x_rand_1&quot;, &quot;x_rand_2&quot;, &quot;x_rand_3&quot;, &quot;x_rand_10&quot;, &quot;x_rand_11&quot;, &quot;x_rand_12&quot;, &quot;Scale&quot;, &quot;Normality&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 35, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 3) Based on the distributions for the random predictors, it looks like our brms horseshoe prior regularized more aggressively than Kruschke’s hierarchical prior in the text. And interestingly, look how our marginal posterior for Spend is bimodal. For kicks and giggles, here’s the corresponding coefficient plot for \\(\\beta_1\\) through \\(\\beta_{14}\\). post %&gt;% select(Spend:x_rand_12) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = reorder(key, value))) + geom_vline(xintercept = 0, color = &quot;white&quot;) + stat_pointintervalh(point_interval = mode_hdi, .width = .95, color = &quot;grey50&quot;) + labs(x = NULL, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid.major.y = element_line(color = &quot;grey98&quot;, linetype = 2)) But anyways, here’s that final Bayesian \\(R^2\\) histogram for Figure 18.12. bayes_R2(fit18.5, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = R2, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 20, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(paste(&quot;Bayesian &quot;, italic(R)^2)), x = NULL) + coord_cartesian(xlim = c(.6, 1)) Just recall, though, that our fit18.5 was not exactly like Kruschke’s model. Whereas we hard coded the scale of our Student-\\(t\\) horseshoe prior to be 1, Kruschke estimated it with help from the gamma distribution. I’m not aware that’s currently possible in brms. If I’m at fault and you know how to do it, please share your code. 18.4 Variable selection We can rewrite the linear regression model to accommodate whether it includes a predictor as \\[\\mu_i = \\beta_0 + \\sum_j \\delta_j \\beta_j x_{j, i},\\] where \\(\\delta\\) is a dummy for which 0 = not included 1 = included. I’m not aware of a way to use \\(\\delta\\) as an inclusion indicator in brms the way Kruschke implemented it in JAGS. And in fact, it appears this might be unfeasible within the Stan framework. But if you know of a way, please share your code. However, this issue can lead to a similar approach: information criteria. To do so, let’s follow Kruschke’s basic flow and use the first model from way back in subsection 18.1.1 as a starting point. The model formula was as follows. fit18.1$formula ## satt_z ~ 1 + spend_z + prcnt_take_z Taking interactions off the table for a moment, we can specify four model types with various combinations of the two predictors, prcnt_take_z and spend_z. fit18.1 was the first, which we might denote as \\(\\langle 1, 1 \\rangle\\). That leads to the remaining possibilities as \\(\\langle 1, 0 \\rangle:\\) satt_z ~ 1 + spend_z \\(\\langle 0, 1 \\rangle:\\) satt_z ~ 1 + prcnt_take_z \\(\\langle 0, 0 \\rangle:\\) satt_z ~ 1 Let’s fit those models. fit18.6 &lt;- update(fit18.1, formula = satt_z ~ 1 + spend_z, seed = 18, file = &quot;fits/fit18.06&quot;) fit18.7 &lt;- update(fit18.1, formula = satt_z ~ 1 + prcnt_take_z, seed = 18, file = &quot;fits/fit18.07&quot;) fit18.8 &lt;- brm(data = my_data, family = student, satt_z ~ 1, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 18, file = &quot;fits/fit18.08&quot;) We’ll compare our models with the LOO information criterion. Like other information criteria, the LOO values aren’t of interest in and of themselves. However, the values of one model’s LOO relative to that of another is of great interest. We generally prefer models with lower estimates. fit18.1 &lt;- add_criterion(fit18.1, &quot;loo&quot;) fit18.6 &lt;- add_criterion(fit18.6, &quot;loo&quot;) fit18.7 &lt;- add_criterion(fit18.7, &quot;loo&quot;) fit18.8 &lt;- add_criterion(fit18.8, &quot;loo&quot;) loo_compare(fit18.1, fit18.6, fit18.7, fit18.8) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit18.1 0.0 0.0 -31.9 5.4 4.0 0.7 63.8 10.7 ## fit18.7 -3.3 2.8 -35.2 4.4 2.8 0.5 70.4 8.8 ## fit18.6 -37.7 6.3 -69.6 4.0 2.1 0.3 139.3 8.1 ## fit18.8 -40.9 5.7 -72.7 3.1 1.4 0.2 145.5 6.1 In this case, fit18.1 and fit18.7 clearly have the lowest estimates, but the standard error of their difference score is about the same size as their difference. So the LOO difference score puts them on similar footing. Recall that you can do a similar analysis with the waic() function. Let’s compare that with the insights from the model_weights() function. (mw &lt;- model_weights(fit18.1, fit18.6, fit18.7, fit18.8)) ## fit18.1 fit18.6 fit18.7 fit18.8 ## 8.979279e-01 1.181674e-07 1.020720e-01 1.338500e-09 If you don’t like scientific notation, you can always wrangle and plot. mw %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% set_names(&quot;fit&quot;, &quot;estimate&quot;) %&gt;% ggplot(aes(x = estimate, y = reorder(fit, estimate))) + geom_text(aes(label = estimate %&gt;% round(3) %&gt;% as.character())) + scale_x_continuous(&quot;stacking weight&quot;, limits = c(0, 1)) + ylab(NULL) + theme(axis.ticks.y = element_blank()) Based on this weighting scheme, almost all the weight went to the full model, fit18.1. But note, in the intro of their vignette on the topic, Vehtari &amp; Gabry (2019) opined: Ideally, we would avoid the Bayesian model combination problem by extending the model to include the separate models as special cases, and preferably as a continuous expansion of the model space. For example, instead of model averaging over different covariate combinations, all potentially relevant covariates should be included in a predictive model (for causal analysis more care is needed) and a prior assumption that only some of the covariates are relevant can be presented with regularized horseshoe prior (Piironen &amp; Vehtari, 2017). For variable selection we recommend projective predictive variable selection (Piironen and Vehtari, 2017; projpred package). Perhaps unsurprisingly, their thoughts on the topic are similar with the Gelman et al quotation Kruschke provided on page 536: Some prominent authors eschew the variable-selection approach for typical applications in their fields. For example, (Gelman et al., 2013, p. 369) said, “For the regressions we typically see, we do not believe any coefficients to be truly zero and we do not generally consider it a conceptual (as opposed to computational) advantage to get point estimates of zero—but regularized estimates such as obtained by lasso can be much better than those resulting from simple least squares and flat prior distributions …we are not comfortable with an underlying model in which the coefficients can be exactly zero.” For more on some of these methods, check out Vehtari’s GitHub repository, Tutorial on model assessment, model selection and inference after model selection. But anyways, our model weighting methods cohered with Kruschke’s \\(\\delta\\)-inclusion-indicator method in that both suggested the full model, fit1, and the model with prcnt_take as the sole predictor, fit7, were given the greatest weight. I’m not aware that our information criteria weighting/model stacking methods provide probability distributions of the type Kruschke displayed in the left portions of Figure 18.13. But we can at least recreate the plots in the other panels. # first we&#39;ll get the posterior draws from `fit18.1` and wrangle them posterior_samples(fit18.1) %&gt;% transmute(Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% gather() %&gt;% # within `bind_rows()`, we extract and wrangle the posterior draws from `fit18.7` and them insert them below those from `fit18.1` bind_rows( posterior_samples(fit18.7) %&gt;% transmute(value = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% mutate(key = &quot;PrcntTake&quot;) ) %&gt;% # now we just need a little indexing and factor ordering mutate(model = rep(c(&quot;fit18.1&quot;, &quot;fit18.7&quot;), times = c(8000, 4000)), key = factor(key, levels = c(&quot;Spend&quot;, &quot;PrcntTake&quot;))) %&gt;% # we finally plot! ggplot(aes(x = value, y = 0, fill = model)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_fill_viridis_d(option = &quot;D&quot;, begin = .35, end = .65) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(legend.position = &quot;none&quot;) + facet_grid(model~key, scales = &quot;free&quot;) 18.4.1 Inclusion probability is strongly affected by vagueness of prior. To follow along, let’s fit the models with the updated \\(SD = 1\\) on the \\(\\beta_{1+}\\) priors code. fit18.9 &lt;- update(fit18.1, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 18, file = &quot;fits/fit18.09&quot;) fit18.10 &lt;- update(fit18.9, formula = satt_z ~ 1 + spend_z, seed = 18, file = &quot;fits/fit18.10&quot;) fit18.11 &lt;- update(fit18.9, formula = satt_z ~ 1 + prcnt_take_z, seed = 18, file = &quot;fits/fit18.11&quot;) fit18.12 &lt;- update(fit18.8, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), seed = 18, file = &quot;fits/fit18.12&quot;) And now we’ll fit the models with the updated \\(SD = 10\\). fit18.13 &lt;- update(fit18.9, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 10), class = sigma), prior(exponential(one_over_twentynine), class = nu)), seed = 18, file = &quot;fits/fit18.13&quot;) fit18.14 &lt;- update(fit18.13, formula = satt_z ~ 1 + spend_z, seed = 18, file = &quot;fits/fit18.14&quot;) fit18.15 &lt;- update(fit18.13, formula = satt_z ~ 1 + prcnt_take_z, seed = 18, file = &quot;fits/fit18.15&quot;) fit18.16 &lt;- update(fit18.12, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = sigma), prior(exponential(one_over_twentynine), class = nu)), seed = 18, file = &quot;fits/fit18.16&quot;) Now we’ve fit the models, we’re ready to examine how altering the \\(SD\\)s on the \\(\\beta_j\\) priors influenced the model comparisons via model_weights(). Here we’ll use the default stacking method. mw %&gt;% rbind(model_weights(fit18.9, fit18.10, fit18.11, fit18.12), model_weights(fit18.13, fit18.14, fit18.15, fit18.16)) %&gt;% as_tibble() %&gt;% set_names(&quot;1, 1&quot;, &quot;1, 0&quot;, &quot;0, 1&quot;, &quot;0, 0&quot;) %&gt;% gather() %&gt;% mutate(prior = rep(str_c(&quot;SD = &quot;, c(10, 2, 1)), times = 4) %&gt;% factor(., levels = str_c(&quot;SD = &quot;, c(10, 2, 1)))) %&gt;% ggplot(aes(x = value, y = reorder(key, value))) + geom_text(aes(label = value %&gt;% round(3) %&gt;% as.character())) + coord_cartesian(xlim = c(0, 1)) + labs(x = &quot;Stacking weight&quot;, y = expression(paste(&quot;Models defined by Kruschke&#39;s &quot;, delta, &quot; notation&quot;))) + theme(axis.ticks.y = element_blank()) + facet_grid(prior~.) So unlike in the depictions in Figure 18.14, the stacking method was insensitive to the \\(SD\\)s on our \\(\\beta_j\\) priors. We might compare LOO difference scores, too. fit18.9 &lt;- add_criterion(fit18.9, &quot;loo&quot;) fit18.10 &lt;- add_criterion(fit18.10, &quot;loo&quot;) fit18.11 &lt;- add_criterion(fit18.11, &quot;loo&quot;) fit18.12 &lt;- add_criterion(fit18.12, &quot;loo&quot;) fit18.13 &lt;- add_criterion(fit18.13, &quot;loo&quot;) fit18.14 &lt;- add_criterion(fit18.14, &quot;loo&quot;) fit18.15 &lt;- add_criterion(fit18.15, &quot;loo&quot;) fit18.16 &lt;- add_criterion(fit18.16, &quot;loo&quot;) loo_compare(fit18.1, fit18.6, fit18.7, fit18.8) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit18.1 0.0 0.0 -31.9 5.4 4.0 0.7 63.8 10.7 ## fit18.7 -3.3 2.8 -35.2 4.4 2.8 0.5 70.4 8.8 ## fit18.6 -37.7 6.3 -69.6 4.0 2.1 0.3 139.3 8.1 ## fit18.8 -40.9 5.7 -72.7 3.1 1.4 0.2 145.5 6.1 loo_compare(fit18.9, fit18.10, fit18.11, fit18.12) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit18.9 0.0 0.0 -32.0 5.4 4.0 0.7 63.9 10.7 ## fit18.11 -3.2 2.8 -35.1 4.4 2.7 0.4 70.2 8.8 ## fit18.10 -37.6 6.3 -69.5 4.0 2.1 0.3 139.0 8.1 ## fit18.12 -40.7 5.7 -72.7 3.1 1.3 0.2 145.3 6.1 loo_compare(fit18.13, fit18.14, fit18.15, fit18.16) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit18.13 0.0 0.0 -31.9 5.4 3.9 0.7 63.8 10.7 ## fit18.15 -3.2 2.9 -35.1 4.4 2.7 0.5 70.3 8.8 ## fit18.14 -37.6 6.3 -69.5 4.0 2.0 0.3 139.1 8.0 ## fit18.16 -40.9 5.7 -72.8 3.0 1.4 0.2 145.7 6.0 The LOO difference score patterns were also about the same across the \\(SD\\)s on our \\(\\beta_j\\) priors. Let’s finish up with the histograms comparing the model predictors. Here’s the code for those in the top portion of Figure 18.14. # first we&#39;ll get the posterior draws from `fit18.9` and wrangle them posterior_samples(fit18.9) %&gt;% transmute(Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% gather() %&gt;% # within `bind_rows()`, we extract and wrangle the posterior draws from `fit18.11` and # then insert them below those from `fit18.9` bind_rows( posterior_samples(fit18.11) %&gt;% transmute(value = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% mutate(key = &quot;PrcntTake&quot;) ) %&gt;% # now we just need a little indexing and factor ordering mutate(model = rep(c(&quot;fit18.9&quot;, &quot;fit18.11&quot;), times = c(8000, 4000)) %&gt;% factor(., levels = c(&quot;fit18.9&quot;, &quot;fit18.11&quot;)), key = factor(key, levels = c(&quot;Spend&quot;, &quot;PrcntTake&quot;))) %&gt;% # we finally plot! ggplot(aes(x = value, y = 0, fill = model)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_fill_viridis_d(option = &quot;D&quot;, begin = .35, end = .65) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(legend.position = &quot;none&quot;) + facet_grid(model~key, scales = &quot;free&quot;) And now we’ll do the histograms for the bottom portion of Figure 18.14. posterior_samples(fit18.13) %&gt;% transmute(Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% gather() %&gt;% bind_rows( posterior_samples(fit18.15) %&gt;% transmute(value = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% mutate(key = &quot;PrcntTake&quot;) ) %&gt;% mutate(model = rep(c(&quot;fit18.13&quot;, &quot;fit18.15&quot;), times = c(8000, 4000)) %&gt;% factor(., levels = c(&quot;fit18.13&quot;, &quot;fit18.15&quot;)), key = factor(key, levels = c(&quot;Spend&quot;, &quot;PrcntTake&quot;))) %&gt;% ggplot(aes(x = value, y = 0, fill = model)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_fill_viridis_d(option = &quot;D&quot;, begin = .35, end = .65) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(legend.position = &quot;none&quot;) + facet_grid(model~key, scales = &quot;free&quot;) Kurschke concluded this subsection with Bayesian model comparison can be strongly affected by the degree of vagueness in the priors, even though explicit estimates of the parameter values may be minimally affected. Therefore, be very cautious when interpreting the results of Bayesian variable selection. The next section discusses a way to inform the prior by using concurrent data instead of previous data. (p. 542) We should note that while the method in text was “strongly affected by the degree of vagueness in the priors”, the information-criteria and model weighting methods, above, were not. If you’re interested in comparing models within the Bayesian paradigm, choose your method with care. 18.4.2 Variable selection with hierarchical shrinkage. Kruschke opened the subsection with a few good points: If you have strong previous research that can inform the prior, then it should be used. But if previous knowledge is weak, then the uncertainty should be expressed in the prior. This is an underlying mantra of the Bayesian approach: Any uncertainty should be expressed in the prior. (p. 543) Here we’ll standardize our new predictors, StuTeaRat and Salary. my_data &lt;- my_data %&gt;% mutate(stu_tea_rat_z = standardize(StuTeaRat), salary_z = standardize(Salary)) We can use Kruschke’s gamma_s_and_r_from_mode_sd() function to return the exact shape and rate parameters to make a gamma with a mode of 1 and an \\(SD\\) of 10. gamma_s_and_r_from_mode_sd &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } Here are the values. (p &lt;- gamma_s_and_r_from_mode_sd(mode = 1, sd = 10) %&gt;% as.numeric()) ## [1] 1.1051249 0.1051249 That gamma distribution looks like this. tibble(x = seq(from = 0, to = 55, length.out = 1e3)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = dgamma(x, p[1], p[2]))) + geom_ribbon(size = 0, fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Our gamma prior&quot;) + coord_cartesian(xlim = c(0, 50)) We can code those values in with arbitrary precision with the stanvar() function. stanvars &lt;- stanvar(1/29, name = &quot;one_over_twentynine&quot;) + stanvar(p[1], name = &quot;my_shape&quot;) + stanvar(p[2], name = &quot;my_rate&quot;) + stanvar(scode = &quot; real&lt;lower=0&gt; tau;&quot;, block = &quot;parameters&quot;) Note that last stanvar() line. Bürkner recently posted an exemplar of how to set a hierarchical prior on a regression coefficient in a brm() model: # define a hierachical prior on the regression coefficients bprior &lt;- set_prior(&quot;normal(0, tau)&quot;, class = &quot;b&quot;) + set_prior(&quot;target += normal_lpdf(tau | 0, 10)&quot;, check = FALSE) stanvars &lt;- stanvar(scode = &quot; real&lt;lower=0&gt; tau;&quot;, block = &quot;parameters&quot;) make_stancode(count ~ Trt + log_Base4_c, epilepsy, prior = bprior, stanvars = stanvars) Following the method, we tell brm() we’d like to estimate the \\(SD\\) of our \\(\\beta_{1+}\\) priors with prior(normal(0, tau), class = b), where the tau is a stand-in for the \\(SD\\). In the next line, set_prior(&quot;target += gamma_lpdf(tau | my_shape, my_rate)&quot;, check = FALSE), we tell brm() we’d like to estimate tau with a gamma(my_shape, my_rate), the values for which were saved in our stanvars object, above. And it’s that stanvar() line in that code wherein we told brm() we’d like that parameter to have a lower bound of 0. Let’s put it to use. fit18.1111 &lt;- brm(data = my_data, family = student, satt_z ~ 1 + spend_z + prcnt_take_z + stu_tea_rat_z + salary_z, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, tau), class = b), set_prior(&quot;target += gamma_lpdf(tau | my_shape, my_rate)&quot;, check = FALSE), prior(normal(0, 1), class = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, control = list(adapt_delta = .99), stanvars = stanvars, seed = 18, file = &quot;fits/fit18.1111&quot;) fit18.0111 &lt;- update(fit18.1111, formula = satt_z ~ 1 + prcnt_take_z + stu_tea_rat_z + salary_z, file = &quot;fits/fit18.0111&quot;) fit18.1011 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + stu_tea_rat_z + salary_z, file = &quot;fits/fit18.1011&quot;) fit18.1101 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + prcnt_take_z + salary_z, file = &quot;fits/fit18.1101&quot;) fit18.1110 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + prcnt_take_z + stu_tea_rat_z , file = &quot;fits/fit18.1110&quot;) fit18.0011 &lt;- update(fit18.1111, formula = satt_z ~ 1 + stu_tea_rat_z + salary_z, file = &quot;fits/fit18.0011&quot;) fit18.0101 &lt;- update(fit18.1111, formula = satt_z ~ 1 + prcnt_take_z + salary_z, file = &quot;fits/fit18.0101&quot;) fit18.0110 &lt;- update(fit18.1111, formula = satt_z ~ 1 + prcnt_take_z + stu_tea_rat_z , file = &quot;fits/fit18.0110&quot;) fit18.1001 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + salary_z, file = &quot;fits/fit18.1001&quot;) fit18.1010 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + stu_tea_rat_z , file = &quot;fits/fit18.1010&quot;) fit18.1100 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z + prcnt_take_z , file = &quot;fits/fit18.1100&quot;) fit18.0001 &lt;- update(fit18.1111, formula = satt_z ~ 1 + salary_z, file = &quot;fits/fit18.0001&quot;) fit18.0010 &lt;- update(fit18.1111, formula = satt_z ~ 1 + stu_tea_rat_z , file = &quot;fits/fit18.0010&quot;) fit18.0100 &lt;- update(fit18.1111, formula = satt_z ~ 1 + prcnt_take_z , file = &quot;fits/fit18.0100&quot;) fit18.1000 &lt;- update(fit18.1111, formula = satt_z ~ 1 + spend_z , file = &quot;fits/fit18.1000&quot;) fit18.0000 &lt;- update(fit18.1111, formula = satt_z ~ 1 , file = &quot;fits/fit18.0000&quot;) In order to keep track of the next 16 models, we switched our usual naming convention. Instead of continuing on keeping on calling them fit18.17 through fit18.33, we used Kruschke’s \\(\\delta\\) 0/1 convention. If we set the formula for the full model as satt_z ~ 1 + spend_z + prcnt_take_z + stu_tea_rat_z + salary_z, the name becomes fit18.1111. Accordingly, we called the model omitting spend_z, the first predictor, fit18.0111, and so on. Before we go any further, here are the correlations among the \\(\\beta\\)s for the full model, fit18.1111. vcov(fit18.1111, correlation = T) %&gt;% round(digits = 3) ## Intercept spend_z prcnt_take_z stu_tea_rat_z salary_z ## Intercept 1.000 0.009 -0.007 0.019 -0.023 ## spend_z 0.009 1.000 0.039 0.688 -0.883 ## prcnt_take_z -0.007 0.039 1.000 0.242 -0.344 ## stu_tea_rat_z 0.019 0.688 0.242 1.000 -0.683 ## salary_z -0.023 -0.883 -0.344 -0.683 1.000 Once again, the HMC correlations differ from Kruschke’s JAGS correlations. Moving on–behold the model weights. ( mw &lt;- model_weights(fit18.1111, fit18.0111, fit18.1011, fit18.1101, fit18.1110, fit18.0011, fit18.0101, fit18.0110, fit18.1001, fit18.1010, fit18.1100, fit18.0001, fit18.0010, fit18.0100, fit18.1000, fit18.0000) ) ## fit18.1111 fit18.0111 fit18.1011 fit18.1101 fit18.1110 fit18.0011 ## 4.560016e-04 2.773483e-01 5.293648e-05 1.447872e-05 2.004514e-03 2.494391e-03 ## fit18.0101 fit18.0110 fit18.1001 fit18.1010 fit18.1100 fit18.0001 ## 2.187080e-03 1.017260e-02 9.408875e-06 1.607469e-06 2.005920e-01 8.190033e-02 ## fit18.0010 fit18.0100 fit18.1000 fit18.0000 ## 1.457537e-06 3.404205e-01 8.234444e-02 1.259481e-11 We’ll plot our model weights like before. mw %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% set_names(&quot;fit&quot;, &quot;weight&quot;) %&gt;% mutate(fit = str_remove(fit, &quot;fit18.&quot;)) %&gt;% ggplot(aes(x = weight, y = reorder(fit, weight))) + geom_text(aes(label = weight %&gt;% round(3) %&gt;% as.character()), size = 3) + coord_cartesian(xlim = c(0, 1)) + labs(x = &quot;Stacking weight&quot;, y = &quot;fit18.[xxxx]&quot;) + theme(axis.ticks.y = element_blank()) As you might notice, pattern among model weights is similar with but not identical to the one among the model probabilities Kruschke displayed in Figure 18.15. Here we’ll plot the histograms for our top six. # first, we need to redefine `sd_x_3` and `sd_x_4` in terms of our two new predictors sd_x_3 &lt;- sd(my_data$StuTeaRat) sd_x_4 &lt;- sd(my_data$Salary) ## Now we&#39;ll start extracting our posterior samples and wrangling them, by model # fit18.0100 posterior_samples(fit18.0100) %&gt;% transmute(PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% gather() %&gt;% mutate(fit = &quot;fit18.0100&quot;) %&gt;% # fit18.0111 bind_rows( posterior_samples(fit18.0111) %&gt;% transmute(PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), StuTeaRat = make_beta_j(zeta_j = b_stu_tea_rat_z, sd_j = sd_x_3, sd_y = sd_y), Salary = make_beta_j(zeta_j = b_salary_z, sd_j = sd_x_4, sd_y = sd_y)) %&gt;% gather() %&gt;% mutate(fit = &quot;fit18.0111&quot;) ) %&gt;% # fit18.1100 bind_rows( posterior_samples(fit18.1100) %&gt;% transmute(Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y), PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y)) %&gt;% gather() %&gt;% mutate(fit = &quot;fit18.1100&quot;) ) %&gt;% # fit18.0001 bind_rows( posterior_samples(fit18.0001) %&gt;% transmute(Salary = make_beta_j(zeta_j = b_salary_z, sd_j = sd_x_4, sd_y = sd_y)) %&gt;% gather() %&gt;% mutate(fit = &quot;fit18.0001&quot;) ) %&gt;% # fit18.1000 bind_rows( posterior_samples(fit18.1000) %&gt;% transmute(Spend = make_beta_j(zeta_j = b_spend_z, sd_j = sd_x_1, sd_y = sd_y)) %&gt;% gather() %&gt;% mutate(fit = &quot;fit18.1000&quot;) ) %&gt;% # fit18.0110 # spend_z + prcnt_take_z + stu_tea_rat_z bind_rows( posterior_samples(fit18.0110) %&gt;% transmute(PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z, sd_j = sd_x_2, sd_y = sd_y), StuTeaRat = make_beta_j(zeta_j = b_stu_tea_rat_z, sd_j = sd_x_3, sd_y = sd_y)) %&gt;% gather() %&gt;% mutate(fit = &quot;fit18.0110&quot;) ) %&gt;% # the next two lines just help order the grid the plots appear in mutate(key = factor(key, levels = c(&quot;Spend&quot;, &quot;PrcntTake&quot;, &quot;StuTeaRat&quot;, &quot;Salary&quot;)), fit = factor(fit, levels = c(&quot;fit18.0100&quot;, &quot;fit18.0111&quot;, &quot;fit18.1100&quot;, &quot;fit18.0001&quot;, &quot;fit18.1000&quot;, &quot;fit18.0110&quot;))) %&gt;% # finally, the plot! ggplot(aes(x = value, y = 0, fill = fit)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_fill_viridis_d(option = &quot;D&quot;, begin = .2, end = .8) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(legend.position = &quot;none&quot;) + facet_grid(fit~key, scales = &quot;free&quot;) Like Kruschke’s results in the text, PrcntTake was the most prominant predictor. 18.4.3 What to report and what to conclude. Kruschke made a great point in the opening paragraph of this subsection. It might make sense to use the single most credible model, especially if it is notably more credible than the runner up, and if the goal is to have a parsimonious explanatory description of the data. But it is important to recognize that using the single best model, when it excludes some predictors, is concluding that the regression coefficients on the excluded predictors are exactly zero. (p. 546) Later he added “A forthright report should state the posterior probabilities of the several top models. Additionally, it can be useful to report, for each model, the ratio of its posterior probability relative to that of the best model” (p. 546). With our information criteria and model weights approach, we don’t have posterior probabilities for the models themselves. But we can report on their information criteria comparisons and weights. In the final paragraph of the subsection, Kruschke wrote: When the goal is prediction of \\(y\\) for interesting values of the predictors, as opposed to parsimonious explanation, then it is usually not appropriate to use only the single most probable model. Instead, predictions should be based on as much information as possible, using all models to the extent that they are credible. This approach is called Bayesian model averaging (BMA). (p. 547) It’s worth it to walk this out a bit. With brms, one can use brms::pp_average() to get the weighted posterior distributions for the model parameters. This is a natural extension of our model weights comparisons. # how many points on the x-axis? n_points &lt;- 30 # what vales of the predictors would we like to evaluate the weighted posterior over? nd &lt;- tibble(spend_z = seq(from = -3, to = 3, length.out = n_points), prcnt_take_z = 0, stu_tea_rat_z = 0, salary_z = 0) pp &lt;- # the first things we feed into `pp_average()` are the `brm()` fits we&#39;d like to average over pp_average(fit18.1111, fit18.0111, fit18.1011, fit18.1101, fit18.1110, fit18.0011, fit18.0101, fit18.0110, fit18.1001, fit18.1010, fit18.1100, fit18.0001, fit18.0010, fit18.0100, fit18.1000, fit18.0000, # here we tell it to evaluate the posterior over these predictor values newdata = nd, # we can get the mean trends using the &quot;fitted&quot; method method = &quot;fitted&quot;, # by `robust`, we mean we&#39;d like the Estimate in terms of posterior medians, rather than means robust = T) str(pp) ## num [1:30, 1:4] -0.0155 -0.0155 -0.0155 -0.0154 -0.0155 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## - attr(*, &quot;weights&quot;)= Named num [1:16] 4.56e-04 2.77e-01 5.29e-05 1.45e-05 2.00e-03 ... ## ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;fit18.1111&quot; &quot;fit18.0111&quot; &quot;fit18.1011&quot; &quot;fit18.1101&quot; ... ## - attr(*, &quot;nsamples&quot;)= Named num [1:16] 2 1109 0 0 8 ... ## ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;fit18.1111&quot; &quot;fit18.0111&quot; &quot;fit18.1011&quot; &quot;fit18.1101&quot; ... The pp object will require a little wrangling before it’s of use for ggplot2. pp %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = spend_z, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_pointrange(color = &quot;grey50&quot;) + labs(x = &quot;Value of Spend_z&quot;, y = &quot;Standardized SATT&quot;) We can build on this to make a plot considering each of the four predictors. But first that requires we make a new nd tibble to feed into pp_average(). # how many points on the x-axis? n_points &lt;- 30 # what vales of the predictors would we like to evaluate the weighted posterior over? nd &lt;- tibble(spend_z = c(seq(from = -3, to = 3, length.out = n_points), rep(0, times = n_points * 3)), prcnt_take_z = c(rep(0, times = n_points), seq(from = -3, to = 3, length.out = n_points), rep(0, times = n_points * 2)), stu_tea_rat_z = c(rep(0, times = n_points * 2), seq(from = -3, to = 3, length.out = n_points), rep(0, times = n_points)), salary_z = c(rep(0, times = n_points * 3), seq(from = -3, to = 3, length.out = n_points))) pp &lt;- pp_average(fit18.1111, fit18.0111, fit18.1011, fit18.1101, fit18.1110, fit18.0011, fit18.0101, fit18.0110, fit18.1001, fit18.1010, fit18.1100, fit18.0001, fit18.0010, fit18.0100, fit18.1000, fit18.0000, newdata = nd, method = &quot;fitted&quot;, robust = T, # note the `probs` argument probs = c(.025, .975, .1, .9, .25, .75)) str(pp) ## num [1:120, 1:8] -0.0144 -0.0144 -0.0144 -0.0144 -0.0144 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:8] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ... ## - attr(*, &quot;weights&quot;)= Named num [1:16] 4.56e-04 2.77e-01 5.29e-05 1.45e-05 2.00e-03 ... ## ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;fit18.1111&quot; &quot;fit18.0111&quot; &quot;fit18.1011&quot; &quot;fit18.1101&quot; ... ## - attr(*, &quot;nsamples&quot;)= Named num [1:16] 2 1109 0 0 8 ... ## ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;fit18.1111&quot; &quot;fit18.0111&quot; &quot;fit18.1011&quot; &quot;fit18.1101&quot; ... In each panel of the plot, below, we focus on one predictor. For that predictor, we hold all other three at their mean, which, since they are all standardized, is zero. We consider the posterior predictions for standardized SAT scores across a range of values each focal predictor. The posterior predictions are depicted in terms of 95%, 80%, and 50% percentile-based interval bands and a line at the median. pp %&gt;% as_tibble() %&gt;% mutate(x = seq(from = -3, to = 3, length.out = n_points) %&gt;% rep(., times = 4), predictor = rep(c(&quot;Spend_z&quot;, &quot;PrcntTake_z&quot;, &quot;StuTeaRat_z&quot;, &quot;Salary_z&quot;), each = n_points)) %&gt;% ggplot(aes(x = x)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = predictor), alpha = 1/5) + geom_ribbon(aes(ymin = Q10, ymax = Q90, fill = predictor), alpha = 1/4) + geom_ribbon(aes(ymin = Q25, ymax = Q75, fill = predictor), alpha = 1/3) + geom_line(aes(y = Estimate, color = predictor), size = 1) + scale_fill_viridis_d(option = &quot;D&quot;, begin = .1, end = .6) + scale_color_viridis_d(option = &quot;D&quot;, begin = .1, end = .6) + labs(x = &quot;Standardized value of the focal predictor&quot;, y = &quot;Standardized SATT&quot;) + theme(legend.position = &quot;none&quot;) + facet_grid(predictor ~ ., scales = &quot;free&quot;) Based on the weighted average across the models, the PrcntTake_z predictor was the pmost potent. 18.4.4 Caution: Computational methods. To conclude this section regarding variable selection, it is appropriate to recapitulate the considerations at the beginning of the section. Variable selection is a reasonable approach only if it is genuinely plausible and meaningful that candidate predictors have zero relation to the predicted variable. The results can be surprisingly sensitive to the seemingly innocuous choice of prior for the regression coefficients, and, of course, the prior for the inclusion probability. Because of these limitations, hierarchical shrinkage priors may be a more meaningful approach. (p. 548) 18.4.5 Caution: Interaction variables. When interaction terms are included in a model that also has hierarchical shrinkage on regression coefficients, the interaction coefficients should not be put under the same higher-level prior distribution as the individual component coefficients, because interaction coefficients are conceptually from a different class of variables than individual components. (pp. 548–549) Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggridges_0.5.2 bayesplot_1.7.1 tidybayes_2.0.3.9000 ## [4] brms_2.12.0 Rcpp_1.0.4.6 patchwork_1.0.0 ## [7] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 ## [10] purrr_0.3.4 readr_1.3.1 tidyr_1.0.2 ## [13] tibble_3.0.1 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 rsconnect_0.8.16 ## [4] markdown_1.1 base64enc_0.1-3 fs_1.4.1 ## [7] rstudioapi_0.11 farver_2.0.3 rstan_2.19.3 ## [10] svUnit_1.0.3 DT_0.13 fansi_0.4.1 ## [13] mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 ## [16] bridgesampling_1.0-0 mnormt_1.5-6 knitr_1.28 ## [19] shinythemes_1.1.2 jsonlite_1.6.1 broom_0.5.5 ## [22] dbplyr_1.4.2 shiny_1.4.0.2 compiler_3.6.3 ## [25] httr_1.4.1 backports_1.1.6 assertthat_0.2.1 ## [28] Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 ## [31] later_1.0.0 prettyunits_1.1.1 htmltools_0.4.0 ## [34] tools_3.6.3 igraph_1.2.5 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 ## [40] cellranger_1.1.0 vctrs_0.3.0 nlme_3.1-144 ## [43] crosstalk_1.1.0.1 psych_1.9.12.31 xfun_0.13 ## [46] ps_1.3.3 rvest_0.3.5 mime_0.9 ## [49] miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 ## [52] MASS_7.3-51.5 zoo_1.8-7 scales_1.1.1 ## [55] colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [58] Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 ## [64] StanHeaders_2.21.0-1 loo_2.2.0 stringi_1.4.6 ## [67] dygraphs_1.1.1.6 pkgbuild_1.0.8 rlang_0.4.6 ## [70] pkgconfig_2.0.3 matrixStats_0.56.0 HDInterval_0.2.0 ## [73] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [76] htmlwidgets_1.5.1 labeling_0.3 tidyselect_1.0.0 ## [79] processx_3.4.2 plyr_1.8.6 magrittr_1.5 ## [82] bookdown_0.18 R6_2.4.1 generics_0.0.2 ## [85] DBI_1.1.0 pillar_1.4.4 haven_2.2.0 ## [88] withr_2.2.0 xts_0.12-0 abind_1.4-5 ## [91] modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 ## [94] utf8_1.1.4 rmarkdown_2.1 grid_3.6.3 ## [97] readxl_1.3.1 callr_3.4.3 threejs_0.3.3 ## [100] reprex_0.3.0 digest_0.6.25 xtable_1.8-4 ## [103] httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 ## [106] viridisLite_0.3.0 shinyjs_1.1 References "],
["metric-predicted-variable-with-one-nominal-predictor.html", "19 Metric Predicted Variable with One Nominal Predictor 19.1 Describing multiple groups of metric data 19.2 Traditional analysis of variance 19.3 Hierarchical Bayesian approach 19.4 Including a metric predictor 19.5 Heterogeneous variances and robustness against outliers 19.6 Exercises Walk out an effect size Session info", " 19 Metric Predicted Variable with One Nominal Predictor This chapter considers data structures that consist of a metric predicted variable and a nominal predictor…. This type of data structure can arise from experiments or from observational studies. In experiments, the researcher assigns the categories (at random) to the experimental subjects. In observational studies, both the nominal predictor value and the metric predicted value are generated by processes outside the direct control of the researcher. In either case, the same mathematical description can be applied to the data (although causality is best inferred from experimental intervention). The traditional treatment of this sort of data structure is called single-factor analysis of variance (ANOVA), or sometimes one-way ANOVA. Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter will also consider the situation in which there is also a metric predictor that accompanies the primary nominal predictor. The metric predictor is sometimes called a covariate, and the traditional treatment of this data structure is called analysis of covariance (ANCOVA). The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups, etc. (Kruschke, 2015, pp. 553–554) 19.1 Describing multiple groups of metric data Figure 19.1 illustrates the conventional description of grouped metric data. Each group is represented as a position on the horizontal axis. The vertical axis represents the variable to be predicted by group membership. The data are assumed to be normally distributed within groups, with equal standard deviation in all groups. The group means are deflections from overall baseline, such that the deflections sum to zero. Figure 19.1 provides a specific numerical example, with data that were randomly generated from the model. (p. 554) We’ll want a custom data-generating function for our primary group data. library(tidyverse) generate_data &lt;- function(seed, mean) { set.seed(seed) rnorm(n, mean = grand_mean + mean, sd = 2) } n &lt;- 20 grand_mean &lt;- 101 d &lt;- tibble(group = 1:5, deviation = c(4, -5, -2, 6, -3)) %&gt;% mutate(d = map2(group, deviation, generate_data)) %&gt;% unnest(d) %&gt;% mutate(iteration = rep(1:n, times = 5)) glimpse(d) ## Rows: 100 ## Columns: 4 ## $ group &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2… ## $ deviation &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, -5, -5, -5, -5, -5,… ## $ d &lt;dbl&gt; 103.74709, 105.36729, 103.32874, 108.19056, 105.65902, 103.35906, 105.97486, 10… ## $ iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3,… Here we’ll make a tibble containing the necessary data for the rotated Gaussians. As far as I can tell, Kruschke’s Gaussians only span to the bounds of percentile-based 98% intervals. We partition off those bounds for each group by the ll and ul columns in the first mutate() function. In the second mutate(), we expand the dataset to include a sequence of 100 values between those lower- and upper-limit points. In the third mutate(), we feed those points into the dnorm() function, with group-specific means and a common sd. densities &lt;- d %&gt;% distinct(group, deviation) %&gt;% mutate(ll = qnorm(.01, mean = grand_mean + deviation, sd = 2), ul = qnorm(.99, mean = grand_mean + deviation, sd = 2)) %&gt;% mutate(d = map2(ll, ul, seq, length.out = 100)) %&gt;% mutate(density = map2(d, grand_mean + deviation, dnorm, sd = 2)) %&gt;% unnest(c(d, density)) head(densities) ## # A tibble: 6 x 6 ## group deviation ll ul d density ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 100. 110. 100. 0.0133 ## 2 1 4 100. 110. 100. 0.0148 ## 3 1 4 100. 110. 101. 0.0165 ## 4 1 4 100. 110. 101. 0.0183 ## 5 1 4 100. 110. 101. 0.0203 ## 6 1 4 100. 110. 101. 0.0224 We’ll need two more supplementary tibbles to add the flourishes to the plot. The arrow tibble will specify our light-gray arrows. The text tibble will contain our annotation information. arrow &lt;- tibble(d = grand_mean, group = 1:5, deviation = c(4, -5, -2, 6, -3), offset = .1) head(arrow) ## # A tibble: 5 x 4 ## d group deviation offset ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 101 1 4 0.1 ## 2 101 2 -5 0.1 ## 3 101 3 -2 0.1 ## 4 101 4 6 0.1 ## 5 101 5 -3 0.1 text &lt;- tibble(d = grand_mean, group = c(0:5, 0), deviation = c(0, 4, -5, -2, 6, -3, 10), offset = rep(c(1/4, 0), times = c(6, 1)), angle = rep(c(90, 0), times = c(6, 1)), label = c(&quot;beta[0]==101&quot;, &quot;beta[&#39;[1]&#39;]==4&quot;,&quot;beta[&#39;[2]&#39;]==-5&quot;, &quot;beta[&#39;[3]&#39;]==-2&quot;, &quot;beta[&#39;[4]&#39;]==6&quot;, &quot;beta[&#39;[5]&#39;]==3&quot;, &quot;sigma[&#39;all&#39;]==2&quot;)) head(text) ## # A tibble: 6 x 6 ## d group deviation offset angle label ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 101 0 0 0.25 90 beta[0]==101 ## 2 101 1 4 0.25 90 beta[&#39;[1]&#39;]==4 ## 3 101 2 -5 0.25 90 beta[&#39;[2]&#39;]==-5 ## 4 101 3 -2 0.25 90 beta[&#39;[3]&#39;]==-2 ## 5 101 4 6 0.25 90 beta[&#39;[4]&#39;]==6 ## 6 101 5 -3 0.25 90 beta[&#39;[5]&#39;]==3 Now we’re ready to plot. library(ggridges) d %&gt;% ggplot(aes(x = d, y = group, group = group)) + geom_vline(xintercept = grand_mean, color = &quot;white&quot;) + geom_jitter(height = .05, alpha = 1/4, shape = 1) + # the Gausians geom_ridgeline(data = densities, aes(height = -density), min_height = NA, scale = 3/2, size = 3/4, fill = &quot;transparent&quot;, color = &quot;grey50&quot;) + # the small arrows geom_segment(data = arrow, aes(xend = d + deviation, y = group + offset, yend = group + offset), color = &quot;grey50&quot;, size = 1, arrow = arrow(length = unit(.2, &quot;cm&quot;))) + # the large arrow on the left geom_segment(aes(x = 80, xend = grand_mean, y = 0, yend = 0), color = &quot;grey50&quot;, size = 3/4, arrow = arrow(length = unit(.2, &quot;cm&quot;))) + # the text geom_text(data = text, aes(x = grand_mean + deviation, y = group + offset, label = label, angle = angle), size = 4, parse = T) + scale_y_continuous(NULL, breaks = 1:5, labels = c(&quot;&lt;1,0,0,0,0&gt;&quot;, &quot;&lt;0,1,0,0,0&gt;&quot;, &quot;&lt;0,0,1,0,0&gt;&quot;, &quot;&lt;0,0,0,1,0&gt;&quot;, &quot;&lt;0,0,0,0,1&gt;&quot;)) + xlab(NULL) + coord_flip(xlim = c(90, 112), ylim = c(-0.2, 5.5)) + theme(panel.grid = element_blank()) The descriptive model presented in Figure 19.1 is the traditional one used by classical ANOVA (which is described a bit more in the next section). More general models are straight forward to implement in Bayesian software. For example, outliers could be accommodated by using heavy-tailed noise distributions (such as a \\(t\\) distribution) instead of a normal distribution, and different groups could be given different standard deviations. (p. 556) 19.2 Traditional analysis of variance The terminology, “analysis of variance,” comes from a decomposition of overall data variance into within-group variance and between-group variance (Fisher, 1925). Algebraically, the sum of squared deviations of the scores from their overall mean equals the sum of squared deviations of the scores from their respective group means plus the sum of squared deviations of the group means from the overall mean. In other words, the total variance can be partitioned into within-group variance plus between-group variance. Because one definition of the word “analysis” is separation into constituent parts, the term ANOVA accurately describes the underlying algebra in the traditional methods. That algebraic relation is not used in the hierarchical Bayesian approach presented here. The Bayesian method can estimate component variances, however. Therefore, the Bayesian approach is not ANOVA, but is analogous to ANOVA. (p. 556) 19.3 Hierarchical Bayesian approach “Our goal is to estimate its parameters in a Bayesian framework. Therefore, all the parameters need to be given a meaningfully structured prior distribution” (p. 557). However, our approach will depart a little from the one in the text. All our parameters will not “have generic noncommittal prior distributions” (p. 557). Most importantly, we will not follow the example in (Gelman, 2006) of putting a broad uniform prior on \\(\\sigma_y\\). Rather, we will continue using the half-Gaussian prior, as recommended by the Stan team. However, we will follow Kruschke’s lead for the overall intercept and use a Gaussian prior “made broad on the scale of the data” (p. 557). And like Kruschke, we will estimate \\(\\sigma_\\beta\\) from the data. Later on, Kruschke opined: A crucial pre-requisite for estimating \\(\\sigma_\\beta\\) from all the groups is an assumption that all the groups are representative and informative for the estimate. It only makes sense to influence the estimate of one group with data from the other groups if the groups can be meaningfully described as representative of a shared higher-level distribution. (p. 559) Although I agree with him in spirit, this doesn’t appear to strictly be the case. As odd and paradoxical as this sounds, partial pooling can be of use even when the some of the cases are of a different kind. For more on the topic, see Efron and Morris’s classic (1977) paper, Stein’s paradox in statistics, and my blog post walking out one of their examples in brms. 19.3.1 Implementation in JAGS brms. The brms setup, of course, differs a bit from JAGS. fit &lt;- brm(data = my_data, family = gaussian, y ~ 1 + (1 | categirical_variable), prior = c(prior(normal(0, x), class = Intercept), prior(normal(0, x), class = b), prior(cauchy(0, x), class = sd), prior(cauchy(0, x), class = sigma))) The noise standard deviation \\(\\sigma_y\\) is depicted in the prior statement including the argument class = sigma. The grand mean is depicted by the first 1 in the model formula and its prior is indicated by the class = Intercept argument. We indicate we’d like group-based deviations from the grand mean with the (1 | categirical_variable) syntax, where the 1 on the left side of the bar indicates we’d like our intercepts to vary by group and the categirical_variable part simply represents the name of a given categorical variable we’d like those intercepts to vary by. The brms default is to do this with deviance scores, the mean for which will be zero. Although it’s not obvious in the formula syntax, the model presumes the group-based deviations are normally distributed with a mean of zero and a standard deviation, which Kruschke termed \\(\\sigma_\\beta\\). There is no prior for the mean. It’s set at zero. But there is a prior for \\(\\sigma_\\beta\\), which is denoted by the argument class = sd. We, of course, are not using a uniform prior on any of our variance parameters. But in order to be weakly informative, we will use the half-Cauchy. Recall that since the brms default is to set the lower bound for any variance parameter to 0, there’s no need to worry about doing so ourselves. So even though the syntax only indicates cauchy, it’s understood to mean Cauchy with a lower bound at zero; since the mean is usually 0, that makes is a half-Cauchy. Kruschke set the upper bound for his \\(\\sigma_y\\) to 10 times the standard deviation of the criterion variable. The tails of the half-Cauchy are sufficiently fat that, in practice, I’ve found it doesn’t matter much what you set the \\(SD\\) of its prior to. One is often a sensible default for reasonably-scaled data. But if we want to take a more principled approach, we can set it to the size of the criterion’s \\(SD\\) or perhaps even 10 times that. Kruschke suggested using a gamma on \\(\\sigma_\\beta\\), which is a sensible alternative to half-Cauchy often used within the Stan universe. Especially in situations in which you would like to (a) keep the variance parameter above zero, but (b) still allow it to be arbitrarily close to zero, and also (c) let the likelihood dominate the posterior, the Stan team recommends the gamma(2, 0) prior, based on the paper by Chung and colleagues (2013, click here). But you should note that I don’t mean a literal 0 for the second parameter in the gamma distribution, but rather some small value like 0.1 or so. This is all clarified in Chung et al. (2013). Here’s what \\(\\operatorname{gamma} (2, 0.1)\\) looks like. tibble(x = seq(from = 0, to = 110, by = .1)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = dgamma(x, 2, 0.1))) + geom_ribbon(size = 0, fill = &quot;grey67&quot;) + annotate(geom = &quot;text&quot;, x = 14.25, y = 0.015, label = &quot;&#39;gamma&#39;~(2*&#39;, &#39;*0.1)&quot;, parse = T, color = &quot;grey92&quot;, size = 4.25) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 100)) + theme(panel.grid = element_blank()) If you’d like that prior be even less informative, just reduce it to like \\(\\operatorname{gamma} (2, 0.01)\\) or so. Kruschke goes further to recommend “the shape and rate parameters of the gamma distribution are set so its mode is sd(y)/2 and its standard deviation is 2*sd(y), using the function gammaShRaFromModeSD explained in Section 9.2.2.” (pp. 560–561). Let’s make that function. gamma_a_b_from_omega_sigma &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } So in the case of standardized data where sd(1) = 1, we’d use our gamma_a_b_from_omega_sigma() function like so. sd_y &lt;- 1 omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y (s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)) ## $shape ## [1] 1.283196 ## ## $rate ## [1] 0.5663911 And that produces the following gamma distribution. tibble(x = seq(from = 0, to = 50, by = .01)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = dgamma(x, s_r$shape, s_r$rate))) + geom_ribbon(size = 0, fill = &quot;grey67&quot;) + scale_x_continuous(breaks = c(0, 1, 5, 10, 20)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 20)) + theme(panel.grid = element_blank()) In the parameter space that matters, from zero to one, that gamma is pretty noninformative. It peaks between the two, slopes very gently rightward, but has the nice steep slope on the left keeping the estimates off the zero boundary. And even though that right slope is very gentle given the scale of the data, it’s aggressive enough that it should keep the MCMC chains from spending a lot of time in ridiculous parts of the parameter space. I.e., when working with finite numbers of iterations, we want our MCMC chains wasting exactly zero iterations investigating what the density might be for \\(\\sigma_\\beta \\approx 1e10\\) for standardized data. 19.3.2 Example: Sex and death. Let’s load and glimpse() at Hanley and Shapiro’s (1994) fruit-fly data. my_data &lt;- read_csv(&quot;data.R/FruitflyDataReduced.csv&quot;) glimpse(my_data) ## Rows: 125 ## Columns: 3 ## $ Longevity &lt;dbl&gt; 35, 37, 49, 46, 63, 39, 46, 56, 63, 65, 56, 65, 70, 63, 65, 70, 77, 81, 8… ## $ CompanionNumber &lt;chr&gt; &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnan… ## $ Thorax &lt;dbl&gt; 0.64, 0.68, 0.68, 0.72, 0.72, 0.76, 0.76, 0.76, 0.76, 0.76, 0.80, 0.80, 0… We can use geom_density_ridges() to help get a sense of how our criterion Longevity is distributed across groups of CompanionNumber. my_data %&gt;% group_by(CompanionNumber) %&gt;% mutate(group_mean = mean(Longevity)) %&gt;% ungroup() %&gt;% mutate(CompanionNumber = fct_reorder(CompanionNumber, group_mean)) %&gt;% ggplot(aes(x = Longevity, y = CompanionNumber, fill = group_mean)) + geom_density_ridges(scale = 3/2, size = .2, color = &quot;grey92&quot;) + scale_fill_viridis_c(option = &quot;A&quot;, end = .92) + ylab(NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;, panel.grid = element_blank()) Let’s fire up brms. library(brms) We’ll want to do the preparatory work to define our stanvars. (mean_y &lt;- mean(my_data$Longevity)) ## [1] 57.44 (sd_y &lt;- sd(my_data$Longevity)) ## [1] 17.56389 omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y (s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)) ## $shape ## [1] 1.283196 ## ## $rate ## [1] 0.03224747 With the prep work is done, here are our stanvars. stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Now fit the model, our hierarchical Bayesian alternative to an ANOVA. fit19.1 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 1 + (1 | CompanionNumber), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, control = list(adapt_delta = 0.99), stanvars = stanvars, file = &quot;fits/fit19.01&quot;) Much like Kruschke’s JAGS chains, our brms chains are well behaved. plot(fit19.1) Also like Kruschke, our chains appear moderately autocorrelated. post &lt;- posterior_samples(fit19.1, add_chain = T) library(bayesplot) theme_set(theme_gray() + theme(panel.grid = element_blank())) mcmc_acf(post, pars = c(&quot;b_Intercept&quot;, &quot;sd_CompanionNumber__Intercept&quot;, &quot;sigma&quot;), lags = 10) Here’s the model summary. print(fit19.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 1 + (1 | CompanionNumber) ## Data: my_data (Number of observations: 125) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~CompanionNumber (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 15.03 8.04 6.24 36.42 1.00 2311 3554 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 57.59 7.51 43.03 73.39 1.00 2505 3147 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 14.93 0.97 13.22 16.98 1.00 5824 5872 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With the ranef() function, we can get the summaries of the group-specific deflections. ranef(fit19.1) ## $CompanionNumber ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## None0 5.5794036 7.818598 -10.238748 20.800280 ## Pregnant1 6.7890585 7.816206 -9.059062 22.125098 ## Pregnant8 5.3766714 7.839145 -10.687290 20.717351 ## Virgin1 -0.7754446 7.856840 -17.380637 14.139118 ## Virgin8 -17.7177658 7.939150 -34.940259 -3.181382 And with the coef() function, we can get those same group-level summaries in a non-deflection metric. coef(fit19.1) ## $CompanionNumber ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## None0 63.17168 2.921267 57.43048 68.80815 ## Pregnant1 64.38133 2.927249 58.70815 70.16666 ## Pregnant8 62.96894 2.920661 57.21388 68.65320 ## Virgin1 56.81683 2.934214 51.12637 62.52715 ## Virgin8 39.87451 3.091971 33.85827 46.00826 Those are all estimates of the group-specific means. Since it wasn’t modeled, all have the same parameter estimates for \\(\\sigma_y\\). posterior_summary(fit19.1)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 14.9346100 0.9662639 13.2174322 16.9832791 To prepare for our version of the top panel of Figure 19.3, we’ll use sample_n() to randomly sample from the posterior draws. # how many random draws from the posterior would you like? n_draws &lt;- 20 set.seed(19) post_draws &lt;- post %&gt;% sample_n(size = n_draws, replace = F) glimpse(post_draws) ## Rows: 20 ## Columns: 11 ## $ b_Intercept &lt;dbl&gt; 51.37055, 42.46025, 58.43885, 56.01002, 48.52338… ## $ sd_CompanionNumber__Intercept &lt;dbl&gt; 16.262790, 17.931260, 10.294159, 14.104043, 27.4… ## $ sigma &lt;dbl&gt; 13.71128, 13.91828, 16.24448, 14.03107, 16.30758… ## $ `r_CompanionNumber[None0,Intercept]` &lt;dbl&gt; 14.6642542, 18.4782479, 0.3908729, 6.5999786, 14… ## $ `r_CompanionNumber[Pregnant1,Intercept]` &lt;dbl&gt; 12.207309, 23.184843, 9.985389, 15.058426, 19.47… ## $ `r_CompanionNumber[Pregnant8,Intercept]` &lt;dbl&gt; 9.7663362, 15.5679928, 7.3901457, 7.7165676, 15.… ## $ `r_CompanionNumber[Virgin1,Intercept]` &lt;dbl&gt; 6.04861233, 15.33460132, -3.33245392, 0.58641261… ## $ `r_CompanionNumber[Virgin8,Intercept]` &lt;dbl&gt; -12.425826, -3.181759, -14.673815, -15.790020, -… ## $ lp__ &lt;dbl&gt; -526.6386, -528.8238, -531.0766, -528.5373, -526… ## $ chain &lt;fct&gt; 3, 4, 2, 3, 1, 2, 2, 4, 1, 4, 4, 4, 2, 4, 2, 2, … ## $ iter &lt;dbl&gt; 2677, 1910, 2483, 2557, 3745, 2803, 3677, 2255, … Before we make our version of the top panel, let’s make a corresponding plot of the fixed intercept, the grand mean. The most important lines in the code, below are the ones where we used stat_function() within mapply(). tibble(x = c(0, 150)) %&gt;% ggplot(aes(x = x)) + mapply(function(mean, sd) { stat_function(fun = dnorm, args = list(mean = mean, sd = sd), alpha = 2/3, size = 1/3, color = &quot;grey50&quot;) }, # enter means and standard deviations here mean = post_draws[, &quot;b_Intercept&quot;], sd = post_draws[, &quot;sigma&quot;] ) + geom_jitter(data = my_data, aes(x = Longevity, y = -0.001), height = .001, alpha = 1/2) + scale_x_continuous(&quot;Longevity&quot;, breaks = seq(from = 0, to = 100, by = 25)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Posterior Predictive Distribution&quot;, subtitle = &quot;The jittered dots are the ungrouped Longevity data. The\\nGaussians are posterior draws depicting the overall\\ndistribution, the grand mean.&quot;) + coord_cartesian(xlim = c(0, 110)) Unfortunately, we can’t extend our mapply(stat_function()) method to the group-level estimates. To my knowledge, there isn’t a way to show the group estimates at different spots along the y-axis. And our mapply(stat_function()) approach has other limitations, too. Happily, we have some great alternatives. To use them, we’ll need a little help from tidybayes. library(tidybayes) For the first part, we’ll take tidybayes::add_fitted_draws() for a whirl. densities &lt;- my_data %&gt;% distinct(CompanionNumber) %&gt;% add_fitted_draws(fit19.1, n = 20, seed = 19, dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) glimpse(densities) ## Rows: 100 ## Columns: 8 ## Groups: CompanionNumber, .row [5] ## $ CompanionNumber &lt;chr&gt; &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnan… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ .draw &lt;int&gt; 1115, 2133, 2685, 2745, 4031, 4436, 4483, 4803, 5677, 5784, 7173, 7557, 7… ## $ .value &lt;dbl&gt; 60.74737, 63.17496, 59.68698, 63.73562, 60.19609, 59.72371, 65.82900, 61.… ## $ mu &lt;dbl&gt; 60.74737, 63.17496, 59.68698, 63.73562, 60.19609, 59.72371, 65.82900, 61.… ## $ sigma &lt;dbl&gt; 15.11354, 14.27563, 16.01779, 16.30758, 14.23716, 15.40663, 16.24448, 14.… With the first two lines, we made a \\(5 \\times 1\\) tibble containing the five levels of the experimental grouping variable, CompanionNumber. The add_fitted_draws() function comes from tidybayes (see the Posterior fits section of Kay, 2020a). The first argument of the add_fitted_draws() is newdata, which works much like it does in brms::fitted(); it took our \\(5 \\times 1\\) tibble. The next argument took our brms model fit, fit19.1. With the n argument, we indicated we just wanted 20 random draws from the posterior. The seed argument makes those random draws reproducible. With dpar, we requested distributional regression parameters in the output. In our case, those were the \\(\\mu\\) and \\(\\sigma\\) values for each level of CompanionNumber. Since we took 20 draws across 5 groups, we ended up with a 100-row tibble. The next steps are a direct extension of the method we used to make our Gaussians for our version of Figure 19.1. densities &lt;- densities %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Longevity = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Longevity) %&gt;% mutate(density = dnorm(Longevity, mu, sigma)) glimpse(densities) ## Rows: 10,000 ## Columns: 12 ## Groups: CompanionNumber, .row [5] ## $ CompanionNumber &lt;chr&gt; &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnan… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ .draw &lt;int&gt; 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1… ## $ .value &lt;dbl&gt; 60.74737, 60.74737, 60.74737, 60.74737, 60.74737, 60.74737, 60.74737, 60.… ## $ mu &lt;dbl&gt; 60.74737, 60.74737, 60.74737, 60.74737, 60.74737, 60.74737, 60.74737, 60.… ## $ sigma &lt;dbl&gt; 15.11354, 15.11354, 15.11354, 15.11354, 15.11354, 15.11354, 15.11354, 15.… ## $ ll &lt;dbl&gt; 31.12538, 31.12538, 31.12538, 31.12538, 31.12538, 31.12538, 31.12538, 31.… ## $ ul &lt;dbl&gt; 90.36936, 90.36936, 90.36936, 90.36936, 90.36936, 90.36936, 90.36936, 90.… ## $ Longevity &lt;dbl&gt; 31.12538, 31.72381, 32.32223, 32.92066, 33.51908, 34.11750, 34.71593, 35.… ## $ density &lt;dbl&gt; 0.003867068, 0.004175850, 0.004502224, 0.004846502, 0.005208934, 0.005589… If you look at the code we used to make ll and ul, you’ll see we used 95% intervals, this time. Our second mutate() function is basically the same. After unnesting the tibble, we just needed to plug in the Longevity, mu, and sigma values into the dnorm() function to compute the corresponding density values. densities %&gt;% ggplot(aes(x = Longevity, y = CompanionNumber)) + # here we make our density lines geom_ridgeline(aes(height = density, group = interaction(CompanionNumber, .draw)), fill = NA, color = adjustcolor(&quot;grey50&quot;, alpha.f = 2/3), size = 1/3, scale = 25) + # the original data with little jitter thrown in geom_jitter(data = my_data, height = .04, alpha = 1/2) + # pretty much everything below this line is aesthetic fluff scale_x_continuous(breaks = seq(from = 0, to = 100, by = 25)) + labs(title = &quot;Data with Posterior Predictive Distrib.&quot;, y = NULL) + coord_cartesian(xlim = c(0, 110), ylim = c(1.25, 5.25)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) Do be aware that when you use this method, you may have to fiddle around with the geom_ridgeline() scale argument to get the Gaussian’s heights on reasonable-looking relative heights. Stick in different numbers to get a sense of what I mean. I also find that I’m often not a fan of the way the spacing on the y axis ends up with default geom_ridgeline(). It’s easy to overcome this with a little ylim fiddling. To return to the more substantive interpretation, the top panel of Figure 19.3 suggests that the normal distributions with homogeneous variances appear to be reasonable descriptions of the data. There are no dramatic outliers relative to the posterior predicted curves, and the spread of the data within each group appears to be reasonably matched by the width of the posterior normal curves. (Be careful when making visual assessments of homogeneity of variance because the visual spread of the data depends on the sample size; for a reminder see the [see the right panel of Figure 17.1, p. 478].) The range of credible group means, indicated by the peaks of the normal curves, suggests that the group Virgin8 is clearly lower than the others, and the group Virgin1 might be lower than the controls. To find out for sure, we need to examine the differences of group means, which we do in the next section. (p. 564) For clarity, the “see the right panel of Figure 17.1, p. 478” part was changed following Kruschke’s Corrigenda. 19.3.3 Contrasts. It is straight forward to examine the posterior distribution of credible differences. Every step in the MCMC chain provides a combination of group means that are jointly credible, given the data. Therefore, every step in the MCMC chain provides a credible difference between groups… To construct the credible differences of group 1 and group 2, at every step in the MCMC chain we compute \\[\\begin{align*} \\mu_1 - \\mu_2 &amp; = (\\beta_0 + \\beta_1) - (\\beta_0 + \\beta_2) \\\\ &amp; = (+1) \\cdot \\beta_1 + (-1) \\cdot \\beta_2 \\end{align*}\\] In other words, the baseline cancels out of the calculation, and the difference is a sum of weighted group deflections. Notice that the weights sum to zero. To construct the credible differences of the average of groups 1-3 and the average of groups 4-5, at every step in the MCMC chain we compute \\[\\begin{align*} (\\mu_1 + \\mu_2 + \\mu_3) / 3 - (\\mu_4 + \\mu_5) / 2 &amp; = ((\\beta_0 + \\beta_1) + (\\beta_0 + \\beta_2) + (\\beta_0 + \\beta_3) ) / 3 - ((\\beta_0 + \\beta_4) + (\\beta_0 + \\beta_5) ) / 2 \\\\ &amp; = (\\beta_1 + \\beta_2 + \\beta_3) / 3 - (\\beta_4 + \\beta_5) / 2 \\\\ &amp; = (+ 1/3) \\cdot \\beta_1 + (+ 1/3) \\cdot \\beta_2 + (+ 1/3) \\cdot \\beta_3 + (- 1/2) \\cdot \\beta_4 + (- 1/2) \\cdot \\beta_5 \\end{align*}\\] Again, the difference is a sum of weighted group deflections. The coefficients on the group deflections have the properties that they sum to zero, with the positive coefficients summing to +1 and the negative coefficients summing to −1. Such a combination is called a contrast. The differences can also be expressed in terms of effect size, by dividing the difference by \\(\\sigma_y\\) at each step in the chain. (pp. 565–566) To warm up, here’s how to compute the first contrast shown in the lower portion of Kruschke’s Figure 19.3–the contrast between the two pregnant conditions and the none-control condition. post %&gt;% transmute(c = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) %&gt;% ggplot(aes(x = c, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Pregnant1.Pregnant8 vs None0&quot;, x = &quot;Difference&quot;) In case you were curious, here are the HMC-based posterior mode and 95% HDIs. post %&gt;% transmute(difference = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) %&gt;% mode_hdi(difference) ## difference .lower .upper .width .point .interval ## 1 1.67085 -6.825623 9.071875 0.95 mode hdi Little difference, there. Now let’s quantify the same contrast as an effect size. post %&gt;% transmute(es = ((`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) / sigma) %&gt;% ggplot(aes(x = es, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Pregnant1.Pregnant8 vs None0&quot;, x = &quot;Effect Size&quot;) Tiny. Okay, now let’s do the rest in bulk. First we’ll do the difference scores. differences &lt;- post %&gt;% transmute(`Pregnant1.Pregnant8.None0 vs Virgin1` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - `r_CompanionNumber[Virgin1,Intercept]`, `Virgin1 vs Virgin8` = `r_CompanionNumber[Virgin1,Intercept]` - `r_CompanionNumber[Virgin8,Intercept]`, `Pregnant1.Pregnant8.None0 vs Virgin1.Virgin8` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - (`r_CompanionNumber[Virgin1,Intercept]` + `r_CompanionNumber[Virgin8,Intercept]`) / 2) differences %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + facet_wrap(~key, scales = &quot;free&quot;) Because we save our data wrangling labor from above as differences, it won’t take much more effort to compute and plot the corresponding effect sizes as displayed in the bottom row of Figure 19.3. differences %&gt;% mutate_all(.funs = ~ . / post$sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Effect Size&quot;) + facet_wrap(~key, scales = &quot;free_x&quot;) In traditional ANOVA, analysts often perform a so-called omnibus test that asks whether it is plausible that all the groups are simultaneously exactly equal. I find that the omnibus test is rarely meaningful, however…. In the hierarchical Bayesian estimation used here, there is no direct equivalent to an omnibus test in ANOVA, and the emphasis is on examining all the meaningful contrasts. (p. 567) Speaking of all meaningful contrasts, if you’d like to make all pairwise comparisons in a hierarchical model of this form, tidybayes offers a convenient way to do so (see the Comparing levels of a factor section of Kay, 2020a). Here we’ll demonstrate with geom_halfeyeh(). fit19.1 %&gt;% # these two lines are where the magic is at spread_draws(r_CompanionNumber[CompanionNumber,]) %&gt;% compare_levels(r_CompanionNumber, by = CompanionNumber) %&gt;% ggplot(aes(x = r_CompanionNumber, y = CompanionNumber)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.95, .5)) + labs(x = &quot;Contrast&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) But back to that omnibus test notion. If you really wanted to, I suppose one rough analogue would be to use information criteria to compare the hierarchical model to one that includes a single intercept with no group-level deflections. Here’s what the simpler model would look like. fit19.2 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 1, prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, stanvars = stanvars, file = &quot;fits/fit19.02&quot;) Here’s the model summary. print(fit19.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 1 ## Data: my_data (Number of observations: 125) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 57.42 1.59 54.32 60.56 1.00 9746 7638 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 17.69 1.16 15.56 20.12 1.00 7742 7149 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here are their LOO values and their difference score. fit19.1 &lt;- add_criterion(fit19.1, criterion = &quot;loo&quot;) fit19.2 &lt;- add_criterion(fit19.2, criterion = &quot;loo&quot;) loo_compare(fit19.1, fit19.2) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit19.1 0.0 0.0 -517.7 6.9 5.5 0.6 1035.4 13.8 ## fit19.2 -19.4 5.3 -537.1 7.1 1.8 0.3 1074.1 14.1 The hierarchical model has a better LOO. Here are the stacking-based model weights. (mw &lt;- model_weights(fit19.1, fit19.2)) ## fit19.1 fit19.2 ## 9.999994e-01 6.438484e-07 If you don’t like scientific notation, just round(). mw %&gt;% round(digits = 3) ## fit19.1 fit19.2 ## 1 0 Yep, in complimenting the LOO difference, virtually all the stacking weight went to the hierarchical model. You might think of this another way. The conceptual question we’re asking is Does it make sense to say that the \\(\\sigma_\\beta\\) parameter is zero? Is zero a credible value? We’ll, I suppose we could just look at the posterior to assess for that. post %&gt;% ggplot(aes(x = sd_CompanionNumber__Intercept, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 100, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 50)) + labs(title = expression(&quot;Behold the fit19.1 posterior for &quot;*sigma[beta]*&quot;.&quot;), subtitle = &quot;This parameter&#39;s many things, but zero isn&#39;t one of them.&quot;, x = NULL) Yeah, zero and other values close to zero don’t look credible for that parameter. 95% of the mass is between 5 and 30, with the bulk hovering around 10. We don’t need an \\(F\\)-test or even a LOO model comparison to see the writing on wall. 19.3.4 Multiple comparisons and shrinkage. The previous section suggested that an analyst should investigate all contrasts of interest. This recommendation can be thought to conflict with traditional advice in the context on null hypothesis significance testing, which instead recommends that a minimal number of comparisons should be conducted in order to maximize the power of each test while keeping the overall false alarm rate capped at 5% (or whatever maximum is desired)…. Instead, a Bayesian analysis can mitigate false alarms by incorporating prior knowledge into the model. In particular, hierarchical structure (which is an expression of prior knowledge) produces shrinkage of estimates, and shrinkage can help rein in estimates of spurious outlying data. For example, in the posterior distribution from the fruit fly data, the modal values of the posterior group means have a range of 23.2. The sample means of the groups have a range of 26.1. Thus, there is some shrinkage in the estimated means. The amount of shrinkage is dictated only by the data and by the prior structure, not by the intended tests. (p. 568) We may as well compute those ranges by hand. Here’s the range of the observed data. my_data %&gt;% group_by(CompanionNumber) %&gt;% summarise(mean = mean(Longevity)) %&gt;% summarise(range = max(mean) - min(mean)) ## # A tibble: 1 x 1 ## range ## &lt;dbl&gt; ## 1 26.1 For our hierarchical model fit19.1, the posterior means are rank ordered in the same way as the empirical data. coef(fit19.1)$CompanionNumber[, , &quot;Intercept&quot;] %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;companion_number&quot;) %&gt;% arrange(Estimate) %&gt;% mutate_if(is.double, round, digits = 1) ## companion_number Estimate Est.Error Q2.5 Q97.5 ## 1 Virgin8 39.9 3.1 33.9 46.0 ## 2 Virgin1 56.8 2.9 51.1 62.5 ## 3 Pregnant8 63.0 2.9 57.2 68.7 ## 4 None0 63.2 2.9 57.4 68.8 ## 5 Pregnant1 64.4 2.9 58.7 70.2 If we compute the range by a difference of the point estimates of the highest and lowest posterior means, we can get a quick number. coef(fit19.1)$CompanionNumber[, , &quot;Intercept&quot;] %&gt;% as_tibble() %&gt;% summarise(range = max(Estimate) - min(Estimate)) ## # A tibble: 1 x 1 ## range ## &lt;dbl&gt; ## 1 24.5 Note that wasn’t fully Bayesian of us. Those means and their difference carry uncertainty with them and that uncertainty can be fully expressed if we use all the posterior draws (i.e., use summary = F and wrangle). coef(fit19.1, summary = F)$CompanionNumber[, , &quot;Intercept&quot;] %&gt;% as_tibble() %&gt;% transmute(range = Pregnant1 - Virgin8) %&gt;% mode_hdi(range) ## # A tibble: 1 x 6 ## range .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 24.7 15.7 32.9 0.95 mode hdi Happily, the central tendency of the range is near equivalent with both methods, but now we have 95% intervals, too. Do note how wide they are. This is why we work with the full set of posterior draws. 19.3.5 The two-group case. A special case of our current scenario is when there are only two groups. The model of the present section could, in principle, be applied to the two-group case, but the hierarchical structure would do little good because there is virtually no shrinkage when there are so few groups (and the top-level prior on \\(\\sigma_\\beta\\) is broad as assumed here). (p. 568) For kicks and giggles, let’s practice. Since Pregnant1 and Virgin8 had the highest and lowest empirical means—making them the groups best suited to define our range, we’ll use them to fit the 2-group hierarchical model. To fit it with haste, just use update(). fit19.3 &lt;- update(fit19.1, newdata = my_data %&gt;% filter(CompanionNumber %in% c(&quot;Pregnant1&quot;, &quot;Virgin8&quot;)), control = list(adapt_delta = 0.999, max_treedepth = 12), seed = 19, file = &quot;fits/fit19.03&quot;) Even with just two groups, there were no gross issues with fitting the model. print(fit19.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 1 + (1 | CompanionNumber) ## Data: my_data %&gt;% filter(CompanionNumber %in% c(&quot;Pregnan (Number of observations: 50) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~CompanionNumber (Number of levels: 2) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 33.07 22.62 8.86 93.59 1.00 3269 4606 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 52.23 24.82 1.13 105.83 1.00 3447 3274 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 14.24 1.49 11.66 17.51 1.00 5500 4602 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you compare the posteriors for \\(\\sigma_\\beta\\) across the two models, you’ll see how the one for fit19.3 is substantially larger. posterior_summary(fit19.1)[&quot;sd_CompanionNumber__Intercept&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 15.031865 8.041133 6.238274 36.416484 posterior_summary(fit19.3)[&quot;sd_CompanionNumber__Intercept&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 33.065688 22.615175 8.861131 93.587013 Here that is in a coefficient plot using tidybayes::stat_intervalh(). bind_rows(posterior_samples(fit19.1) %&gt;% select(sd_CompanionNumber__Intercept), posterior_samples(fit19.3) %&gt;% select(sd_CompanionNumber__Intercept)) %&gt;% mutate(fit = rep(c(&quot;fit19.1&quot;, &quot;fit19.3&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = sd_CompanionNumber__Intercept, y = fit)) + stat_intervalh(point_interval = mode_hdi, .width = c(.5, .8, .95)) + scale_color_grey(&quot;HDI&quot;, start = 2/3, end = 0, labels = c(&quot;95%&quot;, &quot;80%&quot;, &quot;50%&quot;)) + labs(x = expression(sigma[beta]), y = NULL) + coord_cartesian(xlim = c(0, 80)) + theme(legend.key.size = unit(0.45, &quot;cm&quot;)) This all implies less shrinkage and a larger range. coef(fit19.3, summary = F)$CompanionNumber[, , &quot;Intercept&quot;] %&gt;% as_tibble() %&gt;% transmute(range = Pregnant1 - Virgin8) %&gt;% mode_hdi(range) ## # A tibble: 1 x 6 ## range .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 25.9 17.6 33.4 0.95 mode hdi And indeed, the range between the two groups is larger. Now the posterior mode for their difference has almost converged to that of the raw data. Kruschke then went on to recommend using a single-level model in such situations, instead. That is why the two-group model in Section 16.3 did not use hierarchical structure, as illustrated in Figure 16.11 (p. 468). That model also used a \\(t\\) distribution to accommodate outliers in the data, and that model allowed for heterogeneous variances across groups. Thus, for two groups, it is more appropriate to use the model of Section 16.3. The hierarchical multi-group model is generalized to accommodate outliers and heterogeneous variances in Section 19.5. (p. 568) As a refresher, here’s what the brms code for that Chapter 16 model looked like. fit16.3 &lt;- brm(data = my_data, family = student, bf(Score ~ 0 + Group, sigma ~ 0 + Group), prior = c(prior(normal(mean_y, sd_y * 100), class = b), prior(normal(0, log(sd_y)), class = b, dpar = sigma), prior(exponential(one_over_twentynine), class = nu)), chains = 4, cores = 4, stanvars = stanvars, seed = 16, file = &quot;fits/fit16.03&quot;) Let’s adjust it for our data. Since we have a reduced data set, we’ll need to re-compute our stanvars values, which were based on the raw data. # it&#39;s easier to just make a reduced data set my_small_data &lt;- my_data %&gt;% filter(CompanionNumber %in% c(&quot;Pregnant1&quot;, &quot;Virgin8&quot;)) (mean_y &lt;- mean(my_small_data$Longevity)) ## [1] 51.76 (sd_y &lt;- sd(my_small_data$Longevity)) ## [1] 19.11145 omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y (s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)) ## $shape ## [1] 1.283196 ## ## $rate ## [1] 0.02963623 Here we update stanvars. stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) Note that our priors, here, are something of a blend of those from Chapter 16 and those from our hierarchical model, fit19.1. fit19.4 &lt;- brm(data = my_small_data, family = student, bf(Longevity ~ 0 + CompanionNumber, sigma ~ 0 + CompanionNumber), prior = c(prior(normal(mean_y, sd_y * 10), class = b), prior(normal(0, log(sd_y)), class = b, dpar = sigma), prior(exponential(one_over_twentynine), class = nu)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, stanvars = stanvars, file = &quot;fits/fit19.04&quot;) Here’s the model summary. print(fit19.4) ## Family: student ## Links: mu = identity; sigma = log; nu = identity ## Formula: Longevity ~ 0 + CompanionNumber ## sigma ~ 0 + CompanionNumber ## Data: my_small_data (Number of observations: 50) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## CompanionNumberPregnant1 64.65 3.26 58.29 71.19 1.00 12438 8155 ## CompanionNumberVirgin8 38.80 2.53 33.88 43.78 1.00 13341 8746 ## sigma_CompanionNumberPregnant1 2.73 0.15 2.45 3.05 1.00 13345 9114 ## sigma_CompanionNumberVirgin8 2.48 0.16 2.18 2.80 1.00 13031 8729 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## nu 39.61 31.02 6.01 120.48 1.00 12182 8961 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Man, look at those Bulk_ESS values! As it turns out, they can be greater than the number of post-warmup samples. And here’s the range in posterior means. fixef(fit19.4, summary = F) %&gt;% as_tibble() %&gt;% transmute(range = CompanionNumberPregnant1 - CompanionNumberVirgin8) %&gt;% mode_hdi(range) ## # A tibble: 1 x 6 ## range .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 25.7 18.1 34.4 0.95 mode hdi The results are pretty much the same as that of the two-group hierarchical model, maybe a touch larger. Yep, Kruschke was right. Hierarchical models with two groups and permissive priors on \\(\\sigma_\\beta\\) don’t shrink the estimates to the grand mean all that much. 19.4 Including a metric predictor “In Figure 19.3, the data within each group have a large standard deviation. For example, longevities in the Virgin8 group range from 20 to 60 days” (p. 568). Turns out Kruschke’s slightly wrong on this. Probably just a typo. my_data %&gt;% group_by(CompanionNumber) %&gt;% summarise(min = min(Longevity), max = max(Longevity), range = max(Longevity) - min(Longevity)) ## # A tibble: 5 x 4 ## CompanionNumber min max range ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 None0 37 96 59 ## 2 Pregnant1 42 97 55 ## 3 Pregnant8 35 86 51 ## 4 Virgin1 21 81 60 ## 5 Virgin8 16 60 44 But you get the point. For each group, there was quite a range. We might add predictors to the model to help account for those ranges. The additional metric predictor is sometimes called a covariate. In the experimental setting, the focus of interest is usually on the nominal predictor (i.e., the experimental treatments), and the covariate is typically thought of as an ancillary predictor to help isolate the effect of the nominal predictor. But mathematically the nominal and metric predictors have equal status in the model. Let’s denote the value of the metric covariate for subject \\(i\\) as \\(x_\\text{cov}(i)\\). Then the expected value of the predicted variable for subject \\(i\\) is \\[\\mu (i) = \\beta_0 + \\sum_j \\beta_{[j]} x_{[j]} (i) + \\beta_\\text{cov} x_\\text{cov}(i)\\] with the usual sum-to-zero constraint on the deflections of the nominal predictor stated in Equation 19.2. In words, Equation 19.5 says that the predicted value for subject \\(i\\) is a baseline plus a deflection due to the group of \\(i\\) plus a shift due to the value of \\(i\\) on the covariate. (p. 569) And the \\(j\\) subscript, recall, denotes group membership. In this context, it often makes sense to set the intercept as the mean of predicted values if the covariate is re-centered at its mean value, which is denoted \\(\\overline x_\\text{cov}\\). Therefore Equation 19.5 is algebraically reformulated to make the baseline respect those constraints…. The first equation below is simply Equation 19.5 with \\(x_\\text{cov}\\) recentered on its mean, \\(\\overline x_\\text{cov}\\). The second line below merely algebraically rearranges the terms so that the nominal deflections sum to zero and the constants are combined into the overall baseline: \\[\\begin{align*} \\mu &amp; = \\alpha_0 + \\sum_j \\alpha_{[j]} x_{[j]} + \\alpha_\\text{cov} (x_\\text{cov} - \\overline{x}_\\text{cov}) \\\\ &amp; = \\underbrace{\\alpha_0 + \\overline{\\alpha} - \\alpha_\\text{cov} \\overline{x}_\\text{cov}}_{\\beta_0} + \\sum_j \\underbrace{(\\alpha_{[j]} - \\overline{\\alpha})}_{\\beta_[j]} x_{[j]} + \\underbrace{\\alpha_\\text{cov}}_{\\beta_{\\text{cov}}} x_\\text{cov} \\\\ &amp; \\text{where } \\overline{\\alpha} = \\frac{1}{J} \\sum^J_{j = 1} \\alpha_{[j]} \\end{align*}\\] (pp. 569–570) 19.4.1 Example: Sex, death, and size. Kruschke recalled fit19.1’s estimate for \\(\\sigma_y\\) had a posterior mode around 14.8. Let’s confirm with a plot. posterior_samples(fit19.1) %&gt;% ggplot(aes(x = sigma, y = 0)) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.5, .95)) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma[y])) + theme(panel.grid = element_blank()) Yep, that looks about right. That large of a difference in days would indeed make it difficult to detect between-group differences if those differences were typically on the scale of just a few days. Since Thorax is moderately correlated with Longevity, including Thorax in the statistical model should help shrink that \\(\\sigma_y\\) estimate, making it easier to compare group means. Following the sensibilities from the equations just above, here we’ll mean-center our covariate, first. my_data &lt;- my_data %&gt;% mutate(thorax_c = Thorax - mean(Thorax)) head(my_data) ## # A tibble: 6 x 4 ## Longevity CompanionNumber Thorax thorax_c ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 35 Pregnant8 0.64 -0.181 ## 2 37 Pregnant8 0.68 -0.141 ## 3 49 Pregnant8 0.68 -0.141 ## 4 46 Pregnant8 0.72 -0.101 ## 5 63 Pregnant8 0.72 -0.101 ## 6 39 Pregnant8 0.76 -0.0610 Our model code follows the structure of that in Kruschke’s Jags-Ymet-Xnom1met1-MnormalHom-Example.R and Jags-Ymet-Xnom1met1-MnormalHom.R files. As a preparatory step, we redefine the values necessary for stanvars. (mean_y &lt;- mean(my_data$Longevity)) ## [1] 57.44 (sd_y &lt;- sd(my_data$Longevity)) ## [1] 17.56389 (sd_thorax_c &lt;- sd(my_data$thorax_c)) ## [1] 0.07745367 omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y (s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)) ## $shape ## [1] 1.283196 ## ## $rate ## [1] 0.03224747 stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(sd_thorax_c, name = &quot;sd_thorax_c&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Now we’re ready to fit the brm() model, our hierarchical alternative to ANCOVA. fit19.5 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 1 + thorax_c + (1 | CompanionNumber), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(normal(0, 2 * sd_y / sd_thorax_c), class = b), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, control = list(adapt_delta = 0.99), stanvars = stanvars, file = &quot;fits/fit19.05&quot;) Here’s the model summary. print(fit19.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 1 + thorax_c + (1 | CompanionNumber) ## Data: my_data (Number of observations: 125) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~CompanionNumber (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 14.07 7.53 5.92 33.51 1.00 2575 4157 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 57.35 6.82 44.01 70.95 1.00 2429 3081 ## thorax_c 136.07 12.48 111.30 160.49 1.00 7984 7348 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 10.61 0.69 9.34 12.05 1.00 7505 7602 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s see if that \\(\\sigma_y\\) posterior shrank. posterior_samples(fit19.5) %&gt;% ggplot(aes(x = sigma, y = 0)) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.5, .95)) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma[y])) Yep, sure did! Now our between-group comparisons should be more precise. Heck, if we wanted to we could even make a difference plot. bind_cols(posterior_samples(fit19.1) %&gt;% select(sigma), posterior_samples(fit19.5) %&gt;% select(sigma)) %&gt;% transmute(dif = sigma - sigma1) %&gt;% ggplot(aes(x = dif, y = 0)) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.5, .95)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;This is a difference distribution&quot;, x = expression(sigma[y][&quot; | fit19.1&quot;]-sigma[y][&quot; | fit19.5&quot;])) If you want a quick and dirty plot of the relation between thorax_c and Longevity, you might employ brms::conditional_effects(). conditional_effects(fit19.5) But to make plots like the ones at the top of Figure 19.5, we’ll have to work a little harder. First, we need some intermediary values marking off the three values along the Thorax-axis Kruschke singled out in his top panel plots. As far as I can tell, they were the min(), the max(), and their mean(). (r &lt;- range(my_data$Thorax)) ## [1] 0.64 0.94 mean(r) ## [1] 0.79 Next, we’ll make the data necessary for our side-tipped Gaussians. For kicks and giggles, we’ll choose 80 draws instead of 20. But do note how we used our r values, from above, to specify both Thorax and thorax_c values in addition to the CompanionNumber categories for the newdata argument. Otherwise, this workflow is very much the same as in previous plots. n_draws &lt;- 80 densities &lt;- my_data %&gt;% distinct(CompanionNumber) %&gt;% expand(CompanionNumber, Thorax = c(r[1], mean(r), r[2])) %&gt;% mutate(thorax_c = Thorax - mean(my_data$Thorax)) %&gt;% add_fitted_draws(fit19.5, n = n_draws, seed = 19, dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Longevity = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Longevity) %&gt;% mutate(density = dnorm(Longevity, mu, sigma)) glimpse(densities) ## Rows: 120,000 ## Columns: 14 ## Groups: CompanionNumber, Thorax, thorax_c, .row [15] ## $ CompanionNumber &lt;chr&gt; &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;… ## $ Thorax &lt;dbl&gt; 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0… ## $ thorax_c &lt;dbl&gt; -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ .draw &lt;int&gt; 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 3… ## $ .value &lt;dbl&gt; 38.56409, 38.56409, 38.56409, 38.56409, 38.56409, 38.56409, 38.56409, 38.… ## $ mu &lt;dbl&gt; 38.56409, 38.56409, 38.56409, 38.56409, 38.56409, 38.56409, 38.56409, 38.… ## $ sigma &lt;dbl&gt; 12.08948, 12.08948, 12.08948, 12.08948, 12.08948, 12.08948, 12.08948, 12.… ## $ ll &lt;dbl&gt; 14.86915, 14.86915, 14.86915, 14.86915, 14.86915, 14.86915, 14.86915, 14.… ## $ ul &lt;dbl&gt; 62.25903, 62.25903, 62.25903, 62.25903, 62.25903, 62.25903, 62.25903, 62.… ## $ Longevity &lt;dbl&gt; 14.86915, 15.34783, 15.82652, 16.30520, 16.78389, 17.26258, 17.74126, 18.… ## $ density &lt;dbl&gt; 0.004834375, 0.005220395, 0.005628408, 0.006058804, 0.006511894, 0.006987… Here, we’ll use a simplified workflow to extract the fitted() values in order to make the regression lines. Since these are straight lines, all we need are two values for each draw, one at the extremes of the Thorax axis. f &lt;- my_data %&gt;% distinct(CompanionNumber) %&gt;% expand(CompanionNumber, Thorax = c(r[1], mean(r), r[2])) %&gt;% mutate(thorax_c = Thorax - mean(my_data$Thorax)) %&gt;% add_fitted_draws(fit19.5, n = n_draws, seed = 19, value = &quot;Longevity&quot;) glimpse(f) ## Rows: 1,200 ## Columns: 8 ## Groups: CompanionNumber, Thorax, thorax_c, .row [15] ## $ CompanionNumber &lt;chr&gt; &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;None0&quot;, &quot;… ## $ Thorax &lt;dbl&gt; 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0… ## $ thorax_c &lt;dbl&gt; -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.18096, -0.… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ .draw &lt;int&gt; 31, 208, 586, 800, 1099, 1115, 1353, 1800, 2120, 2133, 2195, 2220, 2492, … ## $ Longevity &lt;dbl&gt; 38.56409, 30.73483, 41.54440, 39.10625, 36.16563, 31.61745, 39.23632, 36.… Now we’re ready to make our plots for the top row of Figure 19.3. densities %&gt;% ggplot(aes(x = Longevity, y = Thorax)) + # the Gaussians geom_ridgeline(aes(height = -density, group = interaction(Thorax, .draw)), fill = NA, size = 1/5, scale = 5/3, color = adjustcolor(&quot;grey50&quot;, alpha.f = 1/5), min_height = NA) + # the vertical lines below the Gaussians geom_line(aes(group = interaction(Thorax, .draw)), color = &quot;grey50&quot;, alpha = 1/5, size = 1/5) + # the regression lines geom_line(data = f, aes(group = .draw), alpha = 1/5, size = 1/5, color = &quot;grey50&quot;) + # the data geom_point(data = my_data, alpha = 1/2) + coord_flip(xlim = c(0, 110), ylim = c(.58, 1)) + facet_wrap(~CompanionNumber, ncol = 5) Now we have a covariate in the model, we have to decide on which of its values we want to base our group comparisons. Unless there’s a substantive reason for another value, the mean is a good standard choice. And since the covariate thorax_c is already mean centered, that means we can effectively leave it out of the equation. Here we make and save them in the simple difference metric. post &lt;- posterior_samples(fit19.5) differences &lt;- post %&gt;% transmute(`Pregnant1.Pregnant8 vs None0` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`, `Pregnant1.Pregnant8.None0 vs Virgin1` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - `r_CompanionNumber[Virgin1,Intercept]`, `Virgin1 vs Virgin8` = `r_CompanionNumber[Virgin1,Intercept]` - `r_CompanionNumber[Virgin8,Intercept]`, `Pregnant1.Pregnant8.None0 vs Virgin1.Virgin8` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - (`r_CompanionNumber[Virgin1,Intercept]` + `r_CompanionNumber[Virgin8,Intercept]`) / 2) p1 &lt;- differences %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + theme(strip.text = element_text(size = 6.4)) + facet_wrap(~key, scales = &quot;free_x&quot;, ncol = 4) Now we’ll look at the differences in the effect size metric. Since we saved our leg work above, it’s really easy to just convert the differences in bulk with mutate_all(). After the conversion, we’ll bind the two rows of subplots together with a little patchwork and display the results. p2 &lt;- differences %&gt;% mutate_all(.funs = ~. / post$sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Effect Size&quot;) + theme(strip.text = element_text(size = 6.4)) + facet_wrap(~key, scales = &quot;free_x&quot;, ncol = 4) library(patchwork) p1 / p2 “The HDI widths of all the contrasts have gotten smaller by virtue of including the covariate in the analysis” (p. 571). 19.4.2 Analogous to traditional ANCOVA. In contrast with ANCOVA, Bayesian methods do not partition the least-squares variance to make estimates, and therefore the Bayesian method is analogous to ANCOVA but is not ANCOVA. Frequentist practitioners are urged to test (with \\(p\\) values) whether the assumptions of (a) equal slope in all groups, (b) equal standard deviation in all groups, and (c) normally distributed noise can be rejected. In a Bayesian approach, the descriptive model is generalized to address these concerns, as will be discussed in Section 19.5. (p. 572) 19.4.3 Relation to hierarchical linear regression. Here Kruschke contrasts our last model with the one from way back in Chapter 17, section 3. As a refresher, here’s what that code looked like. fit17.4 &lt;- brm(data = my_data, family = student, y_z ~ 1 + x_z + (1 + x_z || Subj), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 1), class = sigma), # the next line is new prior(normal(0, 1), class = sd), prior(exponential(one_over_twentynine) + 1, class = nu)), chains = 4, cores = 4, stanvars = stanvar(1/29, name = &quot;one_over_twentynine&quot;), seed = 17, file = &quot;fits/fit17.04&quot;) And for convenience, here’s the code from the model we just fit. fit19.5 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 1 + thorax_c + (1 | CompanionNumber), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(normal(0, 2 * sd_y / sd_thorax_c), class = b), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, control = list(adapt_delta = 0.99), stanvars = stanvars, file = &quot;fits/fit19.05&quot;) It’s easy to get lost in the differences in the priors and the technical details with the model chains and such. The main thing to notice, here, is the differences in the model formulas (i.e., the likelihoods). Both models had intercepts and slopes. But whereas the model from 17.3 set both parameters to random, only the intercept in our last model was random. The covariate thorax_c was fixed–it did not vary by group. Had we wanted it to, our formula syntax would have been something like Longevity ~ 1 + thorax_c + (1 + thorax_c || CompanionNumber). And again, as noted in Chapter 17, the || portion of the syntax set the random intercepts and slopes to be orthogonal (i.e., correlate exactly at zero). As we’ll see, this will often not be the case. But let’s not get ahead of ourselves. Conceptually, the main difference between the models is merely the focus of attention. In the hierarchical linear regression model, the focus was on the slope coefficient. In that case, we were trying to estimate the magnitude of the slope, simultaneously for individuals and overall. The intercepts, which describe the levels of the nominal predictor, were of ancillary interest. In the present section, on the other hand, the focus of attention is reversed. We are most interested in the intercepts and their differences between groups, with the slopes on the covariate being of ancillary interest. (p. 573) 19.5 Heterogeneous variances and robustness against outliers On page 574, Kruschke laid out the schematic for a hierarchical Student’s-\\(t\\) model in for which both the \\(\\mu\\) and \\(\\sigma\\) parameters are random. If you recall, Bürkner (2020c) calls these distributional models and they are indeed available within the brms framework. But there’s a catch. Though we can model \\(\\sigma\\) all day long and we can even make it hierarchical, brms limits us to modeling the hierarchical \\(\\sigma\\) parameters within the typical Gaussian framework. That is, we will depart from Kruschke’s schematic in that we will be modeling the log of \\(\\sigma\\), indicating its grand mean with the sigma ~ 1 syntax, modeling the group-level deflections as Gaussian with a mean of 0 and standard deviation \\(\\sigma_\\sigma\\) estimated from the data, and choosing a sensible prior for \\(\\sigma_\\sigma\\) that is left-bound at 0 and gently slopes to the right (i.e., a folded \\(t\\) or gamma distribution). Since we’re modeling \\(\\log (\\sigma)\\), we might use Gaussian prior centered on sd(my_data$y) %&gt;% log() and a reasonable spread like 1. We can simulate a little to get a sense of what those distributions look like. n_draws &lt;- 1e3 set.seed(19) tibble(prior = rnorm(n_draws, mean = log(1), sd = 1)) %&gt;% mutate(prior_exp = exp(prior)) %&gt;% gather(key, value) %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey50&quot;, color = &quot;transparent&quot;) + facet_wrap(~key, scales = &quot;free&quot;) Here’s what is looks like with sd = 2. set.seed(19) tibble(prior = rnorm(n_draws, mean = log(1), sd = 2)) %&gt;% mutate(prior_exp = exp(prior)) %&gt;% ggplot(aes(x = prior_exp)) + geom_density(fill = &quot;grey50&quot;, color = &quot;transparent&quot;) + coord_cartesian(xlim = c(0, 17)) Though we’re still peaking around 1, there’s more mass in the tail, making it easier for the likelihood to pull away from the prior mode. But all this is the prior on the fixed effect, the grand mean of \\(\\log (\\sigma)\\). Keep in mind we’re also estimating group-level deflections using a hierarchical model. The good old folded \\(t\\) on the unit scale is already pretty permissive for an estimate that is itself on the log scale. To make it more conservative, set \\(\\nu\\) to infinity and go with a folded Gaussian. Or keep your regularization loose and go with a low-\\(\\nu\\) folded \\(t\\) or even a folded Cauchy. And, of course, one could even go with a gamma. Consider we have data my_data for which our primary variable of interest is y. Starting from preparing our stanvars values, here’s what the model code might look like. # get ready for `stanvars` mean_y &lt;- mean(my_data$y) sd_y &lt;- sd(my_data$y) omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma) # define `stanvars` stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) # fit the model fit &lt;- brm(data = my_data, family = student, bf(Longevity ~ 1 + (1 | CompanionNumber), sigma ~ 1 + (1 | CompanionNumber)), prior = c(# grand means prior(normal(mean_y, sd_y * 5), class = Intercept), prior(normal(log(sd_y), 1), class = Intercept, dpar = sigma), # the priors controlling the spread for our hierarchical deflections prior(gamma(alpha, beta), class = sd), prior(normal(0, 1), class = sd, dpar = sigma), # don&#39;t forget our student-t nu prior(exponential(one_over_twentynine), class = nu)), stanvars = stanvars) 19.5.1 Example: Contrast of means with different variances. Let’s load and take a look at Kruschke’s simulated group data. my_data &lt;- read_csv(&quot;data.R/NonhomogVarData.csv&quot;) head(my_data) ## # A tibble: 6 x 2 ## Group Y ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 97.8 ## 2 A 99.9 ## 3 A 92.4 ## 4 A 96.9 ## 5 A 101. ## 6 A 80.7 Here are the means and \\(SD\\)s for each Group. my_data %&gt;% group_by(Group) %&gt;% summarise(mean = mean(Y), sd = sd(Y)) ## # A tibble: 4 x 3 ## Group mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 97.0 8.00 ## 2 B 99.0 1.00 ## 3 C 102. 1. ## 4 D 104. 8.00 First we’ll fit the model with homogeneous variances. To keep things simple, here we’ll fit a conventional model following the form of our original fit1. Here are our stanvars. (mean_y &lt;- mean(my_data$Y)) ## [1] 100.5 (sd_y &lt;- sd(my_data$Y)) ## [1] 6.228965 omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y (s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)) ## $shape ## [1] 1.283196 ## ## $rate ## [1] 0.09092861 # define `stanvars` stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Now fit the ANOVA-like homogeneous-variances model. fit19.6 &lt;- brm(data = my_data, family = gaussian, Y ~ 1 + (1 | Group), prior = c(prior(normal(mean_y, sd_y * 10), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, control = list(adapt_delta = 0.999), stanvars = stanvars, file = &quot;fits/fit19.06&quot;) Here’s the model summary. print(fit19.6) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Y ~ 1 + (1 | Group) ## Data: my_data (Number of observations: 96) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~Group (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 4.96 3.56 1.46 14.67 1.00 1814 2546 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 100.47 3.19 94.43 106.42 1.00 2255 1885 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.76 0.43 4.99 6.68 1.00 5581 5873 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s get ready to make our version of the top of Figure 19.7. First we wrangle. # how many model-implied Gaussians would you like? n_draws &lt;- 20 densities &lt;- my_data %&gt;% distinct(Group) %&gt;% add_fitted_draws(fit19.6, n = n_draws, seed = 19, dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Y = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Y) %&gt;% mutate(density = dnorm(Y, mu, sigma)) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) glimpse(densities) ## Rows: 8,000 ## Columns: 12 ## Groups: .draw [20] ## $ Group &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .draw &lt;int&gt; 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, … ## $ .value &lt;dbl&gt; 99.42235, 99.42235, 99.42235, 99.42235, 99.42235, 99.42235, 99.42235, 99.42235… ## $ mu &lt;dbl&gt; 99.42235, 99.42235, 99.42235, 99.42235, 99.42235, 99.42235, 99.42235, 99.42235… ## $ sigma &lt;dbl&gt; 6.640072, 6.640072, 6.640072, 6.640072, 6.640072, 6.640072, 6.640072, 6.640072… ## $ ll &lt;dbl&gt; 86.40805, 86.40805, 86.40805, 86.40805, 86.40805, 86.40805, 86.40805, 86.40805… ## $ ul &lt;dbl&gt; 112.4367, 112.4367, 112.4367, 112.4367, 112.4367, 112.4367, 112.4367, 112.4367… ## $ Y &lt;dbl&gt; 86.40805, 86.67096, 86.93388, 87.19679, 87.45971, 87.72262, 87.98554, 88.24846… ## $ density &lt;dbl&gt; 0.1465288, 0.1582290, 0.1705958, 0.1836410, 0.1973740, 0.2118018, 0.2269281, 0… In our wrangling code, the main thing to notice is those last two lines. If you look closely to Kruschke’s Gaussians, you’ll notice they all have the same maximum height. Up to this point, ours haven’t. This has to do with technicalities on how densities are scaled. In brief, the wider densities have been shorter. So those last two lines scaled all the densities within the same group to the same metric. Otherwise the code was business as usual. Anyway, here’s our version of the top panel of Figure 19.7. densities %&gt;% ggplot(aes(x = Y, y = Group)) + geom_ridgeline(aes(height = density, group = interaction(Group, .draw)), fill = NA, color = adjustcolor(&quot;grey50&quot;, alpha.f = 2/3), size = 1/3, scale = 3/4) + geom_jitter(data = my_data, height = .04, alpha = 1/2) + scale_x_continuous(breaks = seq(from = 80, to = 120, by = 10)) + labs(title = &quot;Data with Posterior Predictive Distrib.&quot;, y = NULL) + coord_cartesian(xlim = c(75, 125), ylim = c(1.25, 4.5)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) Here are the difference distributions in the middle of Figure 19.7. post &lt;- posterior_samples(fit19.6) differences &lt;- post %&gt;% transmute(`D vs A` = `r_Group[D,Intercept]` - `r_Group[A,Intercept]`, `C vs B` = `r_Group[C,Intercept]` - `r_Group[B,Intercept]`) differences %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;D vs A&quot;, &quot;C vs B&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.5, .95), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;Difference&quot;) + facet_wrap(~key, scales = &quot;free_x&quot;, ncol = 4) Now here are the effect sizes at the bottom of the figure. differences %&gt;% mutate_all(.funs = ~. / post$sigma) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;D vs A&quot;, &quot;C vs B&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.5, .95), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;Effect Size&quot;) + facet_wrap(~key, scales = &quot;free_x&quot;, ncol = 4) Oh and remember, if you’d like to get all the possible contrasts in bulk, tidybayes has got your back. fit19.6 %&gt;% spread_draws(r_Group[Group,]) %&gt;% compare_levels(r_Group, by = Group) %&gt;% # these next two lines allow us to reorder the contrasts along the y ungroup() %&gt;% mutate(Group = reorder(Group, r_Group)) %&gt;% ggplot(aes(x = r_Group, y = Group)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.95, .5)) + labs(x = &quot;Contrast&quot;, y = NULL) + coord_cartesian(ylim = c(1.33, 6.5)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) But to get back on track, here are the stanvars for the robust hierarchical variances model. stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) Now fit that robust better-than-ANOVA model. fit19.7 &lt;- brm(data = my_data, family = student, bf(Y ~ 1 + (1 | Group), sigma ~ 1 + (1 | Group)), prior = c(# grand means prior(normal(mean_y, sd_y * 10), class = Intercept), prior(normal(log(sd_y), 1), class = Intercept, dpar = sigma), # the priors controlling the spread for our hierarchical deflections prior(gamma(alpha, beta), class = sd), prior(normal(0, 1), class = sd, dpar = sigma), # don&#39;t forget our student-t nu prior(exponential(one_over_twentynine), class = nu)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 19, control = list(adapt_delta = 0.99, max_treedepth = 12), stanvars = stanvars, file = &quot;fits/fit19.07&quot;) The chains look good. plot(fit19.7) Here’s the parameter summary. print(fit19.7) ## Family: student ## Links: mu = identity; sigma = log; nu = identity ## Formula: Y ~ 1 + (1 | Group) ## sigma ~ 1 + (1 | Group) ## Data: my_data (Number of observations: 96) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~Group (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 4.55 3.14 1.32 13.02 1.00 3148 4776 ## sd(sigma_Intercept) 1.23 0.40 0.65 2.18 1.00 4350 5529 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 100.48 2.73 94.73 105.97 1.00 4094 4128 ## sigma_Intercept 1.23 0.54 0.21 2.37 1.00 4568 5951 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## nu 32.67 28.51 4.65 109.84 1.00 10110 8384 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s get ready to make our version of the top of Figure 19.7. First we wrangle. densities &lt;- my_data %&gt;% distinct(Group) %&gt;% add_fitted_draws(fit19.7, n = n_draws, seed = 19, dpar = c(&quot;mu&quot;, &quot;sigma&quot;, &quot;nu&quot;)) %&gt;% mutate(ll = qt(.025, df = nu), ul = qt(.975, df = nu)) %&gt;% mutate(Y = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Y) %&gt;% mutate(density = dt(Y, nu)) %&gt;% # notice the conversion mutate(Y = mu + Y * sigma) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) glimpse(densities) ## Rows: 8,000 ## Columns: 13 ## Groups: .draw [20] ## $ Group &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .draw &lt;int&gt; 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, … ## $ .value &lt;dbl&gt; 97.33906, 97.33906, 97.33906, 97.33906, 97.33906, 97.33906, 97.33906, 97.33906… ## $ mu &lt;dbl&gt; 97.33906, 97.33906, 97.33906, 97.33906, 97.33906, 97.33906, 97.33906, 97.33906… ## $ sigma &lt;dbl&gt; 6.611823, 6.611823, 6.611823, 6.611823, 6.611823, 6.611823, 6.611823, 6.611823… ## $ nu &lt;dbl&gt; 24.33915, 24.33915, 24.33915, 24.33915, 24.33915, 24.33915, 24.33915, 24.33915… ## $ ll &lt;dbl&gt; -2.062378, -2.062378, -2.062378, -2.062378, -2.062378, -2.062378, -2.062378, -… ## $ ul &lt;dbl&gt; 2.062378, 2.062378, 2.062378, 2.062378, 2.062378, 2.062378, 2.062378, 2.062378… ## $ Y &lt;dbl&gt; 83.70299, 83.97846, 84.25394, 84.52941, 84.80489, 85.08037, 85.35584, 85.63132… ## $ density &lt;dbl&gt; 0.1299849, 0.1401936, 0.1510374, 0.1625369, 0.1747115, 0.1875784, 0.2011531, 0… If you look closely at our code, above, you’ll note switching from the Gaussian to the Student \\(t\\) required changes in our flow. Most obviously, we switched from qnorm() and dnorm() to qt() and dt(), respectively. The base R Student \\(t\\) functions don’t take arguments for \\(\\mu\\) and \\(\\sigma\\). Rather, they’re presumed to be 0 and 1, respectively. That means that for our first three mutate() functions, the computations were all based on the standard Student \\(t\\), with only the \\(\\nu\\) parameter varying according to the posterior. The way we corrected for that was with the fourth mutate(). Now we’re ready to make and save our version of the top panel of Figure 19.7. p1 &lt;- densities %&gt;% ggplot(aes(x = Y, y = Group)) + geom_ridgeline(aes(height = density, group = interaction(Group, .draw)), fill = NA, color = adjustcolor(&quot;grey50&quot;, alpha.f = 2/3), size = 1/3, scale = 3/4) + geom_jitter(data = my_data, height = .04, alpha = 1/2) + scale_x_continuous(breaks = seq(from = 80, to = 120, by = 10)) + labs(title = &quot;Data with Posterior Predictive Distrib.&quot;, y = NULL) + coord_cartesian(xlim = c(75, 125), ylim = c(1.25, 4.5)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) Here we make the difference distributions in the middle of Figure 19.8. post &lt;- posterior_samples(fit19.7) p2 &lt;- post %&gt;% transmute(`D vs A` = `r_Group[D,Intercept]` - `r_Group[A,Intercept]`, `C vs B` = `r_Group[C,Intercept]` - `r_Group[B,Intercept]`) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;D vs A&quot;, &quot;C vs B&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.5, .95), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + facet_wrap(~key, scales = &quot;free_x&quot;, ncol = 4) And here we make the plots of the corresponding effect sizes at the bottom of the Figure 19.8. p3 &lt;- post %&gt;% transmute(`D vs A` = (`r_Group[D,Intercept]` - `r_Group[A,Intercept]`) / exp((b_sigma_Intercept + `r_Group__sigma[D,Intercept]` + b_sigma_Intercept + `r_Group__sigma[A,Intercept]`) / 2), `C vs B` = (`r_Group[C,Intercept]` - `r_Group[B,Intercept]`) / exp((b_sigma_Intercept + `r_Group__sigma[C,Intercept]` + b_sigma_Intercept + `r_Group__sigma[B,Intercept]`) / 2)) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;D vs A&quot;, &quot;C vs B&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.5, .95), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Effect Size&quot;) + facet_wrap(~key, scales = &quot;free_x&quot;, ncol = 4) Combine them all and plot! p1 / p2 / p3 + plot_layout(heights = c(2, 1, 1)) Notice that because (a) the sigma parameters were heterogeneous and (b) they were estimated on the log scale, we had to do quite a bit more data processing before they effect size estimates were ready. “Finally, because each group has its own estimated scale (i.e., \\(\\sigma_j\\)), we can investigate differences in scales across groups” (p. 578). That’s not a bad idea. Even though Kruschke didn’t show this in the text, we may as well give it a go. post %&gt;% transmute(`D vs A` = exp(b_sigma_Intercept + `r_Group__sigma[D,Intercept]`) - exp(b_sigma_Intercept + `r_Group__sigma[A,Intercept]`), `C vs B` = exp(b_sigma_Intercept + `r_Group__sigma[C,Intercept]`) - exp(b_sigma_Intercept + `r_Group__sigma[B,Intercept]`), `D vs C` = exp(b_sigma_Intercept + `r_Group__sigma[D,Intercept]`) - exp(b_sigma_Intercept + `r_Group__sigma[C,Intercept]`), `B vs A` = exp(b_sigma_Intercept + `r_Group__sigma[B,Intercept]`) - exp(b_sigma_Intercept + `r_Group__sigma[A,Intercept]`)) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;D vs A&quot;, &quot;C vs B&quot;, &quot;D vs C&quot;, &quot;B vs A&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.5, .95), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(Differences~&#39;in&#39;~sigma)) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 4) For more on models including a hierarchical structure on both the mean and scale structures, check out Donald Williams and colleagues’ work on what they call Mixed Effect Location and Scale Models (MELSM; e.g., Williams, Liu, et al., 2019; Williams, Zimprich, et al., 2019). They’re quite handy and I’ve begun using them in my applied work (e.g., here). 19.6 Exercises Walk out an effect size We computed a lot of effect sizes in this chapter. They were all standardized mean differences. Cohen (1988) discussed these kinds of effect sizes in this way: We need a “pure” number, one free of our original measurement unit, with which to index what can be alternately called the degree of departure from the null hypothesis of the alternate hypothesis, or the ES (effect size) we wish to detect. This is accomplished by standardizing the raw effect size as expressed in the measurement unit of the dependent variable by dividing it by the (common) standard deviation of the measures in their respective populations, the latter also in the original measurement unit. (p. 20) Though Cohen framed his discussion in terms of null-hypothesis significance testing, we can just as easily apply it to our Bayesian modeling framework. The main thing is we can use his definitions form above to define a particular kind of effect size—the standardized mean difference between two groups. This is commonly referred to as a Cohen’s \\(d\\), which follows the formula \\[d = \\frac{\\bar y_A - \\bar y_B}{s_y},\\] where the unstandardized means of the variable of interest \\(y\\) are compared between two groups, A and B. From the raw data, we compute their two means, \\(\\bar y_A\\) and \\(\\bar y_B\\), and divide their difference by the common (i.e., pooled) standard deviation \\(s_y\\). As is the typical case, the empirically-derived means and standard deviations are stand-ins (i.e., estimates) of the population parameters. If we’re willing to ignore uncertainty, we can do this all by hand. Let’s walk this out with the fruit-fly data from section 19.3.2. my_data &lt;- read_csv(&quot;data.R/FruitflyDataReduced.csv&quot;) glimpse(my_data) ## Rows: 125 ## Columns: 3 ## $ Longevity &lt;dbl&gt; 35, 37, 49, 46, 63, 39, 46, 56, 63, 65, 56, 65, 70, 63, 65, 70, 77, 81, 8… ## $ CompanionNumber &lt;chr&gt; &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnant8&quot;, &quot;Pregnan… ## $ Thorax &lt;dbl&gt; 0.64, 0.68, 0.68, 0.72, 0.72, 0.76, 0.76, 0.76, 0.76, 0.76, 0.80, 0.80, 0… Recall we have five groups indexed by CompanionNumber, each with \\(n = 25\\). my_data %&gt;% count(CompanionNumber) ## # A tibble: 5 x 2 ## CompanionNumber n ## &lt;chr&gt; &lt;int&gt; ## 1 None0 25 ## 2 Pregnant1 25 ## 3 Pregnant8 25 ## 4 Virgin1 25 ## 5 Virgin8 25 Let’s focus on just two groups, the male fruit flies for which individual males were supplied access to one or with virgin female fruit flies per day. In the data, these are CompanionNumber == Virgin1 and CompanionNumber == Virgin8, respectively. Here’s a look at their mean Longevity values. my_data %&gt;% filter(str_detect(CompanionNumber, &quot;Virgin&quot;)) %&gt;% group_by(CompanionNumber) %&gt;% summarise(mean = mean(Longevity)) ## # A tibble: 2 x 2 ## CompanionNumber mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Virgin1 56.8 ## 2 Virgin8 38.7 If we’re willing to treat the males in the Virgin1 group as group “a” and those in the Virgin8 group as group “b”, we can save those mean values like so. y_bar_a &lt;- 56.76 y_bar_b &lt;- 38.72 Now we’ll compute their pooled standard deviation. my_data %&gt;% filter(str_detect(CompanionNumber, &quot;Virgin&quot;)) %&gt;% summarise(s = sd(Longevity)) ## # A tibble: 1 x 1 ## s ## &lt;dbl&gt; ## 1 16.2 Save that value. s_y &lt;- 16.24533 Now computing Cohen’s \\(d\\) is just simple arithmetic. (y_bar_a - y_bar_b) / s_y ## [1] 1.110473 Though I’m not up on contemporary standards in fruit fly research, a Cohen’s \\(d\\) of that size would be considered [conspicuously] large in most areas of my field (psychology). If we’d like to compute the \\(d\\) estimates for any other combination of experimental conditions, we’d just follow the corresponding arithmetic. As I hinted at earlier, the problem with this approach is it ignores uncertainty. Frequentists use various formulas to express this in terms of 95% confidence intervals. Our approach will be to express it with the posterior distribution of a Bayesian model. We’ve already accomplished this with our fit19.1 from above. Here we’ll use three other approaches. Instead of the Bayesian hierarchical alternative to the frequentist ANOVA, we can use a single-level model where we predict a metric variable with separate intercepts for the two levels of CompanionNumber. First, we subset the data and define our stanvars. my_data &lt;- my_data %&gt;% filter(str_detect(CompanionNumber, &quot;Virgin&quot;)) mean_y &lt;- (y_bar_a + y_bar_b) / 2 stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(s_y, name = &quot;sd_y&quot;) Fit the model with brm(). fit19.8 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 0 + CompanionNumber, prior = c(prior(normal(mean_y, sd_y * 5), class = b), prior(cauchy(0, sd_y), class = sigma)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 19, stanvars = stanvars, file = &quot;fits/fit19.08&quot;) Check the summary. print(fit19.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 0 + CompanionNumber ## Data: my_data (Number of observations: 50) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## CompanionNumberVirgin1 56.74 2.87 51.12 62.41 1.00 6734 5909 ## CompanionNumberVirgin8 38.70 2.81 33.14 44.20 1.00 7202 5730 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 13.85 1.45 11.38 17.07 1.00 6368 5482 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract the posterior draws. post &lt;- posterior_samples(fit19.8) Here we’ll plot the three dimensions of the posterior, each with the corresponding value from the Cohen’s \\(d\\) formula marked off as a vertical line in the foreground. lines &lt;- tibble(name = c(&quot;b_CompanionNumberVirgin1&quot;, &quot;b_CompanionNumberVirgin8&quot;, &quot;sigma&quot;), value = c(y_bar_a, y_bar_b, s_y)) post %&gt;% pivot_longer(-lp__) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeyeh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, normalize = &quot;panels&quot;) + geom_vline(data = lines, aes(xintercept = value), color = &quot;grey33&quot;, linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;posterior&quot;) + facet_wrap(~name, scales = &quot;free&quot;) The model did a great job capturing the group means. Notice how the posterior sigma is a bit lower than the value \\(s_y\\). This is because \\(\\sigma\\) in the model is conditioned on the mean. Because we have two means, one for each group, that has ‘explained some of the variation’ in the criterion variable Longevity. Thus, our posterior for \\(\\sigma\\) might be more fully expressed as \\(\\sigma | \\text{CompanionNumber}\\). If we would like to compute our Cohen’s \\(d\\) using the posterior iterations from fit19.8, we’d execute something like this. post %&gt;% mutate(d = (b_CompanionNumberVirgin1 - b_CompanionNumberVirgin8) / s_y) %&gt;% ggplot(aes(x = d, y = 0)) + stat_halfeyeh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;) + geom_vline(xintercept = (y_bar_a - y_bar_b) / s_y, color = &quot;grey33&quot;, linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(paste(&quot;Cohen&#39;s &quot;, italic(d), &quot; expressed as a posterior&quot;))) Similar to the previous plots, this time we superimposed the density with the estimate for \\(d\\) we computed above, (y_bar_a - y_bar_b) / s_y. Happily, the hand-calculated estimate coheres nicely with the central tendency of our posterior distribution. But now we get a full measure of uncertainty. Notice how wide those 95% HDIs are. Hopefully this isn’t a surprise given our noncommittal priors and only \\(n = 25\\) for both groups. There’s a lot of uncertainty in that posterior. A second way we might use a single-level model to compute a Cohen’s \\(d\\) effect size is using a dummy variable. We’ll convert our nominal variable CompanionNumber into a binary variable Virgin1 for which 1 corresponds to CompanionNumber == Virgin1 and 0 corresponds to CompanionNumber == Virgin8. Compute the dummy. my_data &lt;- my_data %&gt;% mutate(Virgin1 = if_else(CompanionNumber == &quot;Virgin1&quot;, 1, 0)) Now fit the dummy-predictor model with brm(). fit19.9 &lt;- brm(data = my_data, family = gaussian, Longevity ~ 1 + Virgin1, prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(normal(0, sd_y * 5), class = b), prior(cauchy(0, sd_y), class = sigma)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 19, stanvars = stanvars, file = &quot;fits/fit19.09&quot;) print(fit19.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Longevity ~ 1 + Virgin1 ## Data: my_data (Number of observations: 50) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 38.73 2.79 33.19 44.27 1.00 7735 5541 ## Virgin1 17.97 3.94 10.19 25.59 1.00 7058 5512 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 13.85 1.47 11.32 17.08 1.00 6433 5111 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With this parameterization, our posterior for Intercept is the same, within simulation variation, as CompanionNumberVirgin8 from fit7. The posterior for sigma is about the same for both models, too. But focus on Virgin1. This is the unstandardized mean difference, what we called \\(\\bar y_A - \\bar y_B\\) in our formula for Cohen’s \\(d\\). Here’s a look at its posterior distribution with its empirical estimate superimposed with a vertical line. post &lt;- posterior_samples(fit19.9) post %&gt;% ggplot(aes(x = b_Virgin1, y = 0)) + stat_halfeyeh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;) + geom_vline(xintercept = y_bar_a - y_bar_b, color = &quot;grey33&quot;, linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Unstandardized mean difference&quot;) Here’s how to standardize that unstandardized effect size into a Cohen’s-\\(d\\) metric. post %&gt;% mutate(d = b_Virgin1 / s_y) %&gt;% ggplot(aes(x = d, y = 0)) + stat_halfeyeh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;) + geom_vline(xintercept = (y_bar_a - y_bar_b) / s_y, color = &quot;grey33&quot;, linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(paste(&quot;Cohen&#39;s &quot;, italic(d), &quot; expressed as a posterior&quot;))) Let’s work this one more way. By simple algebra, a standardized mean difference is the same as the difference between two standardized means. If we standardize the criterion Longevity before fitting the model and continue using the dummy variable approach, the Virgin1 posterior will be the same as a Cohen’s \\(d\\). Standardize the criterion. my_data &lt;- my_data %&gt;% mutate(Longevity_s = (Longevity - mean(Longevity)) / sd(Longevity)) Because our criterion in a standardized metric, we no longer need our stanvars. fit19.10 &lt;- brm(data = my_data, family = gaussian, Longevity_s ~ 1 + Virgin1, prior = c(prior(normal(0, 1 * 5), class = Intercept), prior(normal(0, 1 * 5), class = b), prior(cauchy(0, 1), class = sigma)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 19, file = &quot;fits/fit19.10&quot;) Behold our out-of-the-box Bayesian Cohen’s \\(d\\). # no transformation necessary posterior_samples(fit19.10) %&gt;% ggplot(aes(x = b_Virgin1, y = 0)) + stat_halfeyeh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;) + geom_vline(xintercept = (y_bar_a - y_bar_b) / s_y, color = &quot;grey33&quot;, linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(paste(&quot;Cohen&#39;s &quot;, italic(d), &quot; expressed as a posterior&quot;))) If you work them through, the approaches we took for fit19.8 and fit19.9 can be generalized to models with more than two groups. You just need to be careful how to compute the \\(s_y\\) for each comparison. The same cannot be said for our fit19.10 approach. If you were to, say, fit a model that produced estimates for all five groups, you could not just standardize the data beforehand using the overall standard deviation. If you did, the pooled standard deviation you’d be basing your not-quite-Cohen’s \\(d\\) posteriors on would be pooled across all groups, not just the two groups entailed in a given two-group comparison. It’s also the case the that standardized mean differences we computed for fit19.1, above, are not quite Cohen’s \\(d\\) effect sizes in the same way these have been. This is because the hierarchical approach we used partially pooled the estimates for each group toward the grand mean. You might say they were hierarchically-regularized Cohen’s \\(d\\)s. But then again, Cohen’s formula for his \\(d\\) statistic did not account for Bayesian priors, either. So perhaps a purist would deny that any of the standardized mean differences we’ve computed in this chapter were proper Cohen’s \\(d\\) effect sizes. To be on the safe side, tell your readers exactly how you computed your models and what formulas you used to compute your effect sizes. 19.6.1 Populations and samples. You may have noticed that in our equation for \\(d\\), above, we defined it in terms of sample statistics. Here it is again: \\[d = \\frac{\\bar y_A - \\bar y_B}{s_y}\\] Sometimes we speak of the population parameter \\(\\delta\\), which is correspondingly defined as \\[\\delta = \\frac{\\mu_A - \\mu_B}{\\sigma},\\] where \\(\\mu_A\\) and \\(\\mu_B\\) are the population means for the two groups under consideration and \\(\\sigma\\) is the pooled standard deviation in the population. Often times we don’t have access to these values, which is why we run experiments and fit statistical models. But sometimes we do have access to the population parameters. In those cases, we can just plug them into the formula rather than estimate them in our models or with our sample statistics. Back in section 16.1.2, we saw an example of this with the TwoGroupIQ data. Let’s load them, again. my_data &lt;- read_csv(&quot;data.R/TwoGroupIQ.csv&quot;) glimpse(my_data) ## Rows: 120 ## Columns: 2 ## $ Score &lt;dbl&gt; 102, 107, 92, 101, 110, 68, 119, 106, 99, 103, 90, 93, 79, 89, 137, 119, 126, 110, … ## $ Group &lt;chr&gt; &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;, &quot;Smart Drug&quot;,… The data are IQ scores for participants in two groups. They look like this. my_data %&gt;% ggplot(aes(x = Score, Group)) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.95, .5)) + xlab(&quot;IQ score&quot;) + coord_cartesian(ylim = c(1.5, 2.25)) If we wanted to compute the point estimate for Cohen’s \\(d\\) by hand, we might extract the necessary sample statistics and go. # save the sample means for the groups y_bar_a &lt;- filter(my_data, Group == &quot;Smart Drug&quot;) %&gt;% summarise(mean = mean(Score)) %&gt;% pull() y_bar_b &lt;- filter(my_data, Group == &quot;Placebo&quot;) %&gt;% summarise(mean = mean(Score)) %&gt;% pull() # save the pooled standard deviation in the sample s_y &lt;- sd(my_data$Score) # compute Cohen&#39;s d (y_bar_a - y_bar_b) / s_y ## [1] 0.3479417 To get full posteriors for \\(d\\), you might use any of the three methods we practiced with, above. Because we’re only working with sample statistics, these are all only approximations of \\(\\delta\\). The thing about IQ scores is we actually know the population mean and standard deviation for IQ. They are 100 and 15, respectively. We know this because the people who make IQ tests design them that way. Let’s see how well out sample statistics approximate the population parameters. my_data %&gt;% group_by(Group) %&gt;% summarise(mean = mean(Score), sd = sd(Score)) ## # A tibble: 2 x 3 ## Group mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Placebo 100. 17.9 ## 2 Smart Drug 108. 25.4 Unsurprisingly, the values for the Smart Drug group are notably different from the population parameters. But notice how close the values from the Placebo group are to the population parameters. If they weren’t, we’d be concerned the Placebo condition was not a valid control. Looks like it was. However, notice that the mean and standard deviation for the Placebo group are not the exact values of 100 and 15 the way they are in the population. If we wanted to compute a standardized mean difference between our Smart Drug group and folks in the population, we could just plug those values directly into our effect size equation. Here’s what that would look like if we plug in the population mean for the control group. (y_bar_a - 100) / s_y ## [1] 0.3495057 The result is very close to the one above. But this time our equation for \\(d\\) was \\[d = \\frac{\\bar y_A - \\mu_B}{s_y},\\] where we used the population mean \\(\\mu_B\\), but the other two terms were based on values from the sample. As long as you are defining the Placebo control as a stand-in for the population, this is a more precice way to compute \\(d\\). Going further, we can also replace our \\(s_y\\) with \\(\\sigma\\) (i.e., 15). (y_bar_a - 100) / 15 ## [1] 0.5227513 Now our estimate is quite different. Why? Recall that sample standard deviations for both groups were larger than 15. That resulted in a \\(s_y\\) value that was larger than 15, which meant we divided the unstandardized difference by a larger denominator. That larger denominator returned a smaller product. We have more than simple algebra to contend with, though. Replacing \\(s_y\\) with 15 actually changed the meaning of the effect size. We were no longer using a pooled standard deviation. Rather, we were using the unconditional population standard deviation. Again, as long as we’re equating our Placebo group with the population, we might restate that 15 as \\(\\sigma_B\\). Sometimes you might see this expressed as \\(\\sigma_C\\), where \\(C\\) indicates “Control” (i.e., the reference category). That changed our formula to this: \\[d = \\frac{\\bar y_A - \\mu_B}{\\sigma_B}\\] So yes, that’s right. We don’t have to use pooled standard deviations to compute our standardized mean differences. If we have a good reference category, we could just use the sample estimate or population parameter for that, instead. Confusingly, you might see all these variants referred to as Cohen’s \\(d\\) within the literature. As with all the other decisions you make with experimental design and data analysis, use carefully reasoning to decide on how you’d like to compute your effect sizes, report them transparently to your audience, and prepare yourself for a collegial debate with Reviewer #2. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.0.0 tidybayes_2.0.3.9000 bayesplot_1.7.1 brms_2.12.0 ## [5] Rcpp_1.0.4.6 ggridges_0.5.2 forcats_0.5.0 stringr_1.4.0 ## [9] dplyr_0.8.5 purrr_0.3.4 readr_1.3.1 tidyr_1.0.2 ## [13] tibble_3.0.1 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 rsconnect_0.8.16 markdown_1.1 ## [5] base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 farver_2.0.3 ## [9] rstan_2.19.3 svUnit_1.0.3 DT_0.13 fansi_0.4.1 ## [13] mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 bridgesampling_1.0-0 ## [17] knitr_1.28 shinythemes_1.1.2 jsonlite_1.6.1 broom_0.5.5 ## [21] dbplyr_1.4.2 shiny_1.4.0.2 compiler_3.6.3 httr_1.4.1 ## [25] backports_1.1.6 assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [29] cli_2.0.2 later_1.0.0 prettyunits_1.1.1 htmltools_0.4.0 ## [33] tools_3.6.3 igraph_1.2.5 coda_0.19-3 gtable_0.3.0 ## [37] glue_1.4.0 reshape2_1.4.4 cellranger_1.1.0 vctrs_0.3.0 ## [41] nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ps_1.3.3 ## [45] rvest_0.3.5 mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 ## [49] gtools_3.8.2 zoo_1.8-7 scales_1.1.1 colourpicker_1.0 ## [53] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 ## [57] inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 ## [61] StanHeaders_2.21.0-1 loo_2.2.0 stringi_1.4.6 dygraphs_1.1.1.6 ## [65] pkgbuild_1.0.8 rlang_0.4.6 pkgconfig_2.0.3 matrixStats_0.56.0 ## [69] HDInterval_0.2.0 evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [73] htmlwidgets_1.5.1 labeling_0.3 tidyselect_1.0.0 processx_3.4.2 ## [77] plyr_1.8.6 magrittr_1.5 bookdown_0.18 R6_2.4.1 ## [81] generics_0.0.2 DBI_1.1.0 pillar_1.4.4 haven_2.2.0 ## [85] withr_2.2.0 xts_0.12-0 abind_1.4-5 modelr_0.1.6 ## [89] crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 rmarkdown_2.1 ## [93] grid_3.6.3 readxl_1.3.1 callr_3.4.3 threejs_0.3.3 ## [97] reprex_0.3.0 digest_0.6.25 xtable_1.8-4 httpuv_1.5.2 ## [101] stats4_3.6.3 munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.1 References "],
["metric-predicted-variable-with-multiple-nominal-predictors.html", "20 Metric Predicted Variable with Multiple Nominal Predictors 20.1 Describing groups of metric data with multiple nominal predictors 20.2 Hierarchical Bayesian approach 20.3 Rescaling can change interactions, homogeneity, and normality 20.4 Heterogeneous variances and robustness against outliers 20.5 Within-subject designs Session info Footnote", " 20 Metric Predicted Variable with Multiple Nominal Predictors This chapter considers data structures that consist of a metric predicted variable and two (or more) nominal predictors. This chapter extends ideas introduced in the previous chapter, so please read the previous chapter if you have not already… The traditional treatment of this sort of data structure is called multifactor analysis of variance (ANOVA). Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups. (Kruschke, 2015, pp. 583–584) 20.1 Describing groups of metric data with multiple nominal predictors Quick reprise: Suppose we have two nominal predictors, denoted \\(\\overrightarrow x_1\\) and \\(\\overrightarrow x_2\\). A datum from the \\(j\\)th level of \\(\\overrightarrow x_1\\) is denoted \\(x_{1[j]}\\), and analogously for the second factor. The predicted value is a baseline plus a deflection due to the level of factor 1 plus a deflection due to the level of factor 2 plus a residual deflection due to the interaction of factors: \\[\\begin{align*} \\mu &amp; = \\beta_0 + \\overrightarrow \\beta_1 \\cdot \\overrightarrow x_1 + \\overrightarrow \\beta_2 \\cdot \\overrightarrow x_2 + \\overrightarrow \\beta_{1 \\times 2} \\cdot \\overrightarrow x_{1 \\times 2} \\\\ &amp; = \\beta_0 + \\sum_j \\beta_{1[j]} x_{1[j]} + \\sum_k \\beta_{2[k]} x_{2[k]} + \\sum_{j, k} \\beta_{1 \\times 2[j, k]} x_{1 \\times 2[j, k]} \\end{align*}\\] The deflections within factors and within the interaction are constrained to sum to zero: \\[\\begin{align*} \\sum_j \\beta_{1[j]} = 0 &amp;&amp;&amp; \\text{and} &amp;&amp; \\sum_k \\beta_{2[k]} = 0 \\;\\;\\; \\text{and} \\\\ \\sum_j \\beta_{1 \\times 2[j, k]} = 0 \\text{ for all } k &amp;&amp;&amp; \\text{and} &amp;&amp; \\sum_k \\beta_{1 \\times 2[j, k]} = 0 \\text{ for all } j \\end{align*}\\] ([these equations] are repetitions of Equations 15.9 and 15.10, p. 434). The actual data are assumed to be randomly distributed around the predicted value. (pp. 584–585) 20.1.1 Interaction. An important concept of models with multiple predictors is interaction. Interaction means that the effect of a predictor depends on the level of another predictor. A little more technically, interaction is what is left over after the main effects of the factors are added: interaction is the nonadditive influence of the factors. (p. 585) Here are the data necessary for our version of Figure 20.1, which displays an interaction of two 2-level factors. library(tidyverse) grand_mean &lt;- 5 deflection_1 &lt;- 1.8 deflection_2 &lt;- 0.2 nonadditive_component &lt;- -1 ( d &lt;- tibble(x1 = rep(c(-1, 1), each = 2), x2 = rep(c(1, -1), times = 2)) %&gt;% mutate(mu_additive = grand_mean + (x1 * deflection_1) + (x2 * deflection_2)) %&gt;% mutate(mu_multiplicative = mu_additive + (x1 * x2 * nonadditive_component), # we&#39;ll need this to accommodate `position = &quot;dodge&quot;` within `geom_col()` x1_offset = x1 + x2 * -.45, # we&#39;ll need this for the fill x2_c = factor(x2, levels = c(1, -1))) ) ## # A tibble: 4 x 6 ## x1 x2 mu_additive mu_multiplicative x1_offset x2_c ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -1 1 3.4 4.4 -1.45 1 ## 2 -1 -1 3 2 -0.55 -1 ## 3 1 1 7 6 0.55 1 ## 4 1 -1 6.6 7.6 1.45 -1 There’s enough going on with the lines, arrows, and titles across the three panels that to my mind it seems easier to make three distinct plots and them join them at the end with syntax from the patchwork package. But enough of the settings are common among the panels that it also makes sense to keep from repeating that part of the code. So we’ll take a three-step solution. For the first step, we’ll make the baseline or foundational plot, which we’ll call p. p &lt;- d %&gt;% ggplot(aes(x = x1, y = mu_multiplicative)) + geom_col(aes(fill = x2_c), position = &quot;dodge&quot;) + scale_fill_viridis_d(NULL, option = &quot;C&quot;, begin = 1/3, end = 2/3, labels = c(&quot;x2[1]&quot;, &quot;x2[2]&quot;)) + scale_x_continuous(breaks = c(-1, 1), labels = c(&quot;x1[1]&quot;, &quot;x1[2]&quot;)) + scale_y_continuous(expression(mu), breaks = seq(from = 0, to = 10, by = 2)) + coord_cartesian(ylim = c(0, 10)) + theme(axis.ticks.x = element_blank(), legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(size = 0), legend.position = c(.17, .875), panel.grid = element_blank()) p Now we have p, we’ll add panel-specific elements to it, which we’ll save as individual objects, p1, p2, and p3. That’s step 2. Then for step 3, we’ll bring them all together with patchwork. # deflection from additive p1 &lt;- p + geom_segment(aes(x = x1_offset, xend = x1_offset, y = mu_additive, yend = mu_multiplicative), size = 1.25, arrow = arrow(length = unit(.275, &quot;cm&quot;))) + geom_line(aes(x = x1_offset, y = mu_additive, group = x2), linetype = 2) + geom_line(aes(x = x1_offset, y = mu_additive, group = x1), linetype = 2) + coord_cartesian(ylim = c(0, 10)) + ggtitle(&quot;Deflection from additive&quot;) # effect of x1 depends on x2 p2 &lt;- p + geom_segment(aes(x = x1_offset, xend = x1_offset, y = mu_additive, yend = mu_multiplicative), size = .5, arrow = arrow(length = unit(.15, &quot;cm&quot;))) + geom_line(aes(x = x1_offset, y = mu_additive, group = x2), linetype = 2) + geom_line(aes(x = x1_offset, y = mu_multiplicative, group = x2), size = 1.25) + ggtitle(&quot;Effect of x1 depends on x2&quot;) # effect of x2 depends on x1 p3 &lt;- p + geom_segment(aes(x = x1_offset, xend = x1_offset, y = mu_additive, yend = mu_multiplicative), size = .5, arrow = arrow(length = unit(.15, &quot;cm&quot;))) + geom_line(aes(x = x1_offset, y = mu_multiplicative, group = x1), size = 1.25) + geom_line(aes(x = x1_offset, y = mu_additive, group = x1), linetype = 2) + ggtitle(&quot;Effect of x2 depends on x1&quot;) library(patchwork) p1 + p2 + p3 And in case it’s not clear, “the average deflection from baseline due to a predictor… is called the main effect of the predictor. The main effects of the predictors correspond to the dashed lines in the left panel of Figure 20.1” (p. 587). And further The left panel of Figure 20.1 highlights the interaction as the nonadditive component, emphasized by the heavy vertical arrows that mark the departure from additivity. The middle panel of Figure 20.1 highlights the interaction by emphasizing that the effect of \\(x_1\\) depends on the level of \\(x_2\\). The heavy lines mark the effect of \\(x_1\\), that is, the changes from level 1 of \\(x_1\\) to level 2 of \\(x_1\\). Notice that the heavy lines have different slopes: The heavy line for level 1 of \\(x_2\\) has a shallower slope than the heavy line for level 2 of \\(x_2\\). The right panel of Figure 20.1 highlights the interaction by emphasizing that the effect of \\(x_2\\) depends on the level of \\(x_1\\). (p. 587) 20.1.2 Traditional ANOVA. As was explained in Section 19.2 (p. 556), the terminology, “analysis of variance,” comes from a decomposition of overall data variance into within-group variance and between-group variance… The Bayesian approach is not ANOVA, but is analogous to ANOVA. Traditional ANOVA makes decisions about equality of groups (i.e., null hypotheses) on the basis of \\(p\\) values using a null hypothesis that assumes (i) the data are normally distributed within groups, and (ii) the standard deviation of the data within each group is the same for all groups. The second assumption is sometimes called “homogeneity of variance.” The entrenched precedent of ANOVA is why basic models of grouped data make those assumptions, and why the basic models presented in this chapter will also make those assumptions. Later in the chapter, those constraints will be relaxed. (pp. 587–588) 20.2 Hierarchical Bayesian approach “Our goal is to estimate the main and interaction deflections, and other parameters, based on the observed data” (p. 588). 20.2.1 Implementation in JAGS brms. Below is how to implement the model based on the code from Kruschke’s Jags-Ymet-Xnom2fac-MnormalHom.R and Jags-Ymet-Xnom2fac-MnormalHom-Example.R scripts. With brms, we’ll need to specify the stanvars. mean_y &lt;- mean(my_data$y) sd_y &lt;- sd(my_data$y) omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) And before that, of course, make sure you’ve defined the gamma_a_b_from_omega_sigma() function. E.g., gamma_a_b_from_omega_sigma &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } With the preparatory work done, now all we’d need to do is run the brm() code. fit &lt;- brm(data = my_data, family = gaussian, y ~ 1 + (1 | factor_1) + (1 | factor_2) + (1 | factor_1:factor_2), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), stanvars = stanvars) If you have reason to use different priors for the random effects, you can always specify multiple lines of class = sd, each with the appropriate coef argument. The big new element is multiple (|) parts in the formula. In this simple model type, we’re only working random intercepts, in this case with two factors and their interaction. The formula above presumes the interaction is not itself coded within the data. But consider the case you have data including a term for the interaction of the two lower-level factors, called interaction. In that case, you’d have that last part of the formula read (1 | interaction), instead. 20.2.2 Example: It’s only money. Load the salary data.3 my_data &lt;- read_csv(&quot;data.R/Salary.csv&quot;) glimpse(my_data) ## Rows: 1,080 ## Columns: 6 ## $ Org &lt;chr&gt; &quot;PL&quot;, &quot;MUTH&quot;, &quot;ENG&quot;, &quot;CMLT&quot;, &quot;LGED&quot;, &quot;MGMT&quot;, &quot;INFO&quot;, &quot;CRIN&quot;, &quot;CRIN&quot;, &quot;PSY&quot;, &quot;SOC&quot;… ## $ OrgName &lt;chr&gt; &quot;Philosophy&quot;, &quot;Music Theory&quot;, &quot;English&quot;, &quot;Comparative Literature&quot;, &quot;Language Educ… ## $ Cla &lt;chr&gt; &quot;PC&quot;, &quot;PC&quot;, &quot;PC&quot;, &quot;PC&quot;, &quot;PT&quot;, &quot;PR&quot;, &quot;PT&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PC… ## $ Pos &lt;chr&gt; &quot;FT2&quot;, &quot;FT2&quot;, &quot;FT2&quot;, &quot;FT2&quot;, &quot;FT3&quot;, &quot;NDW&quot;, &quot;FT3&quot;, &quot;FT1&quot;, &quot;NDW&quot;, &quot;FT1&quot;, &quot;FT1&quot;, &quot;FT1… ## $ ClaPos &lt;chr&gt; &quot;PC.FT2&quot;, &quot;PC.FT2&quot;, &quot;PC.FT2&quot;, &quot;PC.FT2&quot;, &quot;PT.FT3&quot;, &quot;PR.NDW&quot;, &quot;PT.FT3&quot;, &quot;PR.FT1&quot;, &quot;… ## $ Salary &lt;dbl&gt; 72395, 61017, 82370, 68805, 63796, 219600, 98814, 107745, 114275, 173302, 117240,… We’ll follow Kruschke’s example on page 593 and modify the Pos variable a bit. my_data &lt;- my_data %&gt;% mutate(Pos = factor(Pos, levels = c(&quot;FT3&quot;, &quot;FT2&quot;, &quot;FT1&quot;, &quot;NDW&quot;, &quot;DST&quot;) , ordered = T, labels = c(&quot;Assis&quot;, &quot;Assoc&quot;, &quot;Full&quot;, &quot;Endow&quot;, &quot;Disting&quot;))) With 1080 cases, two factors, and a criterion, these data are a little too unwieldy to look at the individual case level. But if we’re tricky on how we aggregate, we can get a good sense of their structure with a geom_tile() plot. Here our strategy is to aggregate by our two factors, Pos and Org. Since our criterion is Salary, we’ll compute the mean value of the cases within each unique paring, encoded as m_salary. Also, we’ll get a sense of how many cases there are within each factor pairing with n. my_data %&gt;% group_by(Pos, Org) %&gt;% summarise(m_salary = mean(Salary), n = n()) %&gt;% ungroup() %&gt;% mutate(Org = fct_reorder(Org, m_salary), Pos = fct_reorder(Pos, m_salary)) %&gt;% ggplot(aes(x = Org, y = Pos, fill = m_salary)) + geom_tile() + geom_text(aes(label = n, color = m_salary &gt; 170000), size = 2.75) + # everything below this is really just aesthetic flourish scale_fill_viridis_c(&quot;median Salary&quot;, option = &quot;D&quot;, breaks = c(55e3, 15e4, 26e4), labels = c(&quot;$55K&quot;, &quot;$150K&quot;, &quot;$260K&quot;)) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;), guide = &#39;none&#39;) + scale_x_discrete(&quot;Org&quot;, expand = c(0, 0)) + scale_y_discrete(&quot;Pos&quot;, expand = c(0, 0)) + theme(axis.text.x = element_text(angle = 90, hjust = 0), axis.text.y = element_text(hjust = 0), axis.ticks = element_blank(), legend.position = &quot;top&quot;, panel.grid = element_blank()) Hopefully it’s clear that each cell is a unique pairing of Org and Pos. The cells are color coded by the mean Salary. The numbers in the cells give the \\(n\\) cases they represent. When there’s no data for a unique combination of Org and Pos, the cells are left light gray and blank. Load brms. library(brms) Define our stanvars. mean_y &lt;- mean(my_data$Salary) sd_y &lt;- sd(my_data$Salary) omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Now fit the model. fit20.1 &lt;- brm(data = my_data, family = gaussian, Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 20, control = list(adapt_delta = 0.999, max_treedepth = 13), stanvars = stanvars, file = &quot;fits/fit20.01&quot;) The chains look fine. plot(fit20.1) Here’s the model summary. print(fit20.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org) ## Data: my_data (Number of observations: 1080) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~Org (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 30648.04 3042.28 25442.24 37414.75 1.00 2059 3368 ## ## ~Pos (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 56119.95 24966.14 26767.66 120147.98 1.00 3423 5330 ## ## ~Pos:Org (Number of levels: 216) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 9697.15 1203.43 7393.97 12147.09 1.00 2523 4289 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 126255.52 27552.64 69727.11 181336.74 1.00 3225 4886 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 17975.97 436.80 17152.78 18853.66 1.00 7103 5660 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This was a difficult model to fit with brms. Stan does well when the criteria are on or close to a standardized metric and these Salary data are a far cry from that. Tuning adapt_delta and max_treedepth went a long way to help the model out. Okay, let’s get ready for our version of Figure 20.3. First, we’ll use tidybayes::add_fitted_draws() to help organize the necessary posterior draws. library(tidybayes) # how many draws would you like? n_draw &lt;- 20 # wrangle f &lt;- my_data %&gt;% distinct(Pos) %&gt;% expand(Pos, Org = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;)) %&gt;% add_fitted_draws(fit20.1, n = n_draw, seed = 20, allow_new_levels = T, dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Salary = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest() %&gt;% mutate(density = dnorm(Salary, mu, sigma)) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) %&gt;% mutate(Org = factor(Org, levels = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;))) glimpse(f) ## Rows: 40,000 ## Columns: 13 ## Groups: .draw [20] ## $ Pos &lt;ord&gt; Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, Assis, A… ## $ Org &lt;fct&gt; BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, BFIN, … ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .draw &lt;int&gt; 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127… ## $ .value &lt;dbl&gt; 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1… ## $ mu &lt;dbl&gt; 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1, 181687.1… ## $ sigma &lt;dbl&gt; 18226.54, 18226.54, 18226.54, 18226.54, 18226.54, 18226.54, 18226.54, 18226.54… ## $ ll &lt;dbl&gt; 145963.7, 145963.7, 145963.7, 145963.7, 145963.7, 145963.7, 145963.7, 145963.7… ## $ ul &lt;dbl&gt; 217410.4, 217410.4, 217410.4, 217410.4, 217410.4, 217410.4, 217410.4, 217410.4… ## $ Salary &lt;dbl&gt; 145963.7, 146685.4, 147407.1, 148128.8, 148850.5, 149572.1, 150293.8, 151015.5… ## $ density &lt;dbl&gt; 0.1465288, 0.1582290, 0.1705958, 0.1836410, 0.1973740, 0.2118018, 0.2269281, 0… We’re ready to plot. library(ggridges) f %&gt;% ggplot(aes(x = Salary, y = Pos)) + geom_vline(xintercept = fixef(fit20.1)[, 1], color = &quot;white&quot;) + geom_ridgeline(aes(height = density, group = interaction(Pos, .draw), color = Pos), fill = NA, show.legend = F, size = 1/4, scale = 3/4) + geom_jitter(data = my_data %&gt;% filter(Org %in% c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;)) %&gt;% mutate(Org = factor(Org, levels = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;))), height = .025, alpha = 1/4) + scale_color_viridis_d(option = &quot;B&quot;, end = .9) + scale_x_continuous(breaks = seq(from = 0, to = 300000, length.out = 4), labels = c(&quot;$0&quot;, &quot;$100K&quot;, &quot;200K&quot;, &quot;$300K&quot;)) + coord_cartesian(xlim = c(0, 35e4), ylim = c(1.25, 5.5)) + labs(title = &quot;Data with Posterior Predictive Distributions&quot;, subtitle = &quot;The white vertical line is the model-implied grand mean.&quot;, y = &quot;Pos&quot;) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) + facet_wrap(~Org, ncol = 2) The brms package doesn’t have a convenience function that returns output quite like what Kruschke displayed in his Table 20.2. But we can get close. The posterior_summary() will return posterior means, \\(SD\\)s, and percentile-based 95% intervals for all model parameters. Due to space concerns, I’ll just show the first ten lines. posterior_summary(fit20.1)[1:10,] ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 126255.523 27552.6419 69727.105 181336.736 ## sd_Org__Intercept 30648.036 3042.2779 25442.242 37414.752 ## sd_Pos__Intercept 56119.953 24966.1418 26767.655 120147.980 ## sd_Pos:Org__Intercept 9697.154 1203.4267 7393.966 12147.087 ## sigma 17975.971 436.8002 17152.785 18853.664 ## r_Org[ACTG,Intercept] 80254.691 7533.7446 65447.882 95019.103 ## r_Org[AFRO,Intercept] -14880.629 9964.0507 -34291.306 4570.176 ## r_Org[AMST,Intercept] -16510.435 10044.9806 -36225.773 3517.428 ## r_Org[ANTH,Intercept] -18933.854 7700.2653 -34071.502 -4130.609 ## r_Org[APHS,Intercept] 2678.886 7948.2368 -13079.343 18192.566 I’m not aware of a simple way to extract the effective samples for all parameters within a brms fit object. There is no effsamples() function. However, we do have neff_ratio(), which returns a named vector of the ratios. For space constraints, here we look at the first ten values. neff_ratio(fit20.1)[1:10] ## b_Intercept sd_Org__Intercept sd_Pos__Intercept sd_Pos:Org__Intercept ## 0.4175883 0.2579337 0.4780342 0.3140237 ## sigma r_Org[ACTG,Intercept] r_Org[AFRO,Intercept] r_Org[AMST,Intercept] ## 0.8754821 0.3452653 0.5259244 0.5291086 ## r_Org[ANTH,Intercept] r_Org[APHS,Intercept] ## 0.3124535 0.3494390 The brms::neff_ratio() function returns ratios of the effective samples over the total number of post-warmup iterations. So if we know the neff_ratio() values and the number of post-warmup iterations, the ‘Eff.Sample’ values are just a little algebra away. A quick solution is to look at the ‘total post-warmup samples’ line at the top of our print() output. Another way is to extract that information from our brm() fit object. I’m not aware of a way to do that directly, but we can extract the iter value (i.e., fit20.1$fit@sim$iter), the warmup value (i.e., fit20.1$fit@sim$warmup), and the number of chains (i.e., fit20.1$fit@sim$chains). With those values in hand, simple algebra will return the total post-warmup samples value. E.g., (n_iter &lt;- (fit20.1$fit@sim$iter - fit20.1$fit@sim$warmup) * fit20.1$fit@sim$chains) ## [1] 8000 And now we have n_iter, we can calculate the ‘Eff.Sample’ values. neff_ratio(fit20.1) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% set_names(&quot;parameter&quot;, &quot;neff_ratio&quot;) %&gt;% mutate(eff_sample = (neff_ratio * n_iter) %&gt;% round(digits = 0)) %&gt;% select(-neff_ratio) %&gt;% arrange(parameter) %&gt;% head() ## parameter eff_sample ## 1 b_Intercept 3341 ## 2 lp__ 1799 ## 3 r_Org[ACTG,Intercept] 2762 ## 4 r_Org[AFRO,Intercept] 4207 ## 5 r_Org[AMST,Intercept] 4233 ## 6 r_Org[ANTH,Intercept] 2500 If we’re careful, we can use left_join() to bind together the posterior summaries and their effective samples like this. post &lt;- posterior_samples(fit20.1) # extract the means and medians post %&gt;% gather() %&gt;% arrange(key) %&gt;% group_by(key) %&gt;% summarise(Mean = mean(value), Median = median(value)) %&gt;% # join to them the mode and HDIs left_join( post %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% select(key:.upper) %&gt;% rename(Mode = value, HDI_low = .lower, HDI_high = .upper), by = &quot;key&quot; ) %&gt;% # now finally join the effective samples left_join( neff_ratio(fit20.1) %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% set_names(&quot;key&quot;, &quot;neff_ratio&quot;) %&gt;% mutate(eff_sample = (neff_ratio * n_iter)) %&gt;% select(-neff_ratio) %&gt;% arrange(key), by = &quot;key&quot; ) %&gt;% # here we just reformat a bit mutate_if(is.double, round, digits = 0) %&gt;% rename(Parameter = key) %&gt;% head(n = 10) ## # A tibble: 10 x 7 ## Parameter Mean Median Mode HDI_low HDI_high eff_sample ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 126256 126790 127983 68383 179760 3341 ## 2 lp__ -12530 -12529 -12528 -12563 -12498 1799 ## 3 r_Org[ACTG,Intercept] 80255 80375 81556 65313 94857 2762 ## 4 r_Org[AFRO,Intercept] -14881 -14899 -14384 -34416 4442 4207 ## 5 r_Org[AMST,Intercept] -16510 -16410 -15106 -37139 2364 4233 ## 6 r_Org[ANTH,Intercept] -18934 -18831 -17860 -33410 -3506 2500 ## 7 r_Org[APHS,Intercept] 2679 2834 3435 -13173 18070 2796 ## 8 r_Org[AST,Intercept] -15 -122 -259 -17091 18328 3591 ## 9 r_Org[BEPP,Intercept] 47848 47723 46539 30509 65360 3251 ## 10 r_Org[BFIN,Intercept] 106924 106987 107801 92028 121476 3071 Our code returned the summaries for all model parameters, including the log posterior (i.e., lp__). Since that’s way too much output to display, here, we just looked at the first 10. If you wanted to subset the parameters, you could just filter() by the Parameter column. And if this kind of a table was really important, to you, you might even use the code above as inspiration for a custom function. As Kruschke then pointed out, “individual salaries vary tremendously around the predicted cell mean” (p. 594), which you can quantify using \\(\\sigma_y\\). Here it is using posterior_summary(). posterior_summary(fit20.1)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 17975.9712 436.8002 17152.7849 18853.6636 And we can get a better sense of the distribution with a plot. post %&gt;% ggplot(aes(x = sigma, y = 0)) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.5, .95)) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma[y])) + theme(panel.grid = element_blank()) As Kruschke pointed out, this parameter is held constant across all subgroups. That is, the subgroups are homogeneous with respect to their variances. We’ll relax this constraint later on. 20.2.3 Main effect contrasts. In applications with multiple levels of the factors, it is virtually always the case that we are interested in comparing particular levels with each other…. These sorts of comparisons, which involve levels of a single factor and collapse across the other factor(s), are called main effect comparisons or contrasts.(p. 595) The fitted() function provides a versatile framework for contrasts among the main effects. Here’s the first contrast. # define the new data nd &lt;- tibble(Pos = c(&quot;Assis&quot;, &quot;Assoc&quot;), Org = &quot;mu&quot;) # feed the new data into `fitted()` f &lt;- fitted(fit20.1, newdata = nd, summary = F, allow_new_levels = T) %&gt;% as_tibble() %&gt;% set_names(&quot;Assis&quot;, &quot;Assoc&quot;) %&gt;% mutate(`Assoc vs Assis` = Assoc - Assis) # plot f %&gt;% ggplot(aes(x = `Assoc vs Assis`, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Assoc vs Assis&quot;, x = &quot;Difference&quot;) + theme(panel.grid = element_blank()) In case you were curious, here are the summary statistics. f %&gt;% mode_hdi(`Assoc vs Assis`) %&gt;% select(`Assoc vs Assis`:.upper) %&gt;% mutate_if(is.double, round, digits = 0) ## # A tibble: 1 x 3 ## `Assoc vs Assis` .lower .upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 13291 8397 18213 Now make the next two contrasts. nd &lt;- tibble(Org = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;ENG&quot;, &quot;PSY&quot;), Pos = &quot;mu&quot;) f &lt;- fitted(fit20.1, newdata = nd, summary = F, allow_new_levels = T) %&gt;% as_tibble() %&gt;% set_names(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;ENG&quot;, &quot;PSY&quot;) %&gt;% transmute(`CHEM vs PSY` = CHEM - PSY, `BFIN vs other 3` = BFIN - (CHEM + ENG + PSY) / 3) # plot f %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;Difference&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;) And here are their numeric summaries. f %&gt;% gather(contrast, mode) %&gt;% group_by(contrast) %&gt;% mode_hdi(mode) %&gt;% select(contrast:.upper) %&gt;% mutate_if(is.double, round, digits = 0) ## # A tibble: 2 x 4 ## contrast mode .lower .upper ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BFIN vs other 3 103790 90498 119161 ## 2 CHEM vs PSY 12410 -2189 27319 For further discussion on marginal contrasts in brms, see the discussion in issue #552 in the brms GitHub repo and this discussion in the Stan user forums. 20.2.4 Interaction contrasts and simple effects. If we’d like to make the simple effects and interaction contrasts like Kruschke displayed in Figure 20.5 within our tidyverse/brms paradigm, it’ll be simplest to just redefine our nd data and use fitted(), again. # define our new data nd &lt;- crossing(Pos = c(&quot;Assis&quot;, &quot;Full&quot;), Org = c(&quot;CHEM&quot;, &quot;PSY&quot;)) # we need to update our col_names brief_col_names &lt;- crossing(Pos = c(&quot;Assis&quot;, &quot;Full&quot;), Org = c(&quot;CHEM&quot;, &quot;PSY&quot;)) %&gt;% unite(key, Pos, Org) %&gt;% pull() # get the draws with `fitted()` f1 &lt;- fitted(fit20.1, newdata = nd, summary = F) %&gt;% # wrangle as_tibble() %&gt;% set_names(brief_col_names) %&gt;% mutate(`Full - Assis @ PSY` = Full_PSY - Assis_PSY, `Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM) %&gt;% mutate(`Full.v.Assis (x) CHEM.v.PSY` = `Full - Assis @ CHEM` - `Full - Assis @ PSY`) # what have we done? head(f1) ## # A tibble: 6 x 7 ## Assis_CHEM Assis_PSY Full_CHEM Full_PSY `Full - Assis @ P… `Full - Assis @ C… `Full.v.Assis (x) C… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 85196. 87625. 156697. 113805. 26180. 71501. 45322. ## 2 90696. 84075. 156326. 118037. 33963. 65630. 31667. ## 3 89715. 80157. 156637. 118550. 38393. 66922. 28529. ## 4 84612. 90458. 164782. 115114. 24656. 80170. 55515. ## 5 94628. 82272. 161350. 112559. 30287. 66722. 36435. ## 6 84340. 86217. 147959. 118240. 32024. 63619. 31596. It’ll take just a tiny bit more wrangling before we’re ready to plot. text &lt;- tibble(key = &quot;Full - Assis @ PSY&quot;, value = 15500, y = .95, label = &quot;ROPE&quot;) %&gt;% mutate(key = factor(key, levels = c(&quot;Full - Assis @ PSY&quot;, &quot;Full - Assis @ CHEM&quot;, &quot;Full.v.Assis (x) CHEM.v.PSY&quot;))) f1 %&gt;% select(-contains(&quot;_&quot;)) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;Full - Assis @ PSY&quot;, &quot;Full - Assis @ CHEM&quot;, &quot;Full.v.Assis (x) CHEM.v.PSY&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + # for kicks and giggles we&#39;ll throw in the ROPE geom_rect(xmin = -1e3, xmax = 1e3, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + geom_text(data = text, aes(y = y, label = label), color = &quot;white&quot;, size = 5) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;) If it was really important that the labels in the x-axes were different, like they are in Kruschke’s Figure 20.5, you could always make the three plots separately and then bind them together with patchwork syntax. Though he didn’t show the results, on page 598 Kruschke mentioned a few other contrasts we might consider. The example entailed comparing the differences within BFIN to the average of the other three. Let’s walk it out. # define our new data nd &lt;- crossing(Pos = c(&quot;Assis&quot;, &quot;Full&quot;), Org = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;ENG&quot;, &quot;PSY&quot;)) # we need to update our `brief_col_names` brief_col_names &lt;- crossing(Pos = c(&quot;Assis&quot;, &quot;Full&quot;), Org = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;ENG&quot;, &quot;PSY&quot;)) %&gt;% unite(key, Pos, Org) %&gt;% pull() # get the draws with `fitted()` f2 &lt;- fitted(fit20.1, newdata = nd, summary = F) %&gt;% # wrangle as_tibble() %&gt;% set_names(brief_col_names) %&gt;% mutate(`Full - Assis @ BFIN` = Full_BFIN - Assis_BFIN, `Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM, `Full - Assis @ ENG` = Full_ENG - Assis_ENG, `Full - Assis @ PSY` = Full_PSY - Assis_PSY) %&gt;% mutate(`Full.v.Assis (x) BFIN.v.the rest` = `Full - Assis @ BFIN` - (`Full - Assis @ CHEM` + `Full - Assis @ ENG` + `Full - Assis @ PSY`) / 3) # what have we done? glimpse(f2) ## Rows: 8,000 ## Columns: 13 ## $ Assis_BFIN &lt;dbl&gt; 204013.3, 194167.9, 190908.6, 204986.8, 206590.5, 1990… ## $ Assis_CHEM &lt;dbl&gt; 85196.23, 90695.72, 89715.38, 84612.13, 94628.19, 8433… ## $ Assis_ENG &lt;dbl&gt; 61319.08, 66098.68, 61094.59, 70835.14, 65593.89, 6069… ## $ Assis_PSY &lt;dbl&gt; 87625.17, 84074.65, 80157.10, 90458.46, 82272.07, 8621… ## $ Full_BFIN &lt;dbl&gt; 234638.3, 243410.2, 239923.8, 227318.5, 248931.7, 2354… ## $ Full_CHEM &lt;dbl&gt; 156697.5, 156325.8, 156637.2, 164782.3, 161350.2, 1479… ## $ Full_ENG &lt;dbl&gt; 110973.1, 106921.4, 107845.9, 105359.4, 104676.9, 1198… ## $ Full_PSY &lt;dbl&gt; 113804.9, 118037.4, 118549.9, 115114.0, 112559.1, 1182… ## $ `Full - Assis @ BFIN` &lt;dbl&gt; 30625.00, 49242.23, 49015.15, 22331.71, 42341.18, 3635… ## $ `Full - Assis @ CHEM` &lt;dbl&gt; 71501.25, 65630.09, 66921.85, 80170.22, 66722.06, 6361… ## $ `Full - Assis @ ENG` &lt;dbl&gt; 49654.02, 40822.74, 46751.28, 34524.27, 39083.05, 5912… ## $ `Full - Assis @ PSY` &lt;dbl&gt; 26179.68, 33962.77, 38392.82, 24655.50, 30287.04, 3202… ## $ `Full.v.Assis (x) BFIN.v.the rest` &lt;dbl&gt; -18486.6456, 2437.0316, -1673.4980, -24118.2823, -3022… Now plot. f2 %&gt;% select(-contains(&quot;_&quot;)) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = 0)) + geom_rect(xmin = -1e3, xmax = 1e3, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 3) So while the overall pay averages for those in BFIN were larger than those in the other three departments, the differences between full and associate professors within BFIN wasn’t substantially different from the differences within the other three departments. To be sure, the interquartile range of that last difference distribution fell below both zero and the ROPE, but there’s still a lot of spread in the rest of the distribution. 20.2.4.1 Interaction effects: High uncertainty and shrinkage. “It is important to realize that the estimates of interaction contrasts are typically much more uncertain than the estimates of simple effects or main effects” (p. 598). If we start with our fitted() object f1, we can wrangle a bit, compute the HDIs with tidybayes::mode_hdi() and then use simple subtraction to compute the interval range for each difference. f1 %&gt;% select(-contains(&quot;_&quot;)) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;Full - Assis @ PSY&quot;, &quot;Full - Assis @ CHEM&quot;, &quot;Full.v.Assis (x) CHEM.v.PSY&quot;))) %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% select(key:.upper) %&gt;% mutate(`interval range` = .upper - .lower) ## # A tibble: 3 x 5 ## key value .lower .upper `interval range` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Full - Assis @ PSY 32732. 17575. 45755. 28181. ## 2 Full - Assis @ CHEM 68192. 55265. 83841. 28577. ## 3 Full.v.Assis (x) CHEM.v.PSY 37203. 17582. 57824. 40242. Just like Kruschke pointed out in the text, the interval for the interaction estimate was quite larger than the intervals for the simple contrasts. This large uncertainty of an interaction contrast is caused by the fact that it involves at least four sources of uncertainty (i.e., at least four groups of data), unlike its component simple effects which each involve only half of those sources of uncertainty. In general, interaction contrasts require a lot of data to estimate accurately. (p. 598) Gelman has blogged on this, a bit (e.g., You need 16 times the sample size to estimate an interaction than to estimate a main effect). There is also shrinkage. The interaction contrasts also can experience notable shrinkage from the hierarchical model. In the present application, for example, there are 300 interaction deflections (5 levels of seniority times 60 departments) that are assumed to come from a higher- level distribution that has an estimated standard deviation, denoted \\(\\sigma_{\\beta 1 \\times 2}\\) in Figure 20.2. Chances are that most of the 300 interaction deflections will be small, and therefore the estimated standard deviation of the interaction deflections will be small, and therefore the estimated deflections themselves will be shrunken toward zero. This shrinkage is inherently neither good nor bad; it is simply the correct consequence of the model assumptions. The shrinkage can be good insofar as it mitigates false alarms about interactions, but the shrinkage can be bad if it inappropriately obscures meaningful interactions. (p. 598) Here’s that \\(\\sigma_{\\beta 1 \\times 2}\\). post %&gt;% ggplot(aes(x = `sd_Pos:Org__Intercept`, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) 20.3 Rescaling can change interactions, homogeneity, and normality When interpreting interactions, it can be important to consider the scale on which the data are measured. This is because an interaction means non-additive effects when measured on the current scale. If the data are nonlinearly transformed to a different scale, then the non-additivity can also change. (p. 599) Here is Kruschke’s initial example of a possible interaction effect of sex and political party with respect to wages. d &lt;- tibble(monetary_units = c(10, 12, 15, 18), politics = rep(c(&quot;democrat&quot;, &quot;republican&quot;), each = 2), sex = rep(c(&quot;women&quot;, &quot;men&quot;), times = 2)) %&gt;% mutate(sex_number = if_else(sex == &quot;women&quot;, 1, 2), politics = factor(politics, levels = c(&quot;republican&quot;, &quot;democrat&quot;))) d %&gt;% ggplot(aes(x = sex_number, y = monetary_units, color = politics)) + geom_line(size = 2) + scale_color_manual(NULL, values = c(&quot;red2&quot;, &quot;blue2&quot;)) + scale_x_continuous(&quot;sex&quot;, breaks = 1:2, labels = c(&quot;women&quot;, &quot;men&quot;)) + coord_cartesian(ylim = c(0, 20)) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;), legend.position = c(.2, .15), panel.grid = element_blank()) Because the pay discrepancy between men and women is not equal between Democrats and Republicans, in this example, it can be tempting to claim there is a subtle interaction. Not necessarily so. tibble(politics = c(&quot;democrat&quot;, &quot;republican&quot;), female_salary = c(10, 15)) %&gt;% mutate(male_salary = 1.2 * female_salary) ## # A tibble: 2 x 3 ## politics female_salary male_salary ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 democrat 10 12 ## 2 republican 15 18 If we take female salary as the baseline and then add 20% to it for the men, the salary difference between Republican men and women will be larger than that between Democratic men and women. Even though the rate increase from women to men was the same, the increase in absolute value was greater within Republicans because Republican women made more than Democratic women. Look what happens to our original plot when we transform monetary_units with log10(). d %&gt;% ggplot(aes(x = sex_number, y = log10(monetary_units), color = politics)) + geom_line(size = 2) + scale_color_manual(NULL, values = c(&quot;red2&quot;, &quot;blue2&quot;)) + scale_x_continuous(&quot;sex&quot;, breaks = 1:2, labels = c(&quot;women&quot;, &quot;men&quot;)) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;), legend.position = c(.2, .4), panel.grid = element_blank()) “Equal ratios are transformed to equal distances by a logarithmic transformation” (p. 599). 20.4 Heterogeneous variances and robustness against outliers Before we fit the robust hierarchical variances model, we need to define our stanvars. stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) + stanvar(1/29, name = &quot;one_over_twentynine&quot;) Recall that to fit a robust hierarchical variances model, we need to wrap our two formulas within the bf() function. Warning: this one took several hours fit. fit20.2 &lt;- brm(data = my_data, family = student, bf(Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org), sigma ~ 1 + (1 | Pos:Org)), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(normal(log(sd_y), 1), class = Intercept, dpar = sigma), prior(gamma(alpha, beta), class = sd), prior(normal(0, 1), class = sd, dpar = sigma), prior(exponential(one_over_twentynine), class = nu)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 20, control = list(adapt_delta = 0.9995, max_treedepth = 15), stanvars = stanvars, file = &quot;fits/fit20.02&quot;) Behold the summary. print(fit20.2) ## Family: student ## Links: mu = identity; sigma = log; nu = identity ## Formula: Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org) ## sigma ~ 1 + (1 | Pos:Org) ## Data: my_data (Number of observations: 1080) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~Org (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 31447.99 3029.60 26167.92 38068.59 1.00 2752 5125 ## ## ~Pos (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 50657.30 23555.77 23446.66 110351.14 1.00 4707 6450 ## ## ~Pos:Org (Number of levels: 216) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 5303.89 862.72 3741.71 7154.75 1.00 2618 4524 ## sd(sigma_Intercept) 0.97 0.07 0.84 1.12 1.00 3422 5842 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 122225.83 24590.96 72456.12 172261.44 1.00 3407 5453 ## sigma_Intercept 9.11 0.09 8.93 9.29 1.00 3125 5806 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## nu 6.96 4.60 3.71 15.51 1.00 5352 6040 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This time we’ll just feed the results of the wrangling code right into the plotting code for our version of the top panels of Figure 20.8. # how many draws would you like? n_draw &lt;- 20 # wrangle my_data %&gt;% distinct(Pos) %&gt;% expand(Pos, Org = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;)) %&gt;% add_fitted_draws(fit20.2, n = n_draw, seed = 20, allow_new_levels = T, dpar = c(&quot;mu&quot;, &quot;sigma&quot;, &quot;nu&quot;)) %&gt;% mutate(ll = qt(.025, df = nu), ul = qt(.975, df = nu)) %&gt;% mutate(Salary = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest() %&gt;% mutate(density = dt(Salary, nu)) %&gt;% # notice the conversion mutate(Salary = mu + Salary * sigma) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) %&gt;% mutate(Org = factor(Org, levels = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;))) %&gt;% # plot ggplot(aes(x = Salary, y = Pos)) + geom_vline(xintercept = fixef(fit20.2)[1, 1], color = &quot;white&quot;) + geom_ridgeline(aes(height = density, group = interaction(Pos, .draw), color = Pos), fill = NA, show.legend = F, size = 1/4, scale = 3/4) + geom_jitter(data = my_data %&gt;% filter(Org %in% c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;)) %&gt;% mutate(Org = factor(Org, levels = c(&quot;BFIN&quot;, &quot;CHEM&quot;, &quot;PSY&quot;, &quot;ENG&quot;))), height = .025, alpha = 1/4) + scale_color_viridis_d(option = &quot;B&quot;, end = .9) + scale_x_continuous(breaks = seq(from = 0, to = 300000, length.out = 4), labels = c(&quot;$0&quot;, &quot;$100K&quot;, &quot;200K&quot;, &quot;$300K&quot;)) + coord_cartesian(xlim = c(0, 35e4), ylim = c(1.25, 5.5)) + labs(title = &quot;Data with Posterior Predictive Distributions&quot;, subtitle = &quot;The white vertical line is the model-implied grand mean.&quot;, y = &quot;Pos&quot;) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) + facet_wrap(~Org, ncol = 2) Our results for the bottom panel of Figure 20.8 will differ substantially from Kruschke’s. Recall that Kurschke modeled \\(\\sigma_{[j, k](i)}\\) with a hierarchical gamma distribution and using the \\(\\omega + \\sigma\\) parameterization. We, however, modeled our hierarchical \\(\\log (\\sigma)\\) with the typical normal distribution. As such, we have posteriors for \\(\\sigma_\\mu\\) and \\(\\sigma_\\sigma\\). # wrangle posterior_samples(fit20.2) %&gt;% transmute(Normality = log10(nu), `Mean of Cell Sigma&#39;s` = exp(b_sigma_Intercept), `SD of Cell Sigma&#39;s` = exp(`sd_Pos:Org__sigma_Intercept`)) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;Normality&quot;, &quot;Mean of Cell Sigma&#39;s&quot;, &quot;SD of Cell Sigma&#39;s&quot;))) %&gt;% # plot ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;param. value&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 3) Though our \\(\\sigma_\\mu\\) is on a similar metric to Kruschke’s \\(\\sigma_\\omega\\), our \\(\\sigma_\\sigma\\) is just fundamentally different from his. So it goes. If you think I’m in error, here, share your insights. Even though our hierarchical \\(\\sigma\\) parameters look different from Kruschke’s, it turns the contrast distributions are quite similar. Here’s the necessary wrangling to make our version for Figure 20.9. # define our new data nd &lt;- crossing(Pos = c(&quot;Assis&quot;, &quot;Full&quot;), Org = c(&quot;CHEM&quot;, &quot;PSY&quot;)) # we need to update our col_names brief_col_names &lt;- crossing(Pos = c(&quot;Assis&quot;, &quot;Full&quot;), Org = c(&quot;CHEM&quot;, &quot;PSY&quot;)) %&gt;% unite(key, Pos, Org) %&gt;% pull() # get the draws with `fitted()` f &lt;- fitted(fit20.2, newdata = nd, summary = F) %&gt;% # wrangle as_tibble() %&gt;% set_names(brief_col_names) %&gt;% transmute(`Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM, `Full - Assis @ PSY` = Full_PSY - Assis_PSY) %&gt;% mutate(`Full.v.Assis (x) CHEM.v.PSY` = `Full - Assis @ CHEM` - `Full - Assis @ PSY`) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;Full - Assis @ PSY&quot;, &quot;Full - Assis @ CHEM&quot;, &quot;Full.v.Assis (x) CHEM.v.PSY&quot;))) # what have we done? head(f) ## # A tibble: 6 x 2 ## key value ## &lt;fct&gt; &lt;dbl&gt; ## 1 Full - Assis @ CHEM 48941. ## 2 Full - Assis @ CHEM 48521. ## 3 Full - Assis @ CHEM 50871. ## 4 Full - Assis @ CHEM 49690. ## 5 Full - Assis @ CHEM 50048. ## 6 Full - Assis @ CHEM 42843. Now plot. f %&gt;% ggplot(aes(x = value, y = 0)) + geom_rect(xmin = -1e3, xmax = 1e3, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 3) See? Our contrast distributions are really close those in the text. Here are the numeric estimates. f %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% select(key:.upper) %&gt;% mutate_if(is.double, round, digits = 0) ## # A tibble: 3 x 4 ## key value .lower .upper ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Full - Assis @ PSY 33451 22559 43609 ## 2 Full - Assis @ CHEM 53226 40070 67693 ## 3 Full.v.Assis (x) CHEM.v.PSY 18579 3367 38073 In the second half of the middle paragraph on page 605, Kruschke contrasted the \\(\\sigma_{\\beta 1 \\times 2}\\) parameter in the two models (i.e., our fit20.1 and fit20.2). Recall that in the brms output, these are termed sd_Pos:Org__Intercept. Here are the comparisons from our brms models. posterior_summary(fit20.1)[&quot;sd_Pos:Org__Intercept&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 9697.154 1203.427 7393.966 12147.087 posterior_summary(fit20.2)[&quot;sd_Pos:Org__Intercept&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 5303.8945 862.7234 3741.7096 7154.7545 They are similar to the values in the text. And recall, of course, the brms::posterior_summary() function returns posterior means. If you really wanted the posterior modes, like Kruschke reported in the text, you’ll have to work a little harder. bind_rows(posterior_samples(fit20.1) %&gt;% select(`sd_Pos:Org__Intercept`), posterior_samples(fit20.2) %&gt;% select(`sd_Pos:Org__Intercept`)) %&gt;% mutate(fit = rep(c(&quot;fit20.1&quot;, &quot;fit20.2&quot;), each = n() / 2)) %&gt;% group_by(fit) %&gt;% mode_hdi(`sd_Pos:Org__Intercept`) %&gt;% select(fit:.upper) %&gt;% mutate_if(is.double, round, digits = 0) ## # A tibble: 3 x 4 ## fit `sd_Pos:Org__Intercept` .lower .upper ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 fit20.1 9664 4137 6622 ## 2 fit20.1 9664 6927 12114 ## 3 fit20.2 5100 3599 7003 The curious might even look at those in a plot. bind_rows(posterior_samples(fit20.1) %&gt;% select(`sd_Pos:Org__Intercept`), posterior_samples(fit20.2) %&gt;% select(`sd_Pos:Org__Intercept`)) %&gt;% mutate(fit = rep(c(&quot;fit20.1&quot;, &quot;fit20.2&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = `sd_Pos:Org__Intercept`, y = fit)) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.5, .95)) + labs(x = expression(sigma[beta][1%*%2]), y = NULL) + theme(panel.grid = element_blank()) “Which model is a better description of the data?… In principle, an intrepid programmer could do a Bayesian model comparison…” (pp. 605–606). We could also examine iformation criteria, like the LOO. fit20.1 &lt;- add_criterion(fit20.1, &quot;loo&quot;) ## Warning: Found 6 observations with a pareto_k &gt; 0.7 in model &#39;fit20.1&#39;. It is recommended to set ## &#39;reloo = TRUE&#39; in order to calculate the ELPD without the assumption that these observations are ## negligible. This will refit the model 6 times to compute the ELPDs for the problematic observations ## directly. ## Automatically saving the model object in &#39;fits/fit20.01.rds&#39; fit20.2 &lt;- add_criterion(fit20.2, &quot;loo&quot;) ## Warning: Found 60 observations with a pareto_k &gt; 0.7 in model &#39;fit20.2&#39;. With this many problematic ## observations, it may be more appropriate to use &#39;kfold&#39; with argument &#39;K = 10&#39; to perform 10-fold ## cross-validation rather than LOO. ## Automatically saving the model object in &#39;fits/fit20.02.rds&#39; Sigh. Both models had high pareto_k values, suggesting there were outliers relative to what was expected by their likelihoods. Just a little further in the text, Kruschke gives us hints why this might be so: Moreover, both models assume that the data within cells are distributed symmetrically above and below their central tendency, either as a normal distribution or a \\(t\\)-distribution. The data instead seem to be skewed toward larger values, especially for advanced seniorities. (p. 606) Here’s the current LOO difference. loo_compare(fit20.1, fit20.2) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit20.2 0.0 0.0 -11766.9 41.5 314.8 10.9 23533.7 83.1 ## fit20.1 -426.8 34.1 -12193.7 42.8 139.5 11.7 24387.3 85.6 For a more robust model comparison, we might follow the kfold() suggestion in the warning. However, be warned: The kfold() function entails refitting the model K times. fit20.1 took about an hour to fit and fit20.2 took several more. Proceed with caution. But really, “we might want to create a model that describes the data within each cell as a skewed distribution such as a Weibull” (p. 606). Yes, brms can handle Weibull regression (e.g., here). 20.5 Within-subject designs When every subject contributes many measurements to every cell, then the model of the situation is a straight-forward extension of the models we have already considered. We merely add “subject” as another nominal predictor in the model, with each individual subject being a level of the predictor. If there is one predictor other than subject, the model becomes \\[ y = \\beta_0 + \\overrightarrow \\beta_1 \\overrightarrow x_1 + \\overrightarrow \\beta_S \\overrightarrow x_S + \\overrightarrow \\beta_{1 \\times S} \\overrightarrow x_{1 \\times S} \\] This is exactly the two-predictor model we have already considered, with the second predictor being subject. When there are two predictors other than subject, the model becomes \\[\\begin{align*} y = &amp; \\; \\beta_0 &amp; \\text{baseline} \\\\ &amp; + \\overrightarrow \\beta_1 \\overrightarrow x_1 + \\overrightarrow \\beta_2 \\overrightarrow x_2 + \\overrightarrow \\beta_S \\overrightarrow x_S &amp; \\text{main effects} \\\\ &amp; + \\overrightarrow \\beta_{1 \\times 2} \\overrightarrow x_{1 \\times 2} + \\overrightarrow \\beta_{1 \\times S} \\overrightarrow x_{1 \\times S} + \\overrightarrow \\beta_{2 \\times S} \\overrightarrow x_{2 \\times S} &amp; \\text{two-way interactions} \\\\ &amp; + \\overrightarrow \\beta_{1 \\times 2 \\times S} \\overrightarrow x_{1 \\times 2 \\times S} &amp; \\text{three-way interactions} \\end{align*}\\] This model includes all the two-way interactions of the factors, plus the three-way interaction. (p. 607) In situations in which subjects only contribute one observation per condition/cell, we simplify the model to \\[\\begin{align*} y = &amp; \\; \\beta_0 \\\\ &amp; + \\overrightarrow \\beta_1 \\overrightarrow x_1 + \\overrightarrow \\beta_2 \\overrightarrow x_2 + \\overrightarrow \\beta_{1 \\times 2} \\overrightarrow x_{1 \\times 2} \\\\ &amp; + \\overrightarrow \\beta_S \\overrightarrow x_S \\end{align*}\\] “In other words, we assume a main effect of subject, but no interaction of subject with other predictors. In this model, the subject effect (deflection) is constant across treatments, and the treatment effects (deflections) are constant across subjects” (p. 608). 20.5.1 Why use a within-subject design? And why not? Kruschke opined “the primary reason to use a within-subject design is that you can achieve greater precision in the estimates of the effects than in a between-subject design” (p. 608). Well, to that I counterpoint: “No one goes to the circus to see the average dog jump through the hoop significantly oftener than untrained does raised under the same circumstances” (Skinner, 1956, p. 228). And it’s unlikely you’ll make a skillful jumper of your dog without repeated trials. There’s also the related issue that between- and within-person processes aren’t necessarily the same. For an introduction to the issue, see Hamaker’s (2012) chapter, Why researchers should think “within-person”: A paradigmatic rationale, or the paper from Bolger et al. (2019), Causal processes in psychology are heterogeneous. But we digress. Here’s the 4-subject response time data. ( d &lt;- tibble(response_time = c(300, 320, 350, 370, 400, 420, 450, 470), subject = rep(1:4, each = 2), hand = rep(c(&quot;dominant&quot;, &quot;nondominant&quot;), times = 4)) ) ## # A tibble: 8 x 3 ## response_time subject hand ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 300 1 dominant ## 2 320 1 nondominant ## 3 350 2 dominant ## 4 370 2 nondominant ## 5 400 3 dominant ## 6 420 3 nondominant ## 7 450 4 dominant ## 8 470 4 nondominant “For every subject, the difference between dominant and nondominant hands is exactly 20 ms, but there are big differences across subjects in overall response times” (p. 608). Here’s what that looks like. d %&gt;% mutate(subject = factor(subject)) %&gt;% ggplot(aes(x = response_time, y = subject)) + geom_line(aes(group = subject), linetype = 3) + geom_point(aes(color = hand), size = 2) + scale_color_viridis_d(option = &quot;A&quot;, begin = 1/3, end = 2/3) + theme(panel.grid = element_blank()) Here there is more variability between subjects than within them, which you’d never detect without a within-subject design including multiple subjects. 20.5.2 Split-plot design. “Split-plot experiments were invented by Fisher (1925) (p. 610).” Kruschke then wrote this to set the stage for the next subsection: Consider an agricultural experiment investigating the productivity of different soil tilling methods and different fertilizers. It is relatively easy to provide all the farmers with the several different fertilizers. But it might be relatively difficult to provide all farmers with all the machinery for several different tilling methods. Therefore, any particular farmer will use a single (randomly assigned) tilling method on his whole plot, and tilling methods will differ between whole plots. Each farmer will split his field into subplots and apply all the fertilizers to different (randomly assigned) split plots, and fertilizers will differ across split plots within whole plots. This type of experiment inspires the name, split-plot design. The generic experiment-design term for the farmer’s field is “block.” Then, the factor that varies within every field is called the within-block factor and the factor that varies between fields is called the between-block factor. Notice also that each split plot yields a single measurement (in this case the productivity measured in bushels per acre), not multiple measurements. (p. 610) 20.5.2.1 Example: Knee high by the fourth of July. Load the agronomy data. my_data &lt;- read_csv(&quot;data.R/SplitPlotAgriData.csv&quot;) glimpse(my_data) ## Rows: 99 ## Columns: 4 ## $ Field &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10… ## $ Till &lt;chr&gt; &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Chisel&quot;, &quot;Ch… ## $ Fert &lt;chr&gt; &quot;Broad&quot;, &quot;Deep&quot;, &quot;Surface&quot;, &quot;Broad&quot;, &quot;Deep&quot;, &quot;Surface&quot;, &quot;Broad&quot;, &quot;Deep&quot;, &quot;Surface&quot;,… ## $ Yield &lt;dbl&gt; 119, 130, 123, 135, 148, 134, 140, 146, 142, 126, 132, 131, 128, 141, 153, 117, 130… We might use geom_tile() to visualize the data like this. my_data %&gt;% mutate(Fert = str_c(&quot;Fert: &quot;, Fert)) %&gt;% ggplot(aes(x = Till, y = Field)) + geom_hline(yintercept = c(12.5, 22.5), color = &quot;grey85&quot;) + geom_tile(aes(fill = Yield)) + scale_fill_viridis_c(option = &quot;D&quot;, begin = 0) + scale_x_discrete(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + theme(panel.grid = element_blank()) + facet_wrap(~Fert) As Kruschke pointed out in the text, notice how each Field has only one level of Till, but three levels of Fert. 20.5.2.2 The descriptive model. In the classical ANOVA-style model for a split-plot design, the overall variance is conceptually decomposed into five components: the main effect of the between-subjects factor, the main effect of the within-subjects factor, the interaction of the two factors, the effect of subject within levels of the between-subject factor, and the interaction of subject with the within-subject factor. Unfortunately, because there is only a single datum per cell, the five components exactly match the data, which is to say that there are as many parameters as there are data points. (If every subject contributed multiple data points to every cell then the five-component model could be used.) Because there is no residual noise within cells, the classical approach is to treat the final component as noise, that is, treat the interaction of subject with the within-subject factor as noise. That component is not included in the model (at least, not distinct from noise). We will do the same for the descriptive model in our Bayesian analysis. (p. 612) 20.5.2.3 Implementation in JAGS brms. Define the stanvars. mean_y &lt;- mean(my_data$Yield) sd_y &lt;- sd(my_data$Yield) omega &lt;- sd_y / 2 sigma &lt;- 2 * sd_y s_r &lt;- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Here’s how to fit the model with brm(). fit20.3 &lt;- brm(data = my_data, family = gaussian, Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Field) + (1 | Till:Fert), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 20, control = list(adapt_delta = 0.99, max_treedepth = 12), stanvars = stanvars, file = &quot;fits/fit20.03&quot;) 20.5.2.4 Results. Check the summary. print(fit20.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Field) + (1 | Till:Fert) ## Data: my_data (Number of observations: 99) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~Fert (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 15.36 13.52 1.35 51.01 1.00 2495 2714 ## ## ~Field (Number of levels: 33) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 11.74 1.72 8.87 15.60 1.00 1639 3004 ## ## ~Till (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 15.48 13.84 1.25 53.53 1.00 2087 2781 ## ## ~Till:Fert (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 8.79 4.13 3.69 19.67 1.00 1901 3365 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 140.44 15.87 107.70 174.18 1.00 2632 3057 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.69 0.53 4.75 6.83 1.00 3583 5148 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We might compare the \\(\\sigma\\) posteriors with a plot. posterior_samples(fit20.3) %&gt;% pivot_longer(c(sigma, starts_with(&quot;sd&quot;))) %&gt;% ggplot(aes(x = value, y = name)) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.5, .95), scale = .85, normalize = &quot;xy&quot;) + labs(x = NULL, y = NULL) + coord_cartesian(xlim = c(0, 50), ylim = c(1.25, 5.5)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) Now we have the results of fit20.3, we are ready to make our version if Figure 20.10. Note that how within add_fitted_draws(), we used the re_formula argument to average over the random effects of Field (i.e., we left (1 | Field) out of the formula). That’s our equivalent to when Kruschke wrote “The predictive normal distributions are plotted with means at \\(\\beta_0 + \\beta_B + \\beta_W + \\beta_{B \\times W}\\) (collapsed across \\(\\beta_S\\)) and with standard deviation \\(\\sigma\\)” (pp. 614–615). # wrangle my_data %&gt;% distinct(Till, Fert) %&gt;% add_fitted_draws(fit20.3, n = 20, re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert), dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Yield = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(Yield) %&gt;% mutate(density = dnorm(Yield, mu, sigma)) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) %&gt;% # plot ggplot(aes(x = Yield, y = Fert)) + geom_path(data = my_data, aes(group = Field %&gt;% as.factor()), size = 1/4) + geom_ridgeline(aes(height = -density, group = interaction(Fert, .draw), color = Fert), fill = NA, size = 1/3, scale = 3/4, min_height = NA) + geom_jitter(data = my_data, height = .025, alpha = 1/4) + scale_color_viridis_d(option = &quot;A&quot;, begin = .2, end = .8) + scale_x_continuous(breaks = seq(from = 100, to = 180, by = 20)) + coord_flip(xlim = c(90, 190), ylim = c(0.5, 2.75)) + ggtitle(&quot;Data with Posterior Predictive Distribution&quot;) + theme(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~Till) Now let’s make our Figure 20.11 contrasts. col_names &lt;- my_data %&gt;% distinct(Till, Fert) %&gt;% unite(key, Till, Fert) %&gt;% pull() nd &lt;- my_data %&gt;% distinct(Till, Fert) fitted(fit20.3, newdata = nd, summary = F, re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert)) %&gt;% as_tibble() %&gt;% set_names(col_names) %&gt;% transmute( `Moldbrd vs Ridge` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 3) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3), `Moldbrd.Ridge vs Chisel` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface + Ridge_Broad + Ridge_Deep + Ridge_Surface) / 6) - ((Chisel_Broad + Chisel_Deep + Chisel_Surface) / 3), `Deep.Surface vs Broad` = ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6) - ((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3), `Chisel.Moldbrd.v.Ridge (x) Broad.v.Deep.Surface` = (((Chisel_Broad + Chisel_Deep + Chisel_Surface + Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 6) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3)) - (((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3) - ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6)) ) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;Moldbrd vs Ridge&quot;, &quot;Moldbrd.Ridge vs Chisel&quot;, &quot;Deep.Surface vs Broad&quot;, &quot;Chisel.Moldbrd.v.Ridge (x) Broad.v.Deep.Surface&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + geom_rect(xmin = -5, xmax = 5, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + theme(panel.grid = element_blank(), strip.text = element_text(size = 6)) + facet_wrap(~name, scales = &quot;free&quot;, ncol = 4) As far as I can tell, it appears that our contrasts indicate our variance parameter for Till ended up larger than Kruschke’s. Kruschke then posed a model “with field/subject coding suppressed, hence no lines connecting data from the same field/subject” (p. 616). Here’s how to fit that model. fit20.4 &lt;- brm(data = my_data, family = gaussian, Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert), prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept), prior(gamma(alpha, beta), class = sd), prior(cauchy(0, sd_y), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 20, control = list(adapt_delta = 0.999, max_treedepth = 12), stanvars = stanvars, file = &quot;fits/fit20.04&quot;) Behold the summary. print(fit20.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert) ## Data: my_data (Number of observations: 99) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~Fert (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 14.89 12.84 1.35 49.45 1.00 2960 3323 ## ## ~Till (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 16.27 13.75 1.77 52.94 1.00 2855 2968 ## ## ~Till:Fert (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 7.66 4.56 1.28 18.44 1.00 2064 2814 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 140.64 16.03 108.18 174.17 1.00 3718 3685 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 12.71 0.96 11.01 14.76 1.00 8073 5309 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Look at how much larger the posterior is for \\(\\sigma_y\\) in this model compared to fit20.3. posterior_summary(fit20.3)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 5.6934670 0.5314513 4.7521209 6.8286296 posterior_summary(fit20.4)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 12.7086864 0.9606523 11.0091522 14.7612638 Here’s the top portion of Figure 20.12. # wrangle my_data %&gt;% distinct(Till, Fert) %&gt;% add_fitted_draws(fit20.4, n = 20, re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert), dpar = c(&quot;mu&quot;, &quot;sigma&quot;)) %&gt;% mutate(ll = qnorm(.025, mean = mu, sd = sigma), ul = qnorm(.975, mean = mu, sd = sigma)) %&gt;% mutate(Yield = map2(ll, ul, seq, length.out = 200)) %&gt;% unnest(Yield) %&gt;% mutate(density = dnorm(Yield, mu, sigma)) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) %&gt;% # plot ggplot(aes(x = Yield, y = Fert)) + geom_ridgeline(aes(height = -density, group = interaction(Fert, .draw), color = Fert), fill = NA, size = 1/3, scale = 3/4, min_height = NA) + geom_jitter(data = my_data, height = .025, alpha = 1/4) + scale_color_viridis_d(option = &quot;A&quot;, begin = .2, end = .8) + scale_x_continuous(breaks = seq(from = 100, to = 180, by = 20)) + coord_flip(xlim = c(90, 190), ylim = c(0.5, 2.75)) + ggtitle(&quot;Data with Posterior Predictive Distribution&quot;) + theme(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~Till) Now the make the plots for the contrast distributions. fitted(fit20.4, newdata = nd, summary = F, re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert)) %&gt;% as_tibble() %&gt;% set_names(col_names) %&gt;% transmute( `Moldbrd vs Ridge` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 3) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3), `Moldbrd.Ridge vs Chisel` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface + Ridge_Broad + Ridge_Deep + Ridge_Surface) / 6) - ((Chisel_Broad + Chisel_Deep + Chisel_Surface) / 3), `Deep.Surface vs Broad` = ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6) - ((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3), `Chisel.Moldbrd.v.Ridge (x) Broad.v.Deep.Surface` = (((Chisel_Broad + Chisel_Deep + Chisel_Surface + Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 6) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3)) - (((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3) - ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6)) ) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;Moldbrd vs Ridge&quot;, &quot;Moldbrd.Ridge vs Chisel&quot;, &quot;Deep.Surface vs Broad&quot;, &quot;Chisel.Moldbrd.v.Ridge (x) Broad.v.Deep.Surface&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + geom_rect(xmin = -5, xmax = 5, ymin = -Inf, ymax = Inf, color = &quot;transparent&quot;, fill = &quot;white&quot;) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference&quot;) + theme(panel.grid = element_blank(), strip.text = element_text(size = 6)) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 4) Though not identical, these were closer to those Kruschke displayed in the text. 20.5.2.5 Model comparison approach. Like we covered in Chapter 10, I’m not aware that Stan/brms will allow for \\(\\delta\\) factor-inclusion parameters the way JAGS allows. However, if you’d like to compare models with different parameters, you can always use information criteria. fit20.3 &lt;- add_criterion(fit20.3, &quot;loo&quot;) fit20.4 &lt;- add_criterion(fit20.4, &quot;loo&quot;) Executing that yielded the following warning message: Found 4 observations with a pareto_k &gt; 0.7 in model ‘fit20.3’. It is recommended to set ‘reloo = TRUE’ in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model 4 times to compute the ELPDs for the problematic observations directly. Let’s follow the diagnostic warnings to make sure the model comparison is robust to the outlying values. fit20.3 &lt;- add_criterion(fit20.3, &quot;loo&quot;, reloo = T) We’re finally ready for the LOO comparison. loo_compare(fit20.3, fit20.4) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit20.3 0.0 0.0 -335.3 8.2 34.8 4.5 670.5 16.4 ## fit20.4 -61.1 7.9 -396.4 5.7 8.7 1.0 792.7 11.4 All good. Based on the LOO values, we should prefer the fuller model. This, of course, should be no surprise. The posterior for \\(\\sigma_\\text{Field}\\) was a far cry from zero. posterior_summary(fit20.3)[&quot;sd_Field__Intercept&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 11.741891 1.723704 8.868938 15.604229 Kruschke ended this chapter by mentioning Bayes’ factors: Bayes’ factor approaches to hypothesis tests in ANOVA were presented by Rouder et al. (2012) and Wetzels, Grasman, and Wagenmakers (2012). Morey and Rouder’s BayesFactor package for R is available at the Web site http://bayesfactorpcl.r-forge.r-project.org/. (p. 618) If you wanna go that route, you’re on your own. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggridges_0.5.2 tidybayes_2.0.3.9000 brms_2.12.0 Rcpp_1.0.4.6 ## [5] patchwork_1.0.0 forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 ## [9] purrr_0.3.4 readr_1.3.1 tidyr_1.0.2 tibble_3.0.1 ## [13] ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 rsconnect_0.8.16 markdown_1.1 ## [5] base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 farver_2.0.3 ## [9] rstan_2.19.3 svUnit_1.0.3 DT_0.13 fansi_0.4.1 ## [13] mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 bridgesampling_1.0-0 ## [17] knitr_1.28 shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 ## [21] broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 compiler_3.6.3 ## [25] httr_1.4.1 backports_1.1.6 assertthat_0.2.1 Matrix_1.2-18 ## [29] fastmap_1.0.1 cli_2.0.2 later_1.0.0 htmltools_0.4.0 ## [33] prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 cellranger_1.1.0 ## [41] vctrs_0.3.0 nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ## [45] ps_1.3.3 rvest_0.3.5 mime_0.9 miniUI_0.1.1.1 ## [49] lifecycle_0.2.0 gtools_3.8.2 zoo_1.8-7 scales_1.1.1 ## [53] colourpicker_1.0 hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 ## [57] parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [61] gridExtra_2.3 StanHeaders_2.21.0-1 loo_2.2.0 stringi_1.4.6 ## [65] dygraphs_1.1.1.6 pkgbuild_1.0.8 rlang_0.4.6 pkgconfig_2.0.3 ## [69] matrixStats_0.56.0 HDInterval_0.2.0 evaluate_0.14 lattice_0.20-38 ## [73] labeling_0.3 rstantools_2.0.0 htmlwidgets_1.5.1 tidyselect_1.0.0 ## [77] processx_3.4.2 plyr_1.8.6 magrittr_1.5 bookdown_0.18 ## [81] R6_2.4.1 generics_0.0.2 DBI_1.1.0 pillar_1.4.4 ## [85] haven_2.2.0 withr_2.2.0 xts_0.12-0 abind_1.4-5 ## [89] modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 ## [93] rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 callr_3.4.3 ## [97] threejs_0.3.3 reprex_0.3.0 digest_0.6.25 xtable_1.8-4 ## [101] httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 viridisLite_0.3.0 ## [105] shinyjs_1.1 Footnote References "],
["dichotomous-predicted-variable.html", "21 Dichotomous Predicted Variable 21.1 Multiple metric predictors 21.2 Interpreting the regression coefficients 21.3 Robust logistic regression 21.4 Nominal predictors Session info", " 21 Dichotomous Predicted Variable This chapter considers data structures that consist of a dichotomous predicted variable. The early chapters of the book were focused on this type of data, but now we reframe the analyses in terms of the generalized linear model… The traditional treatment of these sorts of data structure is called “logistic regression.” In Bayesian software it is easy to generalize the traditional models so they are robust to outliers, allow different variances within levels of a nominal predictor, and have hierarchical structure to share information across levels or factors as appropriate. (Kruschke, 2015, pp. 621–622) 21.1 Multiple metric predictors “We begin by considering a situation with multiple metric predictors, because this case makes it easiest to visualize the concepts of logistic regression” (p. 623). Figure 21.1 is beyond the scope of our current ggplot2 paradigm. But we will discuss an alternative in the end of section 21.1.2. 21.1.1 The model and implementation in JAGS brms. Our statistical model will follow the form \\[\\begin{align*} \\mu &amp; = \\operatorname{logistic} (\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2) \\\\ y &amp; \\sim \\operatorname{Bernoulli} (\\mu) \\end{align*}\\] where \\[\\operatorname{logistic} (x) = \\frac{1}{(1 + \\exp (-x))}.\\] The generic brms code for logistic regression using the Bernoulli likelihood looks like so. fit &lt;- brm(data = my_data, family = bernoulli, y ~ 1 + x1 + x2, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b))) Note that this syntax presumes the predictor variables have already been standardized. We’d be remiss not to point out that you can also specify the model using the binomial distribution. That code would look like this. fit &lt;- brm(data = my_data, family = binomial, y | trials(1) ~ 1 + x1 + x2, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b))) As long as the data are not aggregated, the results of these two should be the same within simulation variance. In brms, the default link for both family = bernoulli and family = binomial models is logit, which is exactly what we want, here. Also, note the additional | trials(1) syntax on the left side of the model formula. You could get away with omitting this in older versions of brms. But newer versions prompt users to specify how many of trials each row in the data represents. This is because, as with the baseball data we’ll use later in the chapter, the binomial distribution includes an \\(n\\) parameter. When working with un-aggregated data like what we’re about to do, below, it’s presumed that \\(n = 1\\). 21.1.2 Example: Height, weight, and gender. Load the height/weight data. library(tidyverse) my_data &lt;- read_csv(&quot;data.R/HtWtData110.csv&quot;) glimpse(my_data) ## Rows: 110 ## Columns: 3 ## $ male &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1… ## $ height &lt;dbl&gt; 63.2, 68.7, 64.8, 67.9, 68.9, 67.8, 68.2, 64.8, 64.3, 64.7, 66.9, 66.9, 67.1, 70.2… ## $ weight &lt;dbl&gt; 168.7, 169.8, 176.6, 246.8, 151.6, 158.0, 168.6, 137.2, 177.0, 128.0, 168.4, 136.2… Let’s standardize our predictors. my_data &lt;- my_data %&gt;% mutate(height_z = (height - mean(height)) / sd(height), weight_z = (weight - mean(weight)) / sd(weight)) Before we fit a model, we might take a quick look at the data. The ggMarginal() function from the ggExtra package (Attali &amp; Baker, 2019) will be of help, here. # install.packages(&quot;ggExtra&quot;, dependencies = T) library(ggExtra) Here we make a scatter plot with marginal densities. p &lt;- my_data %&gt;% ggplot(aes(x = weight, y = height, fill = male == 1)) + geom_point(aes(color = male == 1), alpha = 2/3) + scale_color_manual(values = c(&quot;red4&quot;, &quot;blue4&quot;)) + scale_fill_manual(values = c(&quot;red4&quot;, &quot;blue4&quot;)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) p %&gt;% ggMarginal(data = my_data, groupFill = T, type = &#39;density&#39;, color = &quot;transparent&quot;) Looks like the data for which male == 1 are concentrated in the upper right and those for which male == 0 are more so in the lower left. What we’d like is a model that would tell us the optimal dividing line(s) between our male categories with respect to those predictor variables. Open brms. library(brms) Our first logistic model with family = bernoulli uses only weight_z as a predictor. fit21.1 &lt;- brm(data = my_data, family = bernoulli, male ~ 1 + weight_z, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.01&quot;) Here’s the model summary. print(fit21.1) ## Family: bernoulli ## Links: mu = logit ## Formula: male ~ 1 + weight_z ## Data: my_data (Number of observations: 110) ## Samples: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.08 0.21 -0.50 0.32 1.00 6358 5436 ## weight_z 1.18 0.28 0.66 1.74 1.00 5678 4925 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now let’s get ready to make our version of Figure 21.3. First, we need to do some fitted()-oriented wrangling. length &lt;- 200 n_iter &lt;- 20 nd &lt;- tibble(weight_z = seq(from = -2, to = 3.5, length.out = length)) f &lt;- fitted(fit21.1, newdata = nd, summary = F, nsamples = n_iter) %&gt;% as_tibble() %&gt;% mutate(iter = 1:n_iter) %&gt;% pivot_longer(-iter) %&gt;% mutate(weight_z = rep(nd$weight_z, times = n_iter)) %&gt;% mutate(weight = weight_z * sd(my_data$weight) + mean(my_data$weight)) head(f) ## # A tibble: 6 x 5 ## iter name value weight_z weight ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 V1 0.0912 -2 87.0 ## 2 1 V2 0.0939 -1.97 88.0 ## 3 1 V3 0.0967 -1.94 88.9 ## 4 1 V4 0.0996 -1.92 89.9 ## 5 1 V5 0.103 -1.89 90.9 ## 6 1 V6 0.106 -1.86 91.9 Here’s a tricky way to get the threshold values. thresholds &lt;- f %&gt;% filter(value &lt; .5) %&gt;% group_by(iter) %&gt;% filter(value == max(value)) Using those thresholds, here’s our version of the top panel of Figure 21.3. f %&gt;% ggplot(aes(x = weight)) + geom_hline(yintercept = .5, color = &quot;white&quot;, size = 1/2) + geom_vline(xintercept = thresholds$weight, color = &quot;white&quot;, size = 2/5) + geom_line(aes(y = value, group = iter), color = &quot;grey50&quot;, size = 1/3, alpha = 2/3) + geom_point(data = my_data, aes(y = male), alpha = 1/3) + labs(title = &quot;Data with Post. Pred.&quot;, y = &quot;male&quot;) + coord_cartesian(xlim = range(my_data$weight)) + theme(panel.grid = element_blank()) We should discuss those thresholds (i.e., the verticla lines) a bit. Kruschke: The spread of the logistic curves indicates the uncertainty of the estimate; the steepness of the logistic curves indicates the magnitude of the regression coefficient. The 50% probability threshold is marked by arrows that drop down from the logistic curve to the \\(x\\)-axis, near a weight of approximately 160 pounds. The threshold is the \\(x\\) value at which \\(\\mu = 0.5\\), which is \\(x = -\\beta_0 / \\beta_1\\). (p. 626) Now here we show the marginal distributions in our versions of the lower panels of Figure 21.3. library(tidybayes) # extract the draws post &lt;- posterior_samples(fit21.1) %&gt;% # convert the parameter draws to their natural metric following Equation 21.1 transmute(Intercept = b_Intercept - (b_weight_z * mean(my_data$weight) / sd(my_data$weight)), weight = b_weight_z / sd(my_data$weight)) %&gt;% pivot_longer(everything()) # plot post %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 50, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, ncol = 2) And here are those exact posterior mode and 95% HDI values. post %&gt;% group_by(name) %&gt;% mode_hdi() %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 2 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Intercept -5.17 -7.78 -2.96 0.95 mode hdi ## 2 weight 0.033 0.018 0.048 0.95 mode hdi Now fit the two-predictor model using both weight_z and height_z fit21.2 &lt;- brm(data = my_data, family = bernoulli, male ~ 1 + weight_z + height_z, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.02&quot;) Here’s the model summary. print(fit21.2) ## Family: bernoulli ## Links: mu = logit ## Formula: male ~ 1 + weight_z + height_z ## Data: my_data (Number of observations: 110) ## Samples: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.35 0.30 -0.97 0.21 1.00 5701 4430 ## weight_z 0.67 0.35 -0.01 1.38 1.00 6570 5203 ## height_z 2.62 0.51 1.71 3.71 1.00 5721 4092 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Before we make our plots for Figure 21.4, we’ll need to extract the posterior samples and transform a little. post &lt;- posterior_samples(fit21.2) %&gt;% transmute(b_weight = b_weight_z / sd(my_data$weight), b_height = b_height_z / sd(my_data$height), Intercept = b_Intercept - ((b_weight_z * mean(my_data$weight) / sd(my_data$weight)) + (b_height_z * mean(my_data$height) / sd(my_data$height)))) head(post) ## b_weight b_height Intercept ## 1 0.018166700 0.7321485 -51.60400 ## 2 0.019467097 0.4831885 -35.37444 ## 3 0.011662826 0.9272223 -64.27622 ## 4 0.007602754 1.0011093 -68.24517 ## 5 0.003033513 0.8470679 -57.08786 ## 6 0.029017547 0.5890523 -44.33986 Here’s our version of Figure 21.4.a. set.seed(21) # we need this for the `sample_n()` function post %&gt;% mutate(iter = 1:n()) %&gt;% sample_n(size = 20) %&gt;% expand(nesting(iter, Intercept, b_weight, b_height), weight = c(80, 280)) %&gt;% # this follows the Equation near the top of p. 629 mutate(height = (-Intercept / b_height) + (-b_weight / b_height) * weight) %&gt;% # now plot ggplot(aes(x = weight, y = height)) + geom_line(aes(group = iter), color = &quot;white&quot;, size = 2/5) + geom_text(data = my_data, aes(label = male, color = male == 1)) + scale_color_manual(values = c(&quot;red4&quot;, &quot;blue4&quot;)) + ggtitle(&quot;Data with Post. Pred.&quot;) + coord_cartesian(xlim = range(my_data$weight), ylim = range(my_data$height)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) With just a tiny bit more wrangling, we’ll be ready to make the bottom panels of Figure 21.4. post %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(str_remove(name, &quot;b_&quot;), levels = c(&quot;Intercept&quot;, &quot;weight&quot;, &quot;height&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, ncol = 3) By now, you know how to use mode_hdi() to return those exact summary values if you’d like them. Now remember how we backed away from Figure 21.1? Well, when you have a logistic regression with two predictors, there is a reasonable way to express those three dimensions on a two-dimensional grid. Now we have the results from fit21.2, let’s try it out. First, we need a grid of values for our two predictors, weight_z and height_z. length &lt;- 100 nd &lt;- crossing(weight_z = seq(from = -3.5, to = 3.5, length.out = length), height_z = seq(from = -3.5, to = 3.5, length.out = length)) Second, we plug those values into fitted() and wrangle. f &lt;- fitted(fit21.2, newdata = nd, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% # note we&#39;re only working with the posterior mean, here transmute(prob = Estimate %&gt;% inv_logit_scaled()) %&gt;% bind_cols(nd) %&gt;% mutate(weight = (weight_z * sd(my_data$weight) + mean(my_data$weight)), height = (height_z * sd(my_data$height) + mean(my_data$height))) glimpse(f) ## Rows: 10,000 ## Columns: 5 ## $ prob &lt;dbl&gt; 7.096100e-06, 8.540527e-06, 1.027897e-05, 1.237127e-05, 1.488945e-05, 1.792021e-… ## $ weight_z &lt;dbl&gt; -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3.5, -3… ## $ height_z &lt;dbl&gt; -3.500000, -3.429293, -3.358586, -3.287879, -3.217172, -3.146465, -3.075758, -3.… ## $ weight &lt;dbl&gt; 33.45626, 33.45626, 33.45626, 33.45626, 33.45626, 33.45626, 33.45626, 33.45626, … ## $ height &lt;dbl&gt; 53.24736, 53.51234, 53.77731, 54.04229, 54.30727, 54.57224, 54.83722, 55.10219, … Third, we’re ready to plot. Here we’ll express the third dimension, probability, on a color spectrum. f %&gt;% ggplot(aes(x = weight, y = height)) + geom_raster(aes(fill = prob), interpolate = T) + geom_text(data = my_data, aes(label = male, color = male == 1), show.legend = F) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;)) + scale_fill_viridis_c(option = &quot;A&quot;, begin = .1, limits = c(0, 1)) + scale_y_continuous(position = &quot;right&quot;) + coord_cartesian(xlim = range(my_data$weight), ylim = range(my_data$height)) + theme(legend.position = &quot;left&quot;) If you look way back to Figure 21.1 (p. 623), you’ll see the following formula at the top: \\[y \\sim \\operatorname{dbern} (m), m = \\operatorname{logistic} (0.018 x_1 + 0.7 x_2 - 50).\\] Now while you keep your finger on that equation, take another look at the last line in Kruschke’s Equation 21.1, \\[ \\operatorname{logit} (\\mu) = \\underbrace{\\zeta_0 - \\sum_j \\frac{\\zeta_j}{s_{x_j}} \\overline x_j}_{\\beta_0} + \\sum_j \\underbrace{\\frac{\\zeta_j}{s_{x_j}} \\overline x_j}_{\\beta_j}, \\] where the \\(\\zeta\\)s are the parameters from the model based on standardized predictors. Our fit21.2 was based on standardized weight and height values (i.e., weight_z and height_z), yielding model coefficients in the \\(\\zeta\\) metric. Here we use the formula above to convert our fit21.2 estimates to their unstandardized \\(\\beta\\) metric. For simplicity, we’ll just take their means. posterior_samples(fit21.2) %&gt;% transmute(beta_0 = b_Intercept - ((b_weight_z * mean(my_data$weight) / sd(my_data$weight)) + ((b_height_z * mean(my_data$height) / sd(my_data$height)))), beta_1 = b_weight_z / sd(my_data$weight), beta_2 = b_height_z / sd(my_data$height)) %&gt;% summarise_all(~mean(.) %&gt;% round(., digits = 3)) ## beta_0 beta_1 beta_2 ## 1 -49.713 0.019 0.699 Within rounding error, those values are the same ones in the formula at the top of Kruschke’s Figure 21.1! That is, our last plot was a version of Figure 21.1. Hopefully this helps make sense of what the thresholds in Figure 21.4.a represented. But do note a major limitation of this visualization approach. By expressing the threshold with multiple lines drawn from the posterior in Figure 21.4.a, we expressed the uncertainty inherent in the posterior distribution. However, for this probability plane approach, we’ve taken a single value from the posterior, the mean (i.e., the Estimate), to compute the probabilities. Though beautiful, our probability-plane plot does a poor job expressing the uncertainty in the model. If you’re curious how one might include uncertainty into a plot like this, check out the intriguing blog post by Adam Pearce, Communicating model uncertainty over space. 21.2 Interpreting the regression coefficients In this section, I’ll discuss how to interpret the parameters in logistic regression. The first subsection explains how to interpret the numerical magnitude of the slope coefficients in terms of “log odds.” The next subsection shows how data with relatively few 1’s or 0’s can yield ambiguity in the parameter estimates. Then an example with strongly correlated predictors reveals tradeoffs in slope coefficients. Finally, I briefly describe the meaning of multiplicative interaction for logistic regression. (p. 629) 21.2.1 Log odds. When the logistic regression formula is written using the logit function, we have \\(\\operatorname{logit} (\\mu) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). The formula implies that whenever \\(x_1\\) goes up by 1 unit (on the \\(x_1\\) scale), then \\(\\operatorname{logit} (\\mu)\\) goes up by an amount \\(\\beta_1\\). And whenever \\(x_2\\) goes up by 1 unit (on the \\(x_2\\) scale), then \\(\\operatorname{logit} (\\mu)\\) goes up by an amount \\(\\beta_2\\). Thus, the regression coefficients are telling us about increases in \\(\\operatorname{logit} (\\mu)\\). To understand the regression coefficients, we need to understand \\(\\operatorname{logit} (\\mu)\\). (pp. 629–630) Given the logit function is the inverse of the logistic, which itself is \\[\\operatorname{logistic} (x) = \\frac{1}{1 + \\exp (−x)},\\] and given the formula \\[\\operatorname{logit} (\\mu) = \\log \\bigg (\\frac{\\mu}{1 - \\mu} \\bigg), \\] where \\[0 &lt; \\mu &lt; 1, \\] it may or may not be clear that the results of our logistic regression models have a nonlinear relation with the actual parameter of interest, \\(\\mu\\), which, recall, is the probability our criterion variable is 1 (e.g., male == 1). To get a sense of that nonlinear relation, we might make a plot. tibble(mu = seq(from = 0, to = 1, length.out = 200)) %&gt;% mutate(logit_mu = log(mu / (1 - mu))) %&gt;% ggplot(aes(x = mu, y = logit_mu)) + geom_line() + labs(x = expression(mu~&quot;(i.e., the probability space)&quot;), y = expression(logit~mu~&quot;(i.e., the parameter space)&quot;)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) So whereas our probability space is bound between 0 and 1, the parameter space shoots off into negative and positive infinity. Also, \\[\\operatorname{logit} (\\mu) = \\log \\Bigg (\\frac{p(y = 1)}{p(y = 0)} \\Bigg ).\\] Thus, “the ratio, \\(p(y = 1) / p(y = 0)\\), is called the odds of outcome 1 to outcome 0, and therefore \\(\\operatorname{logit} (\\mu)\\) is the log odds of outcome 1 to outcome 0” (p. 630). Here’s a table layout of the height/weight examples in the middle of page 630. tibble(b0 = -50, b1 = .02, b2 = .7, weight = 160, inches = c(63:64, 67:68)) %&gt;% mutate(logit_mu = b0 + b1 * weight + b2 * inches) %&gt;% mutate(log_odds = logit_mu) %&gt;% mutate(p_male = 1 / (1 + exp(-log_odds))) %&gt;% knitr::kable() b0 b1 b2 weight inches logit_mu log_odds p_male -50 0.02 0.7 160 63 -2.7 -2.7 0.0629734 -50 0.02 0.7 160 64 -2.0 -2.0 0.1192029 -50 0.02 0.7 160 67 0.1 0.1 0.5249792 -50 0.02 0.7 160 68 0.8 0.8 0.6899745 Thus, a regression coefficient in logistic regression indicates how much a 1 unit change of the predictor increases the log odds of outcome 1. A regression coefficient of 0.5 corresponds to a rate of probability change of about 12.5 percentage points per \\(x\\)-unit at the threshold \\(x\\) value. A regression coefficient of 1.0 corresponds to a rate of probability change of about 24.4 percentage points per \\(x\\)-unit at the threshold \\(x\\) value. When \\(x\\) is much larger or smaller than the threshold \\(x\\) value, the rate of change in probability is smaller, even though the rate of change in log odds is constant. (pp. 630–631) 21.2.2 When there are few 1’s or 0’s in the data. In logistic regression, you can think of the parameters as describing the boundary between the 0’s and the 1’s. If there are many 0’s and 1’s, then the estimate of the boundary parameters can be fairly accurate. But if there are few 0’s or few 1’s, the boundary can be difficult to identify very accurately, even if there are many data points overall. (p. 631) As far as I can tell, Kruschke must have used \\(n = 500\\) to simulate the data he displayed in Figure 21.5. Using the coefficient values he displayed in the middle of page 631, here’s an attempt at replicating them. b0 &lt;- -3 b1 &lt;- 1 n &lt;- 500 set.seed(21) d_rare &lt;- tibble(x = rnorm(n, mean = 0, sd = 1)) %&gt;% mutate(mu = b0 + b1 * x) %&gt;% mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu)))) glimpse(d_rare) ## Rows: 500 ## Columns: 3 ## $ x &lt;dbl&gt; 0.793013171, 0.522251264, 1.746222241, -1.271336123, 2.197389533, 0.433130777, -1.5701… ## $ mu &lt;dbl&gt; -2.2069868, -2.4777487, -1.2537778, -4.2713361, -0.8026105, -2.5668692, -4.5701996, -3… ## $ y &lt;int&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,… We’re ready to fit the model. So far, we’ve been following along with Kruschke by using the Bernoulli distribution (i.e., family = bernoulli) in our brms models. Let’s get frisky and use the \\(n = 1\\) binomial distribution, here. You’ll see it yields the same results. fit21.3 &lt;- brm(data = d_rare, family = binomial, y | trials(1) ~ 1 + x, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.03&quot;) Recall that when you use the binomial distribution in newer versions of brms, you need to use the trials() syntax to tell brm() how many trials each row in the data corresponds to. Anyway, behold the summary. print(fit21.3) ## Family: binomial ## Links: mu = logit ## Formula: y | trials(1) ~ 1 + x ## Data: d_rare (Number of observations: 500) ## Samples: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -3.01 0.24 -3.50 -2.56 1.00 2895 3993 ## x 1.03 0.20 0.63 1.43 1.00 3047 3790 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Looks like the model did a good job recapturing those data-generating b0 and b1 values. Here’s the preparatory work to needed before we can make our version of the main plot in Figure 21.5.a. # unclear if Kruschke still used 20 draws or not # perhaps play with `n_iter` values. n_iter &lt;- 20 length &lt;- 100 nd &lt;- tibble(x = seq(from = -3.5, to = 3.5, length.out = length)) # these will make the logistic curves set.seed(21) f &lt;- fitted(fit21.3, newdata = nd, summary = F, nsamples = n_iter) %&gt;% as_tibble() %&gt;% pivot_longer(everything(), values_to = &quot;y&quot;) %&gt;% mutate(x = rep(nd$x, times = n_iter), iter = rep(1:n_iter, each = length)) # Here&#39;s just a little more work to get the thresholds thresholds &lt;- f %&gt;% filter(y &lt; .5) %&gt;% group_by(iter) %&gt;% filter(y == max(y)) We’re ready to plot the top left panel of Figure 21.5. f %&gt;% ggplot(aes(x = x, y = y)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_vline(xintercept = thresholds$x, color = &quot;white&quot;, size = 2/5) + geom_line(aes(group = iter), color = &quot;grey50&quot;, alpha = 2/3, size = 1/3) + geom_point(data = d_rare, alpha = 1/5) + scale_x_continuous(breaks = -3:3) + ggtitle(&quot;Data with Post. Pred.&quot;) + coord_cartesian(xlim = c(-3, 3)) + theme(panel.grid = element_blank()) Here are the two subplots at the bottom, left. posterior_samples(fit21.3) %&gt;% transmute(Intercept = b_Intercept, x = b_x) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, ncol = 2) Since our data were simulated without the benefit of knowing how Kruschke set his seed and such, our results will only approximate those in the text. Okay, now we need to simulate the complimentary data, those for which \\(y = 1\\) is a less-rare event. b0 &lt;- 0 b1 &lt;- 1 n &lt;- 500 set.seed(21) d_not_rare &lt;- tibble(x = rnorm(n, mean = 0, sd = 1)) %&gt;% mutate(mu = b0 + b1 * x) %&gt;% mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu)))) glimpse(d_not_rare) ## Rows: 500 ## Columns: 3 ## $ x &lt;dbl&gt; 0.793013171, 0.522251264, 1.746222241, -1.271336123, 2.197389533, 0.433130777, -1.5701… ## $ mu &lt;dbl&gt; 0.793013171, 0.522251264, 1.746222241, -1.271336123, 2.197389533, 0.433130777, -1.5701… ## $ y &lt;int&gt; 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,… Fitting this model is just like before. fit21.4 &lt;- update(fit21.3, newdata = d_not_rare, iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.04&quot;) Behold the summary. print(fit21.4) ## Family: binomial ## Links: mu = logit ## Formula: y | trials(1) ~ 1 + x ## Data: d_not_rare (Number of observations: 500) ## Samples: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.08 0.10 -0.11 0.27 1.00 6945 5664 ## x 0.91 0.11 0.69 1.13 1.00 6688 5234 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the code for the main plot in Figure 21.5.b. nd &lt;- tibble(x = seq(from = -3.5, to = 3.5, length.out = length)) set.seed(21) f &lt;- fitted(fit21.4, newdata = nd, summary = F, nsamples = n_iter) %&gt;% as_tibble() %&gt;% pivot_longer(everything(), values_to = &quot;y&quot;) %&gt;% mutate(x = rep(nd$x, times = n_iter), iter = rep(1:n_iter, each = length)) thresholds &lt;- f %&gt;% filter(y &lt; .5) %&gt;% group_by(iter) %&gt;% filter(y == max(y)) p1 &lt;- f %&gt;% ggplot(aes(x = x, y = y)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_vline(xintercept = thresholds$x, color = &quot;white&quot;, size = 2/5) + geom_line(aes(group = iter), color = &quot;grey50&quot;, alpha = 2/3, size = 1/3) + geom_point(data = d_not_rare, aes(y = y), alpha = 1/5) + ggtitle(&quot;Data with Post. Pred.&quot;) + coord_cartesian(xlim = c(-3, 3)) + theme(panel.grid = element_blank()) Now make the two subplots at the bottom. p2 &lt;- posterior_samples(fit21.4) %&gt;% transmute(Intercept = b_Intercept, x = b_x) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, ncol = 2) This time we’ll combine them with patchwork. library(patchwork) p3 &lt;- plot_spacer() p1 / (p2 + p3 + plot_layout(widths = c(2, 1))) + plot_layout(height = c(4, 1)) You can see in Figure 21.5 that the estimate of the slope (and of the intercept) is more certain in the right panel than in the left panel. The 95% HDI on the slope, \\(\\beta_1\\), is much wider in the left panel than in the right panel, and you can see that the logistic curves in the left panel have greater variation in steepness than the logistic curves in the right panel. The analogous statements hold true for the intercept parameter. Thus, if you are doing an experimental study and you can manipulate the \\(x\\) values, you will want to select \\(x\\) values that yield about equal numbers of 0’s and 1’s for the \\(y\\) values overall. If you are doing an observational study, such that you cannot control any independent variables, then you should be aware that the parameter estimates may be surprisingly ambiguous if your data have only a small proportion of 0’s or 1’s. (pp. 631–632) 21.2.3 Correlated predictors. “Another important cause of parameter uncertainty is correlated predictors. This issue was previously discussed at length, but the context of logistic regression provides novel illustration in terms of level contours” (p. 632). As far as I can tell, Kruschke chose about \\(n = 200\\) for the data in this example. After messing around with correlations for a bit, it seems \\(\\rho_{x_1, x_2} = .975\\) looks about right. To my knowledge, the best way to simulate multivariate Gaussian data with a particular correlation is with the MASS::mvrnorm() function. Since we’ll be using standardized \\(x\\)-variables, we’ll need to specify our \\(n\\), the desired correlation matrix, and a vector of means. Then we’ll be ready to do the actual simulation with mvrnorm(). n &lt;- 200 # correlation matrix s &lt;- matrix(c(1, .975, .975, 1), nrow = 2, ncol = 2) # mean vector m &lt;- c(0, 0) # simulate set.seed(21) d &lt;- MASS::mvrnorm(n = n, mu = m, Sigma = s) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;x&quot;, 1:2)) Let’s confirm the correlation coefficient. cor(d) ## x1 x2 ## x1 1.0000000 0.9730091 ## x2 0.9730091 1.0000000 Solid. Now we’ll use the \\(\\beta\\) values from page 633 to simulate the data set by including our dichotomous criterion variable, y. b0 &lt;- 0 b1 &lt;- 1 b2 &lt;- 1 set.seed(21) d &lt;- d %&gt;% mutate(mu = b0 + b1 * x1 + b2 * x2) %&gt;% mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu)))) Fit the model with the highly-correlated predictors. fit21.5 &lt;- brm(data = d, family = binomial, y | trials(1) ~ 1 + x1 + x2, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.05&quot;) Behold the summary. print(fit21.5) ## Family: binomial ## Links: mu = logit ## Formula: y | trials(1) ~ 1 + x1 + x2 ## Data: d (Number of observations: 200) ## Samples: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.04 0.18 -0.39 0.31 1.00 4473 3670 ## x1 -0.05 0.72 -1.45 1.35 1.00 3422 3386 ## x2 2.07 0.78 0.61 3.64 1.00 3461 3472 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We did a good job recapturing Kruschke’s \\(\\beta\\)s in terms of our posterior means, but notice how large those posterior \\(SD\\)s are for \\(\\beta_1\\) and \\(\\beta_2\\). To get a better sense, let’s look at them in a coefficient plot before continuing on with the text. posterior_samples(fit21.5) %&gt;% pivot_longer(-lp__) %&gt;% ggplot(aes(x = value, y = name)) + stat_gradientintervalh(point_interval = mode_hdi, .width = c(.5, .95), fill = &quot;grey25&quot;) + labs(x = NULL, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) Them are some sloppy estimates! But we digress. Here’s our version of Figure 21.6.a. set.seed(21) # we need this for the `sample_n()` function posterior_samples(fit21.5) %&gt;% mutate(iter = 1:n()) %&gt;% sample_n(size = 20) %&gt;% expand(nesting(iter, b_Intercept, b_x1, b_x2), x1 = c(-4, 4)) %&gt;% # this follows the equation near the top of p. 629 mutate(x2 = (-b_Intercept / b_x2) + (-b_x1 / b_x2) * x1) %&gt;% # now plot ggplot(aes(x = x1, y = x2)) + geom_line(aes(group = iter), color = &quot;white&quot;, size = 2/5) + geom_text(data = d, aes(label = y, color = y == 1), size = 2.5) + scale_color_manual(values = c(&quot;red4&quot;, &quot;blue4&quot;)) + ggtitle(&quot;Data with Post. Pred.&quot;) + coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) It can be easy to under-appreciate how sensitive this plot is to the seed you set for sample_n(). To give a better sense of the uncertainty in the posterior for the threshold, here we show the plot for several different seeds. # make a custom function different_seed &lt;- function(i) { set.seed(i) posterior_samples(fit21.5) %&gt;% mutate(iter = 1:n()) %&gt;% sample_n(size = 20) %&gt;% expand(nesting(iter, b_Intercept, b_x1, b_x2), x1 = c(-4, 4)) %&gt;% mutate(x2 = (-b_Intercept / b_x2) + (-b_x1 / b_x2) * x1) } # specify your seeds tibble(seed = 1:9) %&gt;% # pump those seeds into the `different_seed()` function mutate(sim = map(seed, different_seed)) %&gt;% unnest(sim) %&gt;% mutate(seed = str_c(&quot;seed: &quot;, seed)) %&gt;% # plot ggplot(aes(x = x1, y = x2)) + geom_line(aes(group = iter), color = &quot;white&quot;, size = 1/3) + geom_text(data = d, aes(label = y, color = y == 1), size = 1.5) + scale_color_manual(values = c(&quot;red4&quot;, &quot;blue4&quot;)) + ggtitle(&quot;Data with Post. Pred.&quot;) + coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~seed) We’ll use pairs() to make our version of Figure 21.6.b. pairs(fit21.5, off_diag_args = list(size = 1/5, alpha = 1/5)) Finally, here are the Pearson’s correlations among the model parameters. vcov(fit21.5, correlation = T) %&gt;% round(digits = 3) ## Intercept x1 x2 ## Intercept 1.000 0.010 -0.028 ## x1 0.010 1.000 -0.928 ## x2 -0.028 -0.928 1.000 21.2.4 Interaction of metric predictors. “There may be applications in which it is meaningful to consider a multiplicative interaction of metric predictors” (p. 633). Kruschke didn’t walk through an analysis in this section, but it’s worth the practice. Let’s simulate data based on the formula he gave in Figure 21.7, top right. n &lt;- 500 b0 &lt;- 0 b1 &lt;- 0 b2 &lt;- 0 b3 &lt;- 4 set.seed(21) d &lt;- tibble(x1 = rnorm(n, mean = 0, sd = 1), x2 = rnorm(n, mean = 0, sd = 1)) %&gt;% mutate(mu = b1 * x1 + b2 * x2 + b3 * x1 * x2 - b0) %&gt;% mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu)))) To fit the interaction model, let’s go back to the Bernoulli likelihood. fit21.6 &lt;- brm(data = d, family = bernoulli, y ~ 1 + x1 + x2 + x1:x2, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 2), class = b)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.06&quot;) Looks like the model did a nice job recapturing the data-generating parameters. print(fit21.6) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + x1 + x2 + x1:x2 ## Data: d (Number of observations: 500) ## Samples: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.18 0.12 -0.42 0.06 1.00 9164 6145 ## x1 -0.21 0.17 -0.53 0.11 1.00 9497 6131 ## x2 -0.02 0.16 -0.35 0.30 1.00 8567 5382 ## x1:x2 4.27 0.44 3.44 5.15 1.00 7391 5543 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I’m not quite sure how to expand Kruschke’s equation from the top of page 629 to our interaction model. But no worries. We can take a slightly different approach to show the consequences of our interaction model on the probability \\(y = 1\\). First, we define our newdata and then get the Estimates from fitted(). Then we wrangle as usual. length &lt;- 100 nd &lt;- crossing(x1 = seq(from = -3.5, to = 3.5, length.out = length), x2 = seq(from = -3.5, to = 3.5, length.out = length)) f &lt;- fitted(fit21.6, newdata = nd, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% transmute(prob = Estimate %&gt;% inv_logit_scaled()) Now all we have to do is integrate our f results with the nd and original d data and then we can plot. f %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = x1, y = x2)) + geom_raster(aes(fill = prob), interpolate = T) + geom_text(data = d, aes(label = y, color = y == 1), size = 2.75, show.legend = F) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;)) + scale_fill_viridis_c(expression(italic(p)(italic(y)==1)), option = &quot;A&quot;, begin = .15, limits = c(0, 1)) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + coord_cartesian(xlim = range(nd$x1), ylim = range(nd$x2)) Instead of a simple threshold line, we get to visualize our interaction as a checkerboard-like probability plane. 21.3 Robust logistic regression With the robust logistic regression approach, we will describe the data as being a mixture of two different sources. One source is the logistic function of the predictor(s). The other source is sheer randomness or “guessing,” whereby the \\(y\\) value comes from the flip of a fair coin: \\(y \\sim \\operatorname{Bernoulli} (\\mu = 1/2)\\). We suppose that every data point has a small chance, \\(\\alpha\\), of being generated by the guessing process, but usually, with probability \\(1 - \\alpha\\), the \\(y\\) value comes from the logistic function of the predictor. With the two sources combined, the predicted probability that \\(y = 1\\) is \\[\\mu = \\alpha \\cdot \\frac{1}{2} + (1 - \\alpha) \\cdot \\operatorname{logistic} \\bigg ( \\beta_0 + \\sum_j \\beta_j x_j \\bigg )\\] Notice that when the guessing coefficient is zero, then the conventional logistic model is completely recovered. When the guessing coefficient is one, then the y values are completely random. (p. 635) Here’s what Kruschke’s beta(1, 9) prior for \\(\\alpha\\) looks like. tibble(x = seq(from = 0, to = 1, length.out = 200)) %&gt;% ggplot(aes(x = x)) + # check out this `ymax` trick? geom_ribbon(aes(ymin = 0, ymax = dbeta(x, 1, 9)), size = 0, fill = &quot;grey50&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(&quot;beta&quot;*(1*&quot;, &quot;*9)), x = NULL) + theme(panel.grid = element_blank()) To fit the brms analogue to Kruschke’s rogust logistic regression model, we’ll need to adopt what Bürkner calls the non-linear syntax, which you can learn about in detail with his (2020e) vignette, Estimating non-linear models with brms. fit21.7 &lt;- brm(data = my_data, family = bernoulli(link = identity), bf(male ~ a * .5 + (1 - a) * 1 / (1 + exp(-1 * (b0 + b1 * weight_z))), a + b0 + b1 ~ 1, nl = TRUE), prior = c(prior(normal(0, 2), nlpar = &quot;b0&quot;), prior(normal(0, 2), nlpar = &quot;b1&quot;), prior(beta(1, 9), nlpar = &quot;a&quot;)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.07&quot;) And just for clarity, you can do the same thing with family = binomial(link = identity), too. Just don’t forget to specify the number of trials with trials(). But to explain what’s going on with our syntax, above, I think it’s best to quote Bürkner at length: When looking at the above code, the first thing that becomes obvious is that we changed the formula syntax to display the non-linear formula including predictors (i.e., [weight_z]) and parameters (i.e., [a, b0, and b1]) wrapped in a call to [the bf() function]. This stands in contrast to classical R formulas, where only predictors are given and parameters are implicit. The argument [a + b0 + b1 ~ 1] serves two purposes. First, it provides information, which variables in formula are parameters, and second, it specifies the linear predictor terms for each parameter. In fact, we should think of non-linear parameters as placeholders for linear predictor terms rather than as parameters themselves (see also the following examples). In the present case, we have no further variables to predict [a, b0, and b1] and thus we just fit intercepts that represent our estimates of [\\(\\alpha\\), \\(\\beta_0\\), and \\(\\beta_1\\)]. The formula [a + b0 + b1 ~ 1] is a short form of [a ~ 1, b0 ~ 1, b1 ~ 1] that can be used if multiple non-linear parameters share the same formula. Setting nl = TRUE tells brms that the formula should be treated as non-linear. In contrast to generalized linear models, priors on population-level parameters (i.e., ‘fixed effects’) are often mandatory to identify a non-linear model. Thus, brms requires the user to explicitely specify these priors. In the present example, we used a [beta(1, 9) prior on (the population-level intercept of) a, while we used a normal(0, 4) prior on both (population-level intercepts of) b0 and b1]. Setting priors is a non-trivial task in all kinds of models, especially in non-linear models, so you should always invest some time to think of appropriate priors. Quite often, you may be forced to change your priors after fitting a non-linear model for the first time, when you observe different MCMC chains converging to different posterior regions. This is a clear sign of an idenfication problem and one solution is to set stronger (i.e., more narrow) priors. (emphasis in the original) Now, behold the summary. print(fit21.7) ## Warning: There were 632 divergent transitions after warmup. Increasing adapt_delta above 0.8 may ## help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: bernoulli ## Links: mu = identity ## Formula: male ~ a * 0.5 + (1 - a) * 1/(1 + exp(-1 * (b0 + b1 * weight_z))) ## a ~ 1 ## b0 ~ 1 ## b1 ~ 1 ## Data: my_data (Number of observations: 110) ## Samples: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_Intercept 0.20 0.09 0.04 0.39 1.00 2541 2843 ## b0_Intercept 0.34 0.45 -0.41 1.39 1.00 3505 3195 ## b1_Intercept 2.64 0.87 1.14 4.53 1.00 2539 3349 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It turns out Bürkner’s warning on “different posterior regions” applied to our case. While preparing this document, I played around with higher adapt_delta values, as suggested in the warning message. The model showed problems even at adapt_delta = 0.999. Here, our main purpose is to ape Kruschke. But if this was a substantive model of interest, I’d suggest following Bürkner’s advice and think hard about specifying narrower priors. Anyway, here’s a quick and dirty look at the conditional effects for weight_z. conditional_effects(fit21.7) %&gt;% plot(points = T) The way we prep for our version of Figure 21.8 is just the same as what we did for Figure 21.3, above. length &lt;- 200 nd &lt;- tibble(weight_z = seq(from = -2, to = 3.5, length.out = length)) f &lt;- fitted(fit21.7, newdata = nd, summary = F, nsamples = n_iter) %&gt;% as_tibble() %&gt;% mutate(iter = 1:n_iter) %&gt;% pivot_longer(-iter) %&gt;% mutate(weight_z = rep(nd$weight_z, times = n_iter)) %&gt;% mutate(weight = weight_z * sd(my_data$weight) + mean(my_data$weight)) thresholds &lt;- f %&gt;% filter(value &lt; .5) %&gt;% group_by(iter) %&gt;% filter(value == max(value)) Now make the top panel. p1 &lt;- f %&gt;% ggplot(aes(x = weight, y = value)) + geom_hline(yintercept = .5, color = &quot;white&quot;, size = 1/2) + geom_vline(xintercept = thresholds$weight, color = &quot;white&quot;, size = 2/5) + geom_line(aes(group = iter), color = &quot;grey50&quot;, size = 1/3, alpha = 2/3) + geom_point(data = my_data, aes(y = male), alpha = 1/3) + labs(title = &quot;Data with Post. Pred.&quot;, y = &quot;male&quot;) + coord_cartesian(xlim = range(my_data$weight)) + theme(panel.grid = element_blank()) Here we make the marginal-distribution plots for our versions of the lower panels of Figure 21.8. p2 &lt;- posterior_samples(fit21.7) %&gt;% transmute(Intercept = b_b0_Intercept - (b_b1_Intercept * mean(my_data$weight) / sd(my_data$weight)), weight = b_b1_Intercept / sd(my_data$weight), guessing = b_a_Intercept) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;Intercept&quot;, &quot;weight&quot;, &quot;guessing&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, ncol = 3) Now combine them with patchwork and behold the glory. p1 / p2 + plot_layout(height = c(4, 1)) Here are the pairs() plots. pairs(fit21.7, off_diag_args = list(size = 1/5, alpha = 1/5)) Look at how that sweet Stan-based HMC reduced the correlation between \\(\\beta_0\\) and \\(\\beta_1\\). For more on this approach to robust logistic regression in brms and an alternative suggested by Andrew Gelman, check out a thread from the Stan Forums, Translating robust logistic regression from rstan to brms. 21.4 Nominal predictors “We now turn our attention from metric predictors to nominal predictors” (p. 636). 21.4.1 Single group. If we have just a single group and no other predictors, that’s just an intercept-only model. Back in the earlier chapters we thought of such a model as \\[\\begin{align*} y &amp; \\sim \\operatorname{Bernoulli} (\\theta) \\\\ \\theta &amp; \\sim \\operatorname{dbeta} (a, b). \\end{align*}\\] Now we’re expressing the model as \\[\\begin{align*} y &amp; \\sim \\operatorname{Bernoulli} (\\mu) \\\\ \\mu &amp; \\sim \\operatorname{logistic} (\\beta_0). \\end{align*}\\] For that \\(\\beta_0\\), we typically use a Gaussian prior of the form \\[\\beta_0 \\sim \\operatorname{Normal} (M_0, S_0).\\] In a situation where we don’t have strong prior substantive knowledge, we often set \\(M_0 = 0\\), which puts the probability mass around \\(\\theta = .5\\), a reasonable default hypothesis. Often times \\(S_0\\) is some modest single-digit integer like 2 or 4. To get a sense of how different Gaussians translate to the beta distribution, we’ll recreate Figure 21.11. # this will help streamline the conversion logistic &lt;- function(x) { 1 / (1 + exp(-x)) } # wrangle crossing(m_0 = 0:1, s_0 = c(.5, 1, 2)) %&gt;% mutate(key = str_c(&quot;mu == logistic(beta %~%&quot;, &quot; N(&quot;, m_0, &quot;, &quot;, s_0, &quot;))&quot;), sim = pmap(list(2e6, m_0, s_0), rnorm)) %&gt;% unnest(sim) %&gt;% mutate(sim = logistic(sim)) %&gt;% # plot ggplot(aes(x = sim, y = ..density..)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = 1/3, bins = 20, boundary = 0) + geom_line(stat = &quot;density&quot;, size = 3/4) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(mu)) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, labeller = label_parsed) So the prior distribution doesn’t even flatten out until you’re somewhere between \\(S_0 = 1\\) and \\(S_0 = 2\\). Just for kicks, here we break that down a little further. # wrangle tibble(m_0 = 0, s_0 = c(1.25, 1.5, 1.75)) %&gt;% mutate(key = str_c(&quot;mu == logistic(beta %~%&quot;, &quot; N(&quot;, m_0, &quot;, &quot;, s_0, &quot;))&quot;), sim = pmap(list(1e7, m_0, s_0), rnorm)) %&gt;% unnest(sim) %&gt;% mutate(sim = logistic(sim)) %&gt;% # plot ggplot(aes(x = sim, y = ..density..)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = 1/3, bins = 20, boundary = 0) + geom_line(stat = &quot;density&quot;, size = 3/4) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(mu)) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, labeller = label_parsed, ncol = 3) Here’s the basic brms analogue to Kruschke’s JAGS code from the bottom of page 639. fit &lt;- brm(data = my_data, family = bernoulli(link = identity), y ~ 1, prior(beta(1, 1), class = Intercept)) Here’s the brms analogue to Kruschke’s JAGS code at the top of page 641. fit &lt;- brm(data = my_data, family = bernoulli, y ~ 1, prior(normal(0, 2), class = &quot;Intercept&quot;)) 21.4.2 Multiple groups. If there’s only one group, we don’t need a grouping variable. But that’s a special case. Now we show the more general approach with multiple groups. 21.4.2.1 Example: Baseball again. Load the baseball data. my_data &lt;- read_csv(&quot;data.R/BattingAverage.csv&quot;) glimpse(my_data) ## Rows: 948 ## Columns: 6 ## $ Player &lt;chr&gt; &quot;Fernando Abad&quot;, &quot;Bobby Abreu&quot;, &quot;Tony Abreu&quot;, &quot;Dustin Ackley&quot;, &quot;Matt Adams&quot;,… ## $ PriPos &lt;chr&gt; &quot;Pitcher&quot;, &quot;Left Field&quot;, &quot;2nd Base&quot;, &quot;2nd Base&quot;, &quot;1st Base&quot;, &quot;Pitcher&quot;, &quot;Pit… ## $ Hits &lt;dbl&gt; 1, 53, 18, 137, 21, 0, 0, 2, 150, 167, 0, 128, 66, 3, 1, 81, 180, 36, 150, 0… ## $ AtBats &lt;dbl&gt; 7, 219, 70, 607, 86, 1, 1, 20, 549, 576, 1, 525, 275, 12, 8, 384, 629, 158, … ## $ PlayerNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2… ## $ PriPosNumber &lt;dbl&gt; 1, 7, 4, 4, 3, 1, 1, 3, 3, 4, 1, 5, 4, 2, 7, 4, 6, 8, 9, 1, 2, 5, 1, 1, 7, 2… 21.4.2.2 The model. I’m not aware that Kruschke’s modeling approach, here, will work well within the brms paradigm. I suggest we fit a simple hierarchical logistic regression model, instead. With this approach, we use the statistical model \\[\\begin{align*} \\text{Hits}_i &amp; \\sim \\operatorname{Binomial} (\\text{AtBats}_i, p_i) \\\\ \\operatorname{logit} (p_i) &amp; = \\beta_0 + \\beta_{\\text{PriPos}_i} + \\beta_{\\text{PriPos:Player}_i} \\\\ \\beta_0 &amp; \\sim \\operatorname{Normal} (0, 2) \\\\ \\beta_\\text{PriPos} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\text{PriPos}) \\\\ \\beta_\\text{PriPos:Player} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\text{PriPos:Player}) \\\\ \\sigma_\\text{PriPos} &amp; \\sim \\operatorname{HalfCauchy} (0, 1) \\\\ \\sigma_\\text{PriPos:Player} &amp; \\sim \\operatorname{HalfCauchy} (0, 1). \\end{align*}\\] Here’s how to fit the model with brms. Notice we’re finally making good use of the trials() syntax. This is because we’re fitting an aggregated binomial model. Instead of our criterion variable Hits being a vector of 0’s and 1’s, it’s the number of successful trials given the total number of trials, which is listed in the AtBats vector. Aggregated binomial. fit21.8 &lt;- brm(data = my_data, family = binomial(link = &quot;logit&quot;), Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player), prior = c(prior(normal(0, 2), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 2500, warmup = 500, chains = 4, cores = 4, seed = 21, file = &quot;fits/fit21.08&quot;) 21.4.2.3 Results. Before we start plotting, review the model summary. print(fit21.8) ## Family: binomial ## Links: mu = logit ## Formula: Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player) ## Data: my_data (Number of observations: 948) ## Samples: 4 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~PriPos (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.32 0.10 0.19 0.57 1.00 2590 3901 ## ## ~PriPos:Player (Number of levels: 948) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.14 0.01 0.12 0.15 1.00 3978 5910 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.17 0.11 -1.40 -0.94 1.00 1696 2634 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you look closely at our model versus the one in the text, you’ll see ours has fewer parameters. As a down-the-line consequence, our model doesn’t support a direct analogue to the plot at the top of Figure 21.13. However, we can come close. Rather than modeling the position-based probabilities as multiple draws of beta distributions, we can simply summarize our probabilities by their posterior distributions. library(ggridges) # define our new data, `nd` nd &lt;- my_data %&gt;% group_by(PriPos) %&gt;% summarise(AtBats = mean(AtBats) %&gt;% round(0)) # push the model through `fitted()` and wrangle fitted(fit21.8, newdata = nd, re_formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos), scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% gather() %&gt;% transmute(probability = inv_logit_scaled(value)) %&gt;% bind_cols(expand(nd, PriPos, iter = 1:8000)) %&gt;% # plot ggplot(aes(x = probability, y = PriPos)) + geom_vline(xintercept = fixef(fit21.8)[1] %&gt;% inv_logit_scaled(), color = &quot;white&quot;) + geom_density_ridges(size = 0, fill = &quot;grey67&quot;, scale = .9) + geom_jitter(data = my_data, aes(x = Hits / AtBats), height = .025, alpha = 1/6, size = 1/6) + coord_cartesian(xlim = c(0, 1), ylim = c(1, 9.5)) + labs(title = &quot;Data with Posterior Predictive Distrib.&quot;, x = &quot;Hits / AtBats&quot;) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) For kicks and giggles, we depicted the grand mean probability as the white vertical line in the background with the geom_vline() line. However, we can make our plot more directly analogous to Kruschke’s if we’re willing to stretch a little. Recall that Kruschke used the beta distribution with the \\(\\omega-\\kappa\\) parameterization in both his statistical model and his plot code—both of which you can find detailed in his Jags-Ybinom-Xnom1fac-Mlogistic.R. file. We didn’t use the beta distribution in our brm() model and the parameters from that model didn’t have as direct correspondences to the beta distribution the way those from Kruschke’s JAGS model did. However, recall that we can re-parameterize the beta distribution in terms of its mean \\(\\mu\\) and sample size \\(n\\), folowing the form \\[\\begin{align*} \\alpha &amp; = \\mu n \\\\ \\beta &amp; = (1 - \\mu) n . \\end{align*}\\] When we take the inverse logit of our intercepts, we do get vales in a probability metric. We might consider inserting those probabilities into the \\(\\mu\\) parameter. Furthermore, we can take our AtBats sample sizes and insert them directly into \\(n\\). As before, we’ll use the average sample size per position. # wrangle like a boss nd %&gt;% add_fitted_draws(fit21.8, n = 20, re_formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos), dpar = c(&quot;mu&quot;)) %&gt;% # use the equations from above mutate(alpha = mu * AtBats, beta = (1 - mu) * AtBats) %&gt;% mutate(ll = qbeta(.025, shape1 = alpha, shape2 = beta), ul = qbeta(.975, shape1 = alpha, shape2 = beta)) %&gt;% mutate(theta = map2(ll, ul, seq, length.out = 100)) %&gt;% unnest(theta) %&gt;% mutate(density = dbeta(theta, alpha, beta)) %&gt;% group_by(.draw) %&gt;% mutate(density = density / max(density)) %&gt;% # plot! ggplot(aes(y = PriPos)) + geom_ridgeline(aes(x = theta, height = -density, group = interaction(PriPos, .draw)), fill = NA, color = adjustcolor(&quot;grey50&quot;, alpha.f = 1/3), size = 1/4, scale = 3/4, min_height = NA) + geom_jitter(data = my_data, aes(x = Hits / AtBats, size = AtBats), height = .05, alpha = 1/6, shape = 1) + scale_size_continuous(range = c(1/4, 4)) + labs(title = &quot;Data with Posterior Predictive Distrib.&quot;, x = &quot;Hits / AtBats&quot;, y = NULL) + coord_flip(ylim = c(0.67, 8.67)) + theme(axis.text.x = element_text(angle = 90, hjust = 1), axis.ticks.x = element_blank(), legend.background = element_rect(fill = &quot;transparent&quot;, color = &quot;white&quot;), legend.key = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;), legend.position = c(.956, .8), panel.grid = element_blank()) ⚠️ Since we didn’t actually presume the beta distribution anywhere in our brm() statistical model, I would not attempt to present this workflow in a scientific outlet. Go with the previous plot. This attempt seems dishonest. But it is kinda fun to see how far we can push our results. ⚠️ Happily, our contrasts will be less contentious. Here’s the initial wrangling. # define our subset of positions positions &lt;- c(&quot;1st Base&quot;, &quot;Catcher&quot;, &quot;Pitcher&quot;) # redefine `nd` nd &lt;- my_data %&gt;% filter(PriPos %in% positions) %&gt;% group_by(PriPos) %&gt;% summarise(AtBats = mean(AtBats) %&gt;% round(0)) # push the model through `fitted()` and wrangle f &lt;- fitted(fit21.8, newdata = nd, re_formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos), scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% set_names(positions) # what did we do? head(f) ## # A tibble: 6 x 3 ## `1st Base` Catcher Pitcher ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.12 -1.12 -1.92 ## 2 -1.09 -1.14 -1.91 ## 3 -1.07 -1.13 -1.89 ## 4 -1.07 -1.13 -1.90 ## 5 -1.05 -1.17 -1.91 ## 6 -1.10 -1.17 -1.89 Here we make are our versions of the middle two panels of Figure 21.13. p1 &lt;- f %&gt;% # compute the differences and put the data in the long format transmute(`Pitcher vs. Catcher` = Pitcher - Catcher, `Catcher vs. 1st Base` = Catcher - `1st Base`) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;Pitcher vs. Catcher&quot;, &quot;Catcher vs. 1st Base&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .2, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference (in b)&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, ncol = 2) Now make our versions of the bottom two panels of Figure 21.13. p2 &lt;- f %&gt;% # do the transformation before computing the differences mutate_all(inv_logit_scaled) %&gt;% transmute(`Pitcher vs. Catcher` = Pitcher - Catcher, `Catcher vs. 1st Base` = Catcher - `1st Base`) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;Pitcher vs. Catcher&quot;, &quot;Catcher vs. 1st Base&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + stat_histintervalh(point_interval = mode_hdi, .width = c(.95, .5), fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .2, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Difference (in probability)&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, ncol = 2) Combine and plot. p1 / p2 Note how our distributions are described as differences in probability, rather than in \\(\\omega\\). Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggridges_0.5.2 patchwork_1.0.0 tidybayes_2.0.3.9000 brms_2.12.0 ## [5] Rcpp_1.0.4.6 ggExtra_0.9 forcats_0.5.0 stringr_1.4.0 ## [9] dplyr_0.8.5 purrr_0.3.4 readr_1.3.1 tidyr_1.0.2 ## [13] tibble_3.0.1 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 rsconnect_0.8.16 markdown_1.1 ## [5] base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 farver_2.0.3 ## [9] rstan_2.19.3 svUnit_1.0.3 DT_0.13 fansi_0.4.1 ## [13] mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 bridgesampling_1.0-0 ## [17] knitr_1.28 shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 ## [21] broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 compiler_3.6.3 ## [25] httr_1.4.1 backports_1.1.6 assertthat_0.2.1 Matrix_1.2-18 ## [29] fastmap_1.0.1 cli_2.0.2 later_1.0.0 htmltools_0.4.0 ## [33] prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 cellranger_1.1.0 ## [41] vctrs_0.3.0 nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ## [45] ps_1.3.3 rvest_0.3.5 mime_0.9 miniUI_0.1.1.1 ## [49] lifecycle_0.2.0 gtools_3.8.2 MASS_7.3-51.5 zoo_1.8-7 ## [53] scales_1.1.1 colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [57] Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 loo_2.2.0 StanHeaders_2.21.0-1 ## [65] stringi_1.4.6 highr_0.8 dygraphs_1.1.1.6 pkgbuild_1.0.8 ## [69] rlang_0.4.6 pkgconfig_2.0.3 matrixStats_0.56.0 HDInterval_0.2.0 ## [73] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [77] labeling_0.3 tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 ## [81] magrittr_1.5 bookdown_0.18 R6_2.4.1 generics_0.0.2 ## [85] DBI_1.1.0 pillar_1.4.4 haven_2.2.0 withr_2.2.0 ## [89] xts_0.12-0 abind_1.4-5 modelr_0.1.6 crayon_1.3.4 ## [93] arrayhelpers_1.1-0 utf8_1.1.4 rmarkdown_2.1 emo_0.0.0.9000 ## [97] grid_3.6.3 readxl_1.3.1 callr_3.4.3 threejs_0.3.3 ## [101] reprex_0.3.0 digest_0.6.25 xtable_1.8-4 httpuv_1.5.2 ## [105] stats4_3.6.3 munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.1 References "],
["nominal-predicted-variable.html", "22 Nominal Predicted Variable 22.1 Softmax regression 22.2 Conditional logistic regression 22.3 Implementation in JAGS brms 22.4 Generalizations and variations of the models Session info", " 22 Nominal Predicted Variable This chapter considers data structures that have a nominal predicted variable. When the nominal predicted variable has only two possible values, this reduces to the case of the dichotomous predicted variable considered in the previous chapter. In the present chapter, we generalize to cases in which the predicted variable has three or more categorical values… The traditional treatment of this sort of data structure is called multinomial logistic regression or conditional logistic regression. We will consider Bayesian approaches to these methods. As usual, in Bayesian software it is easy to generalize the traditional models so they are robust to outliers, allow different variances within levels of a nominal predictor, and have hierarchical structure to share information across levels or factors as appropriate. (Kruschke, 2015, p. 649) 22.1 Softmax regression “The key descriptor of the [models in this chapter is their] inverse-link function, which is the softmax function (which will be defined below). Therefore, [Kruschke] refer[ed] to the method as softmax regression instead of multinomial logistic regression” (p. 650) Say we have a metric predictor \\(x\\) and a multinomial criterion \\(y\\) with \\(k\\) categories. We can express the basic liner model as \\[\\lambda_k = \\beta_{0, k} + \\beta_{1, k} x,\\] for which the subscripts \\(k\\) indicate there’s a linear model for each of the \\(k\\) categories. We call the possible set of \\(k\\) outcomes \\(S\\). Taking the case where \\(k = 3\\), we’d have \\[\\begin{align*} \\lambda_{[1]} &amp; = \\beta_{0, [1]} + \\beta_{1, [1]} x, \\\\ \\lambda_{[2]} &amp; = \\beta_{0, [2]} + \\beta_{1, [2]} x, \\text{and} \\\\ \\lambda_{[3]} &amp; = \\beta_{0, [3]} + \\beta_{1, [3]} x. \\end{align*}\\] In this scenerio, what we want to know is the probability of \\(\\lambda_{[1]}\\), \\(\\lambda_{[2]}\\), and \\(\\lambda_{[3]}\\). The probability of a given outcome \\(k\\) follows the formula \\[\\phi_k = \\operatorname{softmax}_S (\\{\\lambda_k\\}) = \\frac{\\exp (\\lambda_k)}{\\sum_{c \\in S} \\exp (\\lambda_c)}\\] In words, [the equation] says that the probability of outcome \\(k\\) is the exponentiated linear propensity of outcome \\(k\\) relative to the sum of exponentiated linear propensities across all outcomes in the set \\(S\\). You may be wondering, Why exponentiate? Intuitively, we have to go from propensities that can have negative values to probabilities that can only have non-negative values, and we have to preserve order. The exponential function satisfies that need. (p. 650) You may be wondering what happened to \\(y\\) and where all those \\(\\lambda\\)s came from. Here we’re using \\(\\lambda\\) to describe the propensity of outcome \\(k\\), as indexed within our criterion \\(y\\). So, the output of these models, \\(\\phi_k\\), is the relative probability we’ll see each of our \\(k\\) categories within our criterion \\(y\\). What we want is \\(\\phi_k\\). The way we parameterize that with the softmax function is with \\(\\lambda_k\\). There are are indeterminacies in the system of equations Kruschke covered in this section, the upshot of which is we’ll end up making one of the \\(k\\) categories the reference category, which we term \\(r\\). Continuing on with our univariable model, we choose convenient constants for our parameters for \\(r\\): \\(\\beta_{0, r} = 0\\) and \\(\\beta_{1, r} = 0\\). As such, the regression coefficients for the remaining categories are relative to those for \\(r\\). Kruschke saved the data for Figure 22.1 in the SoftmaxRegData1.csv and SoftmaxRegData2.csv files. library(readr) library(tidyverse) d1 &lt;- read_csv(&quot;data.R/SoftmaxRegData1.csv&quot;) d2 &lt;- read_csv(&quot;data.R/SoftmaxRegData2.csv&quot;) glimpse(d1) ## Rows: 475 ## Columns: 3 ## $ X1 &lt;dbl&gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.53341559, -0.1893265… ## $ X2 &lt;dbl&gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.77465062, -0.53727640,… ## $ Y &lt;dbl&gt; 2, 1, 3, 3, 3, 3, 1, 4, 2, 2, 3, 2, 4, 4, 4, 1, 2, 3, 3, 3, 3, 2, 1, 1, 3, 2, 3, 2, 4,… glimpse(d2) ## Rows: 475 ## Columns: 3 ## $ X1 &lt;dbl&gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.53341559, -0.1893265… ## $ X2 &lt;dbl&gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.77465062, -0.53727640,… ## $ Y &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 2, 3, 2, 1, 3, 2, 4, 3, 3, 2, 2, 1, 1, 3, 3, 4, 2, 2, 3, 2, 3, 4, 4,… Let’s bind the two data frames together and plot in bulk. bind_rows(d1, d2) %&gt;% mutate(data = rep(str_c(&quot;d&quot;, 1:2), each = n() / 2)) %&gt;% ggplot(aes(x = X1, y = X2, label = Y, color = Y)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_text(size = 3) + scale_color_viridis_c(end = .9) + labs(x = expression(x[1]), y = expression(x[2])) + coord_equal() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~data, ncol = 2) 22.1.1 Softmax reduces to logistic for two outcomes. “When there are only two outcomes, the softmax formulation reduces to the logistic regression of Chapter 21” (p. 653) 22.1.2 Independence from irrelevant attributes. An important property of the softmax function of Equation 22.2 is known as independence from irrelevant attributes (Luce, 2012, 2008). The model implies that the ratio of probabilities of two outcomes is the same regardless of what other possible outcomes are included in the set. Let \\(S\\) denote the set of possible outcomes. Then, from the definition of the softmax function, the ratio of outcomes \\(j\\) and \\(k\\) is \\[\\frac{\\phi_j}{\\phi_k} = \\frac{\\exp (\\lambda_j) / \\sum_{c \\in S} \\exp (\\lambda_c)}{\\exp (\\lambda_k) / \\sum_{c \\in S} \\exp (\\lambda_c)}\\] The summation in the denominators cancels and has no effect on the ratio of probabilities. Obviously if we changed the set of outcomes \\(S\\) to any other set \\(S^*\\) that still contains outcomes \\(j\\) and \\(k\\), the summation \\(\\sum_{c \\in S^*}\\) would still cancel and have no effect on the ratio of probabilities. (p. 654) Just to walk out that denominators-canceling business a little further, \\[\\begin{align*} \\frac{\\phi_j}{\\phi_k} &amp; = \\frac{\\exp (\\lambda_j) / \\sum_{c \\in S} \\exp (\\lambda_c)}{\\exp (\\lambda_k) / \\sum_{c \\in S} \\exp (\\lambda_c)} \\\\ &amp; = \\frac{\\exp (\\lambda_j)}{\\exp (\\lambda_k)}. \\end{align*}\\] Thus even in the case of a very different set of possible outcomes \\(S^\\text{very different}\\), it remains that \\(\\frac{\\phi_j}{\\phi_k} = \\frac{\\exp (\\lambda_j)}{\\exp (\\lambda_k)}\\). Getting more applied, here’s a tibble presentation of Kruschke’s commute example with three modes of transportation. tibble(mode = c(&quot;walking&quot;, &quot;bicycling&quot;, &quot;bussing&quot;), preference = 3:1) %&gt;% mutate(`chance %` = (100 * preference / sum(preference)) %&gt;% round(digits = 1)) ## # A tibble: 3 x 3 ## mode preference `chance %` ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 walking 3 50 ## 2 bicycling 2 33.3 ## 3 bussing 1 16.7 Sticking with the example, if we take bicycling out of the picture, the preference values remain, but the chance % values change. tibble(mode = c(&quot;walking&quot;, &quot;bussing&quot;), preference = c(3, 1)) %&gt;% mutate(`chance %` = 100 * preference / sum(preference)) ## # A tibble: 2 x 3 ## mode preference `chance %` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 walking 3 75 ## 2 bussing 1 25 Though we retain the same walking/bussing ratio, we end up with a different model of relative probabilities. 22.2 Conditional logistic regression Softmax regression conceives of each outcome as an independent change in log odds from the reference outcome, and a special case of that is dichotomous logistic regression. But we can generalize logistic regression another way, which may better capture some patterns of data. The idea of this generalization is that we divide the set of outcomes into a hierarchy of two-set divisions, and use a logistic to describe the probability of each branch of the two-set divisions. (p. 655) The model follows the generic equation \\[\\begin{align*} \\phi_{S^* | S} = \\operatorname{logistic} (\\lambda_{S^* | S}) \\\\ \\lambda_{S^* | S} = \\beta_{0, S^* | S} + \\beta_{1, {S^* | S}} x, \\end{align*}\\] where the conditional response probability (i.e., the goal of the analysis) is \\(\\phi_{S^* | S}\\). \\(S^*\\) and \\(S\\) denote the subset of outcomes and larger set of outcomes, respectively, and \\(\\lambda_{S^* | S}\\) is the propensity based on some linear model. Kruschke saved the data for Figure 22.3 in the CondLogistRegData1.csv and CondLogistRegData2.csv files. d3 &lt;- read_csv(&quot;data.R/CondLogistRegData1.csv&quot;) d4 &lt;- read_csv(&quot;data.R/CondLogistRegData2.csv&quot;) glimpse(d3) ## Rows: 475 ## Columns: 3 ## $ X1 &lt;dbl&gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.53341559, -0.1893265… ## $ X2 &lt;dbl&gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.77465062, -0.53727640,… ## $ Y &lt;dbl&gt; 2, 1, 3, 1, 3, 3, 2, 3, 2, 4, 1, 2, 2, 3, 4, 2, 2, 4, 2, 3, 4, 2, 1, 1, 1, 2, 1, 2, 3,… glimpse(d4) ## Rows: 475 ## Columns: 3 ## $ X1 &lt;dbl&gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.53341559, -0.1893265… ## $ X2 &lt;dbl&gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.77465062, -0.53727640,… ## $ Y &lt;dbl&gt; 4, 4, 3, 4, 2, 3, 4, 3, 4, 4, 2, 4, 4, 3, 3, 4, 4, 4, 4, 3, 4, 4, 1, 1, 2, 4, 3, 4, 3,… Let’s bind the two data frames together and plot in bulk. bind_rows(d3, d4) %&gt;% mutate(data = rep(str_c(&quot;d&quot;, 3:4), each = n() / 2)) %&gt;% ggplot(aes(x = X1, y = X2, label = Y, color = Y)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_text(size = 3) + scale_color_viridis_c(end = .9) + coord_equal() + labs(x = expression(x[1]), y = expression(x[2])) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~data, ncol = 2) 22.3 Implementation in JAGS brms 22.3.1 Softmax model. Kruschke pointed out in his Figure 22.4 and the surrounding prose that we speak of the categorical distribution when fitting softmax models. Our brms paradigm will be much the same. To fit a softmax model with the brm() function, you specify family = categorical. The default is to use the logit link. In his (2020h) Parameterization of response distributions in brms vignette, Bürkner clarified: The categorical family is currently only implemented with the multivariate logit link function and has density \\[f(y) = \\mu_y = \\frac{\\exp (\\eta_y)}{\\sum_{k = 1}^K \\exp (\\eta_k)}\\] Note that \\(\\eta\\) does also depend on the category \\(k\\). For reasons of identifiability, \\(\\eta_1\\) is set to \\(0\\). Though there’s no explicit softmax talk in that vignette, you can find it documented in his code here, starting in line 1891. 22.3.2 Conditional logistic model. The conditional logistic regression models are not natively supported in brms at this time. However, if you follow issue #560, you’ll see there are ways to fit them using the nonlinear syntax. If you compare the syntax Bürkner used in that thread on January 30th to the JAGS syntax Kruschke showed on pages 661 and 662, you’ll see they appear to follow contrasting parameterizations. I think that’s about as far as I’m going with this model type at this time. If you work through the solution, please share your code in my GitHub issue #22. 22.3.3 Results: Interpreting the regression coefficients. 22.3.3.1 Softmax model. Load brms. library(brms) Along with Kruschke, we’ll be modeling the d1 data. In case it’s not clear, the X1 and X2 variables are already in a standardized metric. d1 %&gt;% pivot_longer(-Y) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), sd = sd(value)) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 x 3 ## name mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 X1 0 1 ## 2 X2 0 1 This will make it easier to set the priors. Here we’ll just use the rather wide priors Kruschke indicated on page 662. fit22.1 &lt;- brm(data = d1, family = categorical(link = logit), Y ~ 0 + Intercept + X1 + X2, prior(normal(0, 20), class = b), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 22, file = &quot;fits/fit22.01&quot;) Since it’s the default, we didn’t have to include the (link = logit) bit in the family argument. I’m just being explicit for the sake of pedagogy. Take a look at the parameter summary. print(fit22.1) ## Family: categorical ## Links: mu2 = logit; mu3 = logit; mu4 = logit ## Formula: Y ~ 0 + Intercept + X1 + X2 ## Data: d1 (Number of observations: 475) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## mu2_Intercept 3.37 0.59 2.25 4.57 1.00 1610 1593 ## mu2_X1 5.57 0.72 4.25 7.08 1.00 2606 2483 ## mu2_X2 0.81 0.49 -0.13 1.77 1.00 1767 2266 ## mu3_Intercept 2.04 0.66 0.76 3.33 1.00 1716 1604 ## mu3_X1 0.71 0.57 -0.43 1.80 1.00 1961 2340 ## mu3_X2 5.98 0.70 4.68 7.43 1.00 2358 2141 ## mu4_Intercept -0.43 0.89 -2.22 1.24 1.00 2158 2248 ## mu4_X1 12.38 1.15 10.25 14.72 1.00 2567 2542 ## mu4_X2 3.55 0.63 2.35 4.84 1.00 1836 2325 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As indicated in the formulas, above, we get posteriors for each level of Y, except for Y == 1. That serves as the reference category. The values for \\(\\beta_{i, k = 1}\\) are all fixed at \\(0\\). Here’s how we might make the histograms in Figure 22.5. library(tidybayes) # extract the posterior draws post &lt;- posterior_samples(fit22.1) # wrangle post %&gt;% pivot_longer(-lp__) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;)) %&gt;% mutate(lambda = str_extract(name, &quot;[2-4]+&quot;) %&gt;% str_c(&quot;lambda==&quot;, .), parameter = if_else(str_detect(name, &quot;Intercept&quot;), &quot;beta[0]&quot;, if_else(str_detect(name, &quot;X1&quot;), &quot;beta[1]&quot;, &quot;beta[2]&quot;))) %&gt;% # plot ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;marginal posterior&quot;) + theme(panel.grid = element_blank()) + facet_grid(lambda~parameter, labeller = label_parsed, scales = &quot;free_x&quot;) Because the \\(\\beta\\) values for when \\(\\lambda = 1\\) are all fixed to 0, we left those plots out of our version of the figure. If you really wanted them, you’d have to enter the corresponding cells into the data before plotting. If you summarize each parameter by it’s posterior mean, round(), and wrangle a little, you can arrange the results in a similar way that the equations for \\(\\lambda_2\\) through \\(\\lambda_4\\) are displayed on the left side of Figure 22.5 post %&gt;% pivot_longer(-lp__) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;)) %&gt;% mutate(lambda = str_extract(name, &quot;[2-4]+&quot;) %&gt;% str_c(&quot;lambda[&quot;, ., &quot;]&quot;), parameter = if_else(str_detect(name, &quot;Intercept&quot;), &quot;beta[0]&quot;, if_else(str_detect(name, &quot;X1&quot;), &quot;beta[1]&quot;, &quot;beta[2]&quot;))) %&gt;% group_by(lambda, parameter) %&gt;% summarise(mean = mean(value) %&gt;% round(digits = 1)) %&gt;% pivot_wider(names_from = parameter, values_from = mean) ## # A tibble: 3 x 4 ## # Groups: lambda [3] ## lambda `beta[0]` `beta[1]` `beta[2]` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lambda[2] 3.4 5.6 0.8 ## 2 lambda[3] 2 0.7 6 ## 3 lambda[4] -0.4 12.4 3.6 As Kruschke mentioned in the text, “the estimated parameter values should be near the generating values, but not exactly the same because the data are merely a finite random sample” (pp. 662–663). Furthermore, interpreting the parameters is always contextualized relative to the model. For the softmax model, the regression coefficient for outcome \\(k\\) on predictor \\(x_j\\) indicates that rate at which the log odds of that outcome increase relative to the reference outcome for a one unit increase in \\(x_j\\), assuming that a softmax model is a reasonable description of the data. (p. 663) Unfortunately, this makes the parameters difficult to interpret directly. Kruschke didn’t show a plot like this, but it might be helpful to further understand what this model means in terms of probabilities for a given y value. Here we’ll use the fitted() function to return the conditional probabilities for all four response options for Y based on various combinations of X1 and X2. nd &lt;- crossing(X1 = seq(from = -2, to = 2, length.out = 50), X2 = seq(from = -2, to = 2, length.out = 50)) fitted(fit22.1, newdata = nd) %&gt;% as_tibble() %&gt;% select(contains(&quot;Estimate&quot;)) %&gt;% set_names(str_c(&quot;lambda==&quot;, 1:4)) %&gt;% bind_cols(nd) %&gt;% pivot_longer(contains(&quot;lambda&quot;), values_to = &quot;probability&quot;) %&gt;% ggplot(aes(x = X1, y = X2, fill = probability)) + geom_raster(interpolate = T) + scale_fill_viridis_c(expression(phi[italic(k)*&quot;|&quot;*italic(S)]), option = &quot;A&quot;, limits = c(0, 1)) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + theme(panel.grid = element_blank()) + facet_wrap(~name, labeller = label_parsed) Now use that plot while you walk through the final paragraph in this subsection. It is easy to transform the estimated parameter values to a different reference category. Recall from Equation 22.3 (p. 651) that arbitrary constants can be added to all the regression coefficients without changing the model prediction. Therefore, to change the parameters estimates so they are relative to outcome \\(R\\), we simply subtract \\(\\beta_{j, R}\\) from \\(\\beta_{j, k}\\) for all predictors \\(j\\) and all outcomes \\(k\\). We do this at every step in the MCMC chain. For example, in Figure 22.5, consider the regression coefficient on \\(x_1\\) for outcome 2. Relative to reference outcome 1, this coefficient is positive, meaning that the probability of outcome 2 increases relative to outcome 1 when \\(x_1\\) increases. You can see this in the data graph, as the region of 2’s falls to right side (positive \\(x_1\\) direction) of the region of 1’s. But if the reference outcome is changed to outcome 4, then the coefficient on \\(x_1\\) for outcome 2 changes to a negative value. Algebraically this happens because the coefficient on \\(x_1\\) for outcome 4 is larger than for outcome 2, so when the coefficient for outcome 4 is subtracted, the result is a negative value for the coefficient on outcome 2. Visually, you can see this in the data graph, as the region of 2’s falls to the left side (negative \\(x_1\\) direction) of the region of 4’s. Thus, interpreting regression coefficients in a softmax model is rather different than in linear regression. In linear regression, a positive regression coefficient implies that \\(y\\) increases when the predictor increases. But not in softmax regression, where a positive regression coefficient is only positive with respect to a particular reference outcome. (p. 664, emphasis added) 22.3.3.2 Conditional logistic model. I’m not pursuing this model type at this time. If you work through the solution, please share your code in my GitHub issue #22. 22.4 Generalizations and variations of the models These models can be generalized to include different kinds of predictors, variants robust to outliers, and model comparison via information criteria and so forth. Also, you can find a couple more examples with softmax regression in Chapter 10 of McElreath’s Statistical rethinking. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.3.9000 brms_2.12.0 Rcpp_1.0.4.6 forcats_0.5.0 ## [5] stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 tidyr_1.0.2 ## [9] tibble_3.0.1 ggplot2_3.3.0 tidyverse_1.3.0 readr_1.3.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 ## [9] farver_2.0.3 rstan_2.19.3 svUnit_1.0.3 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 ## [17] bridgesampling_1.0-0 knitr_1.28 shinythemes_1.1.2 bayesplot_1.7.1 ## [21] jsonlite_1.6.1 broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 ## [25] compiler_3.6.3 httr_1.4.1 backports_1.1.6 assertthat_0.2.1 ## [29] Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 later_1.0.0 ## [33] htmltools_0.4.0 prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 ## [41] cellranger_1.1.0 vctrs_0.3.0 nlme_3.1-144 crosstalk_1.1.0.1 ## [45] xfun_0.13 ps_1.3.3 rvest_0.3.5 mime_0.9 ## [49] miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 zoo_1.8-7 ## [53] scales_1.1.1 colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [57] Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 StanHeaders_2.21.0-1 loo_2.2.0 ## [65] stringi_1.4.6 dygraphs_1.1.1.6 pkgbuild_1.0.8 rlang_0.4.6 ## [69] pkgconfig_2.0.3 matrixStats_0.56.0 HDInterval_0.2.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 ## [77] tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 magrittr_1.5 ## [81] bookdown_0.18 R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [85] pillar_1.4.4 haven_2.2.0 withr_2.2.0 xts_0.12-0 ## [89] abind_1.4-5 modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 ## [93] utf8_1.1.4 rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 ## [97] callr_3.4.3 threejs_0.3.3 reprex_0.3.0 digest_0.6.25 ## [101] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 ## [105] viridisLite_0.3.0 shinyjs_1.1 References "],
["ordinal-predicted-variable.html", "23 Ordinal Predicted Variable 23.1 Modeling ordinal data with an underlying metric variable 23.2 The case of a single group 23.3 The case of two groups 23.4 The Case of metric predictors 23.5 Posterior prediction 23.6 Generalizations and extensions Session info", " 23 Ordinal Predicted Variable This chapter considers data that have an ordinal predicted variable. For example, we might want to predict people’s happiness ratings on a 1-to-7 scale as a function of their total financial assets. Or we might want to predict ratings of movies as a function of the year they were made. One traditional treatment of this sort of data structure is called ordinal or ordered probit regression. We will consider a Bayesian approach to this model. As usual, in Bayesian software, it is easy to generalize the traditional model so it is robust to outliers, allows different variances within levels of a nominal predictor, or has hierarchical structure to share information across levels or factors as appropriate. In the context of the generalized linear model (GLM) introduced in Chapter 15, this chapter’s situation involves an inverse-link function that is a thresholded cumulative normal with a categorical distribution for describing noise in the data, as indicated in the fourth row of Table 15.2 (p. 443). For a reminder of how this chapter’s combination of predicted and predictor variables relates to other combinations, see Table 15.3 (p. 444). (Kruschke, 2015, p. 671, emphasis in the original) We might follow Kruschke’s advice and rewind a little before tackling this chapter. The cumulative normal function is denoted \\(\\Phi(x; \\mu, \\sigma)\\), where \\(x\\) is some real number and \\(\\mu\\) and \\(\\sigma\\) have their typical meaning. The function is cumulative in that it produces values ranging from zero to 1. Figure 15.8 showed an example of the normal distribution atop of the cumulative normal. Here it is, again. library(tidyverse) d &lt;- tibble(z = seq(from = -3, to = 3, by = .1)) %&gt;% # add the density values mutate(`p(z)` = dnorm(z, mean = 0, sd = 1), # add the CDF values `Phi(z)` = pnorm(z, mean = 0, sd = 1)) head(d) ## # A tibble: 6 x 3 ## z `p(z)` `Phi(z)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -3 0.00443 0.00135 ## 2 -2.9 0.00595 0.00187 ## 3 -2.8 0.00792 0.00256 ## 4 -2.7 0.0104 0.00347 ## 5 -2.6 0.0136 0.00466 ## 6 -2.5 0.0175 0.00621 Make the plot. p1 &lt;- d %&gt;% ggplot(aes(x = z)) + geom_ribbon(data = . %&gt;% filter(z &lt;=1), aes(ymin = 0, ymax = `p(z)`), fill = &quot;grey67&quot;) + geom_line(aes(y = `p(z)`), size = 1, color = &quot;grey50&quot;) + labs(title = &quot;Normal Density&quot;, y = expression(p(italic(z)))) p2 &lt;- d %&gt;% ggplot(aes(x = z)) + geom_ribbon(data = . %&gt;% filter(z &lt;=1), aes(ymin = 0, ymax = `Phi(z)`), fill = &quot;grey67&quot;) + geom_line(aes(y = `Phi(z)`), size = 1, color = &quot;grey50&quot;) + labs(title = &quot;Cumulative Normal&quot;, y = expression(Phi(italic(z)))) # combine and adjust with patchwork library(patchwork) p1 / p2 &amp; scale_x_continuous(breaks = -2:2) &amp; coord_cartesian(xlim = c(-2.5, 2.5)) &amp; theme(panel.grid = element_blank()) For both plots, z is in a standardized metric (i.e., \\(z\\)-score). With the cumulative normal function, the cumulative probability \\(\\Phi(z)\\) increases nonlinearly with the \\(z\\)-scores such that, much like with the logistic curve, the greatest change occurs around \\(z = 0\\) and tapers off in the tails. The inverse of \\(\\Phi(x)\\) is the probit function. As indicated in the above block quote, we’ll be making extensive use of the probit function in this chapter for our Bayesian models. 23.1 Modeling ordinal data with an underlying metric variable You can imagine that the distribution of ordinal values might not resemble a normal distribution, even though the underlying metric values are normally distributed. Figure 23.1 shows some examples of ordinal outcome probabilities generated from an underlying normal distribution. The horizontal axis is the underlying continuous metric value. Thresholds are plotted as vertical dashed lines, labeled \\(\\theta\\). In all examples, the ordinal scale has 7 levels, and hence, there are 6 thresholds. The lowest threshold is set at \\(\\theta_1 = 1.5\\) (to separate outcomes 1 and 2), and the highest threshold is set at \\(\\theta_1 = 6.5\\) (to separate outcomes 6 and 7). The normal curve in each panel shows the distribution of underlying continuous values. What differs across panels are the settings of means, standard deviations, and remaining thresholds. (p. 672) The various Figure 23.1 subplots require a lot of ins and outs. We’ll start with the top panel and build from there. Here is how we might make the values necessary for the density curve. den &lt;- # define the parameters for the underlying normal distribution tibble(mu = 4, sigma = 1.5) %&gt;% mutate(strip = str_c(&quot;mu==&quot;, mu, &quot;~~sigma==&quot;, sigma)) %&gt;% # this will allow us to rescale the density in terms of the bar plot mutate(multiplier = 26 / dnorm(mu, mu, sigma)) %&gt;% # we need values for the x-axis expand(nesting(mu, sigma, strip, multiplier), y = seq(from = -1, to = 9, by = .1)) %&gt;% # compute the density values mutate(density = dnorm(y, mu, sigma)) %&gt;% # use that multiplier value from above to rescale the density values mutate(percent = density * multiplier) head(den) ## # A tibble: 6 x 7 ## mu sigma strip multiplier y density percent ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 1.5 mu==4~~sigma==1.5 97.8 -1 0.00103 0.101 ## 2 4 1.5 mu==4~~sigma==1.5 97.8 -0.9 0.00128 0.125 ## 3 4 1.5 mu==4~~sigma==1.5 97.8 -0.8 0.00159 0.155 ## 4 4 1.5 mu==4~~sigma==1.5 97.8 -0.7 0.00196 0.192 ## 5 4 1.5 mu==4~~sigma==1.5 97.8 -0.6 0.00241 0.236 ## 6 4 1.5 mu==4~~sigma==1.5 97.8 -0.5 0.00295 0.289 Before making the data for the bar portion of the plot, we’ll need to define the \\(\\theta\\)-values they’ll be placed between. We also need to define the exact points on the \\(x\\)-axis from which we’d like those bars to originate. Those points, which we’ll call label_1, will double as names for the individual bars. (theta_1 &lt;- seq(from = 1.5, to = 6.5, by = 1)) ## [1] 1.5 2.5 3.5 4.5 5.5 6.5 (label_1 &lt;- 1:7) ## [1] 1 2 3 4 5 6 7 Now we can define the data for the bars. bar &lt;- # define the parameters for the underlying normal distribution tibble(mu = 4, sigma = 1.5) %&gt;% mutate(strip = str_c(&quot;mu==&quot;, mu, &quot;~~sigma==&quot;, sigma)) %&gt;% # take random draws from the underlying normal distribution mutate(draw = map2(mu, sigma, ~rnorm(1e4, mean = .x, sd = .y))) %&gt;% unnest(draw) %&gt;% # bin those draws into ordinal categories defined by `theta_1` # and named by `label_1` mutate(y = case_when( draw &lt; theta_1[1] ~ label_1[1], draw &lt; theta_1[2] ~ label_1[2], draw &lt; theta_1[3] ~ label_1[3], draw &lt; theta_1[4] ~ label_1[4], draw &lt; theta_1[5] ~ label_1[5], draw &lt; theta_1[6] ~ label_1[6], draw &gt;= theta_1[6] ~ label_1[7] )) %&gt;% # summarize count(y) %&gt;% mutate(percent = (100 * n / sum(n)) %&gt;% round(0)) %&gt;% mutate(percent_label = str_c(percent, &quot;%&quot;), percent_max = max(percent)) head(bar) ## # A tibble: 6 x 5 ## y n percent percent_label percent_max ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 464 5 5% 27 ## 2 2 1062 11 11% 27 ## 3 3 2035 20 20% 27 ## 4 4 2740 27 27% 27 ## 5 5 2119 21 21% 27 ## 6 6 1082 11 11% 27 Make the top subplot. bar %&gt;% ggplot(aes(x = y)) + geom_ribbon(data = den, aes(ymin = 0, ymax = percent), fill = &quot;grey75&quot;) + geom_vline(xintercept = theta_1, color = &quot;grey92&quot;, linetype = 3) + geom_col(aes(y = percent), width = .5, alpha = .85) + geom_text(aes(y = percent + 2, label = percent_label), size = 3.5) + annotate(geom = &quot;text&quot;, x = theta_1, y = -6.5, label = theta_1, size = 3) + scale_x_continuous(NULL, breaks = theta_1, labels = parse(text = str_c(&quot;theta[&quot;, 1:6, &quot;]&quot;))) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(ylim = c(0, 28.5), clip = F) + theme(panel.grid = element_blank(), plot.margin = margin(5.5, 5.5, 11, 5.5)) + facet_wrap(~strip, labeller = label_parsed) This method works okay for plotting one or two panels. The sheer number of code lines and moving parts seem unwieldy for plotting four. It’d be convenient if we could save the density information for all four panels in one data object. Here’s one way how. den &lt;- tibble(panel = 1:4, mu = c(4, 1, 4, 4), sigma = c(1.5, 2.5, 1, 3)) %&gt;% mutate(strip = factor(panel, labels = str_c(&quot;mu==&quot;, mu, &quot;~~sigma==&quot;, sigma), ordered = T)) %&gt;% mutate(multiplier = c(26, 58, 24, 26) / dnorm(mu, mu, sigma)) %&gt;% expand(nesting(panel, mu, sigma, strip, multiplier), y = seq(from = -1, to = 9, by = .1)) %&gt;% mutate(density = dnorm(y, mu, sigma)) %&gt;% mutate(percent = density * multiplier) head(den) ## # A tibble: 6 x 8 ## panel mu sigma strip multiplier y density percent ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 1.5 mu==4~~sigma==1.5 97.8 -1 0.00103 0.101 ## 2 1 4 1.5 mu==4~~sigma==1.5 97.8 -0.9 0.00128 0.125 ## 3 1 4 1.5 mu==4~~sigma==1.5 97.8 -0.8 0.00159 0.155 ## 4 1 4 1.5 mu==4~~sigma==1.5 97.8 -0.7 0.00196 0.192 ## 5 1 4 1.5 mu==4~~sigma==1.5 97.8 -0.6 0.00241 0.236 ## 6 1 4 1.5 mu==4~~sigma==1.5 97.8 -0.5 0.00295 0.289 Notice we added a panel column for indexing the subplots. Next we’ll need to define theta_[i] and label_[i] values for the remaining plots. theta_3 &lt;- c(1.5, 3.1, 3.7, 4.3, 4.9, 6.5) theta_4 &lt;- c(1.5, 2.25, 3, 5, 5.75, 6.5) label_3 &lt;- c(1, 2.2, 3.4, 4, 4.6, 5.7, 7) label_4 &lt;- c(1, 1.875, 2.625, 4, 5.375, 6.125, 7) Since the values are the same for the top two panels, we didn’t bother defining a theta_2 or label_2. Now we have all the theta_[i] and label_[i] values, we’ll want to make a function that can use them within case_when() for any of the four panels. Here’s one way to make such a function, which we’ll call make_ordinal(). make_ordinal &lt;- function(x, panel) { if (panel &lt; 3) { case_when( x &lt; theta_1[1] ~ label_1[1], x &lt; theta_1[2] ~ label_1[2], x &lt; theta_1[3] ~ label_1[3], x &lt; theta_1[4] ~ label_1[4], x &lt; theta_1[5] ~ label_1[5], x &lt; theta_1[6] ~ label_1[6], x &gt;= theta_1[6] ~ label_1[7] ) } else if (panel == 3) { case_when( x &lt; theta_3[1] ~ label_3[1], x &lt; theta_3[2] ~ label_3[2], x &lt; theta_3[3] ~ label_3[3], x &lt; theta_3[4] ~ label_3[4], x &lt; theta_3[5] ~ label_3[5], x &lt; theta_3[6] ~ label_3[6], x &gt;= theta_3[6] ~ label_3[7] ) } else { case_when( x &lt; theta_4[1] ~ label_4[1], x &lt; theta_4[2] ~ label_4[2], x &lt; theta_4[3] ~ label_4[3], x &lt; theta_4[4] ~ label_4[4], x &lt; theta_4[5] ~ label_4[5], x &lt; theta_4[6] ~ label_4[6], x &gt;= theta_4[6] ~ label_4[7] ) } } Now put those values and our make_ordinal() function to work to make the data for the bar plots. set.seed(23) bar &lt;- tibble(panel = 1:4, mu = c(4, 1, 4, 4), sigma = c(1.5, 2.5, 1, 3)) %&gt;% mutate(strip = factor(panel, labels = str_c(&quot;mu==&quot;, mu, &quot;~~sigma==&quot;, sigma), ordered = T)) %&gt;% mutate(draw = map2(mu, sigma, ~rnorm(1e5, mean = .x, sd = .y))) %&gt;% unnest(draw) %&gt;% mutate(y = map2_dbl(draw, panel, make_ordinal)) %&gt;% group_by(panel, strip) %&gt;% count(y) %&gt;% mutate(percent = (100 * n / sum(n)) %&gt;% round(0)) %&gt;% mutate(percent_label = str_c(percent, &quot;%&quot;), percent_max = max(percent)) head(bar) ## # A tibble: 6 x 7 ## # Groups: panel, strip [1] ## panel strip y n percent percent_label percent_max ## &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 mu==4~~sigma==1.5 1 4763 5 5% 26 ## 2 1 mu==4~~sigma==1.5 2 10844 11 11% 26 ## 3 1 mu==4~~sigma==1.5 3 21174 21 21% 26 ## 4 1 mu==4~~sigma==1.5 4 26256 26 26% 26 ## 5 1 mu==4~~sigma==1.5 5 21233 21 21% 26 ## 6 1 mu==4~~sigma==1.5 6 10951 11 11% 26 Like before, we added a panel index. As our final preparatory step, we will make something of a super function with which we’ll plug the desired information into ggplot2, which will then make each subplot. Much of the plotting and data wrangling code will be the same across subplots. As far as I can tell, we only need to vary four parameters. First, we’ll want to be able to subset the data by panel index. We’ll do that with the panel_n argument. Second, we’ll want to select which of the theta_[i] values we’d like to use in geom_vline(), annotate(), and scale_x_continuous(). We’ll do that with the theta argument. We’ll make a y_second_x to pin down exactly where below the \\(x\\)-axis we’d like to put those secondary axis values defined by the theta_[i] values. Finally, we’ll want an ylim_ub parameter to set the upper limit of the \\(y\\)-axis with. The name of our four-parameter super function will be plot_bar_den(). plot_bar_den &lt;- function(panel_n, theta, y_second_x, ylim_ub) { bar %&gt;% filter(panel == panel_n) %&gt;% ggplot(aes(x = y)) + geom_ribbon(data = den %&gt;% filter(panel == panel_n), aes(ymin = 0, ymax = percent), fill = &quot;grey75&quot;) + geom_vline(xintercept = theta, color = &quot;grey92&quot;, linetype = 3) + geom_linerange(aes(ymin = 0, ymax = percent), color = &quot;grey33&quot;, alpha = .85, size = 8) + geom_text(aes(y = percent + (percent_max / 15), label = percent_label), size = 3.5) + annotate(geom = &quot;text&quot;, x = theta, y = y_second_x, label = theta, size = 3) + scale_x_continuous(NULL, breaks = theta, labels = parse(text = str_c(&quot;theta[&quot;, 1:6, &quot;]&quot;))) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(ylim = c(0, ylim_ub), clip = F) + theme(panel.grid = element_blank(), plot.margin = margin(5.5, 5.5, 11, 5.5)) + facet_wrap(~strip, labeller = label_parsed) } Finally, make all four subplots and combine them with patchwork syntax! p1 &lt;- plot_bar_den(panel_n = 1, theta = theta_1, y_second_x = -6.75, ylim_ub = 28) p2 &lt;- plot_bar_den(panel_n = 2, theta = theta_1, y_second_x = -15.5, ylim_ub = 63) p3 &lt;- plot_bar_den(panel_n = 3, theta = theta_3, y_second_x = -6.25, ylim_ub = 25.75) p4 &lt;- plot_bar_den(panel_n = 4, theta = theta_4, y_second_x = -6.75, ylim_ub = 28) p1 / p2 / p3 / p4 Oh mamma. “The crucial concept in Figure 23.1 is that the probability of a particular ordinal outcome is the area under the normal curve between the thresholds of that outcome” (p. 672, emphasis in the original). In each of the subplots, we used six thresholds to descritize the continuous data into seven categories. More generally, we need \\(K\\) thresholds to make \\(K + 1\\) ordinal categories. To make this work, the idea is that we consider the cumulative area under the normal up the high-side threshold, and subtract away the cumulative area under the normal up to the low-side threshold. Recall that the cumulative area under the standardized normal is denoted \\(\\Phi(z)\\), as was illustrated in Figure 15.8 [which we remade at the top of this chapter]. Thus, the area under the normal to the left of \\(\\theta_k\\) is \\(\\Phi((\\theta_k - \\mu) / \\sigma)\\), and the area under the normal to the left of \\(\\theta_{k - 1}\\) is \\(\\Phi((\\theta_{k - 1} - \\mu) / \\sigma)\\). Therefore, the area under the normal curve between the two thresholds, which is the probability of outcome \\(k\\), is \\[p(y = k | \\mu, \\sigma, \\{ \\theta_j \\}) = \\Phi((\\theta_k - \\mu) / \\sigma) - \\Phi((\\theta_{k - 1} - \\mu) / \\sigma)\\] [This equation] applies even to the least and greatest ordinal values if we append two “virtual” thresholds at \\(- \\infty\\) and \\(+ \\infty\\)… Thus, a normally distributed underlying metric value can yield a clearly non-normal distribution of discrete ordinal values. This result does not imply that the ordinal values can be treated as if they were themselves metric and normally distributed; in fact it implies the opposite: We might be able to model a distribution of ordinal values as consecutive intervals of a normal distribution on an underlying metric scale with appropriately positioned thresholds. (pp. 674–675) 23.2 The case of a single group Given a model with no predictors, “if there are \\(K\\) ordinal values, the model has \\(K + 1\\) parameters: \\(\\theta_1,...,\\theta_{K - 1}, \\mu\\), and \\(\\sigma\\). If you think about it a moment, you’ll realize that the parameter values trade-off and are undetermined” (p. 675). The solution Kruschke took throughout this chapter was to fix the two thresholds at the ends, \\(\\theta_1\\) and \\(\\theta_{K - 1}\\), to the constants \\[\\begin{align*} \\theta_1 \\equiv 1 + 0.5 &amp;&amp; \\text{and} &amp;&amp; \\theta_{K - 1} \\equiv K - 0.5. \\end{align*}\\] For example, all four subplots from Figure 23.1 had \\(K = 7\\) categories, ranging from 1 to 7. Following Kruschke’s convention would mean setting the endmost thresholds to \\[\\begin{align*} \\theta_1 \\equiv 1.5 &amp;&amp; \\text{and} &amp;&amp; \\theta_6 \\equiv 6.5. \\end{align*}\\] As we’ll see, there are other ways to parameterize these models. 23.2.1 Implementation in JAGS brms. The syntax to fit a basic ordered probit model with brms::brm() is pretty simple. fit &lt;- brm(data = my_data, family = cumulative(probit), y ~ 1, prior(normal(0, 4), class = Intercept)) The family = cumulative(probit) tells brms you’d like to use the probit link for the ordered-categorical data. It’s important to specify probit because the brms default is to use the logit link, instead. We’ll talk more about that approach at the end of this chapter. Remember how, at the end of the last section, we said there are other ways to parameterize the ordered probit model? As it turns out, brms does not follow Kruschke’s approach for fixing the thresholds on the ends. Rather, brms freely estimates all thresholds, \\(\\theta_1,...,\\theta_{K - 1}\\), by fixing \\(\\mu = 0\\) and \\(\\sigma = 1\\). That is, instead of estimating \\(\\mu\\) and \\(\\sigma\\) from the normal cumulative density function \\(\\Phi(x)\\), brms::brm() uses the standard normal cumulative density function \\(\\Phi(z)\\). This all probably seems abstract. We’ll get a lot of practice comparing the two approaches as we go along. Each has its strengths and weaknesses. At this point, the thing to get is that when fitting a single-group ordered-probit model with the brm() function, there will be no priors for \\(\\mu\\) and \\(\\sigma\\). We only have to worry about setting the priors for all \\(K - 1\\) thresholds. And because those thresholds are conditional on \\(\\Phi(z)\\), we should think about their priors with respect to the scale of standard normal distribution. Thus, to continue on with Kruschke’s minimally-informative prior approach, something like prior(normal(0, 4), class = Intercept) might be a good starting place. Do feel free to experiment with different settings. 23.2.2 Examples: Bayesian estimation recovers true parameter values. The data for Kruschke’s first example come from his OrdinalProbitData-1grp-1.csv file. Load the data. my_data_1 &lt;- read_csv(&quot;data.R/OrdinalProbitData-1grp-1.csv&quot;) glimpse(my_data_1) ## Rows: 100 ## Columns: 1 ## $ Y &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… Plot the distribution for Y. my_data_1 %&gt;% mutate(Y = factor(Y)) %&gt;% ggplot(aes(x = Y)) + geom_bar() + theme(panel.grid = element_blank()) It looks a lot like the distribution of the data from one of the panels from Figure 23.1. Load brms. library(brms) Fit the first cumulative-probit model. fit23.1 &lt;- brm(data = my_data_1, family = cumulative(probit), Y ~ 1, prior(normal(0, 4), class = Intercept), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.01&quot;) Examine the model summary. print(fit23.1) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Y ~ 1 ## Data: my_data_1 (Number of observations: 100) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] 0.18 0.13 -0.06 0.43 1.00 10343 6333 ## Intercept[2] 0.60 0.13 0.34 0.87 1.00 11769 6851 ## Intercept[3] 1.04 0.15 0.74 1.34 1.00 11542 6613 ## Intercept[4] 1.50 0.19 1.14 1.88 1.00 12314 6356 ## Intercept[5] 1.96 0.25 1.50 2.49 1.00 11738 7449 ## Intercept[6] 2.57 0.40 1.89 3.45 1.00 12330 7001 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The brms output for these kinds of models names the thresholds \\(\\theta_{[i]}\\) as Intercept[i]. Again, whereas Kruschke identified his model by fixing \\(\\theta_1 = 1.5\\) (i.e., \\(1 + 0.5\\)) and \\(\\theta_6 = 5.5\\) (i.e., \\(6 - 0.5\\)), we freely estimated all six thresholds by using the cumulative density function for the standard normal. As a result, our thresholds are in a different metric from Kruschke’s. Let’s extract the posterior draws. post &lt;- posterior_samples(fit23.1) glimpse(post) ## Rows: 8,000 ## Columns: 7 ## $ `b_Intercept[1]` &lt;dbl&gt; 0.185364034, 0.165876358, 0.438520914, 0.206087790, 0.131577665, 0.189973985, 0.19… ## $ `b_Intercept[2]` &lt;dbl&gt; 0.6662789, 0.6199828, 0.7796308, 0.6214409, 0.4437972, 0.5546956, 0.7037654, 0.454… ## $ `b_Intercept[3]` &lt;dbl&gt; 1.1525861, 1.1466193, 1.1683288, 1.0015953, 1.0178535, 0.9794249, 1.1422900, 0.864… ## $ `b_Intercept[4]` &lt;dbl&gt; 1.495071, 1.527385, 1.681138, 1.388764, 1.561751, 1.390207, 1.364145, 1.575484, 1.… ## $ `b_Intercept[5]` &lt;dbl&gt; 1.705743, 1.752142, 2.057058, 2.024601, 1.758377, 1.882053, 1.577857, 2.116130, 1.… ## $ `b_Intercept[6]` &lt;dbl&gt; 2.155234, 2.081832, 2.817471, 2.708599, 2.235552, 3.252597, 1.714304, 3.205844, 2.… ## $ lp__ &lt;dbl&gt; -151.4273, -151.4265, -151.7556, -149.8376, -152.3254, -150.8681, -155.6359, -151.… Wrangle post a bit. post &lt;- post %&gt;% select(`b_Intercept[1]`:`b_Intercept[6]`) %&gt;% mutate(iter = 1:n()) Here’s our brms version of the bottom plot of Figure 23.2 means &lt;- post %&gt;% summarise_at(vars(`b_Intercept[1]`:`b_Intercept[6]`), mean) %&gt;% pivot_longer(everything(), values_to = &quot;mean&quot;) post %&gt;% gather(name, threshold, -iter) %&gt;% group_by(iter) %&gt;% mutate(theta_bar = mean(threshold)) %&gt;% ggplot(aes(x = threshold, y = theta_bar, color = name)) + geom_vline(data = means, aes(xintercept = mean, color = name), linetype = 2) + geom_point(alpha = 1/10) + scale_color_viridis_d(option = &quot;D&quot;, end = .85, direction = -1) + ylab(&quot;mean threshold&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) The initial means data at the top contains the \\(\\theta_i\\)-specific means, which we used to make the dashed vertical lines with geom_vline(). Did you see what we did there with those group_by() and mutate() lines? That’s how we computed the mean threshold within each step of the HMC chain, what Kruschke (p. 680) denoted as \\(\\bar \\theta (s) = \\sum_k^{K-1} \\theta_k (s) / (K - 1)\\), where \\(s\\) refers to particular steps in the HMC chain. Perhaps of greater interest, you might have noticed how different our plot is from the one in the text. We might should compare the results of our brms parameterization of \\(\\theta_{[i]}\\) with one based on the parameterization in the text in an expanded version of the bottom plot of Figure 23.2. To convert our brms output to match Kruschke’s, we’ll rescale our \\(\\theta_{[i]}\\) draws with help from the scales::rescale() function, about which you might learn more here. # primary data wrangling p &lt;- bind_rows( # brms parameterization post %&gt;% gather(name, threshold, -iter) %&gt;% group_by(iter) %&gt;% mutate(theta_bar = mean(threshold)), # Kruschke&#39;s parameterization post %&gt;% gather(name, threshold, -iter) %&gt;% group_by(iter) %&gt;% mutate(threshold = scales::rescale(threshold, to = c(1.5, 6.5))) %&gt;% mutate(theta_bar = mean(threshold)) ) %&gt;% # add an index mutate(model = rep(c(&quot;brms parameterization&quot;, &quot;Kruschke&#39;s parameterization&quot;), each = n() / 2)) # compute the means by model and threshold for the vertical lines means &lt;- p %&gt;% ungroup() %&gt;% group_by(model, name) %&gt;% summarise(mean = mean(threshold)) # plot! p %&gt;% ggplot(aes(x = threshold, y = theta_bar)) + geom_vline(data = means, aes(xintercept = mean, color = name), linetype = 2) + geom_point(aes(color = name), alpha = 1/10, size = 1/2) + scale_color_viridis_d(option = &quot;D&quot;, end = .85, direction = -1) + ylab(&quot;mean threshold&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~model, ncol = 1, scales = &quot;free&quot;) We can take our rescaling approach further to convert the posterior distributions for \\(\\mu\\) and \\(\\sigma\\) from the brms \\(\\operatorname{Normal} (0, 1)\\) constants to the metric from Kruschke’s approach. Say \\(y_1\\) and \\(y_2\\) are two draws from some Gaussian and \\(z_1\\) and \\(z_2\\) are their corresponding \\(z\\)-scores. Here’s how to solve for \\(\\sigma\\). \\[\\begin{align*} z_1 - z_2 &amp; = \\frac{(y_1 - \\mu)}{\\sigma} - \\frac{(y_2 - \\mu)}{\\sigma} \\\\ &amp; = \\frac{(y_1 - \\mu) - (y_2 - \\mu)}{\\sigma} \\\\ &amp; = \\frac{y_1 - \\mu - y_2 + \\mu}{\\sigma} \\\\ &amp; = \\frac{y_1 - y_2}{\\sigma}, \\;\\; \\text{therefore} \\\\ \\sigma &amp; = \\frac{y_1 - y_2}{z_1 – z_2}. \\end{align*}\\] If you’d like to compute \\(\\mu\\), it’s even simpler. \\[\\begin{align*} z_1 &amp; = \\frac{y_1 - \\mu}{\\sigma} \\\\ z_1 \\sigma &amp; = y_1 - \\mu \\\\ z_1 \\sigma + \\mu &amp; = y_1, \\;\\; \\text{therefore} \\\\ \\mu &amp; = y_1 - z_1 \\sigma \\end{align*}\\] Big shout out to my math-stats savvy friends academic twitter for the formulas, especially Ph.Demetri, Lukas Neugebauer, and Brenton Wiernik for walking the formulas out (see this twitter thread). For our application, Intercept[1] and Intercept[6] will be our two \\(z\\)-scores and Kruschke’s 1.5 and 6.5 will be their corresponding \\(y\\)-values. library(tidybayes) post %&gt;% select(iter, `b_Intercept[1]`, `b_Intercept[6]`) %&gt;% mutate(`y[1]` = 1.5, `y[6]` = 6.5) %&gt;% mutate(mu = `y[1]` - `b_Intercept[1]` * 1, sigma = (`y[1]` - `y[6]`) / (`b_Intercept[1]` - `b_Intercept[6]`)) %&gt;% mutate(`(mu-2)/sigma` = (mu - 2) / sigma) %&gt;% pivot_longer(mu:`(mu-2)/sigma`) %&gt;% mutate(name = factor(name, levels = c(&quot;mu&quot;, &quot;sigma&quot;, &quot;(mu-2)/sigma&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 50, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, labeller = label_parsed) Our results are similar to Kruschke’s. Given we used a different algorithm, a different parameterization, and different priors, I’m not terribly surprised. If you have more insight on the matter or have spotted a flaw in this method, please share with the rest of us. It’s unclear, to me, how we’d interpret the effect size. The difficulty isn’t that Kruschke’s comparison of \\(C = 2.0\\) is arbitrary, but that we can only interpret the comparison given the model assumption of \\(\\theta_1 = 1.5\\) and \\(\\theta_6 = 6.5\\). If your theory doesn’t allow you to understand the meaning of those constants and why you’d prefer them to slightly different ones, you’d be fooling yourself if you attempted to interpret any effect sizes conditional on those values. Proceed with caution. In the large paragraph on the lower part of page 679, Kruschke discussed why the thresholds tend to have nontrivial covariances. This is what he was trying to convey with the bottom subplot in Figure 23.2. If you’re curious, here is the correlation matrix among the thresholds. vcov(fit23.1, correlation = T) %&gt;% round(digits = 2) ## Intercept[1] Intercept[2] Intercept[3] Intercept[4] Intercept[5] Intercept[6] ## Intercept[1] 1.00 0.72 0.51 0.33 0.21 0.13 ## Intercept[2] 0.72 1.00 0.71 0.45 0.29 0.16 ## Intercept[3] 0.51 0.71 1.00 0.65 0.41 0.21 ## Intercept[4] 0.33 0.45 0.65 1.00 0.64 0.32 ## Intercept[5] 0.21 0.29 0.41 0.64 1.00 0.51 ## Intercept[6] 0.13 0.16 0.21 0.32 0.51 1.00 Kruschke didn’t do this in the text, but it might be informative to plot the probability distributions for the seven categories from Y (i.e., \\(p(y = k | \\mu = 0, \\sigma = 1, \\{ \\theta_i \\})\\)). library(tidybayes) post %&gt;% select(-iter) %&gt;% mutate_all(.funs = ~pnorm(. ,0, 1)) %&gt;% transmute(`p[Y==1]` = `b_Intercept[1]`, `p[Y==2]` = `b_Intercept[2]` - `b_Intercept[1]`, `p[Y==3]` = `b_Intercept[3]` - `b_Intercept[2]`, `p[Y==4]` = `b_Intercept[4]` - `b_Intercept[3]`, `p[Y==5]` = `b_Intercept[5]` - `b_Intercept[4]`, `p[Y==6]` = `b_Intercept[6]` - `b_Intercept[5]`, `p[Y==7]` = 1 - `b_Intercept[6]`) %&gt;% set_names(1:7) %&gt;% pivot_longer(everything(), names_to = &quot;Y&quot;) %&gt;% ggplot(aes(x = value, y = Y)) + geom_halfeyeh(point_interval = mode_hdi, .width = .95, size = 1/2) + xlab(expression(italic(p)*&quot;[&quot;*Y==italic(i)*&quot;]&quot;)) + coord_cartesian(xlim = c(0, 1)) + theme(panel.grid = element_blank()) Happily, the model produces data that look a lot like those from which it was generated. set.seed(23) post %&gt;% mutate(z = rnorm(n(), mean = 0, sd = 1)) %&gt;% mutate(Y = case_when( z &lt; `b_Intercept[1]` ~ 1, z &lt; `b_Intercept[2]` ~ 2, z &lt; `b_Intercept[3]` ~ 3, z &lt; `b_Intercept[4]` ~ 4, z &lt; `b_Intercept[5]` ~ 5, z &lt; `b_Intercept[6]` ~ 6, z &gt;= `b_Intercept[6]` ~ 7 ) %&gt;% as.factor(.)) %&gt;% ggplot(aes(x = Y)) + geom_bar() + theme(panel.grid = element_blank()) Along similar lines, we can use the pp_check() function to make a version of the upper right panel of Figure 23.2. The type = &quot;bars&quot; argument will allow us to summarize the posterior predictions as a dot (mean) and standard error bars superimposed on a bar plot of the original data. Note how this differs a little from Kruschke’s use of the posterior median and 95% HDIs. The nsamples = 1000 argument controls how many posterior predictions we wanted to summarize over. The rest is just formatting. set.seed(23) pp_check(fit23.1, type = &quot;bars&quot;, nsamples = 1000) + scale_x_continuous(&quot;y&quot;, breaks = 1:7) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Data with posterior predictions&quot;, subtitle = &quot;N = 100&quot;) + theme(legend.background = element_blank(), legend.position = c(.9, .8), panel.grid = element_blank()) Load the data for the next model. my_data_2 &lt;- read_csv(&quot;data.R/OrdinalProbitData-1grp-2.csv&quot;) Since we’re reusing all the specifications from the last model for this one, we can just use update(). fit23.2 &lt;- update(fit23.1, newdata = my_data_2, iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.02&quot;) print(fit23.2) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Y ~ 1 ## Data: my_data_2 (Number of observations: 70) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -1.41 0.22 -1.85 -1.00 1.00 5467 5039 ## Intercept[2] -0.18 0.15 -0.47 0.12 1.00 9230 6898 ## Intercept[3] 0.17 0.15 -0.12 0.46 1.00 9458 6989 ## Intercept[4] 0.46 0.16 0.15 0.76 1.00 8971 6719 ## Intercept[5] 0.83 0.17 0.50 1.16 1.00 8841 6441 ## Intercept[6] 2.00 0.31 1.44 2.66 1.00 9194 6731 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract and wrangle the posterior draws. post &lt;- posterior_samples(fit23.2) %&gt;% select(`b_Intercept[1]`:`b_Intercept[6]`) %&gt;% mutate(iter = 1:n()) Now we might compare the brms parameterization of \\(\\theta_{[i]}\\) with Kruschke’s parameterization in an expanded version of the bottom plot of Figure 23.3. As we’ll be making a lot of these plots throughout this chapter, it might be worthwhile to just make a custom function. We’ll call it compare_thresholds(). compare_thresholds &lt;- function(data, lb = 1.5, ub = 6.5) { # we have two parameters: # lb = lower bound # ub = upper bound # primary data wrangling p &lt;- bind_rows( data %&gt;% gather(name, threshold, -iter) %&gt;% group_by(iter) %&gt;% mutate(theta_bar = mean(threshold)), data %&gt;% gather(name, threshold, -iter) %&gt;% group_by(iter) %&gt;% mutate(threshold = scales::rescale(threshold, to = c(lb, ub))) %&gt;% mutate(theta_bar = mean(threshold)) ) %&gt;% mutate(model = rep(c(&quot;brms parameterization&quot;, &quot;Kruschke&#39;s parameterization&quot;), each = n() / 2)) # compute the means by model and threshold for the vertical lines means &lt;- p %&gt;% ungroup() %&gt;% group_by(model, name) %&gt;% summarise(mean = mean(threshold)) # plot! p %&gt;% ggplot(aes(x = threshold, y = theta_bar)) + geom_vline(data = means, aes(xintercept = mean, color = name), linetype = 2) + geom_point(aes(color = name), alpha = 1/10, size = 1/2) + scale_color_viridis_d(option = &quot;D&quot;, end = .85, direction = -1) + ylab(&quot;mean threshold&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~model, ncol = 1, scales = &quot;free&quot;) } Take that puppy for a spin. post %&gt;% compare_thresholds(lb = 1.5, ub = 6.5) Oh man, that works sweet. Now let’s use the same parameter-transformation approach from before to get our un-standardized posteriors for \\(\\mu\\), \\(\\sigma\\), and the effect size. post %&gt;% select(iter, `b_Intercept[1]`, `b_Intercept[6]`) %&gt;% mutate(`y[1]` = 1.5, `y[6]` = 6.5) %&gt;% mutate(mu = `y[1]` - `b_Intercept[1]` * 1, sigma = (`y[1]` - `y[6]`) / (`b_Intercept[1]` - `b_Intercept[6]`)) %&gt;% mutate(`(mu-4)/sigma` = (mu - 4) / sigma) %&gt;% pivot_longer(mu:`(mu-4)/sigma`) %&gt;% mutate(name = factor(name, levels = c(&quot;mu&quot;, &quot;sigma&quot;, &quot;(mu-4)/sigma&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 50, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, labeller = label_parsed) Use pp_check() to make our version of the upper-right panel of Figure 23.3. set.seed(23) pp_check(fit23.2, type = &quot;bars&quot;, nsamples = 1000) + scale_x_continuous(&quot;y&quot;, breaks = 1:7) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Data with posterior predictions&quot;, subtitle = &quot;N = 70&quot;) + theme(legend.background = element_blank(), legend.position = c(.9, .8), panel.grid = element_blank()) Just as in the text, “the posterior predictive distribution in the top-right subpanel accurately describes the bimodal distribution of the outcomes” (p. 680). Here are the probability distributions for each of the 7 categories of Y. post %&gt;% select(-iter) %&gt;% mutate_all(.funs = ~pnorm(. ,0, 1)) %&gt;% transmute(`p[Y==1]` = `b_Intercept[1]`, `p[Y==2]` = `b_Intercept[2]` - `b_Intercept[1]`, `p[Y==3]` = `b_Intercept[3]` - `b_Intercept[2]`, `p[Y==4]` = `b_Intercept[4]` - `b_Intercept[3]`, `p[Y==5]` = `b_Intercept[5]` - `b_Intercept[4]`, `p[Y==6]` = `b_Intercept[6]` - `b_Intercept[5]`, `p[Y==7]` = 1 - `b_Intercept[6]`) %&gt;% set_names(1:7) %&gt;% pivot_longer(everything(), names_to = &quot;Y&quot;) %&gt;% ggplot(aes(x = value, y = Y)) + geom_halfeyeh(point_interval = mode_hdi, .width = .95, size = 1/2) + xlab(expression(italic(p)*&quot;[&quot;*Y==italic(i)*&quot;]&quot;)) + coord_cartesian(xlim = c(0, 1)) + theme(panel.grid = element_blank()) Before we move on, it might be helpful to nail down what the thresholds mean within the context of our brms parameterization. To keep things simple, we’ll focus on their posterior means. tibble(x = seq(from = -3.5, to = 3.5, by = .01)) %&gt;% mutate(d = dnorm(x)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = &quot;grey67&quot;) + geom_vline(xintercept = fixef(fit23.2)[, 1], color = &quot;grey92&quot;, linetype = 3) + scale_x_continuous(NULL, breaks = fixef(fit23.2)[, 1], labels = parse(text = str_c(&quot;theta[&quot;, 1:6, &quot;]&quot;))) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Standard normal distribution underlying the ordinal Y data:&quot;, subtitle = &quot;The dashed vertical lines mark the posterior means for the thresholds.&quot;) + coord_cartesian(xlim = c(-3, 3)) + theme(panel.grid = element_blank()) Compare that to Figure 23.1. 23.2.2.1 Not the same results as pretending the data are metric. “In some conventional approaches to ordinal data, the data are treated as if they were metric and normally distributed” (p. 681). Here’s what that brms::brm() model might look like using methods from back in Chapter 16. First, we’ll define our stanvars. mean_y &lt;- mean(my_data_1$Y) sd_y &lt;- sd(my_data_1$Y) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) Fit the model. fit23.3 &lt;- brm(data = my_data_1, family = gaussian, Y ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.03&quot;) Check the results. print(fit23.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Y ~ 1 ## Data: my_data_1 (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.95 0.14 1.68 2.23 1.00 3377 2534 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.41 0.10 1.23 1.63 1.00 3141 2417 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As Kruschke indicated in the text, it yielded a distributional mean of about 1.95 and a standard deviation of about 1.41. Here we’ll use a posterior predictive check to compare histograms of data generated from this model to that of the original data. pp_check(fit23.3, type = &quot;hist&quot;, nsamples = 10, binwidth = 1) + scale_x_continuous(breaks = seq(from = -3, to = 7, by = 2)) + theme(legend.position = c(.9, .15), panel.grid = element_blank()) Yeah, that’s not a good fit. We won’t be conducting a \\(t\\)-test like Kruschke did on page 681. But we might compromise and take a look at the marginal distribution of the intercept (i.e., for \\(\\mu\\)) and its difference from 2, the reference value. posterior_samples(fit23.3) %&gt;% mutate(`2 - b_Intercept` = 2 - b_Intercept, `effect size` = (2 - b_Intercept) / sigma) %&gt;% pivot_longer(-c(sigma, b_Intercept, lp__)) %&gt;% ggplot(aes(x = value, y = 0)) + geom_halfeyeh(point_interval = mode_hdi, .width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) Yes indeed, 2 is a credible value for the intercept. And as reported in the text, we got a very small \\(d\\) effect size. Now we repeat the process for the second data set. mean_y &lt;- mean(my_data_2$Y) sd_y &lt;- sd(my_data_2$Y) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) fit23.4 &lt;- update(fit23.3, newdata = my_data_2, chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.04&quot;) Let’s just jump to the plot. This time we’re comparing the b_Intercept to the value of 4.0. posterior_samples(fit23.4) %&gt;% mutate(`2 - b_Intercept` = 4 - b_Intercept, `effect size` = (4 - b_Intercept) / sigma) %&gt;% pivot_longer(-c(sigma, b_Intercept, lp__)) %&gt;% ggplot(aes(x = value, y = 0)) + geom_halfeyeh(point_interval = mode_hdi, .width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) As in the text, our \\(d\\) is centered around 0.3. Let’s use a posterior predictive check to see how well fit23.4 summarized these data. pp_check(fit23.4, type = &quot;hist&quot;, nsamples = 10, binwidth = 1) + scale_x_continuous(breaks = seq(from = -3, to = 7, by = 2)) + theme_grey() + theme(legend.position = c(.9, .15), panel.grid = element_blank()) The histograms aren’t as awful as the ones for the previous model. But they’re still not great. We might further inspect the model misspecification with a cumulative distribution function overlay, this time comparing fit23.2 directly to fit23.4. p1 &lt;- pp_check(fit23.2, type = &quot;ecdf_overlay&quot;, nsamples = 50) + ggtitle(&quot;Cumulative-normal (fit23.2)&quot;) p2 &lt;- pp_check(fit23.4, type = &quot;ecdf_overlay&quot;, nsamples = 50) + ggtitle(&quot;Conventional-normal (fit23.4)&quot;) (p1 + p2 &amp; scale_x_continuous(breaks = 0:7, limits = c(0, 7)) &amp; theme_grey() &amp; theme(panel.grid = element_blank(), title = element_text(size = 10.5))) + plot_layout(guides = &#39;collect&#39;) “Which of the analyses yields the more trustworthy conclusion? The one that describes the data better. In these cases, there is no doubt that the cumulative-normal model is the better description of the data” than the conventional Gaussian model (p. 682). 23.2.2.2 Ordinal outcomes versus Likert scales. Just for fun, rate how much you agree with the statement, “Bayesian estimation is more informative than null-hypothesis significance testing,” by selecting one option from the following: 1 = strongly disagree; 2 = disagree; 3 = undecided; 4 = agree; 5 = strongly agree. This sort of ordinal response interface is often called a Likert-type response (Likert, 1932), pronounced LICK-ert not LIKE-ert). Sometimes, it is called a Likert “scale” but the term “scale” in this context is more properly reserved for referring to an underlying metric variable that is indicated by the arithmetic mean of several meaningfully related Likert-type responses (e.g., Carifio &amp; Perla, 2007, 2008; Norman, 2010). (p. 681) Kruschke then briefly introduced how one might combine several such meaningfully-related Likert-type responses with latent variable methods. He then clarified this text will not explore that approach, further. The current version of brms (i.e., 2.12.0) has very limited latent variable capacities. However, they are in the works. Interested modelers can follow Bürkner’s progress in GitHub issue #304. He also has a (2020a) paper on how one might use brms to fit item response theory models, which can be viewed as a special family of latent variable models. One can also fit Bayesian latent variable models with the blavaan package. 23.3 The case of two groups In both examples in the preceding text, the two groups of outcomes were on the same ordinal scale. In the first example, both questionnaire statements were answered on the same disagree–agree scale. In the second example, both groups responded on the same very unhappy–very happy scale. Therefore, we assume that both groups have the same underlying metric variable with the same thresholds. (p. 682) 23.3.1 Implementation in JAGS brms. The brm() syntax for adding a single categorical predictor to an ordered-probit model is much like that for any other likelihood. We just add the variable name to the right side of the ~ in the formula argument. If you’re like me and like to use the verbose 1 syntax for your model intercepts–thresholds in these models–just use the + operator between them. For example, this is what it’d look like for an ordered-categorical criterion y and a single categorical predictor x. fit &lt;- brm(data = my_data, family = cumulative(probit), y ~ 1 + x, prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b))) Also of note, we’ve expanded the prior section to include a line for class = b. As with the thresholds, interpret this prior through the context of the underlying standard normal cumulative distribution, \\(\\Phi(z)\\). Note the interpretation, though. By brms defaults, the underlying Gaussian for the reference category of x will be \\(\\operatorname{Normal} (0, 1)\\). Thus whatever parameter value you get for the other categories in x, those will be standardized mean differences, making them a kind of effect size. Note, the above all presumes you’re only interested in comparing means between groups. Things get more complicated if you want groups to vary by \\(\\sigma\\), too. Hold on tight! First, look back at the output from print(fit1) or print(fit2). The second line for both reads: Links: mu = probit; disc = identity. Hopefully the mu = probit part is no surprise. Probit regression is the primary focus of this chapter. But check out the disc = identity part and notice that nowhere in there is there any mention of sigma = identity like we get when treating the criterion as metric as in conventional Gaussian models (i.e., execute print(fit3) or print(fit4)). Yes, there is a relationship between disc and sigma. disc is shorthand for discrimination. The term comes from the item response theory (IRT) literature and discrimination is the inverse of \\(\\sigma\\) (see Bürkner’s Bayesian item response modelling in R with brms and Stan). In IRT, discrimination is often denoted \\(a\\) or \\(\\alpha\\). Here I’ll adopt the latter, making \\(\\sigma = 1 / \\alpha\\). But focusing back on brms summary output, notice how both disc and sigma are modeled using the identity link. If you recall from earlier chapters, we switched to the log link to constrain the values to zero and above when we allowed \\(\\sigma\\) to vary across groups. It’s the same thing for our discrimination parameter, \\(\\alpha\\). Because \\(\\alpha\\) should always be zero or above, brms defaults to the log link when modeling it with predictors. As with \\(\\sigma\\) in conventional Gaussian models, we’ll be using some version of the bf() syntax when modeling the discrimination parameter in brms. For a general introduction to what Bürkner calls distributional modeling, see his (2020c) vignette, Estimating distributional models with brms. In the case of the discrimination parameter for the cumulative model, we’ll want more focused instructions. Happily, Bürkner &amp; Vuorre (2019) have our backs. We read: Conceptually, unequal variances are incorporated in the model by specifying an additional regression formula for the variance component of the latent variable \\(\\tilde Y\\). In brms, the parameter related to latent variances is called disc (short for “discrimination”), following conventions in item response theory. Note that disc is not the variance itself, but the inverse of the standard deviation, \\(s.\\) That is, \\(s = 1/ \\text{disc}\\). Further, because disc must be strictly positive, it is by default modeled on the log scale. Predicting auxiliary parameters (parameters of the distribution other than the mean/location) in brms is accomplished by passing multiple regression formulas to the brm() function. Each formula must first be wrapped in another function, bf() or lf() (for “linear formula”)–depending on whether it is a main or an auxiliary formula, respectively. The formulas are then combined and passed to the formula argument of brm(). Because the standard deviation of the latent variable is fixed to 1 for the baseline [group, disc cannot be estimated for the baseline group]. We must therefore ensure that disc is estimated only for [non-baseline groups]. To do so, we omit the intercept from the model of disc by writing 0 + ... on the right-hand side of the regression formula. By default, R applies cell-mean coding to factors in formulas without an intercept. That would lead to disc being estimated for [all groups], so we must deactivate it via the cmc argument of lf(). (pp. 11–12) Here’s what that might look like. fit &lt;- brm(data = my_data, family = cumulative(probit), bf(y ~ 1 + x) + lf(disc ~ 0 + x, cmc = F), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(normal(0, 4), class = b, dpar = disc))) Note how when using the disc ~ 0 + ... syntax, the disc parameters are of class = b within the prior() function. If you’d like to assign them priors differing from the other b parameters, you’ll need to specify dpar = disc. Again, though the mean structure for this model is on the probit scale, the discrimination structure is on the log scale. Recalling that \\(\\sigma = 1/\\alpha\\), which means \\(\\alpha = 1/\\sigma\\), and also that we’re modeling \\(\\log (\\alpha)\\), the priors for the standard deviations of the non-reference category groups are on the scale of \\(\\log (1 / \\sigma)\\). To get a better sense of how one might set a prior on such a scale, we might compare \\(\\sigma\\), \\(\\alpha\\), and \\(\\log (\\alpha)\\). Here are the density and cumulative density functions for \\(\\operatorname{Normal} (0, 0.5)\\), \\(\\operatorname{Normal} (0, 1)\\), and \\(\\operatorname{Normal} (0, 2)\\). tibble(mu = 0, sigma = c(0.5, 1, 2)) %&gt;% expand(nesting(mu, sigma), y = seq(from = -5, to = 5, by = 0.1)) %&gt;% mutate(`p(y)` = dnorm(y, mu, sigma), `Phi(y)` = pnorm(y, mu, sigma)) %&gt;% mutate(alpha = 1 / sigma, loga = log(1 / sigma)) %&gt;% mutate(label = str_c(&quot;list(sigma==&quot;, sigma, &quot;,alpha==&quot;, alpha, &quot;,log(alpha)==&quot;, round(loga, 2), &quot;)&quot;)) %&gt;% pivot_longer(`p(y)`:`Phi(y)`) %&gt;% ggplot(aes(x = y, y = value)) + geom_line(size = 1.5, color = &quot;grey50&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + coord_cartesian(xlim = c(-4, 4)) + theme(panel.grid = element_blank()) + facet_grid(name~label, labeller = label_parsed, switch = &quot;y&quot;) Put another way, here’s how \\(\\alpha\\) and \\(\\log (\\alpha)\\) scale on values of \\(\\sigma\\) ranging from 0.0001 to 10. tibble(sigma = seq(from = 0.0001, to = 10, by = 0.01)) %&gt;% mutate(alpha = 1 / sigma, `log(alpha)` = log(1 / sigma)) %&gt;% pivot_longer(-sigma, names_to = &quot;labels&quot;) %&gt;% ggplot(aes(x = sigma, y = value)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_line(size = 1.5, color = &quot;grey50&quot;) + coord_cartesian(ylim = c(-2, 10)) + theme(panel.grid = element_blank()) + facet_grid(~labels, labeller = label_parsed) When \\(\\sigma\\) goes below 1, both explode upward. As \\(\\sigma\\) increases, \\(\\alpha\\) asymptotes at zero and \\(\\log (\\alpha)\\) slowly descends below zero. Put another way, here is how \\(\\sigma\\) scales as a function of \\(\\log (\\alpha)\\). tibble(`log(alpha)` = seq(from = -3, to = 3, by = 0.01)) %&gt;% mutate(sigma = 1 / exp(`log(alpha)`)) %&gt;% ggplot(aes(x = `log(alpha)`, y = sigma)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_line(size = 1.5, color = &quot;grey50&quot;) + labs(x = expression(log(alpha)), y = expression(sigma)) + coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 10)) + theme(panel.grid = element_blank()) In the context where the underlying distribution for the reference category will be the standard normal, it seems like a \\(\\operatorname{Normal} (0, 1)\\) prior would be fairly permissive for \\(\\log (\\alpha)\\). This is what I will use going forward. Choose your priors with care. 23.3.2 Examples: Not funny. Load the data for the next model. my_data &lt;- read_csv(&quot;data.R/OrdinalProbitData1.csv&quot;) glimpse(my_data) ## Rows: 88 ## Columns: 2 ## $ X &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A… ## $ Y &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2… Fit the first ordinal probit model with group-specific \\(\\mu\\) and \\(\\sigma\\) values for the underlying normal distributions for the ordinal variable Y. fit23.5 &lt;- brm(data = my_data, family = cumulative(probit), bf(Y ~ 1 + X) + lf(disc ~ 0 + X, cmc = FALSE), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(normal(0, 1), class = b, dpar = disc)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.05&quot;) Look over the summary. print(fit23.5) ## Family: cumulative ## Links: mu = probit; disc = log ## Formula: Y ~ 1 + X ## disc ~ 0 + X ## Data: my_data (Number of observations: 88) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] 0.49 0.20 0.11 0.89 1.00 9299 6066 ## Intercept[2] 1.30 0.23 0.85 1.76 1.00 10487 5485 ## Intercept[3] 2.20 0.38 1.54 3.01 1.00 5559 5620 ## Intercept[4] 3.43 0.83 2.16 5.42 1.00 4253 5610 ## XB 0.44 0.34 -0.28 1.05 1.00 4716 3738 ## disc_XB -0.31 0.28 -0.86 0.22 1.00 3433 4450 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Because our fancy new parameter disc_XB is on the \\(\\log (\\alpha)\\) scale, we can convert it to the \\(\\sigma\\) scale with \\(\\frac{1}{\\exp (\\log \\alpha)}\\). For a quick and dirty example, here it is with the posterior mean. 1 / (exp(fixef(fit23.5)[&quot;disc_XB&quot;, 1])) ## [1] 1.368896 Before we follow along with Kruschke, let’s hammer the meaning of these model parameters home. Here is a density plot of the two underlying latent distributions for Y, given X. We’ll throw in the thresholds for good measure. To keep things simple, we’ll just express the distributions in terms of the posterior means of each parameter. tibble(X = LETTERS[1:2], mu = c(0, fixef(fit23.5)[&quot;XB&quot;, 1]), sigma = c(1, 1 / (exp(fixef(fit23.5)[&quot;disc_XB&quot;, 1])))) %&gt;% expand(nesting(X, mu, sigma), y = seq(from = -5, to = 5, by = 0.1)) %&gt;% mutate(d = dnorm(y, mu, sigma)) %&gt;% ggplot(aes(x = y, ymin = 0, ymax = d)) + geom_ribbon(aes(fill = X), alpha = 2/3) + geom_vline(xintercept = fixef(fit23.5)[1:4, 1], color = &quot;grey75&quot;, linetype = 3) + scale_x_continuous(sec.axis = dup_axis(breaks = fixef(fit23.5)[1:4, 1] %&gt;% as.double(), labels = parse(text = str_c(&quot;theta[&quot;, 1:4, &quot;]&quot;)))) + scale_y_continuous(NULL, breaks = NULL) + scale_fill_viridis_d(option = &quot;A&quot;, begin = .33, end = .67) + labs(title = &quot;Underlying latent scale for Y, given X&quot;, x = NULL) + theme(panel.grid = element_blank(), axis.ticks.x.top = element_blank()) Returning to our previous workflow, extract the posterior draws and wrangle. post &lt;- posterior_samples(fit23.5) %&gt;% mutate(iter = 1:n()) glimpse(post) ## Rows: 8,000 ## Columns: 8 ## $ `b_Intercept[1]` &lt;dbl&gt; 0.367300818, 0.382564379, 0.409425986, 0.288490883, 0.008978847, 0.643486264, 0.76… ## $ `b_Intercept[2]` &lt;dbl&gt; 1.3844545, 1.0726888, 1.1754398, 0.9113326, 0.9676571, 1.3374801, 1.4323809, 1.501… ## $ `b_Intercept[3]` &lt;dbl&gt; 2.242209, 2.073569, 2.091679, 1.993377, 1.911539, 2.290516, 2.281980, 2.353368, 2.… ## $ `b_Intercept[4]` &lt;dbl&gt; 3.394341, 3.393257, 3.350974, 2.801191, 2.636229, 3.168556, 3.780114, 4.075613, 4.… ## $ b_XB &lt;dbl&gt; 0.67918461, 0.22528956, 0.50615392, 0.01511700, -0.71507783, -0.07491061, 0.151551… ## $ b_disc_XB &lt;dbl&gt; -0.66137007, -0.49036663, -0.05895237, -0.41540676, -0.56076002, -0.58665895, -0.4… ## $ lp__ &lt;dbl&gt; -109.8339, -107.9836, -107.6560, -109.3514, -113.1869, -110.4044, -110.1080, -110.… ## $ iter &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,… Now, let’s use our handy compare_thresholds() function to make an expanded version of the lower-left plot of Figure 23.4. post %&gt;% select(`b_Intercept[1]`:`b_Intercept[4]`, iter) %&gt;% compare_thresholds(lb = 1.5, ub = 4.5) It will no longer be straightforward to use the formulas from 23.2.2 to convert the output from our brms parameterization to match the way Kruschke parameterized his conditional means and standard deviations. I will leave the conversion up to the interested reader. Going forward, we will focus on the output from our brms parameterization. post %&gt;% # simple parameters mutate(`mu[A]` = 0, `mu[B]` = b_XB, `sigma[A]` = 1, `sigma[B]` = 1 / exp(b_disc_XB)) %&gt;% # simple differences mutate(`mu[B]-mu[A]` = `mu[B]` - `mu[A]`, `sigma[B]-sigma[A]` = `sigma[B]` - `sigma[A]`) %&gt;% # effect size mutate(`(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)` = (`mu[B]-mu[A]`) / sqrt((`sigma[A]`^2 + `sigma[B]`^2) / 2)) %&gt;% # wrangle pivot_longer(`mu[A]`:`(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)`) %&gt;% mutate(name = factor(name, levels = c(&quot;mu[A]&quot;, &quot;mu[B]&quot;, &quot;mu[B]-mu[A]&quot;, &quot;sigma[A]&quot;, &quot;sigma[B]&quot;, &quot;sigma[B]-sigma[A]&quot;, &quot;(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)&quot;))) %&gt;% # plot ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 50, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, labeller = label_parsed) \\(\\mu_A\\) and \\(\\sigma_A\\) are both constants, which doesn’t show up well with our geom_histogram() approach with the scales freed across facets. If these plots really mattered for a scientific presentation or something for industry, you could experiment using either a common scale across all facets, or making the plots individually and then combining them with patchwork syntax. Returning to interpretation, because \\(\\mu_A = 0\\), it turns out that \\(\\mu_B - \\mu_A = \\mu_B\\), which is on display on the top row. Because \\(\\sigma_A = 1\\), it turns out that \\(\\sigma_B - \\sigma_A\\) is just \\(\\sigma_B\\) moved over one unit to the left, which is hopefully clear in the panels of the second row. Very happily, the effect size formula worked with our brms parameters the same way it did for Kruschke’s. Both yield an effect size of about 0.5, with 95% intervals extending about \\(\\mp 0.5\\). Here we make good use of the type = &quot;bars_grouped&quot; and group = &quot;X&quot; arguments to make the posterior predictive plots at the top right of Figure 23.4 with the brms::pp_check() function. set.seed(23) pp_check(fit23.5, type = &quot;bars_grouped&quot;, nsamples = 100, group = &quot;X&quot;) + scale_x_continuous(&quot;y&quot;, breaks = 1:7) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Data with posterior predictions&quot;, subtitle = expression(list(italic(N[A])==44, italic(N[B])==44))) + theme(legend.background = element_blank(), legend.position = c(.9, .75), panel.grid = element_blank()) Using more tricks from back in Chapter 16, here’s the corresponding conventional Gaussian model for metric data. mean_y &lt;- mean(my_data$Y) sd_y &lt;- sd(my_data$Y) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) fit23.6 &lt;- brm(data = my_data, family = gaussian, bf(Y ~ 0 + X, sigma ~ 0 + X), prior = c(prior(normal(mean_y, sd_y * 100), class = b), prior(normal(0, 1), class = b, dpar = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.06&quot;) Check the summary. print(fit23.6) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: Y ~ 0 + X ## sigma ~ 0 + X ## Data: my_data (Number of observations: 88) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## XA 1.43 0.12 1.19 1.66 1.00 3892 3041 ## XB 1.86 0.16 1.55 2.18 1.00 4238 2770 ## sigma_XA -0.26 0.11 -0.45 -0.03 1.00 3664 2789 ## sigma_XB 0.08 0.11 -0.13 0.30 1.00 4071 2889 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here are the marginal posteriors, including the effect size. posterior_samples(fit23.6) %&gt;% mutate(`A mean` = b_XA, `B mean` = b_XB, `A Std. Dev.` = exp(b_sigma_XA), `B Std. Dev.` = exp(b_sigma_XB)) %&gt;% mutate(`Difference of Means` = `B mean` - `A mean`, `Difference of Std. Devs` = `B Std. Dev.` - `A Std. Dev.`) %&gt;% mutate(`Effect Size` = `Difference of Means` / ((`A Std. Dev.` + `B Std. Dev.`) / 2)) %&gt;% pivot_longer(`A mean`:`Effect Size`) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 50, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;These are based on the conventional Gaussian model, NOT the cumulative-normal\\nmodel Kruschke displayed in Figure 23.4&quot;, x = &quot;Marginal posterior&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, ncol = 2) Compare those results to those Kruschke reported from an NHST analysis in the note below Figure 23.4: \\(M_1 = 1.43, M_2 = 1.86, t = 2.18, p = 0.032\\), with effect size \\(d = 0.466\\) with 95% CI of \\(0.036-0.895\\). An \\(F\\) test of the variances concludes that the standard deviations are significantly different: \\(S_1 = 0.76, S_2 = 1.07, p = 0.027\\). Notice in this case that treating the values as metric greatly underestimates their variances, as well as erroneously concludes the variances are different. (p. 684) As to the data in the analyses Kruschke reported in Figure 23.5 and the prose in the second paragraph on page 685, I’m not aware that Kruschke provided them. From his footnote #2, we read: “Data in Figure 23.5 are from an as-yet unpublished study I conducted with the collaboration of Allison Vollmer as part of her undergraduate honors project.” In place of the real data, I eyeballed the values based on the upper-right panels in Figure 23.5. Here they are. d &lt;- tibble(x = rep(str_c(&quot;joke &quot;, c(1, 6)), each = 177), y = c(rep(1:7, times = c(95, 19, 18, 10, 17, 10, 8)), rep(1:7, times = c(53, 33, 31, 22, 23, 14, 1)))) glimpse(d) ## Rows: 354 ## Columns: 2 ## $ x &lt;chr&gt; &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1&quot;, &quot;joke 1… ## $ y &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… My approximation to Kruschke’s data looks like this. d %&gt;% ggplot(aes(x = y)) + geom_bar() + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~x, ncol = 1) Here we fit the cumulative-normal model based on our version of the data, continuing to allow both \\(\\mu\\) and \\(\\sigma\\) (i.e., \\(1 / \\exp(\\log \\alpha)\\)) to differ across groups. fit23.7 &lt;- brm(data = d, family = cumulative(probit), bf(y ~ 1 + x) + lf(disc ~ 0 + x, cmc = FALSE), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(normal(0, 1), class = b, dpar = disc)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.07&quot;) Check the model summary. print(fit23.7) ## Family: cumulative ## Links: mu = probit; disc = log ## Formula: y ~ 1 + x ## disc ~ 0 + x ## Data: d (Number of observations: 354) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] 0.07 0.09 -0.11 0.26 1.00 5213 5865 ## Intercept[2] 0.37 0.09 0.19 0.54 1.00 7007 6115 ## Intercept[3] 0.65 0.09 0.48 0.83 1.00 8519 6245 ## Intercept[4] 0.87 0.10 0.69 1.06 1.00 8483 6724 ## Intercept[5] 1.26 0.12 1.04 1.49 1.00 6803 6073 ## Intercept[6] 1.81 0.16 1.51 2.15 1.00 6934 5574 ## xjoke6 0.39 0.10 0.20 0.59 1.00 7524 6368 ## disc_xjoke6 0.49 0.12 0.26 0.72 1.00 4006 5322 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Save and wrangle the posterior draws, then use our compare_thresholds() function to compare the brms parameterization of \\(\\theta_{[i]}\\) with the parameterization in the text in an expanded version of the lower-left plot of Figure 23.5. post &lt;- posterior_samples(fit23.7) %&gt;% mutate(iter = 1:n()) post %&gt;% select(`b_Intercept[1]`:`b_Intercept[6]`, iter) %&gt;% compare_thresholds(lb = 1.5, ub = 6.5) Given our data are only approximations of Kruschke’s, I think we did pretty good. Here are the histograms for our brms versions of the \\(\\mu\\)- and \\(\\sigma\\)-related parameters. post %&gt;% transmute(`mu[Joke~1]` = 0, `mu[Joke~6]` = b_xjoke6, `sigma[Joke~1]` = 1, `sigma[Joke~6]` = 1 / exp(b_disc_xjoke6)) %&gt;% mutate(`mu[Joke~6]-mu[Joke~1]` = `mu[Joke~6]` - `mu[Joke~1]`, `sigma[Joke~6]-sigma[Joke~1]` = `sigma[Joke~6]` - `sigma[Joke~1]`) %&gt;% mutate(`(mu[Joke~6]-mu[Joke~1])/sqrt((sigma[Joke~1]^2+sigma[Joke~6]^2)/2)` = (`mu[Joke~6]-mu[Joke~1]`) / sqrt((`sigma[Joke~1]`^2 + `sigma[Joke~6]`^2) / 2)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;mu[Joke~1]&quot;, &quot;mu[Joke~6]&quot;, &quot;mu[Joke~6]-mu[Joke~1]&quot;, &quot;sigma[Joke~1]&quot;, &quot;sigma[Joke~6]&quot;, &quot;sigma[Joke~6]-sigma[Joke~1]&quot;, &quot;(mu[Joke~6]-mu[Joke~1])/sqrt((sigma[Joke~1]^2+sigma[Joke~6]^2)/2)&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, labeller = label_parsed) Here are our versions of the two panels in the upper right of Figure 23.5. set.seed(23) pp_check(fit23.7, type = &quot;bars_grouped&quot;, nsamples = 100, group = &quot;x&quot;) + scale_x_continuous(&quot;y&quot;, breaks = 1:7) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Data with posterior predictions&quot;, subtitle = expression(list(italic(N[&quot;joke &quot;*1])==177, italic(N[&quot;joke &quot;*6])==177))) + theme_grey() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Now here’s the corresponding model where we treat the y data as metric. mean_y &lt;- mean(d$y) sd_y &lt;- sd(d$y) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) fit23.8 &lt;- brm(data = d, family = gaussian, bf(y ~ 0 + x, sigma ~ 0 + x), prior = c(prior(normal(mean_y, sd_y * 100), class = b), prior(normal(0, exp(sd_y)), class = b, dpar = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.08&quot;) Check the summary. print(fit23.8) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: y ~ 0 + x ## sigma ~ 0 + x ## Data: d (Number of observations: 354) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## xjoke1 2.42 0.14 2.13 2.71 1.00 4069 2870 ## xjoke6 2.86 0.13 2.60 3.11 1.00 4204 2955 ## sigma_xjoke1 0.64 0.05 0.54 0.75 1.00 4299 2812 ## sigma_xjoke6 0.52 0.05 0.42 0.63 1.00 4529 2910 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Make the marginal posteriors, including the effect size. posterior_samples(fit23.8) %&gt;% mutate(`Joke 1 Mean` = b_xjoke1, `Joke 6 Mean` = b_xjoke6, `Joke 1 Std. Dev.` = exp(b_sigma_xjoke1), `Joke 6 Std. Dev.` = exp(b_sigma_xjoke6)) %&gt;% mutate(`Difference of Means` = `Joke 6 Mean` - `Joke 1 Mean`, `Difference of Std. Devs` = `Joke 6 Std. Dev.` - `Joke 1 Std. Dev.`) %&gt;% mutate(`Effect Size` = `Difference of Means` / ((`Joke 1 Std. Dev.` + `Joke 6 Std. Dev.`) / 2)) %&gt;% pivot_longer(`Joke 1 Mean`:`Effect Size`) %&gt;% mutate(name = factor(name, levels = c(&quot;Joke 1 Mean&quot;, &quot;Joke 1 Std. Dev.&quot;, &quot;Joke 6 Mean&quot;, &quot;Joke 6 Std. Dev.&quot;, &quot;Difference of Means&quot;, &quot;Difference of Std. Devs&quot;, &quot;Effect Size&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 50, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;These are based on the conventional Gaussian model, NOT the cumulative-normal\\nmodel Kruschke displayed in Figure 23.5&quot;, x = &quot;Marginal posterior&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;, ncol = 2) If you think you have a better approximation of Kruschke’s data, please share. 23.4 The Case of metric predictors “This type of model is often referred to as ordinal probit regression or ordered probit regression because the probit function is the link function corresponding to the cumulative-normal inverse-link function” (p. 688, emphasis in the original). 23.4.1 Implementation in JAGS brms. This model is easy to specify in brms. Just make sure to think clearly about your priors. 23.4.2 Example: Happiness and money. Load the data for the next model. my_data &lt;- read_csv(&quot;data.R/OrdinalProbitData-LinReg-2.csv&quot;) glimpse(my_data) ## Rows: 200 ## Columns: 2 ## $ X &lt;dbl&gt; 1.386389, 1.223879, 1.454505, 1.112068, 1.222715, 1.545099, 1.360256, 1.533071, 1.501657, 1.42675… ## $ Y &lt;dbl&gt; 1, 1, 5, 5, 1, 4, 6, 2, 5, 4, 1, 4, 4, 4, 4, 6, 1, 1, 6, 2, 1, 7, 1, 3, 1, 1, 7, 5, 7, 1, 4, 6, 7… Take a quick look at the data. my_data %&gt;% ggplot(aes(x = X, y = Y)) + geom_point(alpha = 1/3) + scale_y_continuous(breaks = 1:7) + theme(panel.grid = element_blank()) Kruschke standardized his predictor within his model code. Here we’ll standardize X before fitting the model. my_data &lt;- my_data %&gt;% mutate(X_s = (X - mean(X)) / sd(X)) Fit the model. fit23.9 &lt;- brm(data = my_data, family = cumulative(probit), Y ~ 1 + X_s, prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.09&quot;) Check the summary. print(fit23.9) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Y ~ 1 + X_s ## Data: my_data (Number of observations: 200) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -1.18 0.12 -1.43 -0.94 1.00 6043 6040 ## Intercept[2] -0.76 0.11 -0.99 -0.54 1.00 7685 6760 ## Intercept[3] -0.29 0.11 -0.50 -0.08 1.00 9232 6766 ## Intercept[4] 0.25 0.11 0.04 0.46 1.00 9432 7184 ## Intercept[5] 0.73 0.11 0.51 0.96 1.00 8372 7269 ## Intercept[6] 1.26 0.13 1.02 1.52 1.00 8655 6940 ## X_s 1.16 0.10 0.97 1.35 1.00 7167 5953 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract the posterior draws and compare the brms parameterization of \\(\\theta_{[i]}\\) with the parameterization in the text in an expanded version of the bottom panel of Figure 23.7. post &lt;- posterior_samples(fit23.9) %&gt;% mutate(iter = 1:n()) post %&gt;% select(`b_Intercept[1]`:`b_Intercept[6]`, iter) %&gt;% compare_thresholds(lb = 1.5, ub = 6.5) Here’s the marginal distribution of b_X_s, our effect size for the number of jokes. post %&gt;% ggplot(aes(x = b_X_s, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 50, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) This differs from Kruschks’s \\(\\beta_1\\), which is in an unstandardized metric based on the parameters in his version of the model. But unlike the effect sizes from previous models, this one is not in a Cohen’s-\\(d\\) metric. Rather, this is a fully-standardized regression coefficient. As to the large subplot at the top of Figure 23.7, we can make something like it by nesting conditional_effects() within plot(). conditional_effects(fit23.9) %&gt;% plot(theme = theme(panel.grid = element_blank())) Here’s a more elaborated version of the same plot, this time depicting the model with 100 fitted lines randomly drawn from the posterior. set.seed(23) conditional_effects(fit23.9, spaghetti = TRUE, nsamples = 100) %&gt;% plot(points = T, point_args = c(alpha = 1/3), line_args = c(size = 0), theme = theme(panel.grid = element_blank())) ## Warning: Predictions are treated as continuous variables in &#39;conditional_effects&#39; by default which is likely ## invalid for ordinal families. Please set &#39;categorical&#39; to TRUE. Note the warning message. There was a similar one in the first plot, which I suppressed for simplicity sake. The message suggests treating the fitted lines as “continuous variables” might lead to a deceptive plot. Here’s what happens if we follow the suggestion. set.seed(23) conditional_effects(fit23.9, categorical = T) %&gt;% plot(theme = theme(panel.grid = element_blank())) Recall that our thresholds, \\(\\theta_1,...,\\theta_{K-1}\\), in conjunction with the standard normal density, give us the probability of a given Y value, given X_s (i.e., \\(p(y = k | \\mu, \\sigma, \\{ \\theta_j \\})\\), where \\(\\mu\\) is conditional on \\(x\\)). This plot returned the fitted lines of those conditional probabilities, each depicted by the posterior mean and percentile-based 95% intervals. At lower values of X_s, lower values of Y are more probable. At higher values of X_s, higher values of Y are more probable. It might be useful to get more practice in with this model. Here we’ll use fitted() to make a similar plot, depicting the model with may fitted lines instead of summary statistics. # how many fitted lines do you want? n_iter &lt;- 50 # define the `X_s` values you want to condition on # because the lines are nonlinear, you&#39;ll need many of them nd &lt;- tibble(X_s = seq(from = -2, to = 2, by = 0.05)) f &lt;- fitted(fit23.9, newdata = nd, summary = F, nsamples = n_iter) # inspect the output f %&gt;% str() ## num [1:50, 1:81, 1:7] 0.876 0.811 0.92 0.934 0.837 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : NULL ## ..$ : NULL ## ..$ : chr [1:7] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... Our output came in three dimensions. We have 50 rows, corresponding to n_iter &lt;- 50 (i.e., 50 posterior draws). There are 81 columns, based on how we defined the X_s values within our nd data (i.e., seq(from = -2, to = 2, by = 0.05)). The third dimension has seven levels, one corresponding to each of the seven levels of our criterion variable Y. Here’s a way to rearrange that output into a useful format for plotting. # rearrange the output rbind( f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5], f[, , 6], f[, , 7] ) %&gt;% # wrangle data.frame() %&gt;% set_names(nd %&gt;% pull(X_s)) %&gt;% mutate(iter = rep(1:n_iter, times = 7), rating = rep(1:7, each = n_iter)) %&gt;% pivot_longer(-c(iter, rating), names_to = &quot;X_s&quot;, values_to = &quot;probability&quot;) %&gt;% mutate(rating = str_c(&quot;Y: &quot;, rating), X_s = X_s %&gt;% as.double()) %&gt;% # plot ggplot(aes(x = X_s, y = probability, group = interaction(iter, rating), color = rating)) + geom_line(size = 1/4, alpha = 1/2) + scale_color_viridis_d(end = .85) + scale_y_continuous(breaks = c(0, .5, 1), limits = c(0, 1)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~rating, ncol = 7) So far, we’ve been plotting our model with the context of the default scale = &quot;response&quot; setting within fitted(). Within the context of the response variable Y, our model returns response probabilities. We can also look at the output within the context of scale = &quot;linear&quot;. We’ll plot these fitted lines across our nd values two different ways. For the first, p1, we’ll use summary statistics. For the second, p2, we’ll set summary = T. # adjust the nd nd &lt;- tibble(X_s = seq(from = -2.5, to = 2.5, by = 0.1)) # use summary statistics p1 &lt;- fitted(fit23.9, scale = &quot;linear&quot;, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = X_s, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_smooth(stat = &quot;identity&quot;, alpha = 1/2) + labs(title = &quot;summary statistics&quot;, y = &quot;underlying standard normal&quot;) # set `summary = F` set.seed(23) p2 &lt;- fitted(fit23.9, scale = &quot;linear&quot;, newdata = nd, summary = F, nsamples = n_iter) %&gt;% data.frame() %&gt;% set_names(nd %&gt;% pull(X_s)) %&gt;% mutate(iter = 1:n_iter) %&gt;% pivot_longer(-iter, names_to = &quot;X_s&quot;) %&gt;% mutate(X_s = X_s %&gt;% as.double()) %&gt;% ggplot(aes(x = X_s, y = value, group = iter)) + geom_line(alpha = 1/2, color = &quot;grey50&quot;) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;50 posterior draws&quot;) # combine and plot! p1 + p2 &amp; coord_cartesian(ylim = c(-2, 2)) &amp; theme(panel.grid = element_blank()) Both methods returned the fitted lines in the metric of the underlying standard normal distribution. The fitted lines are nonlinear in the metric of the raw data Y, but they’re linear in the metric of the presumed underlying distribution. If it helps, we’ll make a marginal plot of the standard normal distribution and tack it onto the right. # make Phi p3 &lt;- tibble(x = seq(from = -3, to = 3, by = .01)) %&gt;% mutate(d = dnorm(x)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = &quot;grey67&quot;) + # add the thresholds! geom_vline(xintercept = posterior_summary(fit23.9)[1:6, 1], color = &quot;grey92&quot;, linetype = 3) + # mark the thresholds with the axis breaks scale_x_reverse(NULL, breaks = fixef(fit23.9)[1:6, 1], labels = parse(text = str_c(&quot;theta[&quot;, 1:6, &quot;]&quot;))) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(italic(N)(0,1))) + coord_flip(xlim = c(-2, 2)) # combine, format a bit, and plot ( ((p1 | p2 ) &amp; geom_hline(yintercept = posterior_summary(fit23.9)[1:6, 1], color = &quot;grey75&quot;, linetype = 3) &amp; coord_cartesian(ylim = c(-2, 2)) | p3) &amp; theme(panel.grid = element_blank()) ) + plot_layout(widths = c(4, 4, 1)) The lines intersecting the plots are the posterior means for thresholds, \\(\\theta_1,...,\\theta_6\\). But these still aren’t faithful depictions of the top panel of Figure 23.7, you say. Okay, fine. One of the distinctive elements of that panel is the left-tilted bar-and-error plots. If you look closely at the vertical lines at their bases, you’ll see that the leftmost subplot starts at the minimum value of X and the rightmost subplot starts at the maximum value of X. Since our plots, so far, have been based on X_s, we’ll use the minimum and maximum values from that. Here are those values. (r &lt;- range(my_data$X_s)) ## [1] -1.774444 1.750168 To my eye, it appears that the three middle subplots are equally distributed between those at the ends. If we proceed under that assumption, here’s how we might use fitted() to get us rolling on computing the relevant values. nd &lt;- tibble(X_s = seq(from = r[1], to = r[2], length.out = 5)) f &lt;- fitted(fit23.9, newdata = nd) # inspect the output f %&gt;% str() ## num [1:5, 1:4, 1:7] 0.806616 0.443631 0.123985 0.015642 0.000916 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : NULL ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## ..$ : chr [1:7] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... Here we’ll rearrange the output to make it useful for plotting. # rearrange the output f &lt;- rbind( f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5], f[, , 6], f[, , 7] ) %&gt;% # wrangle data.frame() %&gt;% bind_cols(expand(nd, Y = 1:7, X_s)) head(f) ## Estimate Est.Error Q2.5 Q97.5 Y X_s ## 1 0.8066157820 0.04697296 0.704029954 0.889376843 1 -1.77444380 ## 2 0.4436310447 0.04824394 0.349536653 0.537086920 1 -0.89329094 ## 3 0.1239853476 0.02498454 0.079149727 0.176145937 1 -0.01213809 ## 4 0.0156422414 0.00680478 0.005636675 0.031792247 1 0.86901477 ## 5 0.0009155005 0.00082364 0.000108639 0.003022951 1 1.75016762 ## 6 0.0924873574 0.02486244 0.051015307 0.147259251 2 -1.77444380 Now we can make those bar-and-error plots. f %&gt;% mutate(X_s = round(X_s, digits = 3)) %&gt;% ggplot(aes(x = Y, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_col(fill = &quot;grey67&quot;) + geom_pointrange(fatten = 1.5, size = 1) + scale_x_continuous(breaks = 1:7) + scale_y_reverse(NULL, position = &quot;right&quot;, limits = c(1, 0), breaks = c(1, .5, 0), labels = c(&quot;1&quot;, &quot;.5&quot;, &quot;0&quot;)) + coord_flip() + theme(panel.grid = element_blank()) + facet_wrap(~X_s, ncol = 7, strip.position = &quot;bottom&quot;) The X_s values are depicted in the panel strips on the bottom. The response probabilities are scaled based on the axis on the top. The points and leftmost sides of the bars are the posterior means. The thin, dark horizontal lines are the percentile-based 95% intervals. Here we reformat f a little more to put those bar-and-error plots in a format more similar to that of Figure 23.7. f %&gt;% select(-Est.Error) %&gt;% # rescale the probability summaries mutate_at(vars(Estimate:Q97.5), ~. / 2) %&gt;% # plot! ggplot() + geom_vline(xintercept = seq(from = r[1], to = r[2], length.out = 5), color = &quot;white&quot;) + # bar marking the Estimate geom_segment(aes(x = X_s, xend = X_s - Estimate, y = Y + 0.1, yend = Y + 0.1), size = 8, color = &quot;grey67&quot;) + # bar marking the 95% interval geom_segment(aes(x = X_s - Q2.5, xend = X_s - Q97.5, y = Y + 0.2, yend = Y + 0.2), size = 1, color = &quot;grey33&quot;) + # data geom_point(data = my_data, aes(x = X_s, y = Y), shape = 1, size = 2) + scale_y_continuous(&quot;Y&quot;, breaks = 1:7, limits = c(0.5, 7.5)) + coord_cartesian(xlim = c(-2.4, 2.4)) + theme(panel.grid = element_blank()) I’m not going to attempt superimposing fitted lines on this plot the way Kruschke did. Given that our ordered-probit model is nonlinear on the scale of the criterion, it seems misleading to present linear fitted lines atop the raw data. If you’d like to do so, you’re on your own. Now here’s the corresponding model is we treat the y data as metric with tricks from Chapter 17. sd_x &lt;- sd(my_data$X) sd_y &lt;- sd(my_data$Y) m_x &lt;- mean(my_data$X) m_y &lt;- mean(my_data$Y) beta_0_sigma &lt;- 10 * abs(m_x * sd_y / sd_x) beta_1_sigma &lt;- 10 * abs(sd_y / sd_x) stanvars &lt;- stanvar(beta_0_sigma, name = &quot;beta_0_sigma&quot;) + stanvar(beta_1_sigma, name = &quot;beta_1_sigma&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) fit23.10 &lt;- brm(data = my_data, family = gaussian, Y ~ 1 + X, prior = c(prior(normal(0, beta_0_sigma), class = Intercept), prior(normal(0, beta_1_sigma), class = b), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.10&quot;) It may not have been readily apparent from Kruschke’s prose in the note for Figure 23.7, but his OLS model was based on the fully unstandardized data (i.e., using X as the predictor), not the partially standardized data he used in his JAGS code from 23.4.1. We followed the same sensibilities for fit23.10. Speaking of which, here are the summaries for the marginal posteriors. posterior_summary(fit23.10)[1:3, ] %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -5.430 0.610 -6.614 -4.239 ## b_X 6.711 0.427 5.876 7.537 ## sigma 1.528 0.075 1.392 1.680 These values are very close to those he reported at the bottom of page 690. Here are what the fitted lines from that model look like when superimposed on the data, when presuming both variables are metric. set.seed(23) conditional_effects(fit23.10, spaghetti = TRUE, nsamples = 100) %&gt;% plot(points = T, point_args = c(alpha = 1/3), line_args = c(size = 0), theme = theme(panel.grid = element_blank())) For the next example, we’ll load the HappinessAssetsDebt.csv data from Shi (2009). my_data &lt;- read_csv(&quot;data.R/HappinessAssetsDebt.csv&quot;) glimpse(my_data) ## Rows: 6,759 ## Columns: 3 ## $ Happiness &lt;dbl&gt; 3, 3, 3, 3, 1, 3, 2, 2, 4, 2, 3, 5, 3, 3, 4, 3, 3, 2, 4, 3, 4, 3, 3, 3, 5, 4, 3, 4, 4, 4,… ## $ Assets &lt;dbl&gt; 0, 10000, 30000, 40000, 21000, 20000, 20000, 0, 0, 20000, 5000, 30000, 40000, 5500, 50000… ## $ Debt &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32000, 0, 0, 0… Here’s a quick scatter plot of the data. To help with the overplotting, the points have been horizontally jittered a bit. But as indicated in the text, Happiness is a discrete variable. my_data %&gt;% ggplot(aes(x = Assets, y = Happiness)) + geom_jitter(alpha = 1/4, height = .25) + theme(panel.grid = element_blank()) Standardize our predictor. my_data &lt;- my_data %&gt;% mutate(Assets_s = (Assets - mean(Assets)) / sd(Assets)) Fit the model like before. fit23.11 &lt;- brm(data = my_data, family = cumulative(probit), Happiness ~ 1 + Assets_s, prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.11&quot;) Check the summary. print(fit23.11) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Happiness ~ 1 + Assets_s ## Data: my_data (Number of observations: 6759) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -2.03 0.03 -2.10 -1.97 1.00 5453 5058 ## Intercept[2] -1.17 0.02 -1.20 -1.13 1.00 8415 6880 ## Intercept[3] -0.15 0.02 -0.18 -0.12 1.00 8055 6609 ## Intercept[4] 1.48 0.02 1.44 1.53 1.00 8767 6806 ## Assets_s 0.15 0.01 0.12 0.17 1.00 8513 5692 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract the posterior draws and compare the brms parameterization of \\(\\theta_{[i]}\\) with the parameterization in the text in an expanded version of the bottom panel of Figure 23.8. post &lt;- posterior_samples(fit23.11) %&gt;% mutate(iter = 1:n()) post %&gt;% select(`b_Intercept[1]`:`b_Intercept[4]`, iter) %&gt;% compare_thresholds(lb = 1.5, ub = 4.5) Behold the marginal distribution of b_Assets_s, our effect size for Assets. post %&gt;% ggplot(aes(x = b_Assets_s, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Here’s the fitted()-oriented preparatory work for our version of the top panel of Figure 23.8. # define the range for the predictor r &lt;- range(my_data$Assets_s) # re-define the new data nd &lt;- tibble(Assets_s = seq(from = r[1], to = r[2], length.out = 5)) # compute the fitted summaries f &lt;- fitted(fit23.11, newdata = nd) # rearrange the output f &lt;- rbind( f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5] ) %&gt;% # wrangle data.frame() %&gt;% bind_cols(expand(nd, Happiness = 1:5, Assets_s)) # examine head(f) ## Estimate Est.Error Q2.5 Q97.5 Happiness Assets_s ## 1 2.610257e-02 2.076201e-03 2.214070e-02 3.031010e-02 1 -0.5971712 ## 2 3.150643e-03 7.127502e-04 1.975596e-03 4.707940e-03 1 4.7565107 ## 3 2.318569e-04 1.305224e-04 7.004674e-05 5.510350e-04 1 10.1101925 ## 4 1.127918e-05 1.309096e-05 1.090534e-06 4.334866e-05 1 15.4638743 ## 5 4.055501e-07 9.716135e-07 7.173225e-09 2.291520e-06 1 20.8175561 ## 6 1.147295e-01 4.137412e-03 1.065602e-01 1.229534e-01 2 -0.5971712 Like with the same variant from Figure 23.7, we will not be superimposing linear fitted lines. The model is nonlinear on the scale of the data and I don’t want to confuse readers by pretending otherwise. f %&gt;% select(-Est.Error) %&gt;% # rescale the probability summaries mutate_at(vars(Estimate:Q97.5), ~. * 2.5) %&gt;% # plot! ggplot() + geom_vline(xintercept = seq(from = r[1], to = r[2], length.out = 5), color = &quot;white&quot;) + # bar marking the Estimate geom_segment(aes(x = Assets_s, xend = Assets_s - Estimate, y = Happiness + 0.1, yend = Happiness + 0.1), size = 8, color = &quot;grey67&quot;) + # bar marking the 95% interval geom_segment(aes(x = Assets_s - Q2.5, xend = Assets_s - Q97.5, y = Happiness + 0.2, yend = Happiness + 0.2), size = 1, color = &quot;grey33&quot;) + # data geom_point(data = my_data, aes(x = Assets_s, y = Happiness), shape = 1, size = 2) + scale_y_continuous(&quot;Happiness&quot;, breaks = 1:5, limits = c(0.5, 5.5)) + coord_cartesian(xlim = c(-4, 24)) + theme(panel.grid = element_blank()) Now here’s the corresponding model is we treat Happiness as metric. Unlike our method for the corresponding model from Figure 23.7, fit10, we will use the standardized version of the predictor, Assets_s. The unstandardized values for Happiness and Assets are on vastly different scales, which can be difficulty for HMC with broad priors of the type Kruschke often uses. Standardizing the predictor helps. sd_y &lt;- sd(my_data$Happiness) stanvars &lt;- stanvar(sd_y, name = &quot;sd_y&quot;) fit23.12 &lt;- brm(data = my_data, family = gaussian, Happiness ~ 1 + Assets_s, prior = c(prior(normal(3.5, 5), class = Intercept), prior(normal(0, 5), class = b), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, seed = 23, file = &quot;fits/fit23.12&quot;) Here are the summaries for the marginal posteriors. posterior_summary(fit23.12)[1:3, ] %&gt;% round(digits = 6) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 3.484232 0.010015 3.465201 3.503668 ## b_Assets_s 0.115662 0.010342 0.095176 0.135035 ## sigma 0.847072 0.007229 0.832731 0.861195 They’re just a bit different from those produced by OLS. Here are what the fitted lines from that model look like when superimposed on the data, when presuming both variables are metric. set.seed(23) conditional_effects(fit23.12, spaghetti = TRUE, nsamples = 100) %&gt;% plot(points = T, point_args = c(alpha = 1/3), line_args = c(size = 0), theme = theme(panel.grid = element_blank())) 23.4.3 Example: Movies–They don’t make ’em like they used to. For this section, we’ll load the Moore (2006) Movies.csv data. my_data &lt;- read_csv(&quot;data.R/Movies.csv&quot;) glimpse(my_data) ## Rows: 100 ## Columns: 6 ## $ Title &lt;chr&gt; &quot;A_Ticklish_Affair&quot;, &quot;Action_in_the_North_Atlantic&quot;, &quot;And_the_Ship_Sails_On&quot;, &quot;Autumn_S… ## $ Year &lt;dbl&gt; 1963, 1943, 1984, 1978, 1931, 1930, 1950, 1989, 1940, 1947, 1970, 1940, 1976, 1985, 194… ## $ Length &lt;dbl&gt; 89, 127, 138, 97, 77, 69, 93, 119, 70, 69, 101, 62, 97, 85, 62, 86, 112, 97, 93, 89, 90… ## $ Cast &lt;dbl&gt; 5, 7, 7, 5, 6, 8, 5, 8, 9, 9, 9, 6, 10, 10, 9, 6, 10, 6, 12, 7, 5, 9, 6, 7, 6, 6, 12, 1… ## $ Rating &lt;dbl&gt; 2.0, 3.0, 3.0, 3.0, 2.5, 2.5, 3.0, 2.5, 2.5, 2.0, 3.0, 2.0, 2.5, 1.0, 1.5, 2.5, 3.0, 2.… ## $ Description &lt;dbl&gt; 7, 9, 15, 11, 7, 10, 8, 15, 8, 8, 11, 10, 12, 13, 9, 7, 10, 11, 11, 8, 9, 9, 13, 9, 7, … In footnote #5 at the bottom of page 693, Kruschke explained that whereas the original Ratings data ranged from 1.0 to 4.0 in half-unit increments, he recoded them to range from 1 to 7. Here we recode Ratings in the same way using dplyr::recode(). While we’re at it, we’ll make standardized versions of the predictors, too. my_data &lt;- my_data %&gt;% mutate(Rating = recode(Rating, `1.0` = 1, `1.5` = 2, `2.0` = 3, `2.5` = 4, `3.0` = 5, `3.5` = 6, `4.0` = 7), Year_s = (Year - mean(Year)) / sd(Year), Length_s = (Length - mean(Length)) / sd(Length)) Here’s a scatter plot of the data, with points colored by Rating. my_data %&gt;% mutate(Rating = factor(Rating)) %&gt;% ggplot(aes(x = Year, y = Length)) + geom_point(aes(color = Rating), size = 3) + geom_text(aes(label = Rating), size = 3) + scale_color_viridis_d() + theme(panel.grid = element_blank()) Fitting the multivariable ordered-probit model with brms is about as simple as fitting any other multivariable model. Just tack on predictors with the + operator. fit23.13 &lt;- brm(data = my_data, family = cumulative(probit), Rating ~ 1 + Year_s + Length_s, prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.13&quot;) Check the model summary. print(fit23.13) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Rating ~ 1 + Year_s + Length_s ## Data: my_data (Number of observations: 100) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -1.69 0.20 -2.10 -1.30 1.00 6278 5749 ## Intercept[2] -0.92 0.15 -1.22 -0.63 1.00 8828 6906 ## Intercept[3] -0.22 0.14 -0.49 0.04 1.00 9497 7033 ## Intercept[4] 0.61 0.14 0.34 0.88 1.00 8672 6636 ## Intercept[5] 1.69 0.20 1.30 2.09 1.00 8388 6817 ## Intercept[6] 2.58 0.37 1.95 3.39 1.00 8610 6190 ## Year_s -0.49 0.12 -0.73 -0.24 1.00 6986 6285 ## Length_s 0.62 0.13 0.36 0.88 1.00 7117 6358 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract the posterior draws and compare the brms parameterization of \\(\\theta_{[i]}\\) with the parameterization in the text in an expanded version of the bottom panel of Figure 23.9. post &lt;- posterior_samples(fit23.13) %&gt;% mutate(iter = 1:n()) post %&gt;% select(`b_Intercept[1]`:`b_Intercept[6]`, iter) %&gt;% compare_thresholds(lb = 1.5, ub = 6.5) To sate any curiosity, here are the Pearson’s correlation coefficients for the parameters. vcov(fit23.13, correlation = T) %&gt;% round(digits = 2) ## Intercept[1] Intercept[2] Intercept[3] Intercept[4] Intercept[5] Intercept[6] Year_s Length_s ## Intercept[1] 1.00 0.51 0.26 0.10 -0.01 0.01 0.16 -0.21 ## Intercept[2] 0.51 1.00 0.54 0.25 0.06 0.04 0.11 -0.20 ## Intercept[3] 0.26 0.54 1.00 0.48 0.17 0.08 0.02 -0.06 ## Intercept[4] 0.10 0.25 0.48 1.00 0.40 0.15 -0.12 0.16 ## Intercept[5] -0.01 0.06 0.17 0.40 1.00 0.38 -0.15 0.21 ## Intercept[6] 0.01 0.04 0.08 0.15 0.38 1.00 -0.10 0.05 ## Year_s 0.16 0.11 0.02 -0.12 -0.15 -0.10 1.00 -0.56 ## Length_s -0.21 -0.20 -0.06 0.16 0.21 0.05 -0.56 1.00 Now behold the marginal distribution of our two effect-size parameters. post %&gt;% pivot_longer(ends_with(&quot;_s&quot;)) %&gt;% mutate(name = factor(name, levels = c(&quot;b_Year_s&quot;, &quot;b_Length_s&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 40, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;effect size&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) Before we make the top panel from Figure 23.9, I’d like to wander a bit and look at something related. We’ll use fitted(). # define the new data nd &lt;- crossing(Year_s = seq(from = -3, to = 3, by = 0.25), Length_s = seq(from = -3, to = 3, by = 0.25)) # compute the `Response` probabilities f &lt;- fitted(fit23.13, newdata = nd) # rearrange the output f &lt;- rbind( f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5], f[, , 6], f[, , 7] ) %&gt;% # wrangle data.frame() %&gt;% bind_cols( nd %&gt;% expand(Rating = 1:7, nesting(Year_s, Length_s)) ) # what did we do? head(f) ## Estimate Est.Error Q2.5 Q97.5 Rating Year_s Length_s ## 1 0.11532071 0.07599288 0.019689571 0.30456668 1 -3 -3.00 ## 2 0.08905917 0.06214321 0.014015527 0.24637847 1 -3 -2.75 ## 3 0.06756787 0.05001020 0.009707207 0.19483558 1 -3 -2.50 ## 4 0.05039363 0.03967474 0.006268840 0.15365951 1 -3 -2.25 ## 5 0.03698018 0.03108531 0.004020302 0.12046791 1 -3 -2.00 ## 6 0.02672997 0.02409835 0.002497780 0.09142248 1 -3 -1.75 We just computed the response probabilities across the two-dimensional grid of the predictor values. Now plot using the posterior means. f %&gt;% mutate(strip = str_c(&quot;Rating: &quot;, Rating)) %&gt;% ggplot(aes(x = Year_s, y = Length_s)) + geom_raster(aes(fill = Estimate), interpolate = T) + geom_text(data = my_data %&gt;% mutate(strip = str_c(&quot;Rating: &quot;, Rating)), aes(label = Rating), color = &quot;white&quot;, size = 2.5) + scale_fill_viridis_c(&quot;probability&quot;, option = &quot;A&quot;, limits = c(0, 1), breaks = c(0, .5, 1), labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + scale_x_continuous(breaks = seq(from = -2, to = 2, by = 2), expand = c(0, 0)) + scale_y_continuous(breaks = seq(from = -2, to = 2, by = 2), expand = c(0, 0)) + theme(legend.position = &quot;bottom&quot;) + facet_wrap(~strip, nrow = 1) This model didn’t do a great job capturing the Response probabilities. If you’re curious, you’ll find you can do a little bit better if you allow the two predictors to interact (i.e., add + Year_s:Length_s to the formula line). Even then, the model isn’t great. I leave that as an exercise for the interested reader. For this model, however, we will follow Kruschke and make a more faithful version of the top panel of Figure 23.9. We’ll need to wrangle our post data a bit to get things ready. Here’s the work. post &lt;- post %&gt;% # we just need the data from three steps in the HMC chain slice(1:3) %&gt;% mutate(iter = 1:n() %&gt;% as.factor(), b1 = b_Year_s, b2 = b_Length_s) %&gt;% expand(nesting(iter, b1, b2, `b_Intercept[1]`, `b_Intercept[2]`, `b_Intercept[3]`, `b_Intercept[4]`, `b_Intercept[5]`, `b_Intercept[6]`), # because these are straight lines, two extreme x1-values are all we need x1 = c(-10, 10)) %&gt;% pivot_longer(contains(&quot;[&quot;), names_to = &quot;theta&quot;) %&gt;% # use Kruschke&#39;s Formula 23.5 mutate(x2 = (value / b2) + (-b1 / b2) * x1, # this just renames our x variables for easy plotting Year_s = x1, Length_s = x2) glimpse(post) ## Rows: 36 ## Columns: 9 ## $ iter &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, … ## $ b1 &lt;dbl&gt; -0.5999177, -0.5999177, -0.5999177, -0.5999177, -0.5999177, -0.5999177, -0.5999177, -0.599… ## $ b2 &lt;dbl&gt; 0.5127384, 0.5127384, 0.5127384, 0.5127384, 0.5127384, 0.5127384, 0.5127384, 0.5127384, 0.… ## $ x1 &lt;dbl&gt; -10, -10, -10, -10, -10, -10, 10, 10, 10, 10, 10, 10, -10, -10, -10, -10, -10, -10, 10, 10… ## $ theta &lt;chr&gt; &quot;b_Intercept[1]&quot;, &quot;b_Intercept[2]&quot;, &quot;b_Intercept[3]&quot;, &quot;b_Intercept[4]&quot;, &quot;b_Intercept[5]&quot;, … ## $ value &lt;dbl&gt; -1.54348589, -0.86731565, -0.30290614, 0.71001624, 1.59991220, 2.72062779, -1.54348589, -0… ## $ x2 &lt;dbl&gt; -14.710549, -13.391806, -12.291031, -10.315516, -8.579941, -6.394195, 8.689989, 10.008733,… ## $ Year_s &lt;dbl&gt; -10, -10, -10, -10, -10, -10, 10, 10, 10, 10, 10, 10, -10, -10, -10, -10, -10, -10, 10, 10… ## $ Length_s &lt;dbl&gt; -14.710549, -13.391806, -12.291031, -10.315516, -8.579941, -6.394195, 8.689989, 10.008733,… Now just plot. post %&gt;% ggplot(aes(x = Year_s, y = Length_s)) + geom_line(aes(group = interaction(iter, theta), color = theta, linetype = iter)) + geom_text(data = my_data, aes(label = Rating)) + scale_color_viridis_d(expression(theta), option = &quot;A&quot;, end = .9, labels = 1:6) + coord_cartesian(xlim = range(my_data$Year_s), ylim = range(my_data$Length_s)) + theme(panel.grid = element_blank()) It might be easier to see Kruschke’s main point if we facet by iter. post %&gt;% ggplot(aes(x = Year_s, y = Length_s)) + geom_line(aes(group = interaction(iter, theta), color = theta, linetype = iter)) + geom_text(data = my_data, aes(label = Rating)) + scale_color_viridis_d(expression(theta), option = &quot;A&quot;, end = .9, labels = 1:6) + coord_cartesian(xlim = range(my_data$Year_s), ylim = range(my_data$Length_s)) + theme(legend.direction = &quot;horizontal&quot;, legend.position = c(.75, .25), panel.grid = element_blank()) + facet_wrap(~iter, ncol = 2) Both our versions of the plot show what Kruschke pointed out in the text: Threshold lines from the same step in the chain must be parallel because the regression coefficients are constant at that step, but are different at another step. The threshold lines in Figure 23.9 are level contours on the underlying metric planar surface, and the lines reveal that the ratings increase toward the top left, that is, as \\(x_1\\) decreases and \\(x_2\\) increases. (p. 693) Before we move on to the next section, what these diagonal 2-dimensional threshold lines also hint at is that when we use two predictors to describe ordinal data as having been produces by an underlying unit Gaussian distribution, that underlying distribution is actually bivariate Gaussian. Here we’ll use fitted() one more time to depict that bivariate-Gaussian distribution with a little geom_raster(). # define the new data nd &lt;- crossing(Year_s = seq(from = -5, to = 5, by = 0.1), Length_s = seq(from = -5, to = 5, by = 0.1)) fitted(fit23.13, newdata = nd, # this will yield z-scores scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # convert the z-scores to density values mutate(density = dnorm(Estimate, 0, 1)) %&gt;% ggplot(aes(x = Year_s, y = Length_s)) + geom_raster(aes(fill = density), interpolate = T) + geom_text(data = my_data, aes(label = Rating), size = 2.5) + scale_fill_viridis_c(&quot;density&quot;, option = &quot;A&quot;, limits = c(0, 0.4)) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) As with many of our previous approaches with geom_raster(), this plot is based on the posterior means in each cell and, therefore, does a poor job depicting the uncertainty in the posterior distribution. 23.4.4 Why are some thresholds outside the data? Now load Kruschke’s simulated data. my_data &lt;- read_csv(&quot;data.R/OrdinalProbitData-Movies.csv&quot;) glimpse(my_data) ## Rows: 400 ## Columns: 3 ## $ Year &lt;dbl&gt; 1959, 1946, 1964, 1938, 1946, 1971, 1957, 1970, 1968, 1962, 1929, 1962, 1978, 1972, 1964, 19… ## $ Length &lt;dbl&gt; 88, 117, 130, 85, 111, 105, 93, 119, 78, 77, 138, 107, 70, 60, 138, 122, 72, 109, 71, 61, 60… ## $ Rating &lt;dbl&gt; 4, 5, 5, 4, 5, 4, 3, 4, 3, 3, 7, 4, 2, 2, 5, 5, 3, 4, 3, 2, 2, 3, 5, 3, 5, 5, 5, 4, 5, 4, 4,… These data imitate the movie ratings, but with two key differences. First and foremost, the artificial data have much smaller noise, with \\(\\sigma = 0.20\\) as opposed to \\(\\sigma \\approx 1.25\\) in the real data. Second, the artificial data have points that span the entire range of both predictors, unlike the real data which had points mostly in the central region. (p. 695) Like with the real movie data, we’ll inspect these data with a colored scatter plot. my_data %&gt;% mutate(Rating = factor(Rating)) %&gt;% ggplot(aes(x = Year, y = Length)) + geom_point(aes(color = Rating), size = 3) + geom_text(aes(label = Rating), size = 3) + scale_color_viridis_d() + theme(panel.grid = element_blank()) Unlike in last section, there appears to be a clear trend in Kruschke’s simulated data. Kruschke’s simulated critic liked his movies old and long. Time to standardize the predictors. my_data &lt;- my_data %&gt;% mutate(Year_s = (Year - mean(Year)) / sd(Year), Length_s = (Length - mean(Length)) / sd(Length)) Fitting the multivariable ordered-probit model with brms is about as simple as fitting any other multivariable model. Just tack on predictors with the + operator. fit23.14 &lt;- update(fit23.13, newdata = my_data, iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 23, file = &quot;fits/fit23.14&quot;) Check the model summary. print(fit23.14) ## Family: cumulative ## Links: mu = probit; disc = identity ## Formula: Rating ~ 1 + Year_s + Length_s ## Data: my_data (Number of observations: 400) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -9.79 0.68 -11.17 -8.51 1.00 1604 2263 ## Intercept[2] -5.98 0.43 -6.86 -5.17 1.00 1803 2365 ## Intercept[3] -2.25 0.21 -2.68 -1.84 1.00 2880 4416 ## Intercept[4] 2.42 0.21 2.03 2.85 1.00 2606 4081 ## Intercept[5] 7.80 0.55 6.74 8.92 1.00 1721 2456 ## Intercept[6] 10.89 0.76 9.45 12.45 1.00 1778 2322 ## Year_s -2.80 0.20 -3.20 -2.41 1.00 1699 2504 ## Length_s 4.74 0.32 4.14 5.39 1.00 1627 2261 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Extract the posterior draws and use compare_thresholds() to make our expanded version of the bottom panel of Figure 23.10. post &lt;- posterior_samples(fit23.14) %&gt;% mutate(iter = 1:n()) post %&gt;% select(`b_Intercept[1]`:`b_Intercept[6]`, iter) %&gt;% compare_thresholds(lb = 1.5, ub = 6.5) Make the marginal distribution of our two effect-size parameters. post %&gt;% pivot_longer(ends_with(&quot;_s&quot;)) %&gt;% mutate(name = factor(name, levels = c(&quot;b_Year_s&quot;, &quot;b_Length_s&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;effect size&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) Make the top panel for Figure 23.10 just like we did for its analogue in Figure 23.9. # extract the posterior draws and wrangle posterior_samples(fit23.14) %&gt;% slice(1:3) %&gt;% mutate(iter = 1:n() %&gt;% as.factor(), b1 = b_Year_s, b2 = b_Length_s) %&gt;% expand(nesting(iter, b1, b2, `b_Intercept[1]`, `b_Intercept[2]`, `b_Intercept[3]`, `b_Intercept[4]`, `b_Intercept[5]`, `b_Intercept[6]`), x1 = c(-10, 10)) %&gt;% pivot_longer(contains(&quot;[&quot;), names_to = &quot;theta&quot;) %&gt;% mutate(x2 = (value / b2) + (-b1 / b2) * x1, Year_s = x1, Length_s = x2) %&gt;% # plot! ggplot(aes(x = Year_s, y = Length_s)) + geom_line(aes(group = interaction(iter, theta), color = theta, linetype = iter)) + geom_text(data = my_data, aes(label = Rating)) + scale_color_viridis_d(expression(theta), option = &quot;A&quot;, end = .9, labels = 1:6) + coord_cartesian(xlim = range(my_data$Year_s), ylim = range(my_data$Length_s)) + theme(panel.grid = element_blank()) Those are some tight thresholds. They “very clearly cleave parallel regions of distinct ordinal values. The example demonstrates that the threshold lines do make intuitive sense when there is low noise and a broad range of data” (p. 695, emphasis in the original). With our various bonus plots, we’ve been anticipating Figure 23.11 for some time, now. The thresholds from fit23.14 result in beautifully nonlinear probability curves for the Rating levels. Take a quick look with conditional_effects(). conditional_effects(fit23.14, categorical = T) %&gt;% plot(theme = theme(panel.grid = element_blank())) Because the model had two predictors, we got two plots. What brms::conditional_effects() called Probability on the \\(y\\)-axis is the same as what Kruschke called \\(p(y)\\) on his. Rather than generic predictors \\(x\\) on the \\(x\\)-axis, our plots had either Year_s or Length_s. Whereas Kruschke marked off his different outcomes by line styles, ours were marked by color. Since we don’t have the data Kruschke used to make Figure 23.11, we won’t be able to reproduce it exactly. However, you’ll note that our plot for Length_s corresponded nicely with his subplot on the top (i.e., the one for which \\(\\sigma = 0.1\\)). If we set effects = &quot;Length_s&quot;, we can use conditional_effects() to make a similar plot to Kruschke’s subplot for which \\(\\sigma = 1\\). conditional_effects(fit23.13, categorical = T, effects = &quot;Length_s&quot;) %&gt;% plot(theme = theme(panel.grid = element_blank())) “You can see that each outcome has maximum probability within its corresponding interval, but there is considerable smearing of outcomes into adjacent intervals” (p. 695). Finishing off, Kruschke’s discussion in the text referred to \\(\\sigma\\) as “noise” merely for linguistic ease. Calling the outcomes “noisy” does not mean the underlying generator of the outcomes is inherently wildly random. The “noise” is merely variation in the outcome that cannot be accounted for by the particular model we have chosen with the particular predictors we have chosen. A different model and/or different predictors might account for the outcomes well with little residual noise. In this sense, the noise is in the model, not in the data. (p. 698, emphasis in the original) Through this lens, noisy-looking data are a symptom of weak theory and/or poor data-collection methods. 23.5 Posterior prediction The cumulative-normal model makes posterior predictions for the probabilities of the \\(K\\) categories in the criterion variable by computing \\(p (y | \\mu (x), \\sigma, \\{ \\theta_k \\} )\\) in each step in the HMC chain. In this equation, \\(\\mu (x) = \\beta_0 + \\sum_j \\beta_j x_j\\). Though recall that with our brms parameterization, we have \\(\\beta_0\\) fixed at 0. Kruschke framed part of his discussion in this chapter in terms of a single-predictor model, such as was entertained in Figure 23.8. Recall that corresponds to our fit11. Here’s that formula. fit23.11$formula ## Happiness ~ 1 + Assets_s With brms, you can get this information with fitted(). Let’s say we wanted to focus on response probabilities for Assets_s = -1). Here’s what we get. fitted(fit23.11, newdata = tibble(Assets_s = -1)) ## , , 1 ## ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.02995493 0.00241341 0.02533022 0.03491371 ## ## , , 2 ## ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.1247132 0.004739187 0.1154823 0.1341841 ## ## , , 3 ## ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.3437033 0.006293002 0.3313739 0.3561012 ## ## , , 4 ## ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.4503483 0.007018188 0.4365616 0.4639018 ## ## , , 5 ## ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.05128031 0.002880035 0.04573983 0.05720883 As is typical of brms, those probability summaries were in terms of the posterior mean and percentile-based 95% intervals. If you’re like Kruschke and prefer posterior modes and HDIs, you’ll need to set summary = F and wrangle a bit. f &lt;- fitted(fit23.11, newdata = tibble(Assets_s = -1), summary = F) cbind( f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5] ) %&gt;% data.frame() %&gt;% set_names(str_c(&quot;p(Happiness = &quot;, 1:5, &quot; | Assets_s = -1)&quot;)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 5 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 p(Happiness = 1 | Assets_s = -1) 0.03 0.0252 0.0347 0.95 mode hdi ## 2 p(Happiness = 2 | Assets_s = -1) 0.125 0.115 0.134 0.95 mode hdi ## 3 p(Happiness = 3 | Assets_s = -1) 0.342 0.332 0.356 0.95 mode hdi ## 4 p(Happiness = 4 | Assets_s = -1) 0.451 0.437 0.464 0.95 mode hdi ## 5 p(Happiness = 5 | Assets_s = -1) 0.0514 0.0453 0.0566 0.95 mode hdi 23.6 Generalizations and extensions In this section, Kruschke mentioned extensions of this class of models might include using the cumulative \\(t\\) function to handle outliers or adding a guessing parameter. Full disclosure: I have not fit models like these. Based on my knowledge of brms, I suspect they’re possible. For insights how, you might review Bürkner’s (2020b) Define custom response distributions with brms and his (2020e) Estimating non-linear models with brms vignettes. In addition, there are other likelihoods one might use to model ordinal data using brms. Your first stop should be Bürkner and Vourre’s (2019) Ordinal regression models in psychology: A Tutorial, where, in addition to the cumulative normal model, they cover the sequential and adjacent category models. You might also check out Chapter 11 of my (2020) ebook recoding McElreath’s Statistical rethinking, wherein I show how one might use the logit link (i.e., family = cumulative(logit)) to fit ordered-categorical models with brms. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.3.9000 brms_2.12.0 Rcpp_1.0.4.6 patchwork_1.0.0 forcats_0.5.0 ## [6] stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 readr_1.3.1 tidyr_1.0.2 ## [11] tibble_3.0.1 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 markdown_1.1 ## [6] base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 farver_2.0.3 rstan_2.19.3 ## [11] svUnit_1.0.3 DT_0.13 fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 ## [16] xml2_1.3.1 bridgesampling_1.0-0 knitr_1.28 shinythemes_1.1.2 bayesplot_1.7.1 ## [21] jsonlite_1.6.1 broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 compiler_3.6.3 ## [26] httr_1.4.1 backports_1.1.6 assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] cli_2.0.2 later_1.0.0 htmltools_0.4.0 prettyunits_1.1.1 tools_3.6.3 ## [36] igraph_1.2.5 coda_0.19-3 gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 ## [41] cellranger_1.1.0 vctrs_0.3.0 nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ## [46] ps_1.3.3 rvest_0.3.5 mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 ## [51] gtools_3.8.2 zoo_1.8-7 scales_1.1.1 colourpicker_1.0 hms_0.5.3 ## [56] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 StanHeaders_2.21.0-1 loo_2.2.0 stringi_1.4.6 ## [66] dygraphs_1.1.1.6 pkgbuild_1.0.8 rlang_0.4.6 pkgconfig_2.0.3 matrixStats_0.56.0 ## [71] HDInterval_0.2.0 evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [76] labeling_0.3 tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 magrittr_1.5 ## [81] bookdown_0.18 R6_2.4.1 generics_0.0.2 DBI_1.1.0 pillar_1.4.4 ## [86] haven_2.2.0 withr_2.2.0 xts_0.12-0 abind_1.4-5 modelr_0.1.6 ## [91] crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 rmarkdown_2.1 grid_3.6.3 ## [96] readxl_1.3.1 callr_3.4.3 threejs_0.3.3 reprex_0.3.0 digest_0.6.25 ## [101] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 viridisLite_0.3.0 ## [106] shinyjs_1.1 References "],
["count-predicted-variable.html", "24 Count Predicted Variable 24.1 Poisson exponential model 24.2 Example: Hair eye go again 24.3 Example: Interaction contrasts, shrinkage, and omnibus test 24.4 Log-linear models for contingency tables Bonus: Alternative parameterization Session info", " 24 Count Predicted Variable Consider a situation in which we observe two nominal values for every item measured…. Across the whole sample, the result is a table of counts for each combination of values of the nominal variables. The counts are what we are trying to predict and the nominal variables are the predictors. This is the type of situation addressed in this chapter…. In the context of the generalized linear model (GLM) introduced in Chapter 15, this chapter’s situation involves a predicted value that is a count, for which we will use an inverse-link function that is exponential along with a Poisson distribution for describing noise in the data (Kruschke, 2015, pp. 703–704) 24.1 Poisson exponential model Following Kruschke, we will “refer to the model that will be explained in this section as Poisson exponential because, as we will see, the noise distribution is a Poisson distribution and the inverse-link function is exponential” (p. 704). 24.1.1 Data structure. Kruschke has the Snee (1974) data for Table 24.1 saved as the HairEyeColor.csv file. library(tidyverse) library(janitor) my_data &lt;- read_csv(&quot;data.R/HairEyeColor.csv&quot;) glimpse(my_data) ## Rows: 16 ## Columns: 3 ## $ Hair &lt;chr&gt; &quot;Black&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Blond&quot;, &quot;Blond&quot;, &quot;Blond&quot;, … ## $ Eye &lt;chr&gt; &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green&quot;, &quot;Hazel&quot;, &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green&quot;, &quot;H… ## $ Count &lt;dbl&gt; 20, 68, 5, 15, 94, 7, 16, 10, 84, 119, 29, 54, 17, 26, 14, 14 In order to retain some of the ordering in Table 24.1, we’ll want to make Hair a factor and recode Brown as Brunette. my_data &lt;- my_data %&gt;% mutate(Hair = if_else(Hair == &quot;Brown&quot;, &quot;Brunette&quot;, Hair) %&gt;% factor(., levels = c(&quot;Black&quot;, &quot;Brunette&quot;, &quot;Red&quot;, &quot;Blond&quot;))) Here’s a quick way to use pivot_wider() to make most of the table. my_data %&gt;% pivot_wider(names_from = Hair, values_from = Count) ## # A tibble: 4 x 5 ## Eye Black Blond Brunette Red ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Blue 20 94 84 17 ## 2 Brown 68 7 119 26 ## 3 Green 5 16 29 14 ## 4 Hazel 15 10 54 14 However, that didn’t get us the marginal totals. For those, we’ll uncount() the cells in the data and then make the full table with janitor::tabyl() and janitor::adorn_totals(). my_data %&gt;% uncount(weights = Count, .remove = F) %&gt;% tabyl(Eye, Hair) %&gt;% adorn_totals(c(&quot;row&quot;, &quot;col&quot;)) %&gt;% knitr::kable() Eye Black Brunette Red Blond Total Blue 20 84 17 94 215 Brown 68 119 26 7 220 Green 5 29 14 16 64 Hazel 15 54 14 10 93 Total 108 286 71 127 592 That last knitr::kable() line just formatted the output a bit. 24.1.2 Exponential link function. To analyze data like those above, a natural candidate for the needed likelihood distribution is the Poisson (described later), which takes a non-negative value \\(\\lambda\\) and gives a probability for each integer from zero to infinity. But this motivation may seem a bit arbitrary, even if there’s nothing wrong with it in principle. A different motivation starts by treating the cell counts as representative of underlying cell probabilities, and then asking whether the two nominal variables contribute independent influences to the cell probabilities. (p. 705). The additive model of cell counts of a table of rows \\(r\\) and columns \\(c\\) follows the form \\[\\lambda_{r, c} = \\exp (\\beta_0 + \\beta_r + \\beta_c),\\] where \\(\\lambda_{r, c}\\) is the tendency of counts within row \\(r\\) and column \\(c\\). In the case of an interaction model, the equation expands to \\[\\lambda_{r, c} = \\exp (\\beta_0 + \\beta_r + \\beta_c + \\beta_{r, c}),\\] with the following constraints: \\[\\begin{align*} \\sum_r \\beta_r = 0, &amp;&amp; \\sum_c \\beta_c = 0, &amp;&amp; \\sum_r \\beta_{r, c} = 0 \\text{ for all } c, &amp;&amp; \\text{and} &amp;&amp; \\sum_c \\beta_{r, c} = 0 \\text{ for all } r. \\end{align*}\\] 24.1.3 Poisson noise distribution. Simon-Denis Poisson’s distribution follows the form \\[p(y | \\lambda) = \\frac{\\lambda^y \\exp (-\\lambda)}{y!},\\] where \\(y\\) is a non-negative integer and \\(\\lambda\\) is a non-negative real number. The mean of the Poisson distribution is \\(\\lambda\\). Importantly, the variance of the Poisson distribution is also \\(\\lambda\\) (i.e., the standard deviation is \\(\\sqrt \\lambda\\)). Thus, in the Poisson distribution, the variance is completely yoked to the mean. (p. 707) We can work with that expression directly in base R. Here we use \\(\\lambda = 5.5\\) and \\(y = 2\\). lambda &lt;- 5.5 y &lt;- 2 lambda^y * exp(-lambda) / factorial(y) ## [1] 0.06181242 If we’d like to simulate from the Poisson distribution, we’d use the rpois() function. It takes two arguments, n and lambda. Let’s generate 1,000 draws based on \\(\\lambda = 5\\). set.seed(24) d &lt;- tibble(y = rpois(n = 1000, lambda = 5)) Here are the mean and variance. d %&gt;% summarise(mean = mean(y), variance = var(y)) ## # A tibble: 1 x 2 ## mean variance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4.98 5.17 They’re not exactly the same because of simulation variance, but they get that way real quick with a larger sample. set.seed(24) tibble(y = rpois(n = 100000, lambda = 5)) %&gt;% summarise(mean = mean(y), variance = var(y)) ## # A tibble: 1 x 2 ## mean variance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4.99 4.98 Let’s put rpois() to work and make Figure 24.1. set.seed(24) tibble(lambda = c(1.8, 8.3, 32.1)) %&gt;% mutate(y = map(lambda, rpois, n = 1e5)) %&gt;% unnest(y) %&gt;% ggplot(aes(x = y)) + geom_histogram(aes(y = stat(density)), binwidth = 1, color = &quot;grey92&quot;) + ylab(&quot;p(y)&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~lambda, ncol = 1, labeller = label_bquote(dpois(y*&quot;|&quot;*lambda == .(lambda)))) For more on our labeller = label_bquote syntax, check out this. But anyway, given \\(\\lambda\\), the Poisson distribution gives the probabilities of specific non-negative integers. And instead of using our hand-coded function from above, we can also use dpois() to get precise density values. dpois(2, lambda = 5.5) ## [1] 0.06181242 24.1.4 The complete model and implementation in JAGS brms. Using Kruschke’s method, the prior is supposed to be broad on the scale of the data, but we must be careful about what scale is being modeled by the baseline and deflections. The counts are being directly described by \\(\\lambda\\), but it is \\(\\log (\\lambda)\\) being described by the baseline and deflections. Thus, the prior on the baseline and deflections should be broad on the scale of the logarithm of the data. To establish a generic baseline, consider that if the data points were distributed equally among the cells, the mean count would be the total count divided by the number of cells. The biggest possible standard deviation across cells would occur when all the counts were loaded into a single cell and all the other cells were zero. (pp. 709–710, emphasis added) Before we show how to fit the model, we need the old gamma_a_b_from_omega_sigma() function. gamma_a_b_from_omega_sigma &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } Here are a few intermediate values before we set the stanvars. n_x1_level &lt;- length(unique(my_data$x1)) n_x2_level &lt;- length(unique(my_data$x2)) n_cell &lt;- nrow(my_data) Now we’re ready to define the stanvars. y_log_mean &lt;- log(sum(my_data$y) / (n_x1_level * n_x2_level)) y_log_sd &lt;- c(rep(0, n_cell - 1), sum(my_data$y)) %&gt;% sd() %&gt;% log() s_r &lt;- gamma_a_b_from_omega_sigma(mode = y_log_sd, sd = 2 * y_log_sd) stanvars &lt;- stanvar(y_log_mean, name = &quot;y_log_mean&quot;) + stanvar(y_log_sd, name = &quot;y_log_sd&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) You’d then fit a Poisson model with two nominal predictors using Kruschke’s hierarchical-shrinkage method like this. fit &lt;- brm(data = my_data, family = poisson, y ~ 1 + (1 | x1) + (1 | x2) + (1 | x1:x2), prior = c(prior(normal(y_log_mean, y_log_sd * 2), class = Intercept), prior(gamma(alpha, beta), class = sd)), stanvars = stanvars) By brms default, family = poisson uses the log link. Thus family = poisson(link = &quot;log&quot;) should return the same results. Notice the right-hand side of the model formula. We have three hierarchical variance parameters. This hierarchical-shrinkage approach to frequency-table data has its origins in Gelman (2005), Analysis of variance–why it is more important than ever. 24.2 Example: Hair eye go again We’ll be using the same data, from above. As an alternative to Table 24.1, it might be handy to take a more colorful approach to wading into the data. # wrangle my_data %&gt;% uncount(weights = Count, .remove = F) %&gt;% tabyl(Eye, Hair) %&gt;% adorn_totals(c(&quot;row&quot;, &quot;col&quot;)) %&gt;% data.frame() %&gt;% pivot_longer(-Eye, names_to = &quot;Hair&quot;) %&gt;% mutate(Eye = fct_rev(Eye)) %&gt;% # plot ggplot(aes(x = Hair, y = Eye)) + geom_raster(aes(fill = value)) + geom_text(aes(label = value, color = value &lt; 300)) + scale_fill_viridis_c(option = &quot;D&quot;) + scale_color_manual(values = c(&quot;black&quot;, &quot;white&quot;)) + scale_x_discrete(expand = c(0, 0), position = &quot;top&quot;) + scale_y_discrete(expand = c(0, 0)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks = element_blank(), legend.position = &quot;none&quot;) Load the brms and tidybayes packages. library(brms) library(tidybayes) Now we’ll make save the prepatory values necessary for the stanvars. n_x1_level &lt;- length(unique(my_data$Hair)) n_x2_level &lt;- length(unique(my_data$Eye)) n_cell &lt;- nrow(my_data) n_x1_level ## [1] 4 n_x2_level ## [1] 4 n_cell ## [1] 16 Here are the values we’ll save as stanvars. y_log_mean &lt;- log(sum(my_data$Count) / (n_x1_level * n_x2_level)) y_log_sd &lt;- c(rep(0, n_cell - 1), sum(my_data$Count)) %&gt;% sd() %&gt;% log() s_r &lt;- gamma_a_b_from_omega_sigma(mode = y_log_sd, sd = 2 * y_log_sd) y_log_mean ## [1] 3.610918 y_log_sd ## [1] 4.997212 s_r$shape ## [1] 1.640388 s_r$rate ## [1] 0.1281491 As a quick detour, it might be interesting to see what the kind of gamma distribution is entailed by those last two values. tibble(x = seq(from = 0, to = 70, length.out = 1e3)) %&gt;% mutate(density = dgamma(x, s_r$shape, s_r$rate)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = density)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(&quot;Kruschke&#39;s wide prior for &quot;*sigma[beta*x]), x = NULL) + coord_cartesian(xlim = c(0, 60)) + theme(panel.grid = element_blank()) Save the stanvars. stanvars &lt;- stanvar(y_log_mean, name = &quot;y_log_mean&quot;) + stanvar(y_log_sd, name = &quot;y_log_sd&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Fit Kruschke’s model with brms. fit24.1 &lt;- brm(data = my_data, family = poisson, Count ~ 1 + (1 | Hair) + (1 | Eye) + (1 | Hair:Eye), prior = c(prior(normal(y_log_mean, y_log_sd * 2), class = Intercept), prior(gamma(alpha, beta), class = sd)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 24, stanvars = stanvars, file = &quot;fits/fit24.01&quot;) As it turns out, if you try to fit Kruschke’s model with brms as is, you’ll run into difficulties with divergent transitions and the like. One approach is to try tuning the adapt_delta and max_treedepth parameters. I had no luck with that approach. E.g., cranking adapt_delta up past 0.9999 still returned a divergent transition or two. Another approach is to step back and assess the model. We’re trying to fit a multilevel model with two grouping variables and their interaction with a total of 16 data points. That’s not a lot of data for fitting such a model. If you take a close look at our priors, you’ll notice they’re really quite weak. If you’re willing to tighten them up just a bit, the model can fit more smoothly. That will be our approach. For the \\(\\sigma\\) hyperparameter of the overall intercept’s Gaussian prior, Kruschke would have us multiply y_log_sd by 2. Here we’ll tighten up that \\(\\sigma\\) hyperparameter by simply setting it to y_log_sd. The gamma priors for the upper-level variance parameters were based on a mode of y_log_sd and a standard deviation of the same but multiplied by 2 (i.e., 2 * y_log_sd). We’ll tighten that up a bit by simply defining those gammas by a standard deviation of y_log_sd. When you make those adjustments, the model fits with less fuss. In case you’re curious, here is what those priors look like. # redifine our shape and rate s_r &lt;- gamma_a_b_from_omega_sigma(mode = y_log_sd, sd = y_log_sd) # wrangle bind_rows( # define beta[0] tibble(x = seq(from = y_log_mean - (4 * y_log_sd), to = y_log_mean + (4 * y_log_sd), length.out = 1e3)) %&gt;% mutate(density = dnorm(x, y_log_mean, y_log_sd)), # define sigma[beta[x]] tibble(x = seq(from = 0, to = 40, length.out = 1e3)) %&gt;% mutate(density = dgamma(x, s_r$shape, s_r$rate)) ) %&gt;% mutate(prior = rep(c(&quot;beta[0]&quot;, &quot;sigma[beta*x]&quot;), each = n() / 2)) %&gt;% # plot ggplot(aes(x = x, ymin = 0, ymax = density)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Priors&quot;, x = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~prior, scales = &quot;free&quot;, labeller = label_parsed) Update the stanvars. stanvars &lt;- stanvar(y_log_mean, name = &quot;y_log_mean&quot;) + stanvar(y_log_sd, name = &quot;y_log_sd&quot;) + stanvar(s_r$shape, name = &quot;alpha&quot;) + stanvar(s_r$rate, name = &quot;beta&quot;) Now we’ve updated our stanvars, we’ll fit the modified model. We should note that even this version required some adjustment to the adapt_delta and max_treedepth parameters. But it wasn’t nearly the exercise in frustration entailed in the version, above. fit24.1 &lt;- brm(data = my_data, family = poisson, Count ~ 1 + (1 | Hair) + (1 | Eye) + (1 | Hair:Eye), prior = c(prior(normal(y_log_mean, y_log_sd), class = Intercept), prior(gamma(alpha, beta), class = sd)), iter = 3000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.99, max_treedepth = 11), stanvars = stanvars, seed = 24, file = &quot;fits/fit24.01&quot;) Take a look at the parameter summary. print(fit24.1) ## Family: poisson ## Links: mu = log ## Formula: Count ~ 1 + (1 | Hair) + (1 | Eye) + (1 | Hair:Eye) ## Data: my_data (Number of observations: 16) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~Eye (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.83 1.62 0.29 6.37 1.00 3494 5176 ## ## ~Hair (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.93 1.57 0.36 6.16 1.00 3326 5218 ## ## ~Hair:Eye (Number of levels: 16) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.94 0.29 0.54 1.63 1.00 3344 5137 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.22 1.54 -0.04 6.49 1.00 4685 4300 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). You’ll notice that even though we tightened up the priors, the parameter estimates are still quite small relative to the values they allowed for. So even our tightened priors were quite permissive. Let’s post process in preparation for Figure 24.3. nd &lt;- my_data %&gt;% arrange(Eye, Hair) %&gt;% # make the titles for the facet strips mutate(strip = str_c(&quot;E:&quot;, Eye, &quot; H:&quot;, Hair, &quot;\\nN = &quot;, Count)) f &lt;- fitted(fit24.1, newdata = nd, summary = F) %&gt;% data.frame() %&gt;% set_names(pull(nd, strip)) glimpse(f) ## Rows: 8,000 ## Columns: 16 ## $ `E:Blue H:Black\\nN = 20` &lt;dbl&gt; 24.23364, 13.63719, 19.55832, 26.13812,… ## $ `E:Blue H:Brunette\\nN = 84` &lt;dbl&gt; 86.39334, 97.81758, 72.61219, 79.27535,… ## $ `E:Blue H:Red\\nN = 17` &lt;dbl&gt; 20.54182, 21.13492, 14.66423, 18.26783,… ## $ `E:Blue H:Blond\\nN = 94` &lt;dbl&gt; 77.78793, 89.48316, 110.42520, 95.60393… ## $ `E:Brown H:Black\\nN = 68` &lt;dbl&gt; 73.53992, 64.42050, 71.93415, 70.85845,… ## $ `E:Brown H:Brunette\\nN = 119` &lt;dbl&gt; 107.03246, 141.67926, 98.50453, 105.916… ## $ `E:Brown H:Red\\nN = 26` &lt;dbl&gt; 15.75136, 26.04823, 28.05642, 26.50590,… ## $ `E:Brown H:Blond\\nN = 7` &lt;dbl&gt; 3.623288, 7.148385, 5.402978, 7.771755,… ## $ `E:Green H:Black\\nN = 5` &lt;dbl&gt; 2.225095, 6.290099, 4.262199, 5.220274,… ## $ `E:Green H:Brunette\\nN = 29` &lt;dbl&gt; 33.02071, 31.74593, 33.22965, 27.76614,… ## $ `E:Green H:Red\\nN = 14` &lt;dbl&gt; 19.039984, 14.301296, 13.630850, 19.270… ## $ `E:Green H:Blond\\nN = 16` &lt;dbl&gt; 22.66838, 13.21046, 11.65272, 15.55048,… ## $ `E:Hazel H:Black\\nN = 15` &lt;dbl&gt; 11.094318, 7.580367, 21.340511, 19.2891… ## $ `E:Hazel H:Brunette\\nN = 54` &lt;dbl&gt; 68.04391, 57.82478, 49.07217, 50.65642,… ## $ `E:Hazel H:Red\\nN = 14` &lt;dbl&gt; 14.986971, 16.206171, 10.938308, 14.923… ## $ `E:Hazel H:Blond\\nN = 10` &lt;dbl&gt; 10.105688, 6.760359, 9.542513, 10.05444… Notice that when working with a Poisson model, fitted() defaults to returning estimates in the \\(\\lambda\\) metric. If we want proportions/probabilities, we’ll have to compute them by dividing by the total \\(N\\). In this case \\(N = 592\\), which we get with sum(my_data$Count). Here we convert the data to the long format, compute the proportions, and plot to make the top portion of Figure 24.3. f %&gt;% gather(key, count) %&gt;% mutate(proportion = count / 592) %&gt;% ggplot(aes(x = proportion, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 20, slab_size = .125, outline_bars = T, normalize = &quot;panels&quot;) + scale_x_continuous(breaks = c(0, .1, .2)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, .25)) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;) We’ll have to work a bit to get the deflection differences. If this was a simple multilevel model with a single random grouping variable, we could just use the ranef() function to return the deflections. Like fitted(), it’ll return summaries by default. But you can get the posterior draws with the summary = F argument. But since we used two grouping variables and their interaction, it’d be a bit of a pain to work that way. Happily, we do have a handy alternative. First, if we use the scale = &quot;linear&quot; argument, fitted() will return the draws in the \\(\\lambda\\) scale rather than the original count metric. With the group-level draws in the \\(\\lambda\\) metric, all we need to do is subtract the fixed effect (i.e., the grand mean, the population estimate) from each to convert them to the deflection metric. So below, we’ll make a custom make_deflection() function to do the conversions, redefine our nd data to make our naming conventions a little more streamlined, use fitted() and its scale = &quot;linear&quot; argument to get the draws in the \\(\\lambda\\) metric, wrangle a touch, and use our handy make_deflection() function to convert the results to the deflection metric. I know; that’s a lot. If you get lost, just go step by step and examine the results along the way. # a. make a custom function make_deflection &lt;- function(x) { x - posterior_samples(fit24.1)$b_Intercept } # b. streamline `nd` nd &lt;- my_data %&gt;% arrange(Eye, Hair) %&gt;% mutate(strip = str_c(&quot;E:&quot;, Eye, &quot; H:&quot;, Hair)) # c. use `fitted()` deflections &lt;- fitted(fit24.1, newdata = nd, summary = F, scale = &quot;linear&quot;) %&gt;% # d. wrangle data.frame() %&gt;% set_names(pull(nd, strip)) %&gt;% # e. use the `make_deflection()` function mutate_all(.funs = make_deflection) # what have we done? glimpse(deflections) ## Rows: 8,000 ## Columns: 16 ## $ `E:Blue H:Black` &lt;dbl&gt; 0.4002170, -1.5435698, -0.4282591, -0.2441199, -… ## $ `E:Blue H:Brunette` &lt;dbl&gt; 1.67138593, 0.42673381, 0.88347286, 0.86541257, … ## $ `E:Blue H:Red` &lt;dbl&gt; 0.23493821, -1.10544406, -0.71624886, -0.6023731… ## $ `E:Blue H:Blond` &lt;dbl&gt; 1.5664616, 0.3376799, 1.3026784, 1.0526994, 0.82… ## $ `E:Brown H:Black` &lt;dbl&gt; 1.510303801, 0.009061406, 0.874091152, 0.7531695… ## $ `E:Brown H:Brunette` &lt;dbl&gt; 1.8856075, 0.7971953, 1.1884426, 1.1551389, 0.89… ## $ `E:Brown H:Red` &lt;dbl&gt; -0.03059798, -0.89642081, -0.06744234, -0.230147… ## $ `E:Brown H:Blond` &lt;dbl&gt; -1.5001427, -2.1894841, -1.7147097, -1.4570187, … ## $ `E:Green H:Black` &lt;dbl&gt; -1.98772482, -2.31739364, -1.95187475, -1.854964… ## $ `E:Green H:Brunette` &lt;dbl&gt; 0.70961034, -0.69860606, 0.10178253, -0.18369737… ## $ `E:Green H:Red` &lt;dbl&gt; 0.1590166, -1.4960203, -0.7893244, -0.5489481, -… ## $ `E:Green H:Blond` &lt;dbl&gt; 0.3334465, -1.5753613, -0.9461204, -0.7634230, -… ## $ `E:Hazel H:Black` &lt;dbl&gt; -0.3810916, -2.1308089, -0.3410528, -0.5479710, … ## $ `E:Hazel H:Brunette` &lt;dbl&gt; 1.43262856, -0.09895315, 0.49163215, 0.41755128,… ## $ `E:Hazel H:Red` &lt;dbl&gt; -0.08034337, -1.37097844, -1.00938884, -0.804557… ## $ `E:Hazel H:Blond` &lt;dbl&gt; -0.47442621, -2.24529451, -1.14590312, -1.199499… Now we’re ready to define our difference columns and plot our version of the lower panels in Figure 24.3. deflections %&gt;% transmute(`Blue − Brown @ Black` = `E:Blue H:Black` - `E:Brown H:Black`, `Blue − Brown @ Blond` = `E:Blue H:Blond` - `E:Brown H:Blond`) %&gt;% mutate(`Blue.v.Brown\\n(x)\\nBlack.v.Blond` = `Blue − Brown @ Black` - `Blue − Brown @ Blond`) %&gt;% gather(key, difference) %&gt;% ggplot(aes(x = difference, y = 0)) + geom_rect(aes(xmin = -.1, xmax = .1, ymin = -Inf, ymax = Inf), fill = &quot;white&quot;) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;) If you’re curious, here are the precise summary values. deflections %&gt;% mutate(`Blue − Brown @ Black` = `E:Blue H:Black` - `E:Brown H:Black`, `Blue − Brown @ Blond` = `E:Blue H:Blond` - `E:Brown H:Blond`) %&gt;% mutate(`Blue.v.Brown\\n(x)\\nBlack.v.Blond` = `Blue − Brown @ Black` - `Blue − Brown @ Blond`) %&gt;% pivot_longer(`Blue − Brown @ Black`:`Blue.v.Brown\\n(x)\\nBlack.v.Blond`) %&gt;% group_by(name) %&gt;% mode_hdi(value) ## # A tibble: 3 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;Blue − Brown @ Black&quot; -1.23 -1.70 -0.716 0.95 mode hdi ## 2 &quot;Blue − Brown @ Blond&quot; 2.39 1.73 3.14 0.95 mode hdi ## 3 &quot;Blue.v.Brown\\n(x)\\nBlack.v.Blond&quot; -3.56 -4.50 -2.73 0.95 mode hdi 24.3 Example: Interaction contrasts, shrinkage, and omnibus test “In this section, we consider some contrived data to illustrate aspects of interaction contrasts. Like the eye and hair data, the fictitious data have two attributes with four levels each” (p. 713). Let’s make the data. my_data &lt;- crossing(a = str_c(&quot;a&quot;, 1:4), b = str_c(&quot;b&quot;, 1:4)) %&gt;% mutate(count = c(rep(c(22, 11), each = 2) %&gt;% rep(., times = 2), rep(c(11, 22), each = 2) %&gt;% rep(., times = 2))) head(my_data) ## # A tibble: 6 x 3 ## a b count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 a1 b1 22 ## 2 a1 b2 22 ## 3 a1 b3 11 ## 4 a1 b4 11 ## 5 a2 b1 22 ## 6 a2 b2 22 In the last section, we covered how Kruschke’s broad priors can make fitting these kinds of models difficult when using HMC, particularly with so few cells. Our solution was to reign in the \\(\\sigma\\) hyperparameter for the level-one intercept and to compute the gamma prior for the hierarchical deflections based on a standard deviation of the log of the maximum standard deviation for the data rather than two times that value. Let’s explore more options. This data set has 16 cells. With so few cells, one might argue for a more conservative prior on the hierarchical deflections. Why not ditch the gamma altogether for a half normal centered on zero and with a \\(\\sigma\\) hyperparameter of 1? Even though this is much tighter than Kruschke’s gamma prior approach, it’s still permissive on the \\(\\log\\) scale. As for our intercept, we’ll continue with the same approach from last time. With that in mind, make the stanvars. n_x1_level &lt;- length(unique(my_data$a)) n_x2_level &lt;- length(unique(my_data$b)) n_cell &lt;- nrow(my_data) y_log_mean &lt;- log(sum(my_data$count) / (n_x1_level * n_x2_level)) y_log_sd &lt;- c(rep(0, n_cell - 1), sum(my_data$count)) %&gt;% sd() %&gt;% log() stanvars &lt;- stanvar(y_log_mean, name = &quot;y_log_mean&quot;) + stanvar(y_log_sd, name = &quot;y_log_sd&quot;) Just for kicks, let’s take a quick look at our priors. bind_rows( # define beta[0] tibble(x = seq(from = y_log_mean - (4 * y_log_sd), to = y_log_mean + (4 * y_log_sd), length.out = 1e3)) %&gt;% mutate(density = dnorm(x, y_log_mean, y_log_sd)), # define sigma[beta[x]] tibble(x = seq(from = 0, to = 5, length.out = 1e3)) %&gt;% mutate(density = dnorm(x, 0, 1)) ) %&gt;% mutate(prior = rep(c(&quot;beta[0]&quot;, &quot;sigma[beta*x]&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = density)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Priors&quot;, x = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~prior, scales = &quot;free&quot;, labeller = label_parsed) Fit the model. fit24.2 &lt;- brm(data = my_data, family = poisson, count ~ 1 + (1 | a) + (1 | b) + (1 | a:b), prior = c(prior(normal(y_log_mean, y_log_sd), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.999), stanvars = stanvars, seed = 24, file = &quot;fits/fit24.02&quot;) Review the summary. print(fit24.2) ## Family: poisson ## Links: mu = log ## Formula: count ~ 1 + (1 | a) + (1 | b) + (1 | a:b) ## Data: my_data (Number of observations: 16) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~a (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.17 0.19 0.00 0.66 1.00 2578 4184 ## ## ~a:b (Number of levels: 16) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.31 0.12 0.10 0.57 1.00 2555 2693 ## ## ~b (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.16 0.19 0.00 0.69 1.00 3072 3874 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 2.76 0.20 2.35 3.16 1.00 3655 3416 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We might plot our \\(\\sigma[x]\\) posteriors against our prior to get a sense of how strong it was. posterior_samples(fit24.2) %&gt;% select(starts_with(&quot;sd&quot;)) %&gt;% # set_names(str_c(&quot;expression(sigma&quot;, c(&quot;*a&quot;, &quot;*ab&quot;, &quot;*b&quot;), &quot;)&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;sd&quot;) %&gt;% ggplot(aes(x = value)) + # prior geom_ribbon(data = tibble(value = seq(from = 0, to = 3.5, by =.01)), aes(ymin = 0, ymax = dnorm(value, 0, 1)), fill = &quot;grey50&quot;) + # posterior geom_density(aes(fill = sd), size = 0, alpha = 1/2) + scale_fill_viridis_d(NULL, option = &quot;A&quot;, begin = .2, end = .8, labels = c(expression(sigma[a]), expression(sigma[ab]), expression(sigma[b])), guide = guide_legend(label.hjust = 0)) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank(), legend.position = c(.95, .8), legend.background = element_rect(fill = &quot;transparent&quot;)) Our \\(\\operatorname{Normal}^+ (0, 1)\\) prior is that short medium-gray shape in the background. The posteriors are the taller and more colorful mounds in the foreground. Here’s the top part of Figure 24.4. nd &lt;- my_data %&gt;% mutate(strip = str_c(&quot;a:&quot;, a, &quot; b:&quot;, b, &quot;\\nN = &quot;, count)) fitted(fit24.2, newdata = nd, summary = F) %&gt;% data.frame() %&gt;% set_names(pull(nd, strip)) %&gt;% gather(key, count) %&gt;% mutate(proportion = count / sum(my_data$count)) %&gt;% # plot! ggplot(aes(x = proportion, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 20, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_x_continuous(breaks = c(.05, .1, .15)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, .15)) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;) Like before, we’ll have to work a bit to get the deflection differences. # streamline `nd` nd &lt;- my_data %&gt;% mutate(strip = str_c(&quot;a:&quot;, a, &quot; b:&quot;, b)) # use `fitted()` deflections &lt;- fitted(fit24.2, newdata = nd, summary = F, scale = &quot;linear&quot;) %&gt;% # wrangle data.frame() %&gt;% set_names(pull(nd, strip)) %&gt;% # use the `make_deflection()` function mutate_all(.funs = make_deflection) # what have we done? glimpse(deflections) ## Rows: 8,000 ## Columns: 16 ## $ `a:a1 b:b1` &lt;dbl&gt; 0.25711971, -1.23839562, -0.26126870, -0.50365853, -0.813… ## $ `a:a1 b:b2` &lt;dbl&gt; 0.33022495, -1.14228560, -0.25753173, -0.57421602, -1.134… ## $ `a:a1 b:b3` &lt;dbl&gt; -0.1192869, -1.7308527, -0.4387441, -1.4138614, -1.194692… ## $ `a:a1 b:b4` &lt;dbl&gt; 0.1517327, -1.2943771, -1.2349436, -0.6238803, -1.2244664… ## $ `a:a2 b:b1` &lt;dbl&gt; 0.30888843, -1.15477128, -0.31054063, -0.28530205, -1.082… ## $ `a:a2 b:b2` &lt;dbl&gt; 0.28453788, -1.34459314, -0.17528220, -0.60812582, -0.835… ## $ `a:a2 b:b3` &lt;dbl&gt; 0.1917940, -1.2556402, -1.1079670, -0.7956730, -1.0369094… ## $ `a:a2 b:b4` &lt;dbl&gt; -0.03761425, -1.52218327, -1.06944893, -0.76729942, -1.22… ## $ `a:a3 b:b1` &lt;dbl&gt; 0.01037051, -1.46550902, -0.89236624, -0.90323453, -1.437… ## $ `a:a3 b:b2` &lt;dbl&gt; -0.15321344, -1.61504248, -0.59620940, -1.30486758, -1.11… ## $ `a:a3 b:b3` &lt;dbl&gt; 0.2817608, -1.1612039, -0.1037781, -0.8032772, -1.0008909… ## $ `a:a3 b:b4` &lt;dbl&gt; 0.07675266, -1.33198991, -0.19882258, -0.52332314, -1.105… ## $ `a:a4 b:b1` &lt;dbl&gt; -0.08454566, -1.48251247, -0.85633052, -1.08666326, -1.20… ## $ `a:a4 b:b2` &lt;dbl&gt; -0.05636790, -1.66313517, -0.64427839, -1.37506480, -1.25… ## $ `a:a4 b:b3` &lt;dbl&gt; 0.3882341, -0.9849345, -0.4873845, -0.3698213, -0.8213263… ## $ `a:a4 b:b4` &lt;dbl&gt; 0.16714657, -1.15726058, -0.45468887, -0.67137900, -0.958… Now we’re ready to define some of the difference columns and plot our version of the leftmost lower panel in Figure 24.4. deflections %&gt;% transmute(`a2 - a3 @ b2` = `a:a2 b:b2` - `a:a3 b:b2`, `a2 - a3 @ b3` = `a:a2 b:b3` - `a:a3 b:b3`) %&gt;% transmute(`a2.v.a3\\n(x)\\nb2.v.b3` = `a2 - a3 @ b2` - `a2 - a3 @ b3`) %&gt;% gather(key, difference) %&gt;% ggplot(aes(x = difference, y = 0)) + geom_rect(aes(xmin = -.1, xmax = .1, ymin = -Inf, ymax = Inf), fill = &quot;white&quot;) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-.5, 2.5)) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;) For Figure 24.4, bottom right, we average across the four cells in each quadrant and then compute the contrast. deflections %&gt;% # in this intermediate step, we compute the quadrant averages # `tl` = top left, `br` = bottom right, and so on transmute(tl = (`a:a1 b:b1` + `a:a1 b:b2` + `a:a2 b:b1` + `a:a2 b:b2`) / 4, tr = (`a:a1 b:b3` + `a:a1 b:b4` + `a:a2 b:b3` + `a:a2 b:b4`) / 4, bl = (`a:a3 b:b1` + `a:a3 b:b2` + `a:a4 b:b1` + `a:a4 b:b2`) / 4, br = (`a:a3 b:b3` + `a:a3 b:b4` + `a:a4 b:b3` + `a:a4 b:b4`) / 4) %&gt;% # compute the contrast of interest transmute(`A1.A2.v.A3.A4\\n(x)\\nB1.B2.v.B3.B4` = (tl - bl) - (tr - br)) %&gt;% gather(key, difference) %&gt;% # plot ggplot(aes(x = difference, y = 0)) + geom_rect(aes(xmin = -.1, xmax = .1, ymin = -Inf, ymax = Inf), fill = &quot;white&quot;) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 30, slab_size = .25, outline_bars = T) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-.5, 2.5)) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;) The model presented here has no way to conduct an “ominbus” test of interaction. However, like the ANOVA-style models presented in Chapters 19 and 20, it is easy to extend the model so it has an inclusion coefficient on the interaction deflections. The inclusion coefficient can have values of 0 or 1, and is given a Bernoulli prior. (p. 716) Like we discussed in earlier chapters, this isn’t a feasible approach for brms. However, we can compare this model with a simpler one that omits the interaction. First, fit the model. fit24.3 &lt;- brm(data = my_data, family = poisson, count ~ 1 + (1 | a) + (1 | b), prior = c(prior(normal(y_log_mean, y_log_sd), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.9999), stanvars = stanvars, seed = 24, file = &quot;fits/fit24.03&quot;) Now we can compare them by their stacking weights. model_weights(fit24.2, fit24.3) %&gt;% round(digits = 3) ## fit24.2 fit24.3 ## 1 0 Virtually all the weight went to the interaction model. Also, if we step back and ask ourselves what the purpose of an omnibus text of an interaction is for in this context, we’d conclude such a test is asking the question Is \\(\\sigma_{a \\times b}\\) the same as zero? Let’s look again at that posterior from fit24.2. posterior_samples(fit24.2) %&gt;% ggplot(aes(x = `sd_a:b__Intercept`, y = 0)) + geom_halfeyeh(.width = c(.5, .95)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Does this look the same as zero, to you?&quot;, x = expression(sigma[a%*%b])) + theme(panel.grid = element_blank()) Sure, there’s some uncertainty in that posterior. But that is not zero. We didn’t need an omnibus test or even model comparison via stacking weights to figure that one out. If you wanted to get fancy with it, we might even do a hierarchical variance decomposition. Here the question is what percentage of the hierarchical variance is attributed to a, b and their interaction? Recall that brms returns those variance parameters in the \\(\\sigma\\) metric. So before we can compare them in terms of percentages of the total variance, we have to first have to square them. post &lt;- posterior_samples(fit24.2) %&gt;% transmute(`sigma[a]^2` = sd_a__Intercept^2, `sigma[b]^2` = sd_b__Intercept^2, `sigma[ab]^2` = `sd_a:b__Intercept`^2) %&gt;% mutate(`sigma[total]^2` = `sigma[a]^2` + `sigma[b]^2` + `sigma[ab]^2`) head(post) ## sigma[a]^2 sigma[b]^2 sigma[ab]^2 sigma[total]^2 ## 1 0.0008054698 0.001235870 0.05442256 0.05646390 ## 2 0.0009392091 0.001403585 0.07406198 0.07640477 ## 3 0.0231857141 0.392527369 0.19486678 0.61057987 ## 4 0.0258658175 0.098367269 0.16084684 0.28507993 ## 5 0.0086783402 0.164631688 0.05889993 0.23220996 ## 6 0.0554753005 0.051478990 0.18147490 0.28842919 Now we just need to divide the individual variance parameters by their total and multiply by 100 to get the percent of variance for each. We’ll look at the results in a plot. post %&gt;% pivot_longer(-`sigma[total]^2`) %&gt;% mutate(`% hierarchical variance` = 100 * value / `sigma[total]^2`) %&gt;% ggplot(aes(x = `% hierarchical variance`, y = name)) + geom_halfeyeh(.width = c(.5, .95)) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + theme(panel.grid = element_blank()) Just as each of the variance parameters was estimated with uncertainty, all that uncertainty got propagated into their transformations. Even in the midst of all this uncertainty, it’s clear that a good healthy portion of the hierarchical variance is from the interaction. Again, whatever you might think about \\(a \\times b\\), it’s definitely not zero. 24.4 Log-linear models for contingency tables Bonus: Alternative parameterization The Poisson distribution is widely used for count data. But notice how in our figures, we converted the results to the proportion metric. Once you’re talking about proportions, it’s not hard to further adjust your approach to thinking in terms of probabilities. So instead of thinking about the \\(n\\) within each cell of our contingency table, we might also think about the probability of a given condition. To approach the data this way, we could use a multilevel aggregated binomial model. McElreath covered this in Chapter 10 of his Statistical rethinking. See my (2020) translation of the text into brms code, too. Here’s how to fit that model. fit24.4 &lt;- brm(data = my_data, family = binomial, count | trials(264) ~ 1 + (1 | a) + (1 | b) + (1 | a:b), prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.999), seed = 24, file = &quot;fits/fit24.04&quot;) A few things about the syntax: The aggregated binomial model uses the logit link, just like with typical logistic regression. So when you specify family = binomial, you’re requesting the logit link. The left side of the formula argument, count | trials(264) indicates a few things. First, our criterion is count. The bar | that follows on its right indicates we’d like add additional information about the criterion. In the case of binomial regression, brms requires we specify how many trials the value in each cell of the data is referring to. When we coded trials(264), we indicated each cell was a total count of 264 trials. In case it isn’t clear, here is where the value 264 came from. my_data %&gt;% summarise(total_trials = sum(count)) ## # A tibble: 1 x 1 ## total_trials ## &lt;dbl&gt; ## 1 264 Now look over the summary. print(fit24.4) ## Family: binomial ## Links: mu = logit ## Formula: count | trials(264) ~ 1 + (1 | a) + (1 | b) + (1 | a:b) ## Data: my_data (Number of observations: 16) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~a (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.17 0.18 0.01 0.68 1.00 2658 4026 ## ## ~a:b (Number of levels: 16) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.34 0.12 0.12 0.61 1.00 2399 2487 ## ## ~b (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.17 0.19 0.00 0.66 1.00 2826 3992 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -2.72 0.20 -3.09 -2.27 1.00 3595 3357 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). See that mu = logit part in the second line of the summary? Yep, that’s our link function. Since we used a different likelihood and link function from earlier models, it shouldn’t be surprising the parameters look different. But notice how the aggregated binomial model yields virtually the same results for the top portion of Figure 24.4. nd &lt;- my_data %&gt;% mutate(strip = str_c(&quot;a:&quot;, a, &quot; b:&quot;, b, &quot;\\nN = &quot;, count)) fitted(fit24.4, newdata = nd, summary = F) %&gt;% data.frame() %&gt;% set_names(pull(nd, strip)) %&gt;% gather(key, count) %&gt;% mutate(proportion = count / sum(my_data$count)) %&gt;% # plot! ggplot(aes(x = proportion, y = 0)) + stat_histintervalh(point_interval = mode_hdi, .width = .95, fill = &quot;grey67&quot;, slab_color = &quot;grey92&quot;, breaks = 20, slab_size = .25, outline_bars = T, normalize = &quot;panels&quot;) + scale_x_continuous(breaks = c(.05, .1, .15)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, .15)) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;) To further demonstrate the similarity of this approach to Kruschke’s multilevel Poisson approach, let’s compare the model-based cell estimates for each of the combinations of a and b, by both fit24.2 and fit24.4. # compute the fitted summary statistics rbind(fitted(fit24.2), fitted(fit24.4)) %&gt;% data.frame() %&gt;% # add an augmented version of the data bind_cols(expand(my_data, fit = c(&quot;fit2 (Poisson likelihood)&quot;, &quot;fit4 (binomial likelihood)&quot;), nesting(a, b, count))) %&gt;% mutate(cell = str_c(a, &quot;\\n&quot;, b)) %&gt;% # plot ggplot(aes(x = cell)) + geom_hline(yintercept = c(11, 22), color = &quot;white&quot;, linetype = 2) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, color = fit), fatten = 1.25, position = position_dodge(width = 0.5)) + geom_point(aes(y = count), size = 2) + scale_color_viridis_d(NULL, option = &quot;A&quot;, begin = .4, end = .6) + scale_y_continuous(&quot;count&quot;, breaks = c(0, 11, 22, 33), limits = c(0, 33)) + theme(panel.grid = element_blank(), legend.position = &quot;top&quot;) The black points are the raw data. The colored point-ranges to the left and right of each data point are the posterior means and percentile-based 95% intervals for each of the cells. The results are virtually the same between the two models. Also note how both models partially pooled towards the grand mean. That’s one of the distinctive features of using the hierarchical approach. Wrapping up, this chapter focused on how one might use the Poisson likelihood to model contingency-table data from a multilevel modeling framework. The Poisson likelihood is also handy for count data within a single-level structure, with metric predictors, and with various combinations of metric and nominal predictors. For more practice along those lines, check out Section 10.2 in my project recoding McElreath’s Statistical rethinking. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.3.9000 brms_2.12.0 Rcpp_1.0.4.6 ## [4] janitor_2.0.1 forcats_0.5.0 stringr_1.4.0 ## [7] dplyr_0.8.5 purrr_0.3.4 readr_1.3.1 ## [10] tidyr_1.0.2 tibble_3.0.1 ggplot2_3.3.0 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 snakecase_0.11.0 markdown_1.1 ## [7] base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 ## [10] farver_2.0.3 rstan_2.19.3 svUnit_1.0.3 ## [13] DT_0.13 fansi_0.4.1 mvtnorm_1.1-0 ## [16] lubridate_1.7.8 xml2_1.3.1 bridgesampling_1.0-0 ## [19] knitr_1.28 shinythemes_1.1.2 bayesplot_1.7.1 ## [22] jsonlite_1.6.1 broom_0.5.5 dbplyr_1.4.2 ## [25] shiny_1.4.0.2 compiler_3.6.3 httr_1.4.1 ## [28] backports_1.1.6 assertthat_0.2.1 Matrix_1.2-18 ## [31] fastmap_1.0.1 cli_2.0.2 later_1.0.0 ## [34] prettyunits_1.1.1 htmltools_0.4.0 tools_3.6.3 ## [37] igraph_1.2.5 coda_0.19-3 gtable_0.3.0 ## [40] glue_1.4.0 reshape2_1.4.4 cellranger_1.1.0 ## [43] vctrs_0.3.0 nlme_3.1-144 crosstalk_1.1.0.1 ## [46] xfun_0.13 ps_1.3.3 rvest_0.3.5 ## [49] mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 ## [52] gtools_3.8.2 zoo_1.8-7 scales_1.1.1 ## [55] colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [58] Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 ## [64] StanHeaders_2.21.0-1 loo_2.2.0 stringi_1.4.6 ## [67] highr_0.8 dygraphs_1.1.1.6 pkgbuild_1.0.8 ## [70] rlang_0.4.6 pkgconfig_2.0.3 matrixStats_0.56.0 ## [73] HDInterval_0.2.0 evaluate_0.14 lattice_0.20-38 ## [76] rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 ## [79] tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 ## [82] magrittr_1.5 bookdown_0.18 R6_2.4.1 ## [85] generics_0.0.2 DBI_1.1.0 pillar_1.4.4 ## [88] haven_2.2.0 withr_2.2.0 xts_0.12-0 ## [91] abind_1.4-5 modelr_0.1.6 crayon_1.3.4 ## [94] arrayhelpers_1.1-0 utf8_1.1.4 rmarkdown_2.1 ## [97] grid_3.6.3 readxl_1.3.1 callr_3.4.3 ## [100] threejs_0.3.3 reprex_0.3.0 digest_0.6.25 ## [103] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.3 ## [106] munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.1 References "],
["tools-in-the-trunk.html", "25 Tools in the Trunk 25.1 Reporting a Bayesian analysis 25.2 Functions for computing highest density intervals 25.3 Reparameterization 25.4 Censored Data in JAGS brms 25.5 What Next? Session info", " 25 Tools in the Trunk “This chapter includes some important topics that apply to many different models throughout the book… The sections can be read independently of each other and at any time” (Kruschke, 2015, p. 721). 25.1 Reporting a Bayesian analysis Bayesian data analyses are not yet standard procedure in many fields of research, and no conventional format for reporting them has been established. Therefore, the researcher who reports a Bayesian analysis must be sensitive to the background knowledge of his or her specific audience, and must frame the description accordingly. (p. 721) At the time of this writing (early 2020), this is still the case. See Aczel et al. (2020), Discussion points for Bayesian inference, for a recent discussion from several Bayesian scholars. 25.1.1 Essential points. Recall the basic steps of a Bayesian analysis from Section 2.3 (p. 25): Identify the data, define a descriptive model, specify a prior, compute the posterior distribution, interpret the posterior distribution, and, check that the model is a reasonable description of the data. Those steps are in logical order, with each step building on the previous step. That logical order should be preserved in the report of the analysis. (p. 722) Kruschke then gave recommendations for motivating Bayesian inference. His (2018) paper with Liddell, The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective, might be helpful in this regard. Many of the other points Kruschke made in this section (e.g., adequately reporting the data structure, the priors, evidence for convergence) can be handled by adopting open science practices. If your data and research questions are simple and straightforward, you might find it easy to detail these and other concerns in the primary manuscript. The harsh reality is many journals place tight constraints on word and/or page limits. If your projects are not of the simple and straightforward type, supplemental materials are your friend. Regardless of a journal’s policy on hosting supplemental materials on the official journal website, you can detail your data, priors, MCMC diagnostics, and all the other fine-grained details of your analysis in supplemental documents hosted in publicly-accessible repositories like the Open Science Framework (OSF). If possible, do consider making your data openly available. Regardless of the status of your data, please consider making all your R scripts available as supplementary material. To reiterate from Chapter 3, I strongly recommend checking out R Notebooks for that purpose. They are a type of R Markdown document with augmentations that make them more useful for working scientists. You can learn more about them here and here. And for a more comprehensive overview, check out Xie, Allaire, and Grolemund’s (2020) R markdown: The definitive guide. 25.1.2 Optional points. For more thoughts on robustness checks, check out a couple Gelman’s blog posts, What’s the point of a robustness check? and Robustness checks are a joke, along with the action in the comments section. In addition to posterior predictive checks, which are great (see Kruschke, 2013), consider prior predictive checks, too. For a great introduction to the topic, check out Gabry, Simpson, Vehtari, Betancourt, and Gelman’s (2019) Visualization in Bayesian workflow. 25.1.3 Helpful points. For more ideas on open data, check out Rouder’s (2016) The what, why, and how of born-open data. You might also check out Klein and colleagues’ (2018) A practical guide for transparency in psychological science and Martone, Garcia-Castro, and VandenBos’s (2018) Data sharing in psychology. As to posting your model fits, this could be done in any number of ways, including as official supplemental materials hosted by the journal, on GitHub, or on the OSF. At a base level, this means saving your fits as external files. We’ve already been modeling this with our brm() code throughout this book. With the save argument, we saved the model fits within the fits folder on GitHub. You might adopt a similar approach. But do be warned: brms fit objects contain a copy of the data used to create them. For example, here’s how we might reload fit24.1 from last chapter. fit24.1 &lt;- readRDS(&quot;fits/fit24.01.rds&quot;) By indexing the fit object with $data, you can see the data. library(tidyverse) library(brms) fit24.1$data %&gt;% glimpse() ## Rows: 16 ## Columns: 4 ## $ Count &lt;dbl&gt; 20, 68, 5, 15, 94, 7, 16, 10, 84, 119, 29, 54, 17, 26, 14,… ## $ Hair &lt;fct&gt; Black, Black, Black, Black, Blond, Blond, Blond, Blond, Br… ## $ Eye &lt;chr&gt; &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green&quot;, &quot;Hazel&quot;, &quot;Blue&quot;, &quot;Brown&quot;, &quot;Green… ## $ `Hair:Eye` &lt;chr&gt; &quot;Black_Blue&quot;, &quot;Black_Brown&quot;, &quot;Black_Green&quot;, &quot;Black_Hazel&quot;,… Here’s a quick way to remove the data from the fit object. fit24.1$data &lt;- NULL Confirm it worked. fit24.1$data ## NULL Happily, the rest of the information is still there for you. E.g., here’s the summary. print(fit24.1) ## Family: poisson ## Links: mu = log ## Formula: Count ~ 1 + (1 | Hair) + (1 | Eye) + (1 | Hair:Eye) ## Data: my_data (Number of observations: ) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~Eye (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.83 1.62 0.29 6.37 1.00 3494 5176 ## ## ~Hair (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.93 1.57 0.36 6.16 1.00 3326 5218 ## ## ~Hair:Eye (Number of levels: 16) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.94 0.29 0.54 1.63 1.00 3344 5137 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.22 1.54 -0.04 6.49 1.00 4685 4300 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 25.2 Functions for computing highest density intervals You can find a copy of Kruschke’s scripts, including DBDA2E-utilities.R, at https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/tree/master/data.R. 25.2.1 R code for computing HDI of a grid approximation. We can imagine the grid approximation of a distribution as a landscape of poles sticking up from each point on the parameter grid, with the height of each pole indicating the probability mass at that discrete point. We can imagine the highest density region by visualizing a rising tide: We gradually flood the landscape, monitoring the total mass of the poles that protrude above water, stopping the flood when 95% (say) of the mass remains protruding. The waterline at that moment defines the highest density region (e.g., Hyndman, 1996). (p. 725) HDIofGrid &lt;- function(probMassVec, credMass = 0.95) { # Arguments: # probMassVec is a vector of probability masses at each grid point. # credMass is the desired mass of the HDI region. # Return value: # A list with components: # indices is a vector of indices that are in the HDI # mass is the total mass of the included indices # height is the smallest component probability mass in the HDI # Example of use: For determining HDI of a beta(30,12) distribution # approximated on a grid: # &gt; probDensityVec = dbeta( seq(0,1,length=201) , 30 , 12 ) # &gt; probMassVec = probDensityVec / sum( probDensityVec ) # &gt; HDIinfo = HDIofGrid( probMassVec ) # &gt; show( HDIinfo ) sortedProbMass &lt;- sort(probMassVec, decreasing = TRUE) HDIheightIdx &lt;- min(which(cumsum(sortedProbMass) &gt;= credMass)) HDIheight &lt;- sortedProbMass[HDIheightIdx] HDImass &lt;- sum(probMassVec[probMassVec &gt;= HDIheight]) return(list(indices = which(probMassVec &gt;= HDIheight), mass = HDImass, height = HDIheight)) } I found Kruschke’s description of his HDIofGrid() a bit opaque. Happily, we can understand this function with a little help from an example posted at https://rdrr.io/github/kyusque/DBDA2E-utilities/man/HDIofGrid.html. prob_density_vec &lt;- dbeta(seq(0, 1, length = 201), 30, 12) prob_mass_vec &lt;- prob_density_vec / sum(prob_density_vec) HDI_info &lt;- HDIofGrid(prob_mass_vec) show(HDI_info) ## $indices ## [1] 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 ## [20] 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 ## [39] 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 ## ## $mass ## [1] 0.9528232 ## ## $height ## [1] 0.004448336 To walk that through a bit, prob_density_vec is a vector of density values for \\(\\operatorname{beta} (30, 12)\\) based on 201 evenly-spaced values spanning the parameter space for \\(\\theta\\) (i.e., from 0 to 1). In the second line, we converted those density values to the probability metric by dividing each by their sum, which we then saved as prob_mass_vec. In the third line we shoved those probability values into Kruschke’s HDIofGrid() and saved the results as HDI_info. The output of the fourth line, show(HDI_info), showed us the results (i.e., the contents of HDI_info). As to those results, the values in saved as $indices are the row numbers for all cases in prob_mass_vec that were within the HDI. The value in $mass showed the actual width of the HDI. Because we’re only working with finite samples (i.e., length = 201), we won’t likely get a perfect 95% HDI. The value in $height is the density value for the waterline that defines the highest density region. A plot might make that less abstract. # wrangle tibble(row = 1:length(prob_density_vec), theta = seq(0, 1, length = length(prob_density_vec)), density = prob_mass_vec, cred = if_else(row %in% HDI_info$indices, 1, 0)) %&gt;% # plot ggplot(aes(x = theta, y = density)) + # HDI geom_ribbon(data = . %&gt;% filter(cred == 1), aes(ymin = 0, ymax = density), fill = &quot;grey50&quot;) + # density line geom_line(color = &quot;grey33&quot;) + # waterline geom_hline(yintercept = HDI_info$height, linetype = 2, color = &quot;skyblue&quot;) + # fluff annotate(geom = &quot;text&quot;, x = .2, y = 0.0046, label = &#39;&quot;waterline&quot; that defines all points\\ninside the highest density region&#39;) + annotate(geom = &quot;text&quot;, x = .715, y = 0.01, label = &quot;95.28% HDI&quot;, color = &quot;grey92&quot;, size = 5) + xlab(expression(theta)) + theme(panel.grid = element_blank()) 25.2.2 HDI of unimodal distribution is shortest interval. The algorithms [in the next sections] find the HDI by searching among candidate intervals of mass \\(M\\). The shortest one found is declared to be the HDI. It is an approximation, of course. See Chen &amp; Shao (1999) for more details, and Chen, He, Shao, and Xu (2003) for dealing with the unusual situation of multimodal distributions. (p. 727) 25.2.3 R code for computing HDI of a MCMC sample. In this section, Kruschke provided the code for his HDIofMCMC() function. We recreate it, below, with a few mild formatting changes. HDIofMCMC &lt;- function(sampleVec, credMass = .95) { # Computes highest density interval from a sample of representative values, # estimated as shortest credible interval. # Arguments: # sampleVec # is a vector of representative values from a probability distribution. # credMass # is a scalar between 0 and 1, indicating the mass within the credible # interval that is to be estimated. # Value: # HDIlim is a vector containing the limits of the HDI sortedPts &lt;- sort(sampleVec) ciIdxInc &lt;- ceiling(credMass * length(sortedPts)) nCIs &lt;- length(sortedPts) - ciIdxInc ciWidth &lt;- rep(0, nCIs) for (i in 1:nCIs) { ciWidth[i] &lt;- sortedPts[i + ciIdxInc] - sortedPts[i] } HDImin &lt;- sortedPts[which.min(ciWidth)] HDImax &lt;- sortedPts[which.min(ciWidth) + ciIdxInc] HDIlim &lt;- c(HDImin, HDImax) return(HDIlim) } Let’s continue working with fit24.1 to see how Kruschke’s HDIofMCMC() works. First we need to extract the posterior draws. post &lt;- posterior_samples(fit24.1) Here’s how you might use the function to get the HDIs for the first hierarchical variance parameter. HDIofMCMC(post$sd_Eye__Intercept) ## [1] 0.07972936 5.02625928 Kruschke’s HDIofMCMC() works very much the same as the summary functions from tidybayes. For example, here’s good old tidybayes::mode_hdi(). library(tidybayes) mode_hdi(post$sd_Eye__Intercept) ## y ymin ymax .width .point .interval ## 1 0.8320875 0.07972936 5.026259 0.95 mode hdi If you’d like to use tidybayes to just pull the HDIs without the extra information, just use the hdi() function. hdi(post$sd_Eye__Intercept) ## [,1] [,2] ## [1,] 0.07972936 5.026259 Just in case you’re curious, Kruschke’s HDIofMCMC() function returns the same information as tidybayes::hdi(). Let’s confirm. HDIofMCMC(post$sd_Eye__Intercept) == hdi(post$sd_Eye__Intercept) ## [,1] [,2] ## [1,] TRUE TRUE Identical. 25.2.4 R code for computing HDI of a function. The function described in this section finds the HDI of a unimodal probability density function that is specified mathematically in R. For example, the function can find HDI’s of normal densities or of beta densities or of gamma densities, because those densities are specified as functions in R. (p. 728). If you recall, we’ve been using this funciton off and on since Chapter 4. Here is it, again, with mildly reformated code and parameter names. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { # Arguments: # `name` is R&#39;s name for the inverse cumulative density function # of the distribution. # `width` is the desired mass of the HDI region. # `tol` is passed to R&#39;s optimize function. # Return value: # Highest density iterval (HDI) limits in a vector. # Example of use: For determining HDI of a beta(30, 12) distribution, type # `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)` # Notice that the parameters of the `name` must be explicitly stated; # e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work. # Adapted and corrected from Greg Snow&#39;s TeachingDemos package. incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Here’s how it works for the standard normal distribution. hdi_of_icdf(qnorm, mean = 0, sd = 1) ## [1] -1.959964 1.959964 By default, it returns 95% HDIs. Here’s how it’d work if you wanted the 80% intervals for \\(\\operatorname{beta}(2, 2)\\). hdi_of_icdf(qbeta, shape1 = 2, shape2 = 2, width = .8) ## [1] 0.1958001 0.8041999 25.3 Reparameterization There are situations in which one parameterization is intuitive to express a distribution, but a different parameterization is required for mathematical convenience. For example, we may think intuitively of the standard deviation of a normal distribution, but have to parameterize the distribution in terms of the precision (i.e., reciprocal of the variance). (p. 729) The details in the rest of this section are beyond the scope of this project. 25.4 Censored Data in JAGS brms “In many situations some data are censored, which means that their values are known only within a certain range” (p. 732) Happily, brms is capable of handling censored variables. The setup is a little different from how Kruschke described for JAGS. From the brmsformula section of the brms reference manual (Bürkner, 2020g), we read: With the exception of categorical, ordinal, and mixture families, left, right, and interval censoring can be modeled through y | cens(censored) ~ predictors. The censoring variable (named censored in this example) should contain the values 'left', 'none', 'right', and 'interval' (or equivalently -1, 0, 1, and 2) to indicate that the corresponding observation is left censored, not censored, right censored, or interval censored. For interval censored data, a second variable (let’s call it y2) has to be passed to cens. In this case, the formula has the structure y | cens(censored,y2) ~ predictors. While the lower bounds are given in y, the upper bounds are given in y2 for interval censored data. Intervals are assumed to be open on the left and closed on the right: (y,y2]. We’ll make sense of all this in just a moment. First, let’s see how Kruschke described the example in the text. To illustrate why it is important to include censored data in the analysis, consider a case in which \\(N = 500\\) values are generated randomly from a normal distribution with \\(\\mu = 100\\) and \\(\\sigma = 15\\). Suppose that values above 106 are censored, as are values in the interval between 94 and 100. For the censored values, all we know is the interval in which they occurred, but not their exact value. (p. 732) I’m now aware that we have access to Kruschke’s censored data, so we’ll just make our own based on his description. We’ll start off by simulating the idealized uncensored data, y, based on \\(\\operatorname{Normal} (100, 15)\\). n &lt;- 500 set.seed(25) d &lt;- tibble(y = rnorm(n, mean = 100, sd = 15)) To repeat, Kruschke described two kinds of censoring: “values above 106 are censored”, “as are values in the interval between 94 and 100.” This leaves us with three thresholds. For simplicity, we’ll just name them t1, t2 and t3, with their order based on their numeric values. t1 &lt;- 94 t2 &lt;- 100 t3 &lt;- 106 In the last sentence in the block quote from the brms reference manual, we learned “intervals are assumed to be open on the left and closed on the right: (y,y2].” It’s a little unclear, to me, if this is how Kruschke defined his intervals, but since we’re working with brms we’ll just use this convention. Thus, we will define “values in the interval between 94 and 100” as y &gt;= t1 &amp; y &lt; t2. We will define “values above 106” as y &gt; t3. d &lt;- d %&gt;% mutate(y1 = if_else(y &gt;= t1 &amp; y &lt; t2, t1, if_else(y &gt; t3, t3, y)), y2 = if_else(y &gt;= t1 &amp; y &lt; t2, t2, y), cen = if_else(y &gt;= t1 &amp; y &lt; t2, &quot;interval&quot;, if_else(y &gt; t3, &quot;right&quot;, &quot;none&quot;))) d ## # A tibble: 500 x 4 ## y y1 y2 cen ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 96.8 94 100 interval ## 2 84.4 84.4 84.4 none ## 3 82.7 82.7 82.7 none ## 4 105. 105. 105. none ## 5 77.5 77.5 77.5 none ## 6 93.3 93.3 93.3 none ## 7 126. 106 126. right ## 8 108. 106 108. right ## 9 101. 101. 101. none ## 10 99.1 94 100 interval ## # … with 490 more rows First look at the new cen column. When the values in y are not censored, we see &quot;none&quot;. Otherwise, cen indicates if they are right censored (i.e., &quot;right&quot;) or interval censored (i.e., &quot;interval&quot;). We used those exact terms based on the block quote from the brms reference manual. Now look at y1. When cen == &quot;interval&quot;, those values are the same as the original column y. The same goes for y2. Otherwise, the y1 column contains the relevant lower thresholds values. That is, when cen == &quot;interval&quot;, we see the value for t1 (i.e., 94). When cen == &quot;right&quot;, we see the value for t3 (i.e., 106). For the interval-censored rows, the values in y2 contain the values for the upper threshold (i.e., t2, which is 100). But when the rows are right censored, the values in y2 are simply the same as the original y values. In the rows where cen == &quot;right&quot;, it really doesn’t matter what values you put in the y2 column as long as they aren’t NA. This is because brms will only reference them for rows in which cen == &quot;interval&quot;. I would not spend any time trying to equate this with Kruschke’s exposition at the top of page 734. This is a different coding method from what you might use for JAGS. Let’s make one more data change. Here we’ll make a new variable, y_na, that only has values for which cen == &quot;none&quot;. d &lt;- d %&gt;% mutate(y_na = ifelse(cen == &quot;none&quot;, y, NA)) d ## # A tibble: 500 x 5 ## y y1 y2 cen y_na ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 96.8 94 100 interval NA ## 2 84.4 84.4 84.4 none 84.4 ## 3 82.7 82.7 82.7 none 82.7 ## 4 105. 105. 105. none 105. ## 5 77.5 77.5 77.5 none 77.5 ## 6 93.3 93.3 93.3 none 93.3 ## 7 126. 106 126. right NA ## 8 108. 106 108. right NA ## 9 101. 101. 101. none 101. ## 10 99.1 94 100 interval NA ## # … with 490 more rows In the text, Kruschke reported he had 255 uncensored values (p. 732). Here’s the breakdown of our data. d %&gt;% count(cen) ## # A tibble: 3 x 2 ## cen n ## &lt;chr&gt; &lt;int&gt; ## 1 interval 76 ## 2 none 257 ## 3 right 167 We got really close! Let’s look at what we’ve done with a couple histograms. d %&gt;% pivot_longer(c(y, y_na)) %&gt;% ggplot(aes(x = value)) + geom_histogram(size = .25, binwidth = 2.5, fill = &quot;grey67&quot;, color = &quot;grey92&quot;) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, ncol = 2) Here’s how to fit the first model, which only uses the uncensored values. # define the stanvars mean_y &lt;- mean(d$y_na, na.rm = T) sd_y &lt;- sd(d$y_na, na.rm = T) stanvars &lt;- stanvar(mean_y, name = &quot;mean_y&quot;) + stanvar(sd_y, name = &quot;sd_y&quot;) # fit the model fit25.1 &lt;- brm(data = d, family = gaussian, y_na ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, file = &quot;fits/fit25.01.rds&quot;) Check the summary for the naïve model. print(fit25.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y_na ~ 1 ## Data: d (Number of observations: 257) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 89.87 0.64 88.62 91.14 1.00 2840 2566 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 10.14 0.45 9.30 11.07 1.00 3860 2727 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Relative to the true data-generating process for the original variable y, those parameters look pretty biased. Now let’s practice fitting censored model. This model is one of the rare occasions where we’ll set out initial values for the model intercept. In my first few attempts, brm() had great difficulty initializing the chains using the default initial values. We’ll help it out by setting them at mean_y. Recall that when you set custom initial values in brms, you save them in a list with the number of lists equaling the number of HMC chains. Because we’re using the default chains = 4, well need four lists of intercept start values, mean_y. You can set them to different values, if you’d like. inits &lt;- list(Intercept = mean_y) inits_list &lt;- list(inits, inits, inits, inits) fit25.2 &lt;- brm(data = d, family = gaussian, y1 | cens(cen, y2) ~ 1, prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept), prior(normal(0, sd_y), class = sigma)), chains = 4, cores = 4, stanvars = stanvars, inits = inits_list, # here we insert our start values for the intercept file = &quot;fits/fit25.02.rds&quot;) Now check the summary for the model accounting for the censoring. print(fit25.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y1 | cens(cen, y2) ~ 1 ## Data: d (Number of observations: 500) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 99.46 0.73 98.04 100.90 1.00 2392 2387 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 14.29 0.60 13.18 15.53 1.00 2411 2049 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). All 500 cases were used (Number of observations: 500) and the model did a great job capturing the data-generating parameters. Before we can make our version of Figure 25.4, we’ll need to extract the posterior draws. We’ll start with fit25.1. post &lt;- posterior_samples(fit25.1) %&gt;% mutate(mu = b_Intercept, `(mu-100)/sigma` = (b_Intercept - 100) / sigma) head(post) ## b_Intercept sigma lp__ mu (mu-100)/sigma ## 1 89.01128 10.902672 -969.3481 89.01128 -1.0078924 ## 2 89.78891 9.650831 -967.5287 89.78891 -1.0580531 ## 3 89.96439 9.786381 -967.2537 89.96439 -1.0254674 ## 4 90.20543 9.740210 -967.4664 90.20543 -1.0055812 ## 5 90.07634 10.064154 -967.0646 90.07634 -0.9860402 ## 6 90.43041 9.762487 -967.6899 90.43041 -0.9802415 These subplots look a lot like those from back in Section 16.2. Since this is the last plot of the book, it seems like we should make the effort to stitch all the subplots together with patchwork. To reduce some of the code redundancy with the six subplots of the marginal posteriors, we’ll make a custom geom, geom_hist(). geom_hist &lt;- function(xintercept = xintercept, binwidth = binwidth, ...) { list( geom_vline(xintercept = xintercept, color = &quot;white&quot;, size = 1), geom_histogram(fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = .2, binwidth = binwidth), stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95), scale_y_continuous(NULL, breaks = NULL), theme(panel.grid = element_blank()) ) } Now we have our geom_hist(), here are the first three histograms for the marginal posteriors from fit25.1. p1 &lt;- post %&gt;% ggplot(aes(x = mu)) + geom_hist(xintercept = 100, binwidth = 0.25) + xlab(expression(mu)) p3 &lt;- post %&gt;% ggplot(aes(x = sigma)) + geom_hist(xintercept = 15, binwidth = 0.1) + xlab(expression(sigma)) p4 &lt;- post %&gt;% ggplot(aes(x = `(mu-100)/sigma`)) + geom_hist(xintercept = 0, binwidth = 0.025) + xlab(expression((mu-100)/sigma)) The histogram of the censored data with the posterior predictive density curves superimposed will take a little more work. n_lines &lt;- 50 p2 &lt;- post %&gt;% mutate(iter = 1:n()) %&gt;% slice(1:n_lines) %&gt;% expand(nesting(mu, sigma, iter), y_na = seq(from = 40, to = 120, by = 1)) %&gt;% mutate(density = dnorm(x = y_na, mean = mu, sd = sigma)) %&gt;% ggplot(aes(x = y_na)) + geom_histogram(data = d, aes(y = stat(density)), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = 2.5) + geom_line(aes(y = density, group = iter), size = 1/4, alpha = 1/3, color = &quot;grey25&quot;) + scale_x_continuous(&quot;data with posterior predictive lines&quot;, limits = c(40, 110)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Now extract the posterior draws from our censored model, fit25.2, and repeat the process. post &lt;- posterior_samples(fit25.2) %&gt;% mutate(mu = b_Intercept, `(mu-100)/sigma` = (b_Intercept - 100) / sigma) p5 &lt;- post %&gt;% ggplot(aes(x = mu)) + geom_hist(xintercept = 100, binwidth = 0.15) + xlab(expression(mu)) p7 &lt;- post %&gt;% ggplot(aes(x = sigma)) + geom_hist(xintercept = 15, binwidth = 0.1) + xlab(expression(sigma)) p8 &lt;- post %&gt;% ggplot(aes(x = `(mu-100)/sigma`)) + geom_hist(xintercept = 0, binwidth = 0.01) + xlab(expression((mu-100)/sigma)) p6 &lt;- post %&gt;% mutate(iter = 1:n()) %&gt;% slice(1:n_lines) %&gt;% expand(nesting(mu, sigma, iter), y_na = seq(from = 40, to = 120, by = 1)) %&gt;% mutate(density = dnorm(x = y_na, mean = mu, sd = sigma)) %&gt;% ggplot(aes(x = y_na)) + geom_histogram(data = d, aes(y = stat(density)), color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = 2.5) + geom_line(aes(y = density, group = iter), size = 1/4, alpha = 1/3, color = &quot;grey25&quot;) + scale_x_continuous(&quot;data with posterior predictive lines&quot;, limits = c(40, 110)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Load patchwork, combine the subplots, and annotate a bit. library(patchwork) ((p1 | p2) / (p3 | p4) / (p5 | p6) / (p7 | p8)) + plot_annotation(title = &quot;This is our final plot, together.&quot;, caption = expression(atop(italic(&quot;Upper quartet&quot;)*&quot;: Censored data omitted from analysis; parameter estimates are too small. &quot;, italic(&quot;Lower quartet&quot;)*&quot;: Censored data imputed in known bins; parameter estimates are accurate.&quot;))) &amp; theme(plot.caption = element_text(hjust = 0)) 25.5 What Next? “If you have made it this far and you are looking for more, you might peruse posts at [Kruschke’s] blog, [https://doingbayesiandataanalysis.blogspot.com/], and search there for topics that interest you.” In addition to the other references Kruschke mentioned, you might also check out McElreath’s (2015) Statistical rethinking. The first edition came out in 2015 and the second was released sometime in early 2020. Much like this project, I have recoded Statistical rethinking in a bookdown form, here (Kurz, 2020). You can also find other tutorial material at my academic blog, https://solomonkurz.netlify.com/post/. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.0.0 tidybayes_2.0.3.9000 brms_2.12.0 ## [4] Rcpp_1.0.4.6 forcats_0.5.0 stringr_1.4.0 ## [7] dplyr_0.8.5 purrr_0.3.4 readr_1.3.1 ## [10] tidyr_1.0.2 tibble_3.0.1 ggplot2_3.3.0 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.4.1 rstudioapi_0.11 farver_2.0.3 ## [10] rstan_2.19.3 svUnit_1.0.3 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 ## [16] xml2_1.3.1 bridgesampling_1.0-0 knitr_1.28 ## [19] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 ## [22] broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 ## [25] compiler_3.6.3 httr_1.4.1 backports_1.1.6 ## [28] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] cli_2.0.2 later_1.0.0 htmltools_0.4.0 ## [34] prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.4.0 ## [40] reshape2_1.4.4 cellranger_1.1.0 vctrs_0.3.0 ## [43] nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ## [46] ps_1.3.3 rvest_0.3.5 mime_0.9 ## [49] miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 ## [52] zoo_1.8-7 scales_1.1.1 colourpicker_1.0 ## [55] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 ## [58] parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 loo_2.2.0 ## [64] StanHeaders_2.21.0-1 stringi_1.4.6 dygraphs_1.1.1.6 ## [67] pkgbuild_1.0.8 rlang_0.4.6 pkgconfig_2.0.3 ## [70] matrixStats_0.56.0 HDInterval_0.2.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [76] labeling_0.3 processx_3.4.2 tidyselect_1.0.0 ## [79] plyr_1.8.6 magrittr_1.5 bookdown_0.18 ## [82] R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [85] pillar_1.4.4 haven_2.2.0 withr_2.2.0 ## [88] xts_0.12-0 abind_1.4-5 modelr_0.1.6 ## [91] crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 ## [94] rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 ## [97] callr_3.4.3 threejs_0.3.3 reprex_0.3.0 ## [100] digest_0.6.25 xtable_1.8-4 httpuv_1.5.2 ## [103] stats4_3.6.3 munsell_0.5.0 shinyjs_1.1 References "],
["references.html", "References", " References "]
]
