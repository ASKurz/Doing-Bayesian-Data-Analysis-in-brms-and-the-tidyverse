--- 
title: "*Doing Bayesian Data Analysis* in brms and the tidyverse"
subtitle: "version 0.4.0"
author: "A Solomon Kurz"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
    split_bib: yes
documentclass: book
bibliography: bib_zotero.txt
biblio-style: apalike
csl: apa.csl
link-citations: yes
geometry:
  margin = 0.5in
urlcolor: blue
highlight: tango
header-includes:
  \usepackage{underscore}
  \usepackage[T1]{fontenc}
github-repo: ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse
twitter-handle: SolomonKurz
description: "This project is an attempt to re-express the code in Kruschke's (2015) textbook. His models are re-fit in brms, plots are redone with ggplot2, and the general data wrangling code predominantly follows the tidyverse style."
---

# What and why {-}

Kruschke began his text with "This book explains how to actually do Bayesian data analysis, by real people (like you), for realistic data (like yours)." In the same way, this project is designed to help those real people do Bayesian data analysis. My contribution is converting Kruschke's JAGS and Stan code for use in Bürkner's [**brms** package](https://github.com/paul-buerkner/brms) [@R-brms; @burknerBrmsPackageBayesian2017; @burknerAdvancedBayesianMultilevel2018], which makes it easier to fit Bayesian regression models in **R** [@R-base] using Hamiltonian Monte Carlo. I also prefer plotting and data wrangling with the packages from the [**tidyverse**](http://style.tidyverse.org) [@R-tidyverse; @wickhamWelcomeTidyverse2019]. So we'll be using those methods, too.

This project is not meant to stand alone. It's a supplement to the second edition of Kruschke's [-@kruschkeDoingBayesianData2015] [*Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan*](https://sites.google.com/site/doingbayesiandataanalysis/). Please give the source material some love.

## **R** setup {-}

To get the full benefit from this ebook, you'll need some software. Happily, everything will be free (provided you have access to a decent personal computer and an good internet connection).

First, you'll need to install **R**, which you can learn about at [https://cran.r-project.org/](https://cran.r-project.org/).

Though not necessary, your **R** experience might be more enjoyable if done through the free RStudio interface, which you can learn about at [https://rstudio.com/products/rstudio/](https://rstudio.com/products/rstudio/).

Once you have installed **R**, execute the following to install the bulk of the add-on packages. This will probably take a few minutes to finish. Go make yourself a coffee.

```{r, eval = F}
packages <- c("bayesplot", "brms", "coda", "cowplot", "cubelyr", "devtools", "fishualize", "GGally", "ggdist", "ggExtra", "ggforce", "ggmcmc", "ggridges", "ggthemes", "janitor", "lisa", "loo", "palettetown", "patchwork", "psych", "remotes", "rstan", "santoku", "scico", "tidybayes", "tidyverse")

install.packages(packages, dependencies = T)
```

A few of the other packages are not officially available via the Comprehensive R Archive Network (CRAN; https://cran.r-project.org/). You can download them directly from GitHub by executing the following.

```{r, eval = F}
remotes::install_github("clauswilke/colorblindr")
devtools::install_github("dill/beyonce")
devtools::install_github("ropenscilabs/ochRe")
```

It's possible you'll have problems installing some of these packages. Here are some likely suspects and where you can find help:

* for difficulties installing **brms**, go to [https://github.com/paul-buerkner/brms#how-do-i-install-brms](https://github.com/paul-buerkner/brms#how-do-i-install-brms) or search around in the [**brms** section of the Stan forums ](https://discourse.mc-stan.org/c/interfaces/brms/36); and
* for difficulties installing **rstan**, go to [https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started).

## We have updates {-}

For a brief rundown of the version history, we have:

### Version 0.1.0. {-}

I released the 0.1.0 version of this project in February 17, 2020. It was the first [fairly] complete draft including material from all the chapters in Kruschke's text. The supermajority of Kruschke's JAGS and Stan models were fit **brms** 2.11.5. The results were saved in the [`fits` folder on GitHub](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/tree/master/fits) and most of the results are quite comparable to those in the original text. We also reproduced most of the data-related figures and tables and little subpoints and examples sprinkled throughout Kruschke's prose.

### Version 0.2.0. {-}

The 0.2.0 update came in May 19, 2020. Noteworthy changes included:

* reproducing the simulation necessary for Figure 7.3 (see [GitHub issue #14](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/14)) with help from Cardy Moten III ([\@cmoten](https://github.com/cmoten));
* with guidance from Bjørn Peare Bartholdy ([\@bbartholdy](https://github.com/bbartholdy)), Mladen Jovanović ([\@mladenjovanovic](https://github.com/mladenjovanovic)), Cory Whitney ([\@CWWhitney](https://github.com/CWWhitney)), and Brenton M. Wiernik ([\@bwiernik](https://github.com/bwiernik)), we improved in-text citations and reference sections using [BibTex](http://www.bibtex.org/) [@BibTeX2020], [Better BibTeX](https://github.com/retorquere/zotero-better-bibtex) [@heynsBetterBibTeXZotero2020], and [zotero](https://www.zotero.org/) [@ZoteroYourPersonal2020];
* the plot resolution increased with `fig.retina = 2.5`; and
* small code, hyperlink, and typo corrections.

### Version 0.3.0. {-}

The 0.3.0 update came in September 22, 2020. Noteworthy changes included:

* adding the [Kruschke-style model diagrams](https://solomonkurz.netlify.app/post/make-model-diagrams-kruschke-style/) throughout the text (e.g., [Figure 8.5][Example: Difference of biases]);
* adding chapter-specific plotting schemes with help from the [**cowplot** package](https://wilkelab.org/cowplot) [@R-cowplot], Wilke's [-@wilkeFundamentalsDataVisualization2019] [*Fundamentals of data visualization*](https://clauswilke.com/dataviz/), and many other great color-scheme packages; 
* an overhaul to the plotting workflow in [Section 6.4.1][Prior knowledge expressed as a beta distribution.]; and
* updating all model fits with **brms** version 2.13.5.

### Version 0.4.0. {-}

Welcome to version 0.4.0! Noteworthy changes include:

* using the Metropolis algorithm to fit the bivariate Bernoulli model for Figure 7.6 ([Section 7.4.3][The posterior via the Metropolis algorithm.]), thanks to help from [Omid Ghasemi](https://github.com/OmidGhasemi21);
* corrections to mistakes around the `lag()` and `lead()` functions in [Section 7.5.2][MCMC accuracy.];
* an added bonus section clarifying the pooled standard deviation for standardized mean differences ([Section 16.3.0.1][Bonus: Pooled standard deviation.]);
* refining the custom `stat_wilke()` plotting function in [Chapter 18][Metric Predicted Variable with Multiple Metric Predictors];
* an overhaul of the bonus section covering effect sizes ([Section 19.6][~~Exercises~~ Walk out an effect size]);
* refining/correcting the threshold workflow for univariable logistic regression models ([Chapter 21][Dichotomous Predicted Variable]);
* fixing the divergent transitions issue for the robust logistic regression model by adding boundaries on the prior ([Section 21.3][Robust logistic regression]);
* corrections to a few incorrectly computed effect sizes in [Chapter 23][Ordinal Predicted Variable];
* the addition of a new bonus section ([Section 22.3.3.1.1][Bonus: Consider the interceps-only softmax model.]) highlighting the benefits of the intercepts-only softmax model;
* expansions to the material on censored data ([Section 25.4][Censored Data in ~~JAGS~~ brms]) and the addition of a brief introduction to truncated data ([Section 25.4.4][Bonus: Truncation.]); and
* updating all HMC fits to the current version of **brms** (2.15.0).

### We're not done yet and I could use your help. {-}

There are some minor improvements I'd like to add in future versions. Most importantly, I'd like to patch up the content holes. A few simulations, figures, and models are beyond my current skill set. I've opened separate GitHub issues for the most important ones and they are as follows:

* the effective-sample-size simulations in Section 7.5.2 and the corresponding plots in Figures 7.13 and 7.14 ([issue #15](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/15)),
* several of the simulations in Sections 11.1.4, 11.3.1, and 11.3.2 and their corresponding figures (issues [#16](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/16), [#17](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/17), [#18](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/18), and [#19](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/19)),
* the stopping-rule simulations in Section 13.3.2 and their corresponding figures ([issue #20](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/20)),
* the data necessary to properly reproduce the HMC proposal schematic presented in Section 14.1 and Figures 14.1 through 14.3 ([issue #21](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/21)), and
* the conditional logistic models of Section 22.3.3.2 ([issue #22](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/22)), which you might also chime in on in the [Nominal data and Kruschke’s “conditional logistic” approach](https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433) thread in the Stan forums.

If you know how to conquer any of these unresolved challenges, I'd love to hear all about it. In addition, please feel free to open a new [GitHub issue](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues) if you find any flaws in the other sections of the ebook.

## Thank-you's are in order {-}

Before we enter the primary text, I'd like to thank the following for their helpful contributions:

* Bjørn Peare Bartholdy ([\@bbartholdy](https://github.com/bbartholdy)),
* David Baumeister ([\@xdavebx](https://github.com/xdavebx)),
* Paul-Christian Bürkner ([\@paul-buerkner](https://github.com/paul-buerkner)),
* Andrew Gelman ([\@andrewgelman](https://github.com/andrewgelman)),
* Omid Ghasemi ([\@OmidGhasemi21](https://github.com/OmidGhasemi21)),
* Mladen Jovanović ([\@mladenjovanovic](https://github.com/mladenjovanovic)),
* Matthew Kay ([\@mjskay](https://github.com/mjskay)),
* TJ Mahr ([\@tjmahr](https://github.com/tjmahr)),
* Cardy Moten III ([\@cmoten](https://github.com/cmoten)),
* Lukas Neugebauer ([\@LukasNeugebauer](https://github.com/LukasNeugebauer)),
* Demetri Pananos ([\@Dpananos](https://github.com/dpananos)),
* Peter Ralph ([\@petrelharp](https://github.com/petrelharp)),
* Aki Vehtari ([\@avehtari](https://github.com/avehtari)),
* Matti Vuorre ([\@mvuorre](https://github.com/mvuorre)),
* Cory Whitney ([\@CWWhitney](https://github.com/CWWhitney)), and
* Brenton M. Wiernik ([\@bwiernik](https://github.com/bwiernik)).

## License and citation {-}

This book is licensed under the Creative Commons Zero v1.0 Universal license. You can learn the details, [here](https://github.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse/blob/master/LICENSE). In short, you can use my work. Just please give me the appropriate credit the same way you would for any other scholarly resource. Here's the citation information:

```{r, eval = F}
@book{kurzDoingBayesianDataAnalysis2022,
  title = {Doing {{Bayesian}} data analysis in brms and the tidyverse},
  author = {Kurz, A. Solomon},
  year = {2022},
  month = {4},
  edition = {version 1.0.0},
  url = {https://bookdown.org/content/3686/}
}
```


<!--chapter:end:index.Rmd-->


# What's in This Book (Read This First!)

## Real people can read this book

Kruschke began his [-@kruschkeDoingBayesianData2015, p.1] text with "This book explains how to actually do Bayesian data analysis, by real people (like you), for realistic data (like yours)." Agreed. Similarly, this project is designed to help those real people do Bayesian data analysis. While I'm at it, I may as well explicate my assumptions about you.

If you're looking at this project, I'm guessing you’re a graduate student, a post-graduate academic or researcher of some sort. Which means I'm presuming you have at least a 101-level foundation in statistics. In his text, it seems like Kruschke presumed his readers would have a good foundation in calculus, too. I make no such presumption. But if your stats 101 chops are rusty, check out Legler and Roback's free [-@leglerBroadeningYourStatistical2019] **bookdown** text, [*Broadening your statistical horizons*](https://bookdown.org/roback/bookdown-bysh/) or Navarro's free [-@navarroLearningStatistics2019] text, [*Learning statistics with R: A tutorial for psychology students and other beginners*](https://learningstatisticswithr.com/).

I presume a basic working fluency in **R** and a vague idea about what the **tidyverse** is. Kruschke does some **R** warm-up in [Chapter 3][The R Programming Language], and I follow suit. But if you're totally new to **R**, you might also consider starting with Peng's [-@pengProgrammingDataScience2020] [*R programming for data science*](https://bookdown.org/rdpeng/rprogdatascience/). The best introduction to the **tidyvese**-style of data analysis I've found is Grolemund and Wickham's [-@grolemundDataScience2017] [*R for data science*](http://r4ds.had.co.nz). If you prefer learning by video, Navarro has some nice introductory playlists [here](https://www.youtube.com/c/DanielleNavarro77/playlists).

## What's in this book

This ebook is not meant to stand alone. It's a supplement to the second edition of Kruschke's [-@kruschkeDoingBayesianData2015] [*Doing Bayesian data analysis*](https://sites.google.com/site/doingbayesiandataanalysis/). I follow the structure of his text, chapter by chapter, translating his analyses into **brms** and **tidyverse** code. However, many of the sections in the text are composed entirely of equations and prose, leaving us nothing to translate. When we run into those, the corresponding sections in this ebook may be missing.

Also beware the content herein will depart at times from Kruschke's source material. Bayesian data analysis with HMC is an active area of development in terms of both statistical methods and software implementation. There will also be times when my thoughts and preferences on Bayesian data analysis diverge a bit from Kruschke's. In those places of divergence, I will often provide references and explanations.

I use a handful of formatting conventions gleaned from [*R4DS*](http://r4ds.had.co.nz/introduction.html#running-r-code) and Xie, Allaire, and Grolemund's [-@xieMarkdownDefinitiveGuide2022] [*R markdown: The definitive guide*](https://bookdown.org/yihui/rmarkdown/software-info.html). For example:

* I put **R** and **R** packages (e.g., **brms**) in **boldface**.
* **R** code blocks and their output appear in a gray background. E.g., 

```{r}
2 + 2
```

* Did you notice how there were two strips of gray background, there? The first one designated the actual code. The second one was the output of that code, and the output of a code block often begins with `##`.
* Functions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., `brm()`).
* When I want to make explicit what packages a given function comes from, I insert the double-colon operator `::` between the package name and the function (e.g., `tidyr::pivot_longer()`).
* **R** objects, such as data or function arguments, are in typewriter font atop a gray background (e.g., `d` or `size = 2`).
* Hyperlinks are denoted by their typical [blue-colored font](https://rmarkdown.rstudio.com/authoring_basics.html).

## What's new in the second edition

This is my first attempt at this project. There's nothing new from my end.

## Gimme feedback (be polite)

I am not a statistician and I have no formal background in computer science. I finished my PhD in clinical psychology in 2018. During my graduate training I developed an unexpected interest in applied statistics and programming. I became an **R** user in 2015 and started learning about Bayesian statistics around 2013. There is still so much to learn, so my apologies for when my code appears dated or inelegant. There will also be occasions in which I'm not yet sure how to reproduce models or plots in the text. Which is all to say, suggestions on how to improve my code are welcome. My preference is to do so by opening a [GitHub issue](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues). You can find more guidelines on contributing to this ebook in the [GitHub CONTRIBUTING section](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/blob/master/CONTRIBUTING.md). If you'd like to learn more about me, you can find my website at [https://solomonkurz.netlify.com](https://solomonkurz.netlify.com/).

## Thank you!

While in grad school, I benefited tremendously from free online content. This project and others like it (see [here](https://solomonkurz.netlify.app/bookdown/) and [here](https://solomonkurz.netlify.app/post/)) are my attempts to pay it forward. As soon as you've gained a little proficiency, do consider doing to same.

I addition to great texts like Kruschke's, I'd like to point out a few other important resources that have allowed me to complete a project like this:

* Jenny Bryan's [*Happy Git and GitHub for the useR*](https://happygitwithr.com/) [@bryanHappyGitGitHub2020] is the reference that finally got me working on GitHub.
* Again and again, I return to Grolemund and Wickham's [-@grolemundDataScience2017] [*R for data science*](https://r4ds.had.co.nz) to learn about the **tidyverse** way of coding.
* Yihui Xie's [-@xieBookdownAuthoringBooks2022] [*bookdown: Authoring books and technical documents with R Markdown*](https://bookdown.org/yihui/bookdown/) is the primary source from which I learned how to make online books like this.
* While you're at it, also check out Xie, Allaire, and Grolemund's [-@xieMarkdownDefinitiveGuide2022] [*R Markdown: The definitive guide*](https://bookdown.org/yihui/rmarkdown/) to learn more about how you can mix your **R** code with well-formatted prose.

If you haven't already, bookmark these resources and share them with your friends. With time and practice, these resources might help you write books and other online projects with **R**.

## Session info {-}

At the end of every chapter, I use the `sessionInfo()` function to help make my results more reproducible.

```{r}
sessionInfo()
```

```{r, echo = F, message = F, warning = F, results = "hide"}
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:01.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

```{r, echo = F, eval = F}
# (PART) THE BASICS: MODELS, PROBABILITY, BAYES' RULE, AND R {-}

# In this part of the project, we mainly reproduce figures. But we will fit models with increasing regularity as the chapters progress.
```

# Introduction: Credibility, Models, and Parameters

> The goal of this chapter is to introduce the conceptual framework of Bayesian data analysis. Bayesian data analysis has two foundational ideas. The first idea is that Bayesian inference is reallocation of credibility across possibilities. The second foundational idea is that the possibilities, over which we allocate credibility, are parameter values in meaningful mathematical models. [@kruschkeDoingBayesianData2015, p. 15]

## Bayesian inference is reallocation of credibility across possibilities

The first step toward making Figure 2.1 is putting together a data object. To help with that, we'll open up the **tidyverse**.

```{r, message = F, warning = F}
library(tidyverse)

d <-
  crossing(iteration = 1:3,
           stage     = factor(c("Prior", "Posterior"),
                              levels = c("Prior", "Posterior"))) %>% 
  expand(nesting(iteration, stage),
         Possibilities = LETTERS[1:4]) %>% 
  mutate(Credibility   = c(rep(.25, times = 4),
                           0, rep(1/3, times = 3),
                           0, rep(1/3, times = 3),
                           rep(c(0, .5), each = 2),
                           rep(c(0, .5), each = 2),
                           rep(0, times = 3), 1))
```

When making data with many repetitions in the rows, it's good to have the `tidyr::expand()` function up your sleeve. Go [here](https://tidyr.tidyverse.org/reference/expand.html) to learn more.

We can take a look at the top few rows of the data with the `head()` function.

```{r}
head(d)
```

Before we attempt Figure 2.1, we'll need two supplemental data frames. The first one, `text`, will supply the coordinates for the annotation in the plot. The second, `arrow`, will supply the coordinates for the arrows.

```{r}
text <-
  tibble(Possibilities = "B",
         Credibility   = .75,
         label         = str_c(LETTERS[1:3], " is\nimpossible"),
         iteration     = 1:3,
         stage         = factor("Posterior", 
                                levels = c("Prior", "Posterior")))
arrow <-
  tibble(Possibilities = LETTERS[1:3],
         iteration     = 1:3) %>% 
  expand(nesting(Possibilities, iteration),
         Credibility = c(0.6, 0.01)) %>% 
  mutate(stage = factor("Posterior", levels = c("Prior", "Posterior")))
```

Now we're ready to code our version of Figure 2.1.

```{r, fig.width = 6, fig.height = 4}
d %>%
  ggplot(aes(x = Possibilities, y = Credibility)) +
  geom_col(color = "grey30", fill = "grey30") +
  # annotation in the bottom row
  geom_text(data = text, 
            aes(label = label)) +
  # arrows in the bottom row
  geom_line(data = arrow,
            arrow = arrow(length = unit(0.30, "cm"), 
                          ends = "first", type = "closed")) +
  facet_grid(stage ~ iteration) +
  theme(axis.ticks.x = element_blank(),
        panel.grid = element_blank(),
        strip.text.x = element_blank())
```

We will take a similar approach to make our version of Figure 2.2. But this time, we'll define our supplemental data sets directly in `geom_text()` and `geom_line()`. It's good to have both methods up your sleeve. Also notice how we simply fed our primary data set directly into `ggplot()` without saving it, either.

```{r, fig.width = 2.25, fig.height = 4}
# primary data
crossing(stage         = factor(c("Prior", "Posterior"),
                              levels = c("Prior", "Posterior")),
         Possibilities = LETTERS[1:4]) %>% 
  mutate(Credibility = c(rep(0.25, times = 4),
                         rep(0,    times = 3), 
                         1)) %>%
  
  # plot!
  ggplot(aes(x = Possibilities, y = Credibility)) +
  geom_col(color = "grey30", fill = "grey30") +
  # annotation in the bottom panel
  geom_text(data = tibble(
    Possibilities = "B",
    Credibility   = .8,
    label         = "D is\nresponsible",
    stage         = factor("Posterior", levels = c("Prior", "Posterior"))
  ), aes(label = label)
  ) +
  # the arrow
  geom_line(data = tibble(
    Possibilities = LETTERS[c(4, 4)],
    Credibility   = c(.25, .99),
    stage         = factor("Posterior", levels = c("Prior", "Posterior"))
  ),
  arrow = arrow(length = unit(0.30, "cm"), ends = "last", type = "closed"),
  color = "grey92") +
  facet_wrap(~ stage, ncol = 1) +
  theme(axis.ticks.x = element_blank(),
        panel.grid = element_blank())
```

### Data are noisy and inferences are probabilistic.

Now on to Figure 2.3. I'm pretty sure the curves in the plot are Gaussian, which we'll make with the `dnorm()` function. After a little trial and error, their standard deviations look to be 1.2. However, it's tricky placing those curves in along with the probabilities, because the probabilities for the four discrete sizes (i.e., 1 through 4) are in a different metric than the Gaussian density curves. Since the probability metric for the four discrete sizes are the primary metric of the plot, we need to rescale the curves using a little algebra, which we do in the data code below. After that, the code for the plot is relatively simple.

```{r, fig.width = 2.25, fig.height = 2.5}
# data
tibble(mu = 1:4,
       p  = .25) %>% 
  expand(nesting(mu, p), 
         x = seq(from = -2, to = 6, by = .1)) %>% 
  mutate(density = dnorm(x, mean = mu, sd = 1.2)) %>% 
  mutate(d_max = max(density)) %>% 
  mutate(rescale = p / d_max) %>% 
  mutate(density = density * rescale) %>% 
  
  # plot!
  ggplot(aes(x = x)) +
  geom_col(data = . %>% distinct(mu, p),
           aes(x = mu, y = p),
           fill = "grey67", width = 1/3) +
  geom_line(aes(y = density, group = mu)) +
  scale_x_continuous(breaks = 1:4) +
  scale_y_continuous(breaks = 0:5 / 5) +
  coord_cartesian(xlim = c(0, 5),
                  ylim = c(0, 1)) +
  labs(title = "Prior",
       x = "Possibilities", 
       y = "Credibility") +
  theme(axis.ticks.x = element_blank(),
        panel.grid = element_blank())
```

We can use the same basic method to make the bottom panel of Figure 2.3. Kruschke gave the relative probability values on page 21. Notice that they sum perfectly to 1. The only other notable change from the previous plot is our addition of a `geom_point()` section, the data in which we defined on the fly.

```{r, fig.width = 2.25, fig.height = 2.5}
tibble(mu = 1:4,
       p  = c(.11, .56, .31, .02)) %>% 
  expand(nesting(mu, p), 
         x = seq(from = -2, to = 6, by = .1)) %>% 
  mutate(density = dnorm(x, mean = mu, sd = 1.2)) %>% 
  mutate(d_max = max(density)) %>% 
  mutate(rescale = p / d_max) %>% 
  mutate(density = density * rescale) %>% 
  
  # plot!
  ggplot() +
  geom_col(data = . %>% distinct(mu, p),
           aes(x = mu, y = p),
           fill = "grey67", width = 1/3) +
  geom_line(aes(x = x, y = density, group = mu)) +
  geom_point(data = tibble(x = c(1.75, 2.25, 2.75), y = 0),
             aes(x = x, y = y),
             size = 3, color = "grey33", alpha = 3/4) +
  scale_x_continuous(breaks = 1:4) +
  scale_y_continuous(breaks = 0:5 / 5) +
  coord_cartesian(xlim = c(0, 5),
                  ylim = c(0, 1)) +
  labs(title = "Posterior",
       x = "Possibilities", 
       y = "Credibility") +
  theme(axis.ticks.x = element_blank(),
        panel.grid = element_blank())
```

> In summary, the essence of Bayesian inference is reallocation of credibility across possibilities. The distribution of credibility initially reflects prior knowledge about the possibilities, which can be quite vague. Then new data are observed, and the credibility is re-allocated. Possibilities that are consistent with the data garner more credibility, while possibilities that are not consistent with the data lose credibility. Bayesian analysis is the mathematics of re-allocating credibility in a logically coherent and precise way. (p. 22)

## Possibilities are parameter values in descriptive models

"A key step in Bayesian analysis is defining the set of possibilities over which credibility is allocated. *This is not a trivial step*, because there might always be possibilities beyond the ones we include in the initial set" (p. 22, *emphasis* added).

In the last section, we used the `dnorm()` function to make curves following the normal distribution (a.k.a. the Gaussian distribution). Here we'll do that again, but also use the `rnorm()` function to simulate actual data from that same normal distribution. Behold Figure 2.4.a.

```{r, fig.width = 3, fig.height = 2.5}
# set the seed to make the simulation reproducible
set.seed(2)
# simulate the data with `rnorm()`
d <- tibble(x = rnorm(2000, mean = 10, sd = 5))

# plot!
ggplot(data = d, aes(x = x)) +
  geom_histogram(aes(y = ..density..),
                 binwidth = 1, fill = "grey67", 
                 color = "grey92", size = 1/10) +
  geom_line(data = tibble(x = seq(from = -6, to = 26, by = .01)),
            aes(x = x, y = dnorm(x, mean = 10, sd = 5)),
            color = "grey33") +
  coord_cartesian(xlim = c(-5, 25)) +
  labs(subtitle = "The candidate normal distribution\nhas a mean of 10 and SD of 5.",
       x = "Data Values", 
       y = "Data Probability") +
  theme(panel.grid = element_blank())
```

Did you notice how we made the data for the density curve within `geom_line()`? That's one way to do it, and in our next plot we'll take a more elegant approach with the `stat_function()` function. Here's our Figure 2.4.b.

```{r, fig.width = 3, fig.height = 2.5}
ggplot(data = d, aes(x = x)) +
  geom_histogram(aes(y = ..density..),
                 binwidth = 1, fill = "grey67",
                 color = "grey92", size = 1/8) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 8, sd = 6),
                color = "grey33", linetype = 2) +
  coord_cartesian(xlim = c(-5, 25)) +
  labs(subtitle = "The candidate normal distribution\nhas a mean of 8 and SD of 6.",
       x = "Data Values", 
       y = "Data Probability") +
  theme(panel.grid = element_blank())
```

## The steps of Bayesian data analysis

> In general, Bayesian analysis of data follows these steps:
>
> 1. Identify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors?
> 2. Define a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis.
> 3. Specify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists.
> 4. Use Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step).
> 5. Check that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a "posterior predictive check"). If not, then consider a different descriptive
model. 
>
> Perhaps the best way to explain these steps is with a realistic example of Bayesian data analysis. The discussion that follows is abbreviated for purposes of this introductory chapter, with many technical details suppressed. (p. 25)

I will show you a few more details than Kruschke did in the text. But just has he did, we'll cover this workflow in much more detail in the chapters to come.

In order to recreate Figure 2.5, we need to generate the data and fit a model to those data. In his `HtWtDataDenerator.R` script, Kruschke provided the code for a function that will generate height/weight data of the kind in his text. Here is the code in full:

```{r HtWtDataGenerator}
HtWtDataGenerator <- function(nSubj, rndsd = NULL, maleProb = 0.50) {
  # Random height, weight generator for males and females. Uses parameters from
  # Brainard, J. & Burmaster, D. E. (1992). Bivariate distributions for height and
  # weight of men and women in the United States. Risk Analysis, 12(2), 267-275.
  # Kruschke, J. K. (2011). Doing Bayesian data analysis:
  # A Tutorial with R and BUGS. Academic Press / Elsevier.
  # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition:
  # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier.
  
  # require(MASS)
  
  # Specify parameters of multivariate normal (MVN) distributions.
  # Men:
  HtMmu   <- 69.18
  HtMsd   <- 2.87
  lnWtMmu <- 5.14
  lnWtMsd <- 0.17
  Mrho    <- 0.42
  Mmean   <- c(HtMmu, lnWtMmu)
  Msigma  <- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd,
                      Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2)
  # Women cluster 1:
  HtFmu1   <- 63.11
  HtFsd1   <- 2.76
  lnWtFmu1 <- 5.06
  lnWtFsd1 <- 0.24
  Frho1    <- 0.41
  prop1    <- 0.46
  Fmean1   <- c(HtFmu1, lnWtFmu1)
  Fsigma1  <- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1,
                       Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2)
  # Women cluster 2:
  HtFmu2   <- 64.36
  HtFsd2   <- 2.49
  lnWtFmu2 <- 4.86
  lnWtFsd2 <- 0.14
  Frho2    <- 0.44
  prop2    <- 1 - prop1
  Fmean2   <- c(HtFmu2, lnWtFmu2)
  Fsigma2  <- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2,
                       Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2)
  
  # Randomly generate data values from those MVN distributions.
  if (!is.null(rndsd)) {set.seed(rndsd)}
  datamatrix <- matrix(0, nrow = nSubj, ncol = 3)
  colnames(datamatrix) <- c("male", "height", "weight")
  maleval <- 1; femaleval <- 0 # arbitrary coding values
  for (i in 1:nSubj)  {
    # Flip coin to decide sex
    sex <- sample(c(maleval, femaleval), size = 1, replace = TRUE,
                  prob = c(maleProb, 1 - maleProb))
    if (sex == maleval) {datum = MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)}
    if (sex == femaleval) {
      Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2))
      if (Fclust == 1) {datum = MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)}
      if (Fclust == 2) {datum = MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)}
    }
    datamatrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1))
  }
  
  return(datamatrix)
} # end function
```

Now we have the `HtWtDataGenerator()` function, all we need to do is determine how many values to generate and how probable we want the values to be based on those from men. These are controlled by the `nSubj` and `maleProb` parameters.

```{r, message = F, warning = F}
# set your seed to make the data generation reproducible
set.seed(2)

d <-
  HtWtDataGenerator(nSubj = 57, maleProb = .5) %>%
  as_tibble()

d %>%
  head()
```

We're about ready for the model, which we will fit it with the Hamiltonian Monte Carlo (HMC) method via the [**brms** package](https://CRAN.R-project.org/package=brms). We'll introduce **brms** more fully in [Chapter 8][~~JAGS~~ brms] and you can go [here](https://github.com/paul-buerkner/brms) for an introduction, too. In the meantime, let's just load the package and see what happens.

```{r, message = F, warning = F}
library(brms)
```

The traditional use of [diffuse and noninformative priors is discouraged with HMC, as is the uniform distribution for sigma](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations). Instead, we'll use weakly-regularizing priors for the intercept and slope and a half Cauchy with a fairly large scale parameter for $\sigma$.

```{r fit2.1}
fit2.1 <- 
  brm(data = d, 
      family = gaussian,
      weight ~ 1 + height,
      prior = c(prior(normal(0, 100), class = Intercept),
                prior(normal(0, 100), class = b),
                prior(cauchy(0, 10),  class = sigma)),
      chains = 4, cores = 4, iter = 2000, warmup = 1000,
      seed = 2,
      file = "fits/fit02.01")
```

If you wanted a quick model summary, you could execute `print(fit1)`. Again, we'll walk through that and other diagnostics in greater detail starting in [Chapter 8][~~JAGS~~ brms]. For now, here's how we might make Figure 2.5.a.

```{r, fig.width = 3.25, fig.height = 3}
# extract the posterior draws
draws <- as_draws_df(fit2.1)

# this will subset the output
n_lines <- 150

# plot!
draws %>% 
  slice(1:n_lines) %>% 

  ggplot() +
  geom_abline(aes(intercept = b_Intercept, slope = b_height, group = .draw),
              color = "grey50", size = 1/4, alpha = .3) +
  geom_point(data = d,
             aes(x = height, y = weight),
             shape = 1) +
  # the `eval(substitute(paste()))` trick came from: https://www.r-bloggers.com/value-of-an-r-object-in-an-expression/
  labs(subtitle = eval(substitute(paste("Data with", n_lines, "credible regression lines"))),
       x = "Height in inches",
       y = "Weight in pounds") +
  coord_cartesian(xlim = c(55, 80),
                  ylim = c(50, 250)) +
  theme(panel.grid = element_blank())
```

For Figure 2.5.b., we'll mark off the mode and 95% highest density interval (HDI) with help from the handy `stat_histinterval()` function, provided by the [**tidybayes** package](http://mjskay.github.io/tidybayes) [@R-tidybayes].

```{r, warning = F, message = F, fig.width = 3, fig.height = 3}
library(tidybayes)

draws %>% 
  ggplot(aes(x = b_height, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = "grey67", slab_color = "grey92",
                    breaks = 40, slab_size = .2, outline_bars = T) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 8)) +
  labs(title = "The posterior distribution",
       subtitle = "The mode and 95% HPD intervals are\nthe dot and horizontal line at the bottom.",
       x = expression(beta[1]~(slope))) +
  theme(panel.grid = element_blank())
```

To make Figure 2.6, we use the `brms::predict()` function, which we'll cover more fully in the pages to come.

```{r, fig.width = 4, fig.height = 4}
nd <- tibble(height = seq(from = 53, to = 81, length.out = 20))

predict(fit2.1, newdata = nd) %>%
  data.frame() %>%
  bind_cols(nd) %>% 

  ggplot(aes(x = height)) +
  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
                  color = "grey67", shape = 20) +
  geom_point(data = d, 
             aes(y = weight),
             alpha = 2/3) +
  labs(subtitle = "Data with the percentile-based 95% intervals and\nthe means of the posterior predictions",
       x = "Height in inches",
       y = "Weight in inches") +
  theme(panel.grid = element_blank())
```

The posterior predictions might be easier to depict with a ribbon and line, instead.

```{r, fig.width = 4, fig.height = 4}
nd <- tibble(height = seq(from = 53, to = 81, length.out = 30))

predict(fit2.1, newdata = nd) %>%
  data.frame() %>%
  bind_cols(nd) %>% 

  ggplot(aes(x = height)) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = "grey75") +
  geom_line(aes(y = Estimate),
            color = "grey92") +
  geom_point(data =  d, 
             aes(y = weight),
             alpha = 2/3) +
  labs(subtitle = "Data with the percentile-based 95% intervals and\nthe means of the posterior predictions",
       x = "Height in inches",
       y = "Weight in inches") +
  theme(panel.grid = element_blank())
```

"We have seen the five steps of Bayesian analysis in a fairly realistic example. This book explains how to do this sort of analysis for many different applications and types of descriptive models" (p. 30). Are you intrigued and excited? You should be. Welcome to Bayes, friends!

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
# remove our objects
rm(d, text, arrow, HtWtDataGenerator, fit2.1, draws, n_lines, nd)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:02.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

# The R Programming Language

> The material in this chapter is rather dull reading because it basically amounts to a list (although a carefully scaffolded list) of basic commands in R along with illustrative examples. After reading the first few pages and nodding off, you may be tempted to skip ahead, and I wouldn't blame you. But much of the material in this chapter is crucial, and all of it will eventually be useful, so you should at least skim it all so you know where to return when the topics arise later. [@kruschkeDoingBayesianData2015, p. 35]

Most, but not all, of this part of my project will mirror what's in the text. However, I do add **tidyverse**-oriented content, such as a small walk through of plotting with the [**ggplot2** package](https://ggplot2.tidyverse.org) [@wickhamGgplot2ElegantGraphics2016; @R-ggplot2].

## Get the software

The first step to following along with this ebook is to install **R** on your computer. Go to [https://cran.r-project.org/](https://cran.r-project.org/) and follow the instructions, from there. If you get confused, there are any number of brief video tutorials available to lead you through the steps. Just use a search term like "install R."

If you're new to **R** or just curious about its origins, check out Chapter 2 of @pengProgrammingDataScience2020, [*History and overview of R*](https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html). One of the great features about **R**, which might seem odd or confusing if you're more used to working with propriety software like SPSS, is a lot of the functionality comes from the add-on packages users develop to make **R** easier to use. Peng briefly discusses these features in Section 2.7, [*Design of the R system*](https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html#design-of-the-r-system). I make use of a variety of add-on packages in this project. You can install them all by executing this code block.

```{r, eval = F}
packages <- c("bayesplot", "brms", "coda", "cowplot", "cubelyr", "devtools", "fishualize", "GGally", "ggdist", "ggExtra", "ggforce", "ggmcmc", "ggridges", "ggthemes", "janitor", "lisa", "loo", "palettetown", "patchwork", "psych", "remotes", "rstan", "santoku", "scico", "tidybayes", "tidyverse")

install.packages(packages, dependencies = T)

remotes::install_github("clauswilke/colorblindr")
devtools::install_github("dill/beyonce")
devtools::install_github("ropenscilabs/ochRe")
```

### A look at RStudio.

> The R programming language comes with its own basic user interface that is adequate for modest applications. But larger applications become unwieldy in the basic R user interface, and therefore it helps to install a more sophisticated R-friendly editor. There are a number of useful editors available, many of which are free, and they are constantly evolving. At the time of this writing, I recommend RStudio, which can be obtained from [[https://rstudio.com/]](https://rstudio.com/) (p. 35).

I completely agree. **R** programming is easier with **RStudio**. However, I should point out that there are other user interfaces available. You can find several alternatives listed [here](https://datascience.stackexchange.com/questions/5345/ide-alternatives-for-r-programming-rstudio-intellij-idea-eclipse-visual-stud) or [here](https://intro2r.com/alternatives-to-rstudio.html).

## A simple example of R in action

Basic arithmetic is straightforward in **R**.

```{r}
2 + 3
```

Much like Kruschke did in his text, I denote my programming prompts in typewriter font atop a gray background, like this: `2 + 3`.

Anyway, algebra is simple in **R**, too.

```{r}
x <- 2

x + x
```

I don't tend to save lists of commands in text files. Rather, I almost exclusively work within [R Notebook](https://bookdown.org/yihui/rmarkdown/notebook.html) files, which I discuss more fully in [Section 3.7][Programming in R]. As far as *sourcing*, I never use the `source()` approach Kruschke discussed in page 36. I'm not opposed to it. It's just not my style.

Anyway, behold Figure 3.1.

```{r, fig.width = 3.5, fig.height = 3, message = F, warning = F}
library(tidyverse)

d <-
  tibble(x = seq(from = -2, to = 2, by = .1)) %>%
  mutate(y = x^2) 
  
ggplot(data = d,
       aes(x = x, y = y)) +
  geom_line(color = "skyblue") +
  theme(panel.grid = element_blank())
```

If you're new to the **tidyverse** and/or making figures with **ggplot2**, it's worthwhile to walk that code out. With the first line, `library(tidyverse)`, we opened up the [core packages within the tidyverse](https://www.tidyverse.org/packages/), which are:

* **ggplot2** [@wickhamGgplot2ElegantGraphics2016; @R-ggplot2],
* **dplyr** [@R-dplyr],
* **tidyr** [@R-tidyr],
* **readr** [@R-readr],
* **purrr** [@R-purrr],
* **tibble** [@R-tibble],
* **stringr** [@R-stringr], and
* **forcats** [@R-forcats].

With the few lines,

```{r}
d <-
  tibble(x = seq(from = -2, to = 2, by = .1)) %>%
  mutate(y = x^2) 
```

we made our tibble. In **R**, data frames are one of the primary types of data objects (see [Section 3.4.4][List and data frame.], below). We'll make extensive use of data frames in this project. Tibbles are a particular type of data frame, which you might learn more about in the [tibbles section](https://r4ds.had.co.nz/tibbles.html) of Grolemund and Wickham's [-@grolemundDataScience2017] *R4DS*. With those first two lines, we determined what the name of our tibble would be, `d`, and made the first column, `x`.

Note the `%>%` operator at the end of the second line. In prose, we call that the *pipe*. As explained in [Section 5.6.1 of *R4DS*](https://r4ds.had.co.nz/transform.html#combining-multiple-operations-with-the-pipe), "a good way to pronounce `%>%` when reading code is 'then.'" So in words, the those first two lines indicate "Make an object, `d`, which is a tibble with a variable, `x`, defined by the `seq()` function, *then*..."

In the portion after *then* (i.e., the `%>%`), we changed `d`. The `dplyr::mutate()` function let us add another variable, `y`, which is a function of our first variable, `x`.

With the next 4 lines of code, we made our plot. When plotting with **ggplot2**, the first line is always with the `ggplot()` function. This is where you typically tell **ggplot2** what data object you're using--which must be a data frame or tibble--and what variables you want on your axes. The interesting thing about **ggplot2** is that the code is modular. So if we only coded the `ggplot()` portion, we'd get:

```{r, fig.width = 3.5, fig.height = 3, message = F, warning = F}
ggplot(data = d,
       aes(x = x, y = y))
```

Although **ggplot2** knows which variables to put on which axes, it has no idea how we'd like to express the data. The result is an empty coordinate system. The next line of code is the main event. With `geom_line()` we told **ggplot2** to connect the data points with a line. With the `color` argument, we made that line `"skyblue"`. [[Here's a great list](http://sape.inf.usi.ch/quick-reference/ggplot2/colour) of the named colors available in **ggplot2**.] Also, notice the `+` operator at the end of the `ggplot()` function. With **ggplot2**, you add functions by placing the `+` operator on the right of the end of one function, which will then append the next function.

```{r, fig.width = 3.5, fig.height = 3, message = F, warning = F}
ggplot(data = d,
       aes(x = x, y = y)) +
  geom_line(color = "skyblue")
```

Personally, I'm not a fan of gridlines. They occasionally have their place and I do use them from time to time. But on the whole, I prefer to omit them from my plots. The final `theme()` function allowed me to do so.

```{r, fig.width = 3.5, fig.height = 3, message = F, warning = F}
ggplot(data = d,
       aes(x = x, y = y)) +
  geom_line(color = "skyblue") +
  theme(panel.grid = element_blank())
```

[Chapter 3 of *R4DS*](https://r4ds.had.co.nz/data-visualisation.html) is a great introduction to plotting with **ggplot2**. If you want to dive deeper, see the [references at the bottom of this page](https://ggplot2.tidyverse.org). And of course, you might read up in Wickham's [-@wickhamGgplot2ElegantGraphics2016] [*ggplot2: Elegant graphics for data analysis*](https://ggplot2-book.org/).

### Get the programs used with this book.

This subtitle has a double meaning, here. Yes, you should probably get Kruschke's scripts from the book's website, [https://sites.google.com/site/doingbayesiandataanalysis/](https://sites.google.com/site/doingbayesiandataanalysis/). You may have noticed this already, but unlike in Kruschke's text, I will usually show all my code. Indeed, the purpose of my project is to make coding these kinds of models and visualizations easier. But if you're ever curious, you can always find my script files in their naked form at [https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse). For example, the raw file for this very chapter is at [https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/blob/master/03.Rmd](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/blob/master/03.Rmd).

Later in this subsection, Kruschke mentioned working directories. If you don't know what your current working directory is, just execute `getwd()`. I'll have more to say on this topic later on when I make my pitch for [**RStudio** projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects) in [Section 3.7.2][Running a program.].

## Basic commands and operators in R

In addition to the resource link Kruschke provided in the text, Grolemund and Wickham's [*R4DS*](http://r4ds.had.co.nz) is an excellent general introduction to the kinds of **R** functions you'll want to succeed with your data analysis. Other than that, I've learned the most when I had a specific data problem to solve and then sought out the specific code/techniques required to solve it. If already have your own data or can get your hands on some sexy data, learn these techniques by playing around with them. This isn't the time to worry about rigor, preregistration, or all of that. This is time to play.

### Getting help in R.

As with `plot()` you can learn more about the `ggplot()` function with `?`.

```{r, eval = F}
?ggplot
```

`help.start()` can be nice, too.

```{r, eval = F}
help.start()
```

`??geom_line()` can help us learn more about the `geom_line()` function.

```{r, eval = F}
??geom_line()
```

Quite frankly, a bit part of becoming a successful **R** user is learning how to get help, online. In addition to the methods, above, type in the name of your function of interest in your favorite web browser. For example, If I wanted to learn more about the `geom_line()` function, I'd literally do a web search for "geom_line()". In my case, the first search results when doing so was [https://ggplot2.tidyverse.org/reference/geom_path.html](https://ggplot2.tidyverse.org/reference/geom_path.html), which is a nicely-formatted official reference page put out by the **ggplot2** team.

### Arithmetic and logical operators.

With arithmetic, the order of operations is: power first, then multiplication, then addition.

```{r}
1 + 2 * 3^2
```

With parentheses, you can force addition before multiplication.

```{r}
(1 + 2) * 3^2
```

Operations inside parentheses get done before power operations.

```{r}
(1 + 2 * 3)^2
```

One can nest parentheses.

```{r}
((1 + 2) * 3)^2
```

```{r}
?Syntax
```

We can use **R** to perform a variety of logical tests, such as negation.

```{r}
!TRUE
```

We can do conjunction.

```{r}
TRUE & FALSE
```

And we can do disjunction.

```{r}
TRUE | FALSE
```

Conjunction has precedence over disjunction.

```{r}
TRUE | TRUE & FALSE

```

However, with parentheses we can force disjunction first.

```{r}
(TRUE | TRUE) & FALSE
```

### Assignment, relational operators, and tests of equality.

In contrast to Kruschke's preference, I will use the [arrow operator, `<-`, to assign](https://style.tidyverse.org/syntax.html#assignment) values to named variables[^1].

```{r}
x = 1

x <- 1
```

Yep, this ain't normal math.

```{r}
(x = 1)

(x = x + 1)
```

Here we use `==` to test for equality.

```{r}
(x = 2)

x == 2
```

Using `!=`, we can check whether the value of `x` is NOT equal to 3.

```{r}
x != 3
```

We can use `<` to check whether the value of `x` is less than 3.

```{r}
x < 3
```

Similarly, we can use `>` to check whether the value of `x` is greater than 3.

```{r}
x > 3
```

This normal use of the `<-` operator

```{r}
x <- 3
```

is not the same as

```{r}
x < - 3
```

The limited precision of a computer's memory can lead to odd results.

```{r}
x <- 0.5 - 0.3
y <- 0.3 - 0.1
```

Although mathematically `TRUE`, this is `FALSE` for limited precision.

```{r}
x == y
```

However, they are equal up to the precision of a computer.

```{r}
all.equal(x, y)
```

## Variable types

If you'd like to learn more about the differences among vectors, matrices, lists, data frames and so on, you might check out Roger Peng's [-@pengProgrammingDataScience2020] *R Programming for data science*, [Chapter 4](https://bookdown.org/rdpeng/rprogdatascience/r-nuts-and-bolts.html).

### Vector.

"A vector is simply an ordered list of elements of the same type" (p. 42).

#### The combine function.

The *combine* function is `c()`, which makes vectors. Here we'll first make an unnamed vector. Then we'll sve that vector as `x`.

```{r}
c(2.718, 3.14, 1.414)

x <- c(2.718, 3.14, 1.414)
```

You'll note the equivalence.

```{r}
x == c(2.718, 3.14, 1.414)
```

This leads to the next subsection.

#### Component-by-component vector operations.

We can multiply two vectors, component by component.

```{r}
c(1, 2, 3) * c(7, 6, 5)
```

If you have a sole number, a *scaler*, you can multiply an entire vector by it like:

```{r}
2 * c(1, 2, 3)
```

which is a more compact way to perform this.

```{r}
c(2, 2, 2) * c(1, 2, 3)
```

The same sensibilities hold for other operations, such as addition.

```{r}
2 + c(1, 2, 3)
```

#### The colon operator and sequence function.

The colon operator, `:`, is a handy way to make integer sequences. Here we use it to serially list the inters from 4 to 7.

```{r}
4:7
```

The colon operator has precedence over addition.

```{r}
2 + 3:6
```

Parentheses override default precedence.

```{r}
(2 + 3):6 
```

The power operator has precedence over the colon operator.
 
```{r}
1:3^2
```

And parentheses override default precedence.

```{r}
(1:3)^2
```

The `seq()` function is quite handy. If you don't specify the length of the output, it will figure that out the logical consequence of the other arguments.

```{r}
seq(from = 0, to = 3, by = 0.5)
```

This sequence won't exceed `to = 3`.

```{r}
seq(from = 0, to = 3, by = 0.5001) 
```

In each of the following examples, we'll omit one of the core `seq()` arguments: `from`, `to`, `by`, and `length.out`. Here we do not define the end point.

```{r}
seq(from = 0, by = 0.5, length.out = 7)
```

This time we fail to define the increment.

```{r}
seq(from = 0, to = 3, length.out = 7)
```

And this time we omit a starting point.

```{r}
seq(to = 3, by = 0.5, length.out = 7)
```

In this ebook, I will always explicitly name my arguments within `seq()`.

#### The replicate function.

We'll define our pre-replication vector with the `<-` operator.

```{r}
abc <- c("A", "B", "C")
```

With the `times` argument, we repeat the vector as a unit with the `rep()` function.

```{r}
rep(abc, times = 2)
```

But if we mix the `times` argument with `c()`, we can repeat individual components of `abc` differently.

```{r}
rep(abc, times = c(4, 2, 1))
```

With the `each` argument, we repeat the individual components of `abc` one at a time.

```{r}
rep(abc, each = 2)
```

And you can even combine `each` and `length`, repeating each element until the `length` requirement has been fulfilled.

```{r}
rep(abc, each = 2, length = 10)
```

You can also combine `each` and `times`.

```{r}
rep(abc, each = 2, times = 3)
```

I tend to do things like the above as two separate steps. One way to do so is by nesting one `rep()` function within another.

```{r}
rep(rep(abc, each = 2),
    times = 3)
```

As Kruschke points out, this can look confusing.

```{r}
rep(abc, each = 2, times = c(1, 2, 3, 1, 2, 3))
```

But breaking the results up into two steps might be easier to understand,

```{r}
rep(rep(abc, each = 2), 
    times = c(1, 2, 3, 1, 2, 3))
```

And especially earlier in my **R** career, it helped quite a bit to break operation sequences like this up by saving and assessing the intermediary steps.

```{r}
step_1 <- rep(abc, each = 2)
step_1

rep(step_1, times = c(1, 2, 3, 1, 2, 3))
```

#### Getting at elements of a vector.

Behold our exemplar vector, `x`.

```{r}
x <- c(2.718, 3.14, 1.414, 47405)
```

The straightforward way to extract the second and fourth elements is with a combination of brackets, `[]`, and `c()`.

```{r}
x[c(2, 4)]
```

Or you might use reverse logic and omit the first and third elements.

```{r}
x[c(-1, -3 )]
```

It's handy to know that `T` is a stand in for `TRUE` and `F` is a stand in for `FALSE`. You'll probably notice I use the abbreviations most of the time.

```{r}
x[c(F, T, F, T)]
```

The `names()` function makes it easy to name the components of a vector.

```{r}
names(x) <- c("e", "pi", "sqrt2", "zipcode")

x
```

Now we can call the components with their names.

```{r}
x[c("pi", "zipcode")]
```

Once we start working with summaries from our Bayesian models, we'll use this trick a lot.

Here's Kruschke's review:

```{r}
# define a vector
x <- c(2.718, 3.14, 1.414, 47405)

# name the components
names(x) <- c("e", "pi", "sqrt2", "zipcode")

# you can indicate which elements you'd like to include
x[c(2, 4)]

# you can decide which to exclude

x[c(-1, -3)]

# or you can use logical tests
x[c(F, T, F, T)]

# and you can use the names themselves
x[c("pi", "zipcode")]
```

### Factor.

"Factors are a type of vector in R for which the elements are *categorical* values that could also be ordered. The values are stored internally as integers with labeled levels" (p. 46, *emphasis* in the original).

Here are our five-person socioeconomic status data.

```{r}
x <- c("high", "medium", "low", "high", "medium")
x
```

The `factor()` function turns them into a factor, which will return the levels when called.

```{r}
xf <- factor(x)
xf
```

Here are the factor levels as numerals.

```{r}
as.numeric(xf)
```

With the `levels` and `ordered` arguments, we can order the factor elements.

```{r}
xfo <- factor(x, levels = c("low", "medium", "high"), ordered = T)
xfo
```

Now "high" is a larger integer.

```{r}
as.numeric(xfo)
```

We've already specified `xf`.

```{r}
xf
```

And we know how it's been coded numerically.

```{r}
as.numeric(xf)
```

We can have `levels` and `labels`.

```{r}
xfol <- factor(x, 
               levels = c("low", "medium", "high"), ordered = T,
               labels = c("Bottom SES", "Middle SES", "Top SES"))

xfol
```

Factors can come in very handy when modeling with certain kinds of categorical variables, as in [Chapter 22][Nominal Predicted Variable], or when arranging elements within a plot.

### Matrix and array.

Kruschke uses these more often than I do. I'm more of a vector and data frame kinda guy. Even so, here's an example of a matrix.

```{r}
matrix(1:6, ncol = 3)
```

We can get the same thing using `nrow`.

```{r}
matrix(1:6, nrow = 2)
```

Note how the numbers got ordered by rows within each column? We can specify them to be ordered across columns, first.

```{r}
matrix(1:6, nrow = 2, byrow = T)
```

We can name the dimensions. I'm not completely consistent, but I generally follow [*The tidyverse style guide*](https://style.tidyverse.org) [@wickhamTidyverseStyleGuide2020] for naming my **R** objects and their elements. From [Section 2.1](https://style.tidyverse.org/syntax.html#object-names), we read

> Variable and function names should use only lowercase letters, numbers, and `_`. Use underscores (`_`) (so called snake case) to separate words within a name.

By those sensibilities, we'll name our rows and columns like this.

```{r}
matrix(1:6, 
       nrow = 2,
       dimnames = list(TheRowDimName = c("row_1_name", "row_2_name"),
                       TheColDimName = c("col_1_name", "col_2_name", "col_3_name")))
```

You've also probably noticed that I "[always put a space after a comma, never before, just like in regular English](https://style.tidyverse.org/syntax.html#spacing)," as well as "put a space before and after `=` when naming arguments in function calls." IMO, this makes code easier to read. You do you.

We'll name our matrix `x`.

```{r}
x <-
  matrix(1:6, 
       nrow = 2,
       dimnames = list(TheRowDimName = c("row_1_name", "row_2_name"),
                       TheColDimName = c("col_1_name", "col_2_name", "col_3_name")))

```

Since there are 2 dimensions, we'll subset with two dimensions. Numerical indices work.

```{r}
x[2, 3]
```

Row and column names work, too. Just make sure to use quotation marks, `""`, for those.

```{r}
x["row_2_name", "col_3_name"]
```

Here we specify the range of columns to include.

```{r}
x[2, 1:3]
```

Leaving that argument blank returns them all.

```{r}
x[2, ]
```

And leaving the row index blank returns all row values within the specified column(s).

```{r}
x[, 3]
```

Mind your commas! This produces the second row, returned as a vector.

```{r}
x[2, ]
```

This returns both rows of the 2^nd^ column.

```{r}
x[, 2]
```  

Leaving out the comma will return the numbered element.

```{r}
x[2]
```

It'll be important in your **brms** career to have a sense of 3-dimensional arrays. Several **brms** convenience functions often return them (e.g., `ranef()` in multilevel models).

```{r}
a <- array(1:24, dim = c(3, 4, 2), # 3 rows, 4 columns, 2 layers
           dimnames = list(RowDimName = c("r1", "r2", "r3"),
                           ColDimName = c("c1", "c2", "c3", "c4"),
                           LayDimName = c("l1", "l2")))

a
```

Since these have 3 dimensions, you have to use 3-dimensional indexing. As with 2-dimensional objects, leaving the indices for a dimension blank will return all elements within that dimension. For example, this code returns all columns of `r3` and `l2`, as a vector.

```{r}
a["r3", , "l2"]
```

And this code returns all layers of `r3` and `c4`, as a vector.

```{r}
a["r3", "c4", ]
```

This whole topic of subsetting--whether from matrices and arrays, or from data frames and tibbles--can be confusing. For more practice and clarification, you might check out Peng's Chapter 9, [*Subsetting R objects*](https://bookdown.org/rdpeng/rprogdatascience/subsetting-r-objects.html).

### List and data frame.

"The `list` structure is a generic vector in which components can be of different types, and named" (p. 51). Here's `my_list`.

```{r}
my_list <- 
  list("a" = 1:3, 
       "b" = matrix(1:6, nrow = 2), 
       "c" = "Hello, world.")

my_list
```

To return the contents of the `a` portion of `my_list`, just execute this.

```{r}
my_list$a
```

We can index further within `a`.

```{r}
my_list$a[2]
```

To return the contents of the first item in our list with the double bracket, `[[]]`, do:

```{r}
my_list[[1]]
```

You can index further to return only the second element of the first list item.

```{r}
my_list[[1]][2]
```

But double brackets, `[][]`, are no good, here.

```{r}
my_list[1][2]
```

To learn more, Jenny Bryan has a [great talk](https://www.youtube.com/watch?v=4MfUCX_KpdE&t=615s&frags=pl%2Cwn) discussing the role of lists within data wrangling. There's also [this classic pic](https://twitter.com/hadleywickham/status/643381054758363136?lang=en) from Hadley Wickham:

```{r, echo = F, fig.align = "center", out.width = "100%", fig.cap = "Indexing lists in #rstats. Inspired by the Residence Inn"}
knitr::include_graphics("pics/pepper_list.png") 
```

But here's a data frame.

```{r}
d <- 
  data.frame(integers = 1:3, 
             number_names = c("one", "two", "three"))

d
```

With data frames, we can continue indexing with the `$` operator.

```{r}
d$number_names
```

We can also use the double bracket.

```{r}
d[[2]]
```

Notice how the single bracket with no comma indexes columns rather than rows.

```{r}
d[2]
```

But adding the comma returns the factor-level information when indexing columns.

```{r}
d[, 2]
```

It works a touch differently when indexing by row.

```{r}
d[2, ]
```

Let's try with a tibble, instead.

```{r}
t <-
  tibble(integers = 1:3,
         number_names = c("one", "two", "three"))

t
```

One difference is that tibbles default to assigning text columns as character strings rather than factors. Another difference occurs when printing large data frames versus large tibbles. Tibbles yield more compact glimpses. For more, check out [*R4DS* Chapter 10](https://r4ds.had.co.nz/tibbles.html).

It's also worthwhile pointing out that within the **tidyverse**, you can pull out a specific column with the `select()` function. Here we select `number_names`.

```{r}
t %>% 
  select(number_names)
```

Go [here](https://r4ds.had.co.nz/transform.html#select) learn more about `select()`.

## Loading and saving data

### The ~~read.csv~~ `read_csv()` and ~~read.table~~ `read_table()` functions.

Although `read.csv()` is the default CSV reader in **R**, the [`read_csv()` function](https://readr.tidyverse.org/reference/read_delim.html) from the [**readr** package](https://readr.tidyverse.org) (i.e., one of the core **tidyverse** packages) is a new alternative. In [comparison to base **R**](https://r4ds.had.co.nz/data-import.html#compared-to-base-r)'s `read.csv()`, `readr::read_csv()` is faster and returns tibbles (as opposed to data frames with `read.csv()`). The same general points hold for base **R**'s `read.table()` versus `readr::read_table()`.

Using Kruschke's `HGN.csv` example, we'd load the CSV with `read_csv()` like this.

```{r, message = F}
hgn <- read_csv("data.R/HGN.csv")
```

Note again that `read_csv()` defaults to returning columns with character information as characters, not factors.

```{r}
hgn$Hair
```

See? As a character variable, `Hair` no longer has factor level information. But if you knew you wanted to treat `Hair` as a factor, you could easily convert it with `dplyr::mutate()`.

```{r}
hgn <-
  hgn %>% 
  mutate(Hair = factor(Hair))

hgn$Hair
```

And here's a **tidyverse** way to reorder the levels for the `Hair` factor.

```{r}
hgn <-
  hgn %>% 
  mutate(Hair = factor(Hair, levels = c("red", "blond", "brown", "black")))
         
hgn$Hair
as.numeric(hgn$Hair)
```

Since we imported `hgn` with `read_csv()`, the `Name` column is already a character vector, which we can verify with the `str()` function.

```{r}
hgn$Name %>% str()
```

Note how using `as.vector()` did nothing in our case. `Name` was already a character vector.

```{r}
hgn$Name %>% 
  as.vector() %>% 
  str()
```

The `Group` column was imported as composed of integers.

```{r}
hgn$Group %>% str()
```

Switching `Group` to a factor is easy enough.

```{r}
hgn <-
  hgn %>% 
  mutate(Group = factor(Group))

hgn$Group
```

### Saving data from R.

The **readr** package has a `write_csv()` function, too. The arguments are as follows: `write_csv(x, file, na = "NA", append = FALSE, col_names = !append, quote_escape = "double", eol = "\n")`. Learn more by executing `?write_csv`. Saving `hgn` in your working directory is as easy as:

```{r}
write_csv(hgn, "hgn.csv")
```

You could also use `save()`.

```{r}
save(hgn, file = "hgn.Rdata" )
```

Once we start fitting Bayesian models, this method will be an important way to save the results of those models.

The `load()` function is simple.

```{r}
load("hgn.Rdata" )
```

The `ls()` function works very much the same way as the more verbosely-named `objects()` function.

```{r}
ls()
```

## Some utility functions

"A function is a process that takes some input, called the *arguments*, and produces some output, called the *value*" (p. 56, *emphasis* in the original).

```{r}
# this is a more compact way to replicate 100 1's, 200 2's, and 300 3's
x <- rep(1:3, times = c(100, 200, 300))

summary(x)
```

We can use the pipe to convert and then summarize `x`.

```{r}
x %>% 
  factor() %>% 
  summary()
```

The `head()` and `tail()` functions are quite useful.

```{r}
head(x)
tail(x)
```

I used `head()` a lot.

Within the **tidyverse**, the `slice()` function serves a similar role. In order to use `slice()`, we'll want to convert `x`, which is just a vector of integers, into a data frame. Then we'll use `slice()` to return a subset of the rows.

```{r}
x <-
  x %>%
  data.frame() 

x %>% 
  slice(1:6)
```

So that was analogous to what we accomplished with `head()`. Here's the analogue to `tail()`.

```{r}
x %>%
  slice(595:600)
```

The downside of that code was we had to do the math to determine that $600 - 6 = 595$ in order to get the last six rows, as returned by `tail()`. A more general approach is to use `n()`, which will return the total number of rows in the tibble.

```{r}
x %>%
  slice((n() - 6):n())
```

To unpack `(n() - 6):n()`, because `n()` = 600, `(n() - 6)` = 600 - 6 = 595. Therefore `(n() - 6):n()` was equivalent to having coded `595:600`. Instead of having to do the math ourselves, `n()` did it for us. It's often easier to just go with `head()` or `tail()`. But the advantage of this more general approach is that it allows one take more complicated slices of the data, such as returning the first three and last three rows.

```{r}
x %>%
  slice(c(1:3, (n() - 3):n()))
```

We've already used the handy `str()` function a bit. It's also nice to know that `tidyverse::glimpse()` performs a similar function.

```{r}
x %>% str()
x %>% glimpse()
```

Within the **tidyverse**, we'd use `group_by()` and then `summarize()` as alternatives to the base **R** `aggregate()` function. With `group_by()` we group the observations first by `Hair` and then by `Gender` within `Hair`. After that, we summarize the groups by taking the `median()` values of their `Number`.

```{r, message = F}
hgn %>% 
  group_by(Hair, Gender) %>% 
  summarize(median = median(Number))
```

One of the nice things about this workflow is that the code reads somewhat like how we'd explain what we were doing. We, in effect, told **R** to *Take `hgn`, then group the data by `Hair` and `Gender` within `Hair`, and then `summarize()` those groups by their `median()` `Number` values.* There's also the nice quality that we don't have to continually tell **R** where the data are coming from the way the `aggregate()` function required Kruschke to prefix each of his variables with `HGNdf$`. We also didn't have to explicitly rename the output columns the way Kruschke had to.

I'm not aware that our `group_by() %>% summarize()` workflow has a formula format the way `aggregate()` does.

To count how many levels we had in a grouping factor, we'd use the `n()` function in `summarize()`.

```{r, message = F}
hgn %>% 
  group_by(Hair, Gender) %>% 
  summarize(n = n())
```

Alternatively, we could switch out the `group_by()` and `summarize()` lines with `count()`.

```{r}
hgn %>% 
  count(Hair, Gender)
```

We could then use `spread()` to convert that output to a format similar to Kruschke's table of counts.

```{r}
hgn %>% 
  count(Hair, Gender) %>% 
  spread(key = Hair, value = n)
```

With this method, the `NA`s are stand-ins for 0's.

```{r}
a
```

`apply()` is part of a family of functions that offer a wide array of uses. You can learn more about the `apply()` family [here](https://www.datacamp.com/community/tutorials/r-tutorial-apply-family) or [here](https://faculty.nps.edu/sebuttre/home/R/apply.html).

```{r}
apply(a, MARGIN = c(2, 3), FUN = sum)
```

Here's `a`.

```{r}
a
```

The [**reshape2** package](https://github.com/hadley/reshape) [@R-reshape2; @wickhamReshapingDataReshape2007] was [a precursor](https://tidyr.tidyverse.org) to the **tidyr** package (i.e., one of the core **tidyverse** packages). The `reshape2::melt()` function is a quick way to transform the 3-dimensional `a` matrix into a tidy data frame.

```{r}
a %>% 
  reshape2::melt()
```

We have an alternative if you wanted to stay within the **tidyverse**. To my knowledge, the fastest way to make the transformation is to first use ` as.tbl_cube()` and follow that up with `as_tibble()`. The [`as.tbl_cube()` function](https://rdrr.io/cran/cubelyr/man/as.tbl_cube.html) will convert the `a` matrix into a tbl_cube. We will use the `met_name` argument to determine the name of the measure assessed in the data. Since the default is for `as.tbl_cube()` to name the measure name as `.`, it seemed `value` was a more descriptive choice. We'll then use the `as_tibble()` function to convert our `tbl_cube` object into a tidy tibble.

```{r, warning = F, message = F}
library(cubelyr)

a %>% 
  as.tbl_cube(met_name = "value") %>% 
  as_tibble()
```

Notice how the first three columns are returned as characters instead of factors. If you really wanted those to be factors, you could always follow up the code with `mutate_if(is.character, as.factor)`. Also notice that we gained access to the `as.tbl_cube()` function by loading the [**cubelyr** package](https://CRAN.R-project.org/package=cubelyr) [@R-cubelyr]. Though `as.tbl_cube()` was available in earlier versions of `dplyr` (one of the core packages in the **tidyverse**), it was [removed in the version 1.0.0 update](https://dplyr.tidyverse.org/news/index.html) and is now available via **cubelyr**.

## Programming in R

It's worthy to note that this project was done with [R Markdown](https://rmarkdown.rstudio.com), which is an alternative to an **R** script. As [Grolemund and Wickham point out](https://r4ds.had.co.nz/r-markdown.html)

> R Markdown integrates a number of R packages and external tools. This means that help is, by-and-large, not available through ?. Instead, as you work through this chapter, and use R Markdown in the future, keep these resources close to hand:
>
> * R Markdown Cheat Sheet: Help > Cheatsheets > R Markdown Cheat Sheet,
>
> * R Markdown Reference Guide: Help > Cheatsheets > R Markdown Reference Guide.
>
> Both cheatsheets are also available at [https://rstudio.com/resources/cheatsheets/](https://rstudio.com/resources/cheatsheets/).

I also strongly recommend checking out [R Notebooks](https://bookdown.org/yihui/rmarkdown/notebook.html), which is a kind of R Markdown document but with a few bells a whistles that make it more useful for working scientists. You can learn more about it [here](https://rstudio-pubs-static.s3.amazonaws.com/256225_63ebef4029dd40ef8e3679f6cf200a5a.html) and [here](https://www.r-bloggers.com/why-i-love-r-notebooks-2/). And for a more comprehensive overview, check out Xie, Allaire, and Grolemund's [-@xieMarkdownDefinitiveGuide2022] [*R markdown: The definitive guide*](https://bookdown.org/yihui/rmarkdown/).

### Variable names in R.

Kruschke prefers to use camelBack notation for his variable and function names. Though I initially hated it, I largely use [snake_case](https://style.tidyverse.org/syntax.html#object-names). It seems easier to read_prose_in_snake_case than it is to readProseInCamelBack. To each their own.

### Running a program.

I do most of my data analyses using **RStudio** projects. In short, **RStudio** projects provide a handy way to organize your files within a given project, be it an analysis for a scientific paper, an ebook, or even a personal website. See *R4DS* [Chapter 8](https://r4ds.had.co.nz/workflow-projects.html) for a nice overview on working directories within the context of an **RStudio** project. I didn't really *get it*, at first. But after using **RStudio** projects for a couple days, I was hooked. I bet you will be, too.

### Programming a function.

Here's our simple `a_sq_plus_b` function.

```{r}
a_sq_plus_b <- function(a, b = 1) {
  c <- a^2 + b
  return(c)
}
```

If you explicitly denote your arguments, everything works fine.

```{r}
a_sq_plus_b(a = 3, b = 2)
```

Keep things explicit and you can switch up the order of the arguments.

```{r}
a_sq_plus_b(b = 2, a = 3)
```

But here's what happens when you are less explicit.

```{r}
# this
a_sq_plus_b(3, 2)

# is not the same as this
a_sq_plus_b(2, 3)
```

Since we gave `b` a default value, we can be really lazy.

```{r}
a_sq_plus_b(a = 2)
```

But we can't be lazy with `a`. This

```{r, eval = F}
a_sq_plus_b(b = 1)
```

yielded this warning on my computer: "Error in a_sq_plus_b(b = 1) : argument "a" is missing, with no default".

If we're completely lazy, `a_sq_plus_b()` presumes our sole input value is for the `a` argument and it uses the default value of `1` for `b`.

```{r}
a_sq_plus_b(2)
```

The lesson is important because it’s good practice to familiarize yourself with the defaults of the functions you use in statistics and data analysis, more generally.

If you haven't done it much, before, programming your own functions can feel like a superpower. It's a good idea to get comfortable with the basics. I've found it really helps when I'm working on models for real-world scientific projects. For more practice, Check out *R4DS*, [Chapter 19](https://r4ds.had.co.nz/functions.html), or *R programming for data science*, [Chapter 14](https://bookdown.org/rdpeng/rprogdatascience/functions.html).

### Conditions and loops.

Here's our starting point for `if()` and `else()`.

```{r}
if(x <= 3) {     # if x is less than or equal to 3
  show("small")  # display the word "small"
} else {         # otherwise
  show("big")    # display the word "big"
}                # end of ’else’ clause
```

Yep, this is no good.

```{r, eval = F}
if (x <= 3) {show("small")}
else {show("big")}
```

On my computer, it returned this message: "the condition has length > 1 and only the first element will be used[1] "small" Error: unexpected 'else' in "else"".

Here we use the loop.

```{r}
for (count_down in 5:1) {
  show(count_down)
}
```

```{r}
for (note in c("do", "re", "mi")) {
  show(note)
}
```

It's also useful to understand how to use the `ifelse()` function within the context of a data frame. Recall how `x` is a data frame.

```{r}
x <- tibble(x = 1:5)

x
```

We can use the `mutate()` function to make a new variable, `size`, which is itself a function of the original variable, `x`. We'll use the `ifelse()` function to return "small" if `x <= 3`, but to return "big" otherwise.
    
```{r}
x %>% 
  mutate(size = ifelse(x <= 3, "small", "big"))
```

You should also know there's a **dplyr** alternative, called `if_else()`. It works quite similarly, but is stricter about type consistency. If you ever get into a situation where you need to do many `ifelse()` statements or a many-layered `ifelse()` statement, you might check out `dplyr::case_when()`. We'll get some practice with `case_when()` in the next chapter.

### Measuring processing time.

This will be nontrivial to consider in your Bayesian career. Here's the loop.

```{r}
start_time               <- proc.time()
y                        <- vector(mode = "numeric", length = 1.0E6)
for (i in 1:1.0E6) {y[i] <- log(i)}
stop_time <- proc.time()

elapsed_time_loop <- stop_time - start_time
show(elapsed_time_loop)
```

Now we use a vector.

```{r}
start_time <- proc.time()
y          <- log(1:1.0E6)
stop_time  <- proc.time()

elapsed_time_vector <- stop_time - start_time
show(elapsed_time_vector)
```

Here we compare the two times.

```{r}
elapsed_time_vector[1] / elapsed_time_loop[1]
```

For my computer, the vectorized approach took about `r ((elapsed_time_vector[1]/elapsed_time_loop[1]) * 100) %>% round(digits = 1)`% the time the loop approach did. When using **R**, avoid loops for vectorized approaches whenever possible. As an alternative, I tend to just use `Sys.time()` when I'm doing analyses like these.

I'm not going to walk them out, here. But as we go along, you might notice I sometimes use functions from the `purrr::map()` family in places where Kruschke used loops. I think they're pretty great. For more on loops and `purrr::map()` alternatives, check out *R4DS* [Chapter 21](https://r4ds.had.co.nz/iteration.html).

### Debugging.

This should be no surprise by now, but in addition to Kruschke's good advice, I also recommend checking out [*R4DS*](https://r4ds.had.co.nz). I reference it often.

## Graphical plots: Opening and saving

For making and saving plots with **ggplot2**, I recommend reviewing *R4DS* Chapters [3](https://r4ds.had.co.nz/data-visualisation.html) and [28](https://r4ds.had.co.nz/graphics-for-communication.html) or Wickham's [*ggplot2: Elegant graphics for data analysis*](https://ggplot2-book.org/).

## ~~Conclusion~~ **brms**-related needs

Given its central role in this ebook, we'd be remiss not to mention a bit about using **brms**. The main website for **brms** is the GitHub repository at [https://github.com/paul-buerkner/brms](https://github.com/paul-buerkner/brms). The official page on CRAN, [https://CRAN.R-project.org/package=brms](https://CRAN.R-project.org/package=brms) is also handy in that it contains a nice list of vignettes and other documents to help you get started. You can find a few video lectures of Paul Bürkner discussing **brms**, such as [here](https://www.youtube.com/watch?v=OUyB4kiJcWE), [here](https://www.youtube.com/watch?v=FRs1iribZME), and [here](https://www.youtube.com/watch?v=40o0_0XTB6E). Finally, make sure to keep an eye on all the hot **brms** discussions in the **brms** section of the Stan forums, [https://discourse.mc-stan.org/c/interfaces/brms/36](https://discourse.mc-stan.org/c/interfaces/brms/36).,

## Session info {-}

```{r}
sessionInfo()
```

```{r, eval = F, echo = F}
# remove our objects
rm(x, d, y, abc, step_1, xf, xfo, xfol, a, my_list, t, hgn, a_sq_plus_b, count_down, note, start_time, i, stop_time, elapsed_time_vector, elapsed_time_loop)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

## Footnote {-}

[^1]: The whole `=` versus `<-` controversy can spark some strong opinions within the **R** community. If you're curious about the historical roots for `<-`, check out [Colon Fay](https://twitter.com/_ColinFay)'s nice blog post, [*Why do we use arrow as an assignment operator?*](https://colinfay.me/r-assignment/).


<!--chapter:end:03.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

# What is This Stuff Called Probability?

> Inferential statistical techniques assign precise measures to our uncertainty about possibilities. Uncertainty is measured in terms of *probability*, and therefore we must establish the properties of probability before we can make inferences about it. This chapter introduces the basic ideas of probability. [@kruschkeDoingBayesianData2015, p. 71, *emphasis* in the original]

## The set of all possible events

This snip from page 72 is important (*emphasis* in the original):

> Whenever we ask about how likely an outcome is, we always ask with a set of possible outcomes in mind. This set exhausts all possible outcomes, and the outcomes are all mutually exclusive. This set is called the *sample space*.

## Probability: Outside or inside the head

It's worthwhile to quote this section in full.

> Sometimes we talk about probabilities of outcomes that are "out there" in the world. The face of a flipped coin is such an outcome: We can observe the flip, and the probability of coming up heads can be estimated by observing several flips.
>
> But sometimes we talk about probabilities of things that are not so clearly "out there," and instead are just possible beliefs "inside the head." Our belief about the fairness of a coin is an example of something inside the head. The coin may have an intrinsic physical bias, but now I am referring to our *belief* about the bias. Our beliefs refer to a space of mutually exclusive and exhaustive possibilities. It might be strange to say that we randomly sample from our beliefs, like we randomly sample from a sack of coins. Nevertheless, the mathematical properties of probabilities outside the head and beliefs inside the head are the same in their essentials, as we will see. (pp. 73--74, *emphasis* in the original)

### Outside the head: Long-run relative frequency.

> For events outside the head, it's intuitive to think of probability as being the long-run relative frequency of each possible outcome...
>
> We can determine the long-run relative frequency by two different ways. One way is to approximate it by actually sampling from the space many times and tallying the number of times each event happens. A second way is by deriving it mathematically. These two methods are now explored in turn. (p. 74)

#### Simulating a long-run relative frequency.

Before we try coding the simulation, we'll first load the **tidyverse**.

```{r p_unload, echo = F, warning = F, message = F}
# solves difficulties while publishing
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

```{r, message = F, warning = F}
library(tidyverse)
```

Now run the simulation.

```{r end_prop, message = F, warning = F}
n       <- 500  # specify the total number of flips
p_heads <- 0.5  # specify underlying probability of heads

# Kruschke reported this was the seed he used at the top of page 94
set.seed(47405)

# here we use that seed to flip a coin n times and compute the running proportion of heads at each flip. 
# we generate a random sample of n flips (heads = 1, tails = 0)
d <-
  tibble(flip_sequence = sample(x = c(0, 1), 
                                prob = c(1 - p_heads, p_heads), 
                                size = n, 
                                replace = T)) %>% 
  mutate(n = 1:n,
         r = cumsum(flip_sequence)) %>% 
  mutate(run_prop = r / n)

end_prop <-
  d %>% 
  select(run_prop) %>% 
  slice(n()) %>% 
  round(digits = 3) %>% 
  pull()
```

Now we're ready to make Figure 4.1.

```{r, message = F, warning = F, fig.width = 4.5, fig.height = 4}
d %>%
  filter(n < 1000) %>%  # this step cuts down on the time it takes to make the plot
  ggplot(aes(x = n, y = run_prop)) +
  geom_hline(yintercept = .5, color = "white") +
  geom_line(color = "grey50") +
  geom_point(color = "grey50", alpha = 1/4) +
  scale_x_log10(breaks = c(1, 2, 5, 10, 20, 50, 200, 500)) +
  coord_cartesian(xlim = c(1, 500),
                  ylim = c(0, 1)) +
  labs(title = "Running proportion of heads",
       subtitle = paste("Our end proportion =", end_prop),
       x = "Flip number", 
       y = "Proportion of heads") +
  theme(panel.grid = element_blank())
```

#### Deriving a long-run relative frequency.

> Sometimes, when the situation is simple enough mathematically, we can derive the exact long-run relative frequency. The case of the fair coin is one such simple situation. The sample space of the coin consists of two possible outcomes, head and tail. By the assumption of fairness, we know that each outcome is equally likely. Therefore, the long-run relative frequency of heads should be exactly one out of two, i.e., 1/2, and the long-run relative frequency of tails should also be exactly 1/2. (p. 76)

### Inside the head: Subjective belief.

> To specify our subjective beliefs, we have to specify how likely we think each possible outcome is. It can be hard to pin down mushy intuitive beliefs. In the next section, we explore one way to "calibrate" subjective beliefs, and in the subsequent section we discuss ways to mathematically describe degrees of belief. (p. 76)

### Probabilities assign numbers to possibilities.

> In general, a probability, whether it's outside the head or inside the head, is just a way of assigning numbers to a set of mutually exclusive possibilities. The numbers, called "probabilities," merely need to satisfy three properties [@kolmogorovFoundationsTheoryProbability1956]:
>
> 1. A probability value must be nonnegative (i.e., zero or positive).
> 2. The sum of the probabilities across all events in the entire sample space must be 1.0 (i.e., one of the events in the space must happen, otherwise the space does not exhaust all possibilities).
> 3. For any two mutually exclusive events, the probability that one *or* the other occurs is the *sum* of their individual probabilities. For example, the probability that a fair six-sided die comes up 3-dots *or* 4-dots is 1/6 + 1/6 = 2/6.
>
> Any assignment of numbers to events that respects those three properties will also have all the properties of probabilities that we will discuss below. (pp. 77--78, *emphasis* in the original)

## Probability distributions

"A probability *distribution* is simply a list of all possible outcomes and their corresponding probabilities" (p. 78, *emphasis* in the original)

### Discrete distributions: Probability mass.

> When the sample space consists of discrete outcomes, then we can talk about the probability of each distinct outcome. For example, the sample space of a flipped coin has two discrete outcomes, and we talk about the probability of head or tail...
>
> For continuous outcome spaces, we can *discretize* the space into a finite set of mutually exclusive and exhaustive "bins." (p. 78, *emphasis* in the original)

In order to recreate Figure 4.2, we need to generate the heights data with Kruschke's `HtWtDataDenerator()` function. You can find the original code in Kruschke's `HtWtDataDenerator.R` script, and we first used this function in [Section 2.3][The steps of Bayesian data analysis]. Once again, here's how to make the function.

```{r}
HtWtDataGenerator <- function(n_subj, rndsd = NULL, male_prob = 0.50) {
  
  # Random height, weight generator for males and females. Uses parameters from
  # Brainard, J. & Burmaster, D. E. (1992). Bivariate distributions for height and
  # weight of men and women in the United States. Risk Analysis, 12(2), 267-275.
  # Kruschke, J. K. (2011). Doing Bayesian data analysis:
  # A Tutorial with R and BUGS. Academic Press / Elsevier.
  # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition:
  # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier.
  
  # require(MASS)
  
  # Specify parameters of multivariate normal (MVN) distributions.
  # Men:
  HtMmu   <- 69.18
  HtMsd   <- 2.87
  lnWtMmu <- 5.14
  lnWtMsd <- 0.17
  Mrho    <- 0.42
  Mmean   <- c(HtMmu, lnWtMmu)
  Msigma  <- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd,
                      Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2)
  # Women cluster 1:
  HtFmu1   <- 63.11
  HtFsd1   <- 2.76
  lnWtFmu1 <- 5.06
  lnWtFsd1 <- 0.24
  Frho1    <- 0.41
  prop1    <- 0.46
  Fmean1   <- c(HtFmu1, lnWtFmu1)
  Fsigma1  <- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1,
                       Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2)
  # Women cluster 2:
  HtFmu2   <- 64.36
  HtFsd2   <- 2.49
  lnWtFmu2 <- 4.86
  lnWtFsd2 <- 0.14
  Frho2    <- 0.44
  prop2    <- 1 - prop1
  Fmean2   <- c(HtFmu2, lnWtFmu2)
  Fsigma2  <- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2,
                       Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2)
  
  # Randomly generate data values from those MVN distributions.
  if (!is.null(rndsd)) {set.seed(rndsd)}
  data_matrix <- matrix(0, nrow = n_subj, ncol = 3)
  colnames(data_matrix) <- c("male", "height", "weight")
  maleval <- 1; femaleval <- 0 # arbitrary coding values
  for (i in 1:n_subj) {
    # Flip coin to decide sex
    sex = sample(c(maleval, femaleval), size = 1, replace = TRUE,
                 prob = c(male_prob, 1 - male_prob))
    if (sex == maleval) {datum <- MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)}
    if (sex == femaleval) {
      Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2))
      if (Fclust == 1) {datum <- MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)}
      if (Fclust == 2) {datum <- MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)}
    }
    data_matrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1))
  }
  
  return(data_matrix)
  
} # end function
```

Now we have the `HtWtDataGenerator()` function, all we need to do is determine how many values are generated and how probable we want the values to be based on those from men. These are controlled by the `n_subj` and `male_prob` parameters.

```{r, message = F, warning = F}
set.seed(4)
d <-
  HtWtDataGenerator(n_subj = 10000, male_prob = .5) %>%
  data.frame() %>%
  mutate(person = 1:n())

d %>%
  head()
```

For Figure 4.2, we'll discretize the continuous `height` values into bins with the `case_when()` function, which you can learn more about from [hrbrmstr](https://twitter.com/hrbrmstr)'s blog post, [*Making a case for case_when*](https://rud.is/b/2017/03/10/making-a-case-for-case_when/).

```{r, fig.height = 2, fig.width = 7, message = F}
d_bin <-
  d %>%
  mutate(bin = case_when(
    height < 51 ~ 51,
    between(height, 51, 53) ~ 53,
    between(height, 53, 55) ~ 55,
    between(height, 55, 57) ~ 57,
    between(height, 57, 59) ~ 59,
    between(height, 59, 61) ~ 61,
    between(height, 61, 63) ~ 63,
    between(height, 63, 65) ~ 65,
    between(height, 65, 67) ~ 67,
    between(height, 67, 69) ~ 69,
    between(height, 69, 71) ~ 71,
    between(height, 71, 73) ~ 73,
    between(height, 73, 75) ~ 75,
    between(height, 75, 77) ~ 77,
    between(height, 77, 79) ~ 79,
    between(height, 79, 81) ~ 71,
    between(height, 81, 83) ~ 83,
    height > 83 ~ 85)
    ) %>%
  group_by(bin) %>%
  summarise(n = n()) %>%
  mutate(height = bin - 1)

d %>%
  ggplot(aes(x = height, y = person)) +
  geom_point(size = 3/4, color = "grey67", alpha = 1/2) +
  geom_vline(xintercept = seq(from = 51, to = 83, by = 2),
             linetype = 3, color = "grey33") +
  geom_text(data = d_bin, 
            aes(y = 5000, label = n),
            size = 3.25) +
  scale_y_continuous(breaks = c(0, 5000, 10000)) +
  labs(title = "Total N = 10,000",
       x = "Height (inches)",
       y = "Person #") +
  theme(panel.grid = element_blank())
```

Because we're simulating and we don't know what seed number Kruschke used for his plot, ours will differ a little from his. But the overall pattern is the same.

One way to make a version of the histogram in Kruschke's Figure 4.2.b would be to input the `d` data directly into `ggplot()` and set `x = height` and `y = stat(density)` within the `aes()`. Then you could set `binwidth = 2` within `geom_histogram()` to make the bins within the histogram perform like the bins in the plot above. However, since we have already discretized the `height` values into bins in our `d_bin` data, it might make more sense to plot those bins with `geom_col()`. The only other step we need is to manually compute the density values using the formula Kruschke showed in Figure 4.2.b. Here' how:

```{r, fig.height = 1.75, fig.width = 7}
d_bin %>% 
  # density is the probability mass divided by the bin width
  mutate(density = (n / sum(n)) / 2) %>% 
  
  ggplot(aes(x = height, y = density, fill = bin == 65)) +
  geom_col() +
  scale_fill_manual(values = c("gray67", "gray30"), breaks = NULL) +
  scale_y_continuous("Probability density", breaks = c(0, 0.04, 0.08)) +
  xlab("Height (inches)") +
  coord_cartesian(xlim = c(51, 83)) +
  theme(panel.grid = element_blank())
```

In the text, Kruschke singled out the bin for the values between 63 and 65 with an arrow. In our plot, we highlighted that bin with shading, instead. Here's how we computed the exact density value for that bin.

```{r}
d_bin %>% 
  mutate(density = (n / sum(n)) / 2) %>% 
  filter(bin == 65) %>% 
  select(n, density)
```

Due to sampling variation, our density value is a little different from the one in the text.

Our data binning approach for Figure 4.2.c will be a little different than what we did, above. Here we'll make our bins with the `round()` function.

```{r, fig.height = 2, fig.width = 7, message = F}
d_bin <-
  d %>%
  mutate(bin = round(height, digits = 0)) %>%   
  group_by(bin) %>%
  summarise(n = n()) %>%
  mutate(height = bin - 0.5)

d %>%
  ggplot(aes(x = height, y = person)) +
  geom_point(size = 3/4, color = "grey67", alpha = 1/2) +
  geom_vline(xintercept = seq(from = 51, to = 83, by = 1),
             linetype = 3, color = "grey33") +
  geom_text(data = d_bin, 
            aes(y = 5000, label = n, angle = 90),
            size = 3.25) +
  scale_y_continuous(breaks = c(0, 5000, 10000)) +
  labs(title = "Total N = 10,000",
       x = "Height (inches)",
       y = "Person #") +
  theme(panel.grid = element_blank())
```

However, our method for Figure 4.2.d will be like what we did, before.

```{r, fig.height = 1.75, fig.width = 7}
d_bin %>% 
  # density is the probability mass divided by the bin width
  mutate(density = (n / sum(n)) / 1) %>% 
  
  ggplot(aes(x = height, y = density, fill = bin == 64)) +
  geom_col() +
  scale_fill_manual(values = c("gray67", "gray30"), breaks = NULL) +
  scale_y_continuous("Probability density", breaks = c(0, 0.04, 0.08)) +
  xlab("Height (inches)") +
  coord_cartesian(xlim = c(51, 83)) +
  theme(panel.grid = element_blank())
```

Here's the hand-computed density value for the focal bin.

```{r}
d_bin %>% 
  mutate(density = (n / sum(n)) / 1) %>% 
  filter(bin == 64) %>% 
  select(n, density)
```

> The probability of a discrete outcome, such as the probability of falling into an interval on a continuous scale, is referred to as a probability *mass.* Loosely speaking, the term "mass" refers the amount of stuff in an object. When the stuff is probability and the object is an interval of a scale, then the mass is the proportion of the outcomes in the interval. (p. 80, *emphasis* in the original)

### Continuous distributions: Rendezvous with density.

> If you think carefully about a continuous outcome space, you realize that it becomes problematic to talk about the probability of a specific value on the continuum, as opposed to an interval on the continuum... Therefore, what we will do is make the intervals infinitesimally narrow, and instead of talking about the infinitesimal probability mass of each infinitesimal interval, we will talk about the ratio of the probability mass to the interval width. That ratio is called the probability *density*.
>
> Loosely speaking, density is the amount of stuff per unit of space it takes up. Because we are measuring amount of stuff by its mass, then density is the mass divided by the amount space it occupies. (p. 80, *emphasis* in the original)

To make Figure 4.3, we'll need new data.

```{r, message = F, warning = F}
set.seed(4)
d <-
  tibble(height = rnorm(1e4, mean = 84, sd = .1)) %>%
  mutate(door = 1:n())

d %>%
  head()
```

To make the bins for our version of Figure 4.3.a, we could use the `case_when()` approach from above. However, that would require some tedious code. Happily, we have an alternative in the [**santoku** package](https://github.com/hughjonesd/santoku) [@R-santoku], which I learned about with help from the great [Mara Averick](https://twitter.com/dataandme), [Tyson Barrett](https://twitter.com/healthandstats), and [Omar Wasow](https://twitter.com/owasow). We can use the `santoku::chop()` function to discretize our `height` values. Here we'll walk through the first part.

```{r, eval = F}
library(santoku)

d_bin <-
  d %>% 
  mutate(bin = chop(height, 
                    breaks = seq(from = 83.6, to = 84.4, length.out = 32),
                    # label the boundaries with 3 decimal places, separated by a dash
                    labels = lbl_dash(fmt = "%.3f"))

head(d_bin)
```

```{r, warning = F, message = F, echo = F}
d_bin <-
  d %>% 
  mutate(bin = santoku::chop(height, 
                    breaks = seq(from = 83.6, to = 84.4, length.out = 32),
                    # label the boundaries with 3 decimal places, separated by a dash
                    labels = lbl_dash(fmt = "%.3f")))

head(d_bin)
```

With this format, the lower-limit for each level of `bin` is the left side of the dash and the upper-limit is on the right. Though the cut points are precise to many decimal places, the `lbl_dash(fmt = "%.3f")` part of the code rounded the numbers to three decimal places in the `bin` labels. The width of each bin is just a bit over 0.0258.

```{r}
(84.4 - 83.6) / (32 - 1)
```

Now to make use of the `d_bin` data in a plot, we'll have to summarize and separate the values from the `bin` names to compute the midway points. Here's one way how.

```{r}
d_bin <- d_bin %>% 
  group_by(bin) %>%
  summarise(n = n()) %>% 
  separate(bin, c("min", "max"), sep = "—", remove = F, convert = T) %>% 
  mutate(height = (min + max) / 2)

head(d_bin)
```

Now we plot.

```{r, fig.height = 2, fig.width = 7}
d %>%
  ggplot(aes(x = height, y = door)) +
  geom_point(size = 3/4, color = "grey67", alpha = 1/2) +
  geom_vline(xintercept = seq(from = 83.6, to = 84.4, length.out = 32),
             linetype = 3, color = "grey33") +
  geom_text(data = d_bin,
            aes(y = 5000, label = n, angle = 90),
            size = 3.25) +
  scale_y_continuous(breaks = c(0, 5000, 10000)) +
  labs(title = "Total N = 10,000",
       x = "Height (inches)",
       y = "Door #") +
  theme(panel.grid = element_blank())
```

The only tricky thing about Figure 4.3.b is getting the denominator in the density equation correct.

```{r, fig.height = 1.75, fig.width = 7}
d_bin %>% 
  # density is the probability mass divided by the bin width
  mutate(density = (n / sum(n)) / ((84.4 - 83.6) / (32 - 1))) %>% 
  
  ggplot(aes(x = height, y = density, fill = bin == "83.910—83.935")) +
  geom_col() +
  scale_fill_manual(values = c("gray67", "gray30"), breaks = NULL) +
  scale_y_continuous("Probability density", breaks = 0:4) +
  xlab("Height (inches)") +
  coord_cartesian(xlim = c(83.6, 84.4)) +
  theme(panel.grid = element_blank())
```

Here's the density value for that focal bin.

```{r}
d_bin %>% 
  # density is the probability mass divided by the bin width
  mutate(density = (n / sum(n)) / ((84.4 - 83.6) / (32 - 1))) %>% 
  filter(bin == "83.910—83.935") %>% 
  select(bin, n, density)
```

As Kruschke remarked: "There is nothing mysterious about probability densities larger than 1.0; it means merely that there is a high concentration of probability mass relative to the scale" (p. 82).

#### Properties of probability density functions.

> In general, for any continuous value that is split up into intervals, the sum of the probability masses of the intervals must be 1, because, by definition of making a measurement, some value of the measurement scale must occur. (p. 82)

#### The normal probability density function.

"Perhaps the most famous probability density function is the normal distribution, also known as the Gaussian distribution" (p. 83). We'll use `dnorm()` again to make our version of Figure 4.4.

```{r, fig.width = 4.5, fig.height = 3}
tibble(x = seq(from = -.8, to = .8, by = .02)) %>% 
  mutate(p = dnorm(x, mean = 0, sd = .2)) %>% 
  
  ggplot(aes(x = x)) +
  geom_line(aes(y = p),
            color = "grey50", size = 1.25) +
  geom_linerange(aes(ymin = 0, ymax = p),
                 size = 1/3) +
  labs(title = "Normal probability density",
       subtitle = expression(paste(mu, " = 0 and ", sigma, " = 0.2")),
       y = "p(x)") +
  coord_cartesian(xlim = c(-.61, .61)) +
  theme(panel.grid = element_blank())
```

The equation for the normal probability density follows the form

$$
p(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left ( - \frac{1}{2} \left [ \frac{x - \mu}{\sigma}^2 \right ] \right ),
$$

where $\mu$ governs the mean and $\sigma$ governs the standard deviation.

### Mean and variance of a distribution.

The mean of a probability distribution is also called the *expected value*, which follows the form

$$E[x] = \sum_x p(x) x$$

when $x$ is discrete. For continuous $x$ values, the formula is

$$E[x] = \int \text d x \; p(x) x.$$

The variance is defined as the mean squared deviation from the mean,

$$\text{var}_x = \int \text d x \; p(x) (x - E[x])^2.$$

If you take the square root of the variance, you get the standard deviation.

### Highest density interval (HDI).

> The HDI indicates which points of a distribution are most credible, and which cover most of the distribution. Thus, the HDI summarizes the distribution by specifying an interval that spans most of the distribution, say 95% of it, such that every point inside the interval has higher credibility than any point outside the interval. (p. 87)

In Chapter 10 (p. 294), Kruschke briefly mentioned his `HDIofICDF()` function, the code for which you can find in his `DBDA2E-utilities.R` file. It's a handy function which we'll put to use from time to time. Here's a mild reworking of his code.

```{r}
hdi_of_icdf <- function(name, width = .95, tol = 1e-8, ... ) {
  
  # Arguments:
  #   `name` is R's name for the inverse cumulative density function
  #   of the distribution.
  #   `width` is the desired mass of the HDI region.
  #   `tol` is passed to R's optimize function.
  # Return value:
  #   Highest density iterval (HDI) limits in a vector.
  # Example of use: For determining HDI of a beta(30, 12) distribution, type
  #   `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)`
  #   Notice that the parameters of the `name` must be explicitly stated;
  #   e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work.
  # Adapted and corrected from Greg Snow's TeachingDemos package.
  
  incredible_mass <-  1.0 - width
  interval_width <- function(low_tail_prob, name, width, ...) {
    name(width + low_tail_prob, ...) - name(low_tail_prob, ...)
  }
  opt_info <- optimize(interval_width, c(0, incredible_mass), 
                       name = name, width = width, 
                       tol = tol, ...)
  hdi_lower_tail_prob <- opt_info$minimum
  
  return(c(name(hdi_lower_tail_prob, ...),
           name(width + hdi_lower_tail_prob, ...)))
  
}
```


We already know from the text, and perhaps from prior experience, what the 95% HDI is for the unit normal. But it's nice to be able to confirm that with a function.

```{r}
h <-
  hdi_of_icdf(name = qnorm,
              mean = 0,
              sd   = 1)

h
```

Now we've saved those values in `h`, we can use then to make our version of Figure 4.5.a.

```{r, fig.width = 4.5, fig.height = 2, warning = F}
tibble(x = seq(from = -3.5, to = 3.5, by = .05)) %>% 
  mutate(d = dnorm(x, mean = 0, sd = 1)) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = "grey75") +
  geom_area(data = . %>% filter(x >= h[1] & x <= h[2]),
            fill = "grey50") +
  geom_line(data = tibble(x = c(h[1] + .02, h[2] - .02),
                          d = c(.059, .059)),
            arrow = arrow(length = unit(.2, "cm"), 
                          ends = "both", 
                          type = "closed"),
            color = "grey92") +
  annotate(geom = "text", x = 0, y = .09, 
           label = "95% HDI", color = "grey92") +
  xlim(-3.1, 3.1) +
  ylab("p(x)") +
  theme(panel.grid = element_blank())
```

As far as I could tell, Figure 4.5.b is of a beta distribution, which Kruschke covered in greater detail starting in [Chapter 6][Inferring a Binomial Probability via Exact Mathematical Analysis]. I got the `shape1` and `shape2` values from playing around. If you have a more principled approach, [do share](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues). But anyway, we can use our `hdi_of_icdf()` function to ge the correct values.

```{r}
h <-
  hdi_of_icdf(name = qbeta,
              shape1 = 15, 
              shape2 = 4)

h
```

Let's put those `h` values to work.

```{r, fig.width = 4.5, fig.height = 2, warning = F}
tibble(x = seq(from = 0, to = 1, by = .01)) %>% 
  mutate(d = dbeta(x, shape1 = 15, shape2 = 4)) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = "grey75") +
  geom_area(data = . %>% filter(x >= h[1] & x <= h[2]),
           fill = "grey50") +
  geom_line(data = tibble(x = c(h[1] + .01, h[2] - .002),
                          d = c(.75, .75)),
            arrow = arrow(length = unit(.2, "cm"),
                          ends = "both",
                          type = "closed"),
            color = "grey92") +
  annotate(geom = "text", x = .8, y = 1.1, 
           label = "95% HDI", color = "grey92") +
  xlim(.4, 1) +
  ylab("p(x)") +
  theme(panel.grid = element_blank())
```

Figure 4.5.c was also a lot of trial and error. It seemed the easiest way to reproduce the shape was to mash two Gaussians together. After playing around with `rnorm()`, I ended up with this.

```{r}
set.seed(4)
d <-
  tibble(x = c(rnorm(6e5, mean = 1.50, sd = .5),
               rnorm(4e5, mean = 4.75, sd = .5)))

glimpse(d)
```

As you'll see, it's not exactly right. But it's close enough to give you a sense of what's going on. But anyway, since we're working with simulated data rather than an analytic solution, we'll want to use one of the powerful convenience functions from the **tidybayes** package.

```{r, warning = F, message = F}
library(tidybayes)
```

Kay's **tidybayes** package

> provides a family of functions for generating point summaries and intervals from draws in a tidy format. These functions follow the naming scheme `[median|mean|mode]_[qi|hdi]`, for example, `median_qi()`, `mean_qi()`, `mode_hdi()`, and so on. The first name (before the `_`) indicates the type of point summary, and the second name indicates the type of interval. `qi` yields a quantile interval (a.k.a. equi-tailed interval, central interval, or percentile interval) and `hdi` yields a highest (posterior) density interval. [@kayExtractingVisualizingTidy2021, "Point summaries and intervals"]

Here we'll use `mode_hdi()` to compute the HDIs and put them in a tibble. We'll be using a lot of `mode_hdi()` in this project.

```{r}
h <- 
  d %>% 
  mode_hdi()

h
```

Usually, `mode_hdi()` will return a tibble with just one row. But in this case, since we had a bimodal distribution, it returned two rows—one for each of the two distinct regions. Oh, and in case it wasn't clear, that first column `x` is the measure of central tendency—the mode, in this case. Though I acknowledge, it's a little odd to speak of central tendency in a bimodal distribution. Again, this won't happen much.

In order to fill the bimodal density with the split HDIs, you need to use the `density()` function to transform the `d` data to a tibble with the values for the $x$-axis in an `x` vector and the corresponding density values in a `y` vector.

```{r}
dens <-
  d$x %>%
  density() %>%
  with(tibble(x, y))

head(dens)
```

We're finally ready to plot. Forgive me. It's a monster.

```{r, fig.width = 4.5, fig.height = 2, warning = F, message = F}
ggplot(data = dens,
       aes(x = x, y = y)) +
  geom_area(fill = "grey75") +
  # note the use of `pull()`, which extracts the values, rather than return a tibble  
  geom_area(data = dens %>% filter(x > h[1, 2] %>% pull() & 
                                     x < h[1, 3] %>% pull()),
            fill = "grey50") +
  geom_area(data = dens %>% filter(x > h[2, 2] %>% pull() & 
                                     x < h[2, 3] %>% pull()),
            fill = "grey50") +
  geom_line(data = tibble(x = c(h[1, 2] %>% pull(), h[1, 3] %>% pull()),
                          y = c(.06, .06)),
            arrow = arrow(length = unit(.2,"cm"),
                          ends = "both",
                          type = "closed"),
            color = "grey92") +
  geom_line(data = tibble(x = c(h[2, 2] %>% pull(), h[2, 3] %>% pull()),
                          y = c(.06, .06)),
            arrow = arrow(length = unit(.2,"cm"),
                          ends = "both",
                          type = "closed"),
            color = "grey92") +
  annotate(geom = "text", x = c(1.5, 4.75), y = .1, 
           label = "95% HDI", color = "grey92") +
  scale_x_continuous(breaks = 0:6, limits = c(0, 6.3)) +
  scale_y_continuous("p(x)", breaks = c(0, .1, .2, .3, .4, .5)) +
  theme(panel.grid = element_blank())
```

> When the distribution refers to credibility of values, then the width of the HDI is another way of measuring uncertainty of beliefs. If the HDI is wide, then beliefs are uncertain. If the HDI is narrow, then beliefs are relatively certain. (p. 89)

## Two-way distributions

In the note below Table 4.1, Kruschke indicated the data came from @sneeGraphicalDisplayTwoway1974, [*Graphical display of two-way contingency tables*](https://www.researchgate.net/publication/243769696_Graphical_Display_of_Two-Way_Contingency_Tables). Kruschke has those data saved as the `HairEyeColor.csv` file.

```{r, warning = F, message = F}
d <- read_csv("data.R/HairEyeColor.csv")

glimpse(d)
```

We'll need to transform `Hair` and `Eye` a bit to ensure our output matches the order in Table 4.1.

```{r}
d <- 
  d %>% 
  mutate(Hair = if_else(Hair == "Brown", "Brunette", Hair) %>% 
           factor(., levels = c("Black", "Brunette", "Red", "Blond")),
         Eye  = factor(Eye, levels = c("Brown", "Blue", "Hazel", "Green")))
```

Here we'll use the `tabyl()` and `adorn_totals()` functions from the [**janitor** package](https://CRAN.R-project.org/package=janitor) [@R-janitor] to help make the table of proportions by `Eye` and `Hair`.

```{r, warning = F, message = F}
library(janitor)

d <-
  d %>%
  uncount(weights = Count, .remove = F) %>% 
  tabyl(Eye, Hair) %>% 
  adorn_totals(c("row", "col")) %>% 
  data.frame() %>% 
  mutate_if(is.double, ~ . / 592)

d %>% 
  mutate_if(is.double, round, digits = 2)
```

### Conditional probability.

> We often want to know the probability of one outcome, given that we know another outcome is true. For example, suppose I sample a person at random from the population referred to in Table 4.1. Suppose I tell you that this person has blue eyes. Conditional on that information, what is the probability that the person has blond hair (or any other particular hair color)? It is intuitively clear how to compute the answer: We see from the blue-eye row of Table 4.1 that the total (i.e., marginal) amount of blue-eyed people is 0.36, and that 0.16 of the population has blue eyes and blond hair. (p. 91)

Kruschke then showed how to compute such conditional probabilities by hand in Table 4.2. Here's a slightly reformatted version of that information.

```{r}
d %>% 
  filter(Eye == "Blue") %>% 
  pivot_longer(Black:Blond,
               names_to = "Hair",
               values_to = "proportion") %>% 
  rename(`p(Eyes = "Blue")` = Total) %>% 
  mutate(`conditional probability` = proportion / `p(Eyes = "Blue")`) %>% 
  select(Eye, Hair, `p(Eyes = "Blue")`, proportion, `conditional probability`)
```

The only reason our values differ from those in Table 4.2 is because Kruschke rounded.

## Session info {-}

```{r}
sessionInfo()
```

```{r, eval = F, echo = F}
# remove our objects
rm(n, p_heads, d, end_prop, HtWtDataGenerator, d_bin, hdi_of_icdf, h, dens)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:04.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

# Bayes' Rule

"Bayes' rule is merely the mathematical relation between the prior allocation of credibility and the posterior reallocation of credibility conditional on data" [@kruschkeDoingBayesianData2015, pp. 99--100].

## Bayes' rule

> Thomas Bayes (1702–1761) was a mathematician and Presbyterian minister in England. His famous theorem was published posthumously in 1763, thanks to the extensive editorial efforts of his friend, Richard Price [@bayesLIIEssaySolving1763]. The simple rule has vast ramifications for statistical inference, and therefore as long as his name is attached to the rule, we'll continue to see his name in textbooks. But Bayes himself probably was not fully aware of these ramifications, and many historians argue that it is Bayes' successor, Pierre-Simon Laplace (1749–1827), whose name should really label this type of analysis, because it was Laplace who independently rediscovered and extensively developed the methods [e.g., @daleHistoryInverseProbability2012; @mcgrayneTheoryThatWould2011]. (p. 100)

I do recommend checking out McGrayne's book It's an easy and entertaining read. For a sneak preview, why not [listen to her](https://www.youtube.com/watch?v=8oD6eBkjF9o) discuss the main themes she covered in the book?

### Derived from definitions of conditional probability.

With Equations 5.5 and 5.6, Kruschke gave us Bayes' rule in terms of $c$ and $r$. Equation 5.5 was

$$p(c|r) = \frac{p(r|c) \; p(c)}{p(r)}.$$

Since $p(r) = \sum_{c^*}p(r|c^*)p(c^*)$, we can re-express that as Equation 5.6:

$$p(c|r) = \frac{p(r|c) \; p(c)}{\sum_{c^*}p(r|c^*) \; p(c^*)},$$

where $c^*$ "in the denominator is a variable that takes on all possible values" of $c$ (p. 101).

## Applied to parameters and data

Here we get those equations re-expressed in the terms data analysts tend to think with, parameters (i.e., $\theta$) and data (i.e., $D$):

\begin{align*}
p(\theta|D) & = \frac{p(D|\theta) \; p(\theta)}{p(D)} \;\; \text{and since} \\
p(D)        & = \sum\limits_{\theta^*}p(D|\theta^*) \; p(\theta^*), \;\; \text{it's also the case that} \\
p(\theta|D) & = \frac{p(D|\theta) \; p(\theta)}{\sum\limits_{\theta^*}p(D|\theta^*) \; p(\theta^*)}.
\end{align*}

As in the previous section where we spoke in terms of $r$ and $c$, our updated $\theta^*$ notation is meant to indicate all possible values of $\theta$. For practice, it's worth repeating how Kruschke broke this down with Equation 5.7,

$$
\underbrace{p(\theta|D)}_\text{posterior} \; = \; \underbrace{p(D|\theta)}_\text{likelihood} \;\; \underbrace{p(\theta)}_\text{prior} \; / \; \underbrace{p(D)}_\text{evidence}.
$$

> The "prior," $p(\theta)$, is the credibility of the $\theta$ values without the data $D$. The "posterior," $p(\theta|D)$, is the credibility of $\theta$ values with the data $D$ taken into account. The "likelihood," $p(D|\theta)$, is the probability that the data could be generated by the model with parameter value $\theta$. The "evidence" for the model, $p(D)$, is the overall probability of the data according to the model, determined by averaging across all possible parameter values weighted by the strength of belief in those parameter values. (pp. 106--107)

And don't forget, "evidence" is short for "marginal likelihood," which is the term we'll use in some of our code, below.

## Complete examples: Estimating bias in a coin

As we begin to work with Kruschke's coin example, we should clarify that:

> when [Kruschke refered] to the "bias" in a coin, [he] sometimes [referred] to its underlying probability of coming up heads. Thus, *when a coin is fair, it has a "bias" of 0.50*. Other times, [Kruschke used] the term "bias" in its colloquial sense of a *departure from fairness*, as in "head biased" or "tail biased." Although [Kruschke tried] to be clear about which meaning is intended, there will be times that you will have to rely on context to determine whether "bias" means the probability of heads or departure from fairness. (p. 108, *emphasis* in the original)

In this ebook, I will generally avoid Kruschke's idiosyncratic use of the term "bias." Though be warned: it may pop up from time to time.

Here's a way to make Figure 5.1.a.

```{r, message = F, warning = F, fig.width = 4, fig.height = 2}
library(tidyverse)

tibble(theta = seq(from = 0, to = 1, by = .1),
       prior = c(seq(from = 0, to = .2, length.out = 6),
                 seq(from = .16, to = 0, length.out = 5))) %>%
  
  ggplot(aes(x = theta, ymin = -0.0005, ymax = prior)) +
  geom_linerange(size = 4, color = "grey50") +
  scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) +
  labs(title = "Prior",
       y = expression(p(theta))) +
  theme(panel.grid = element_blank())
```

If you were curious, it is indeed the case that those `prior` values sum to 1.

```{r}
tibble(prior = c(seq(from = 0,   to = .2, length.out = 6),
                 seq(from = .16, to = 0,  length.out = 5))) %>% 
  summarise(s = sum(prior))
```

In Equation 5.10 (p. 109), Kruschke defined the Bernoulli function as

$$p(y | \theta) = \theta^y (1 - \theta)^{(1 - y)}.$$

We can express it as a function in **R** like this.

```{r}
bernoulli <- function(theta, y) {
  return(theta^y * (1 - theta)^(1 - y))
}
```

To get a sense of how it works, consider a single coin flip of heads when heads is considered a successful trial. We'll call the single successful trial `y = 1`. We can use our custom `bernoulli()` function to compute the likelihood of different values of $\theta$. We'll look at 11 candidate $\theta$ values, which we'll call `theta_sequence`.

```{r}
theta_sequence <- seq(from = 0, to = 1, by = .1)

bernoulli(theta = theta_sequence, y = 1)
```

Notice how our `theta_sequence` corresponds nicely with the sequence of $\theta$ values on the $x$-axes of Figure 5.1. We can combine `theta_sequence` and our `bernoulli()` function to make the middle panel of Figure 5.1.

```{r, fig.width = 4, fig.height = 2}
tibble(x = theta_sequence) %>%
  mutate(likelihood = bernoulli(theta = theta_sequence, y = 1)) %>% 
  
  ggplot(aes(x = x, y = likelihood)) +
  geom_col(width = .025, color = "grey50", fill = "grey50") +
  scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) +
  labs(title = "Likelihood",
       y = expression(p(D*'|'*theta))) +
  theme(panel.grid = element_blank())
```

In order to compute $p(D)$ (i.e., the *evidence* or the *marginal likelihood*), we'll need to multiply our respective prior and likelihood values for each point in our theta sequence and then sum all that up. That sum will be our *marginal likelihood*. With that cleared up, we can make Figure 5.1.c.

```{r, message = F, warning = F, fig.width = 4, fig.height = 2}
tibble(theta = theta_sequence,
       prior = c(seq(from = 0,   to = .2, length.out = 6),
                 seq(from = .16, to = 0,  length.out = 5))) %>%
  mutate(likelihood = bernoulli(theta = theta_sequence, y = 1)) %>% 
  mutate(marginal_likelihood = sum(prior * likelihood)) %>% 
  mutate(posterior = (prior * likelihood) / marginal_likelihood) %>%
  
  ggplot(aes(x = theta, y = posterior)) +
  geom_col(width = .025, color = "grey50", fill = "grey50") +
  scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) +
  labs(title = "Posterior",
       y = expression(p(theta*'|'*D))) +
  theme(panel.grid = element_blank())
```

> The posterior is a compromise between the prior distribution and the likelihood function. Sometimes this is loosely stated as a compromise between the prior and the data. The compromise favors the prior to the extent that the prior distribution is sharply peaked and the data are few. The compromise favors the likelihood function (i.e., the data) to the extent that the prior distribution is flat and the data are many. (p. 112)

### Influence of sample size on the posterior.

In order to follow along with this section, we're going to have to update our Bernoulli likelihood function so it can accommodate more than a single trial. In anticipation of [Chapter 6][Inferring a Binomial Probability via Exact Mathematical Analysis], we'll call our more general function the `bernoulli_likelihood()`.

```{r}
bernoulli_likelihood <- function(theta, data) {
  
  # `theta` = success probability parameter ranging from 0 to 1
  # `data` = the vector of data (i.e., a series of 0s and 1s)
  n   <- length(data)
  
  return(theta^sum(data) * (1 - theta)^(n - sum(data)))
  
}
```

Here's the work required to make our version of the left portion of Figure 5.2.

```{r, fig.width = 4, fig.height = 4}
small_data <- rep(0:1, times = c(3, 1))

tibble(theta =   seq(from = 0,     to = 1, by = .001),
       Prior = c(seq(from = 0,     to = 1, length.out = 501),
                 seq(from = 0.998, to = 0, length.out = 500))) %>% 
  mutate(Prior      = Prior / sum(Prior),
         Likelihood = bernoulli_likelihood(theta = theta,
                                           data  = small_data)) %>% 
  mutate(marginal_likelihood = sum(Prior * Likelihood)) %>% 
  mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %>% 
  select(theta, Prior, Likelihood, Posterior) %>% 
  pivot_longer(-theta) %>% 
  mutate(name = factor(name, levels = c("Prior", "Likelihood", "Posterior"))) %>% 

  ggplot(aes(x = theta, y = value)) +
  geom_area(fill = "grey67") +
  scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) +
  ylab("probability density") +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free_y", ncol = 1)
```

Just follow the same overall procedure to make the right portion of Figure 5.2. The only difference is how we switch from `small_data` to `large_data`.

```{r, fig.width = 4, fig.height = 4}
large_data <- rep(0:1, times = c(30, 10))

tibble(theta = seq(from = 0, to = 1, by = .001),
       Prior = c(seq(from = 0, to = 1, length.out = 501),
                 seq(from = 0.998, to = 0, length.out = 500))) %>% 
  mutate(Prior      = Prior / sum(Prior),
         Likelihood = bernoulli_likelihood(theta = theta, data = large_data)) %>% 
  mutate(marginal_likelihood = sum(Prior * Likelihood)) %>% 
  mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %>% 
  select(theta, Prior, Likelihood, Posterior) %>% 
  pivot_longer(-theta) %>% 
  mutate(name = factor(name, levels = c("Prior", "Likelihood", "Posterior"))) %>% 
  
  ggplot(aes(x = theta, y = value)) +
  geom_area(fill = "grey67") +
  scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) +
  ylab("probability density") +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free_y", ncol = 1)
```

With just an $N = 40$, the likelihood already dominated the posterior. But this is also a function of our fairly gentle prior. "In general, the more data we have, the more precise is the estimate of the parameter(s) in the model. Larger sample sizes yield greater precision or certainty of estimation" (p. 113).

### Influence of prior on the posterior.

It's not immediately obvious how Kruschke made his prior distributions for Figure 5.3. However, hidden away in his `BernGridExample.R` file he indicated that to get the distribution for the left side of Figure 5.3, you simply raise the prior from the left of Figure 5.2 to the 0.1 power.

```{r, fig.width = 4, fig.height = 4}
small_data <- rep(0:1, times = c(3, 1))

tibble(theta = seq(from = 0, to = 1, by = .001),
       Prior = c(seq(from = 0, to = 1, length.out = 501),
                 seq(from = 0.998, to = 0, length.out = 500))) %>% 
  # here's the important line of code
  mutate(Prior = Prior^0.1 / sum(Prior^0.1)) %>% 
  mutate(Likelihood = bernoulli_likelihood(theta = theta, data = small_data)) %>% 
  mutate(marginal_likelihood = sum(Prior * Likelihood)) %>% 
  mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %>% 
  select(theta, Prior, Likelihood, Posterior) %>% 
  pivot_longer(-theta) %>% 
  mutate(name = factor(name, levels = c("Prior", "Likelihood", "Posterior"))) %>% 

  ggplot(aes(x = theta, y = value)) +
  geom_area(fill = "grey67") +
  scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) +
  ylab("probability density") +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free_y", ncol = 1)
```

The trick is similar for the right half of Figure 5.3.

```{r, fig.width = 4.5, fig.height = 4}
large_data <- rep(0:1, times = c(30, 10))

tibble(theta = seq(from = 0, to = 1, by = .001),
       Prior = c(seq(from = 0, to = 1, length.out = 501),
                 seq(from = 0.998, to = 0, length.out = 500))) %>% 
  mutate(Prior      = Prior / sum(Prior),
         Likelihood = bernoulli_likelihood(theta = theta, data = large_data)) %>% 
  # here's the important line of code
  mutate(Prior = Prior^10) %>% 
  mutate(marginal_likelihood = sum(Prior * Likelihood)) %>% 
  mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %>% 
  select(theta, Prior, Likelihood, Posterior) %>% 
  pivot_longer(-theta) %>% 
  mutate(name = factor(name, levels = c("Prior", "Likelihood", "Posterior"))) %>% 
  
  ggplot(aes(x = theta, y = value)) +
  geom_area(fill = "grey67") +
  scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, by = .2)) +
  ylab("probability density") +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free_y", ncol = 1)
```

> Bayesian inference is intuitively rational: With a strongly informed prior that uses a lot of previous data to put high credibility over a narrow range of parameter values, it takes a lot of novel contrary data to budge beliefs away from the prior. But with a weakly informed prior that spreads credibility over a wide range of parameter values, it takes relatively little data to shift the peak of the posterior distribution toward the data (although the posterior will be relatively wide and uncertain). (p. 114)

## Why Bayesian inference can be difficult

> Determining the posterior distribution directly from Bayes' rule involves computing the evidence (a.k.a. marginal likelihood) in Equations 5.8 and 5.9. In the usual case of continuous parameters, the integral in Equation 5.9 can be impossible to solve analytically. Historically, the difficulty of the integration was addressed by restricting models to relatively simple likelihood functions with corresponding formulas for prior distributions, called *conjugate* priors, that "played nice" with the likelihood function to produce a tractable integral. (p. 115, *emphasis* in the original)

However, the simple model + conjugate prior approach has its limitations. As we'll see, we often want to fit complex models without shackling ourselves with conjugate priors—which can be quite a pain to work with. Happily,

> another kind of approximation involves randomly sampling a large number of representative combinations of parameter values from the posterior distribution. In recent decades, many such algorithms have been developed, generally referred to as Markov chain Monte Carlo (MCMC) methods. What makes these methods so useful is that they can generate representative parameter-value combinations from the posterior distribution of complex models *without* computing the integral in Bayes' rule. It is the development of these MCMC methods that has allowed Bayesian statistical methods to gain practical use. (pp. 115--116, *emphasis* in the original)

## Session info {-}

```{r}
sessionInfo()
```

```{r, eval = F, echo = F}
# remove our objects
rm(bernoulli, theta_sequence, bernoulli_likelihood, small_data, large_data)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:05.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

```{r, echo = F, eval = F}
# (PART) ALL THE FUNDAMENTALS APPLIED TO INFERRING A BINOMIAL PROBABILITY {-}

# > In the next few chapters, we will develop all the foundational concepts and methods of Bayesian data analysis, which are applied to the simplest type of data. Because of the simplicity of the data, we can focus on the Bayesian methods and scaffold the concepts clearly and efficiently. The subsequent part of the book applies the methods developed in this part to more complex data structures. [@kruschkeDoingBayesianData2015, p. 121]

```

# Inferring a Binomial Probability via Exact Mathematical Analysis

> This chapter presents an example of how to do Bayesian inference using pure analytical mathematics without any approximations. Ultimately, we will not use the pure analytical approach for complex applications, but this chapter is important for two reasons. *First*, the relatively simple mathematics in this chapter nicely reveal the underlying concepts of Bayesian inference on a continuous parameter. The simple formulas show how the continuous allocation of credibility changes systematically as data accumulate. The examples provide an important conceptual foundation for subsequent approximation methods, because the examples give you a clear sense of what is being approximated. *Second*, the distributions introduced in this chapter, especially the beta distribution, will be used repeatedly in subsequent chapters. [@kruschkeDoingBayesianData2015, 123, *emphasis* added]

## The likelihood function: The Bernoulli distribution

If we denote a set of possible outcomes as $\{y_i\}$  Kruschke's Bernoulli likelihood function for a set of $N$ trials follows the form

$$p(\{y_i\} | \theta) = \theta^z \cdot (1 - \theta) ^ {N - z},$$

where $z$ is the number of 1's in the data (i.e., heads in a series of coin flips) and the sole parameter a given observation will be a 1 is $\theta$ (i.e., the probability; $p(y_i = 1 | \theta)$).

If you follow that equation closely, here is how we might express it in **R**.

```{r}
bernoulli_likelihood <- function(theta, data) {
  
  # `theta` = success probability parameter ranging from 0 to 1
  # `data`  = the vector of data (i.e., a series of 0s and 1s)
  n <- length(data)
  z <- sum(data)
  
  return(theta^z * (1 - theta)^(n - sum(data)))
  
}
```

This will come in handy in just a bit.

## A description of credibilities: The beta distribution

> In this chapter, we use purely mathematical analysis, with no numerical approximation, to derive the mathematical form of the posterior credibilities of parameter values. To do this, we need a mathematical description of the prior allocation of credibilities...
>
> In principle, we could use any probability density function supported on the interval [0, 1]. When we intend to apply Bayes' rule (Equation 5.7, p. 106), however, there are two desiderata for mathematical tractability. First, it would be convenient if the product of $p(y | \theta)$ and $p(\theta)$, which is in the numerator of Bayes' rule, results in a function of the same form as $p(\theta)$... Second, we desire the denominator of Bayes' rule (Equation 5.9, p. 107), namely $\int \text d \; \theta p(y | \theta) p(\theta)$, to be solvable analytically. This quality also depends on how the form of the function $p(\theta)$ relates to the form of the function $p(y | \theta)$. When the forms of $p(y | \theta)$ and $p(\theta)$ combine so that the posterior distribution has the same form as the prior distribution, then $p(\theta)$ is called a *conjugate prior* for $p(y | \theta)$. (p. 127 *emphasis* in the original)

When we want a conjugate prior for $\theta$ of the Bernoulli likelihood, the *beta distribution* is a handy choice. Beta has two parameters, $a$ and $b$ (also sometimes called $\alpha$ and $\beta$), and the density is defined as

\begin{align*}
p(\theta | a, b) & = \operatorname{beta} (\theta | a, b) \\
                 & = \frac{\theta^{(a - 1)} \; (1 - \theta)^{(b - 1)}}{B(a, b)},
\end{align*}

where $B(a, b)$ is a normalizing constant, keeping the results in a probability metric, and $B(\cdot)$ is the Beta function. Kruschke then clarified that the beta distribution and the Beta function are not the same. In **R**, we use the beta density with the `dbeta()` function, whereas we use the Beta function with `beta()`. In this project, we'll primarily use `dbeta()`. But to give a sense, notice that when given the same input for $a$ and $b$, the two functions return very different values.

```{r}
theta <- .5
a     <- 3
b     <- 3

dbeta(theta, a, b)
beta(a, b)
```

The $a$ and $b$ parameters are also called *shape* parameters. And indeed, if we look at the parameters of the `dbeta()` function in **R**, we'll see that $a$ is called `shape1` and $b$ is called `shape2`.

```{r}
print(dbeta)
```

You can learn more about the `dbeta()` function [here](https://www.rdocumentation.org/packages/stats/versions/3.5.2/topics/Beta).

Before we make Figure 6.1, we'll need some data.

```{r, fig.width = 6, fig.height = 6, warning = F, message = F}
library(tidyverse)

length <- 1e4

d <-
  crossing(shape1 = c(.1, 1:4),
           shape2 = c(.1, 1:4)) %>%
  expand(nesting(shape1, shape2),
         x = seq(from = 0, to = 1, length.out = length)) %>% 
  mutate(a     = str_c("a = ", shape1),
         b     = str_c("b = ", shape2),
         group = rep(1:length, each = 25))

head(d)
```

Now we're ready for our Figure 6.1.

```{r, fig.width = 6, fig.height = 6, warning = F, message = F}
d %>% 
  ggplot(aes(x = x, group = group)) +
  
  geom_line(aes(y = dbeta(x, shape1 = shape1, shape2 = shape2)),
            color = "grey50", size = 1.25) +
  scale_x_continuous(expression(theta), breaks = c(0, .5, 1)) +
  coord_cartesian(ylim = c(0, 3)) +
  labs(title = "Examples of the beta distribution",
       y = expression(p(theta*"|"*a*", "*b))) +
  theme(panel.grid = element_blank()) +
  facet_grid(b ~ a)
```

> Notice that as $a$ gets bigger (left to right across columns of Figure 6.1), the bulk of the distribution moves rightward over higher values of $\theta$, but as $b$ gets bigger (top to bottom across rows of Figure 6.1), the bulk of the distribution moves leftward over lower values of $\theta$. Notice that as $a$ and $b$ get bigger together, the beta distribution gets narrower. (p. 127).

We have a lot of practice with the beta distribution waiting for us in the chapters to come. If you like informal tutorials, you might also check out [Karin Knudson](https://twitter.com/karinknudson)'s nice blog post, [*Beta distributions, Dirichlet distributions and Dirichlet processes*](https://karinknudson.com/dirichletprocesses.html).

### Specifying a beta prior.

> It is useful to know the central tendency and spread of the beta distribution expressed in terms of $a$ and $b$. It turns out that the mean of the $\operatorname{beta}(\theta | a, b)$ distribution is $\mu = a / (a + b)$ and the mode is $\omega = (a − 1) / (a + b − 2)$ for $a > 1$ and $b > 1$ ($\mu$ is Greek letter mu and $\omega$ is Greek letter omega)... The spread of the beta distribution is related to the "concentration" $\kappa = a + b$ ($\kappa$ is Greek letter kappa). You can see from Figure 6.1 that as $\kappa = a + b$ gets larger, the beta distribution gets narrower or more concentrated. (p. 129)

As such, if you'd like to specify a beta distribution in terms of $\omega$ and $\kappa$, it'd follow the form

$$\operatorname{beta} \big (\alpha = \omega (\kappa - 2) + 1, \beta = (1 - \omega) \cdot (\kappa - 2) + 1 \big ),$$

as long as $\kappa > 2$. Kruschke further clarified:

> The value we choose for the prior $\kappa$ can be thought of this way: It is the number of new flips of the coin that we would need to make us teeter between the new data and the prior belief about $\mu$. If we would only need a few new flips to sway our beliefs, then our prior beliefs should be represented by a small $\kappa$. If we would need a large number of new flips to sway us away from our prior beliefs about $\mu$, then our prior beliefs are worth a very large $\kappa$. (p. 129)

He went on to clarify why we might prefer the mode to the mean when discussing the central tendency of a beta distribution.

> The mode can be more intuitive than the mean, especially for skewed distributions, because the mode is where the distribution reaches its tallest height, which is easy to visualize. The mean in a skewed distribution is somewhere away from the mode, in the direction of the longer tail. (pp. 129--130)

Figure 6.2 helped contrast the mean and mode for beta. We'll use the same process for Figure 6.2 and create the data, first.

```{r}
d <-
  tibble(shape1 = c(5.6, 17.6, 5, 17),
         shape2 = c(1.4, 4.4, 2, 5)) %>% 
  mutate(a        = str_c("a = ", shape1),
         b        = str_c("b = ", shape2),
         kappa    = rep(c("kappa==7", "kappa==22"), times = 2),
         mu_omega = rep(c("mu==0.8", "omega==0.8"), each = 2)) %>% 
  mutate(kappa = factor(kappa, levels = c("kappa==7", "kappa==22")),
         label = str_c(a, ", ", b)) %>% 
  expand(nesting(shape1, shape2, a, b, label, kappa, mu_omega), 
         x = seq(from = 0, to = 1, length.out = length))

head(d)
```

Here's Figure 6.2.

```{r, fig.width = 6, fig.height = 4}
d %>%
  ggplot(aes(x = x)) +
  geom_vline(xintercept = .8, color = "white") +
  geom_line(aes(y = dbeta(x, shape1 = shape1, shape2 = shape2)),
            color = "grey50", size = 1.25) +
  geom_text(data = . %>% group_by(label) %>% slice(1),
            aes(x = .025, y = 4.75, label = label),
            hjust = 0, size = 3) +
  scale_x_continuous(expression(theta), breaks = c(0, .8, 1)) +
  ylab(expression(p(theta*"|"*a*", "*b))) +
  coord_cartesian(ylim = c(0, 5)) +
  theme(panel.grid = element_blank()) +
  facet_grid(mu_omega ~ kappa, labeller = label_parsed)
```

In lines 264 to 290 in his `DBDA2E-utilities.R` file, Kruschke provided a series of `betaABfrom...()` functions that will allow us to compute the $a$ and $b$ parameters from measures of central tendency (i.e., mean and mode) and of spread (i.e., $\kappa$ and $\sigma$). Here are those bits of his code.

```{r}
# Shape parameters from central tendency and scale:
betaABfromMeanKappa <- function(mean, kappa) {
  if (mean <= 0 | mean >= 1) stop("must have 0 < mean < 1")
  if (kappa <= 0) stop("kappa must be > 0")
  a <- mean * kappa
  b <- (1.0 - mean) * kappa
  return(list(a = a, b = b))
}

betaABfromModeKappa <- function(mode, kappa) {
  if (mode <= 0 | mode >= 1) stop("must have 0 < mode < 1")
  if (kappa <= 2) stop("kappa must be > 2 for mode parameterization")
  a <- mode * (kappa - 2) + 1
  b <- (1.0 - mode) * (kappa - 2) + 1
  return(list(a = a, b = b))
}

betaABfromMeanSD <- function(mean, sd) {
  if (mean <= 0 | mean >= 1) stop("must have 0 < mean < 1")
  if (sd <= 0) stop("sd must be > 0")
  kappa <- mean * (1 - mean)/sd^2 - 1
  if (kappa <= 0) stop("invalid combination of mean and sd")
  a <- mean * kappa
  b <- (1.0 - mean) * kappa
  return(list(a = a, b = b))
}
```

You can use them like so.

```{r}
betaABfromMeanKappa(mean = .25, kappa = 4)
betaABfromModeKappa(mode = .25, kappa = 4)
betaABfromMeanSD(mean = .5, sd = .1)
```

You can also save the results as an object, which can then be indexed by parameter.

```{r}
beta_param <- betaABfromModeKappa(mode = .25, kappa = 4)

beta_param$a
beta_param$b
```

We'll find this trick quite handy in the sections to come.

## The posterior beta

I'm not going to reproduce all of Formula 6.8. But this a fine opportunity to re-express Bayes' rule in terms of $z$ and $N$,

$$p(\theta | z, N) = \frac{p(z, N | \theta) \; p(\theta)}{p(z, N)}.$$

### Posterior is compromise of prior and likelihood.

You might wonder how Kruschke computed the HDI values for Figure 6.3. Remember our `hdi_of_icdf()` function from back in Chapter 4? Yep, that's how. Here's that code, again.

```{r}
hdi_of_icdf <- function(name, width = .95, tol = 1e-8, ... ) {
  
  # Arguments:
  #   `name` is R's name for the inverse cumulative density function
  #   of the distribution.
  #   `width` is the desired mass of the HDI region.
  #   `tol` is passed to R's optimize function.
  # Return value:
  #   Highest density iterval (HDI) limits in a vector.
  # Example of use: For determining HDI of a beta(30, 12) distribution, type
  #   `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)`
  #   Notice that the parameters of the `name` must be explicitly stated;
  #   e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work.
  # Adapted and corrected from Greg Snow's TeachingDemos package.
  
  incredible_mass <- 1.0 - width
  interval_width <- function(low_tail_prob, name, width, ...) {
    name(width + low_tail_prob, ...) - name(low_tail_prob, ...)
  }
  
  opt_info <- optimize(interval_width, c(0, incredible_mass), 
                       name = name, width = width, 
                       tol = tol, ...)
  
  hdi_lower_tail_prob <- opt_info$minimum
  
  return(c(name(hdi_lower_tail_prob, ...),
           name(width + hdi_lower_tail_prob, ...)))
  
}
```

Recall it's based off of the `HDIofICDF()` function from Kruschke's `DBDA2E-utilities.R` file. I've altered Kruschke's formatting a little bit, but the guts of the code are unchanged. Our `hdi_of_icdf()` function will take the `name` of an "inverse cumulative density function" and its parameters and then return an HDI range, as defined by the `width` parameter. Since the prior at the top of Figure 6.3 is $\operatorname{beta}(5, 5)$, we can use `hdi_of_icdf()` to calculate the HDI like so.

```{r}
hdi_of_icdf(name = qbeta,
            shape1 = 5,
            shape2 = 5,
            width  = .95)
```

Here they are for the posterior distribution at the bottom of the figure.

```{r}
hdi_of_icdf(name = qbeta,
            shape1 = 6,
            shape2 = 14)
```

Note that since we set `width  = .95` as the default, we can leave it out if we want to stick with the conventional 95% intervals.

Here are the mean calculations from the last paragraph on page 134.

```{r}
n <- 10
z <- 1
a <- 5
b <- 5

(proportion_heads <- z / n)
(prior_mean <- a / (a + b))
(posterior_mean <- (z + a) / (n + a + b))
```

In order to make the plots for Figure 6.3, we'll want to compute the prior, likelihood, and posterior density values across a densely-packed range of $\theta$ values.

```{r}
trial_data <- c(rep(0, 9), 1)

d <-
  tibble(theta = seq(from = 0, to = 1, length.out = 100)) %>% 
  mutate(`Prior (beta)`           = dbeta(theta, 
                                          shape1 = a, 
                                          shape2 = b),
         `Likelihood (Bernoulli)` = bernoulli_likelihood(theta = theta, 
                                                         data  = trial_data),
         `Posterior (beta)`       = dbeta(theta, 
                                          shape1 = 6, 
                                          shape2 = 14))

glimpse(d)
```

To make things easier on ourselves, we'll also make two additional data objects to annotate the plots with lines and text.

```{r}
# save the levels
levels <- c("Prior (beta)", "Likelihood (Bernoulli)", "Posterior (beta)")

# the data for the in-plot lines
line <-
  tibble(theta      = c(.212 + .008, .788 - .008, .114 + .004, .497 - .005),
         value      = rep(c(.51, .66), each = 2),
         xintercept = c(.212, .788, .114, .497),
         name       = rep(c("Prior (beta)", "Posterior (beta)"), each = 2)) %>% 
  mutate(name = factor(name, levels = levels))
  
# the data for the annotation
text <-
  tibble(theta = c(.5, .3),
         value = c(.8, 1.125),
         label = "95% HDI",
         name  = c("Prior (beta)", "Posterior (beta)")) %>% 
  mutate(name = factor(name, levels = levels))
```

Finally, here's our Figure 6.3.

```{r, warning = F, message = F, fig.width = 6, fig.height = 5}
library(cowplot)

d %>% 
  pivot_longer(-theta) %>% 
  mutate(name = factor(name, levels = levels)) %>% 
  
  ggplot(aes(x = theta, y = value, )) +
  # densities
  geom_area(fill = "steelblue") +
  # dashed vertical lines
  geom_vline(data = line,
             aes(xintercept = xintercept), 
             linetype = 2, color = "white") +
  # arrows
  geom_line(data = line,
            arrow = arrow(length = unit(.15,"cm"), 
                          ends = "both", 
                          type = "closed"),
            color = "white") +
  # text
  geom_text(data = text,
            aes(label = label),
            color = "white") +
  labs(x = expression(theta),
       y = NULL) +
  facet_wrap(~ name, scales = "free_y", ncol = 1) +
  theme_cowplot()
```

Note how we loaded the [**cowplot** package](https://wilkelab.org/cowplot) [@R-cowplot]. We played around a bit with plotting conventions in the previous chapters. From this chapter onward we'll explore plotting conventions in a more deliberate fashion. One quick way to alter the look and feel of a plot is by altering its theme. The **cowplot** package includes several theme options. In this chapter, we'll focus on making simple and conventional looking plots with the `theme_cowplot()` function.

## Examples

### Prior knowledge expressed as a beta distribution.

If you flip an unaltered freshly-minted coin 20 times and end up with 17 heads, 85% of those trials are heads.

```{r}
 100 * (17 / 20)
```

In the first paragraph of this section, Kruschke suggested we consider a beta prior with a mode of $\omega = .5$ and an effective sample size $\kappa = 500$. We might use Kruschke's `betaABfromModeKappa()` function to use those values to compute the corresponding $a$ and $b$ parameters for the beta distribution.

```{r}
betaABfromModeKappa(mode = .5, kappa = 500)
```

Confusingly, Kruschke switched from $\operatorname{beta(250, 250)}$ in the prose to $\operatorname{beta(100, 100)}$ in Figure 6.4.a, which he acknowledged in his [Corrigenda](https://sites.google.com/site/doingbayesiandataanalysis/corrigenda). We'll stick with $\operatorname{beta(100, 100)}$, which corresponds to $\omega = .5$ and $\kappa = 200$.

```{r}
betaABfromModeKappa(mode = .5, kappa = 200)
```

Here's how to use those values and some of the equations from above to make the data necessary for the left column of Figure 6.4.

```{r}
# define the prior
beta_param <- betaABfromModeKappa(mode = .5, kappa = 200)

# compute the corresponding HDIs
prior_hdi <-
  hdi_of_icdf(name = qbeta,
              shape1 = beta_param$a,
              shape2 = beta_param$b,
              width = .95)

# define the data
n <- 20
z <- 17

trial_data <- c(rep(0, times = n - z), rep(1, times = z))

# compute the HDIs for the posterior
post_hdi <-
  hdi_of_icdf(name = qbeta,
              shape1 = z + beta_param$a,
              shape2 = n - z + beta_param$b,
              width = .95)

# use the above to compute the prior, the likelihood, and the posterior
# densities using the grid approximation approach
d <-
  tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %>% 
  mutate(prior      = dbeta(theta, 
                            shape1 = beta_param$a, 
                            shape2 = beta_param$b),
         likelihood = bernoulli_likelihood(theta = theta, 
                                           data  = trial_data),
         posterior  = dbeta(theta, 
                            shape1 = z + beta_param$a, 
                            shape2 = n - z + beta_param$b))

# what have we done?
glimpse(d)
```

We're finally ready to plot the prior, the likelihood, and the posterior for the left column of Figure 6.4.

```{r, fig.width = 5, fig.height = 2.25}
## Figure 6.4, left column
# prior
d %>% 
  ggplot(aes(x = theta, y = prior)) +
  geom_area(fill = "steelblue", alpha = 1/2) +
  geom_area(data = . %>% filter(theta > prior_hdi[1] & theta < prior_hdi[2]),
            fill = "steelblue") +
  geom_segment(x = prior_hdi[1] + .005, xend = prior_hdi[2] - .005,
               y = 1.8, yend = 1.8,
               arrow = arrow(length = unit(.15,"cm"), 
                             ends = "both", 
                             type = "closed"),
               color = "white") +
  annotate(geom = "text", x = .5, y = 3.5, 
           label = "95% HDI") +
  labs(title = "Prior (beta)",
       x = expression(theta),
       y = expression(dbeta(theta*"|"*100*", "*100))) +
  coord_cartesian(ylim = c(0, 12)) +
  theme_cowplot()

# likelihood
d %>%   
  ggplot(aes(x = theta, y = likelihood)) +
  geom_area(fill = "steelblue") +
  labs(title = "Likelihood (Bernoulli)",
       x = expression(theta),
       y = expression(p(D*"|"*theta))) +
  theme_cowplot()

# posterior
d %>% 
  ggplot(aes(x = theta, y = posterior)) +
  geom_area(fill = "steelblue", alpha = 1/2) +
  geom_area(data = . %>% filter(theta > post_hdi[1] & theta < post_hdi[2]),
            fill = "steelblue") +
  geom_segment(x = post_hdi[1] + .005, xend = post_hdi[2] - .005,
               y = 2, yend = 2,
               arrow = arrow(length = unit(.15, "cm"), 
                             ends = "both", 
                             type = "closed"),
               color = "white") +
  annotate(geom = "text", x = .532, y = 3.5, 
           label = "95% HDI") +
  labs(title = "Posterior (beta)",
       x = expression(theta),
       y = expression(dbeta(theta*"|"*117*", "*103))) +
  coord_cartesian(ylim = c(0, 12)) +
  theme_cowplot()
```

Here are the exact HDI values for the prior and posterior densities.

```{r}
prior_hdi
post_hdi
```

If you double back to page 129 in the text, you'll see Kruschke defined the mode of a beta density as

$$\omega_\text{beta} = (a - 1) / (a + b - 2)$$

whenever $a > 1$ and $b > 1$. Thus we can compute the modes for our prior and posterior densities like this.

```{r}
(beta_param$a - 1) / (beta_param$a + beta_param$b - 2)
(z + beta_param$a - 1) / (z + beta_param$a + n - z + beta_param$b - 2)
```

For the next example, we consider the probability a professional basketball player will make free a throw. We have the same likelihood based on 17 successes our of 20 trials, but this time our prior is based on $\omega = .75$ and $\kappa = 25$. Here we update those values and our `d` data for the plot.

```{r}
# update the beta parameters for the prior
beta_param$a <- 18.25
beta_param$b <- 6.75

# update the HDIs
prior_hdi <-
  hdi_of_icdf(name = qbeta,
              shape1 = beta_param$a,
              shape2 = beta_param$b,
              width = .95)

post_hdi <-
  hdi_of_icdf(name = qbeta,
              shape1 = z + beta_param$a,
              shape2 = n - z + beta_param$b,
              width = .95)

# update the data
d <-
  d %>% 
  mutate(prior     = dbeta(theta, 
                           shape1 = beta_param$a, 
                           shape2 = beta_param$b),
         posterior = dbeta(theta, 
                           shape1 = z + beta_param$a, 
                           shape2 = n - z + beta_param$b))
```

With our updated values in hand, we're ready to make our versions of the middle column of Figure 6.4.

```{r, fig.width = 5, fig.height = 2.25}
## plot Figure 6.4, middle column!
# prior
d %>% 
  ggplot(aes(x = theta, y = prior)) +
  geom_area(fill = "steelblue", alpha = 1/2) +
  geom_area(data = . %>% filter(theta > prior_hdi[1] & theta < prior_hdi[2]),
            fill = "steelblue") +
  geom_segment(x = prior_hdi[1] + .005, xend = prior_hdi[2] - .005,
               y = 0.75, yend = 0.75,
               arrow = arrow(length = unit(.15,"cm"), 
                             ends = "both", 
                             type = "closed"),
               color = "white") +
  annotate(geom = "text", x = .75, y = 1.5, 
           label = "95% HDI", color = "white") +
  labs(title = "Prior (beta)",
       x = expression(theta),
       y = expression(dbeta(theta*"|"*18.25*", "*6.75))) +
  coord_cartesian(ylim = c(0, 7)) +
  theme_cowplot()

# likelihood, which is the same as the last time
d %>%   
  ggplot(aes(x = theta, y = likelihood)) +
  geom_area(fill = "steelblue") +
  labs(title = "Likelihood (Bernoulli)",
       x = expression(theta),
       y = expression(p(D*"|"*theta))) +
  theme_cowplot()

# posterior
d %>% 
  ggplot(aes(x = theta, y = posterior)) +
  geom_area(fill = "steelblue", alpha = 1/2) +
  geom_area(data = . %>% filter(theta > post_hdi[1] & theta < post_hdi[2]),
            fill = "steelblue") +
  geom_segment(x = post_hdi[1] + .005, xend = post_hdi[2] - .005,
               y = 1, yend = 1,
               arrow = arrow(length = unit(.15, "cm"), 
                             ends = "both", 
                             type = "closed"),
               color = "white") +
  annotate(geom = "text", x = .797, y = 2, 
           label = "95% HDI", color = "white") +
  labs(title = "Posterior (beta)",
       x = expression(theta),
       y = expression(dbeta(theta*"|"*35.25*", "*9.75))) +
  coord_cartesian(ylim = c(0, 7)) +
  theme_cowplot()
```

Here are the exact HDI values for the prior and posterior densities.

```{r}
prior_hdi
post_hdi
```

Here are the the modes for our prior and posterior densities.

```{r}
(beta_param$a - 1) / (beta_param$a + beta_param$b - 2)
(z + beta_param$a - 1) / (z + beta_param$a + n - z + beta_param$b - 2)
```

For our final example, we consider the tendency of a newly discovered substance on a distant planet to be blue versus green. Just as in the previous two examples, we discover 17 out of 20 trials come up positive (i.e., blue). This time we have a noncommittal uniform prior, $\operatorname{beta}(1, 1)$. Here's how to plot the results, as shown in the right column of Figure 6.4.

```{r, fig.width = 5, fig.height = 2.25}
# update beta_param
beta_param$a <- 1
beta_param$b <- 1

# update the HDIs
prior_hdi <-
  hdi_of_icdf(name = qbeta,
              shape1 = beta_param$a,
              shape2 = beta_param$b,
              width = .95)

post_hdi <-
  hdi_of_icdf(name = qbeta,
              shape1 = z + beta_param$a,
              shape2 = n - z + beta_param$b,
              width = .95)

# update the data
d <-
  d %>% 
  mutate(prior     = dbeta(theta, 
                           shape1 = beta_param$a, 
                           shape2 = beta_param$b),
         posterior = dbeta(theta, 
                           shape1 = z + beta_param$a, 
                           shape2 = n - z + beta_param$b))


## plot Figure 6.4, rightmost column!
# prior
d %>% 
  ggplot(aes(x = theta, y = prior)) +
  geom_area(fill = "steelblue") +
  labs(title = "Prior (beta)",
       x = expression(theta),
       y = expression(dbeta(theta*"|"*1*", "*1))) +
  coord_cartesian(ylim = c(0, 5)) +
  theme_cowplot()

# likelihood, which is the same as the last two examples
d %>%   
  ggplot(aes(x = theta, y = likelihood)) +
  geom_area(fill = "steelblue") +
  labs(title = "Likelihood (Bernoulli)",
       x = expression(theta),
       y = expression(p(D*"|"*theta))) +
  theme_cowplot()

# posterior
d %>% 
  ggplot(aes(x = theta, y = posterior)) +
  geom_area(fill = "steelblue", alpha = 1/2) +
  geom_area(data = . %>% filter(theta > post_hdi[1] & theta < post_hdi[2]),
            fill = "steelblue") +
  geom_segment(x = post_hdi[1] + .005, xend = post_hdi[2] - .005,
               y = 0.8, yend = 0.8,
               arrow = arrow(length = unit(.15, "cm"), 
                             ends = "both", 
                             type = "closed"),
               color = "white") +
  annotate(geom = "text", x = (post_hdi[1] + post_hdi[2]) / 2, y = 1.5, 
           label = "95% HDI", color = "white") +
  labs(title = "Posterior (beta)",
       x = expression(theta),
       y = expression(dbeta(theta*"|"*18*", "*4))) +
  coord_cartesian(ylim = c(0, 5)) +
  theme_cowplot()
```

Here are the exact HDI values for the posterior density.

```{r}
post_hdi
```

Because both the $a$ and $b$ parameters for our beta prior are 1, we can't use the formula from above to compute the mode. I hope this makes sense if you look back at the plot. The density for $\operatorname{beta}(1, 1)$ is uniform and has no mode. We can, at least, compute the mode for the posterior.

```{r}
(z + beta_param$a - 1) / (z + beta_param$a + n - z + beta_param$b - 2)
```

### Prior knowledge that cannot be expressed as a beta distribution.

> The beauty of using a beta distribution to express prior knowledge is that the posterior distribution is again exactly a beta distribution, and therefore, no matter how much data we include, we always have an exact representation of the posterior distribution and a simple way of computing it. But not all prior knowledge can be expressed by a beta distribution, because the beta distribution can only be in the forms illustrated by Figure 6.1. If the prior knowledge cannot be expressed as a beta distribution, then we must use a different method to derive the posterior. In particular, we might revert to grid approximation as was explained in Section 5.5 (p. 116).

For such a small section in the text, the underlying code is a bit of a beast. Fir kicks, we'll practice two ways. First we'll follow the code Kruschke used in the text. Our second attempt will be in a more **tidyverse** sort of way.

#### Figure 6.5 in Kruschke style.

```{r, fig.width = 4, fig.height = 6}
# Fine teeth for Theta
theta <- seq(0, 1, length = 1000)

# Two triangular peaks on a small non-zero floor
p_theta <-
  c(rep(1, 200), 
    seq(1, 100, length = 50), 
    seq(100, 1, length = 50), 
    rep(1, 200)) %>% 
  rep(., times = 2)

# Make p_theta sum to 1.0
p_theta <- p_theta / sum(p_theta)
```

Here's Kruschke's `BernGrid()` code in all its glory.

```{r}
BernGrid = function( Theta , pTheta , Data , plotType=c("Points","Bars")[2] ,
                     showCentTend=c("Mean","Mode","None")[3] ,
                     showHDI=c(TRUE,FALSE)[2] , HDImass=0.95 ,
                     showpD=c(TRUE,FALSE)[2] , nToPlot=length(Theta) ) {
  # Theta is vector of values between 0 and 1.
  # pTheta is prior probability mass at each value of Theta
  # Data is vector of 0's and 1's.
  
  # Check for input errors:
  if ( any( Theta > 1 | Theta < 0 ) ) {
    stop("Theta values must be between 0 and 1")
  }
  if ( any( pTheta < 0 ) ) {
    stop("pTheta values must be non-negative")
  }
  if ( !isTRUE(all.equal( sum(pTheta) , 1.0 )) )  {
    stop("pTheta values must sum to 1.0")
  }
  if ( !all( Data == 1 | Data == 0 ) ) {
    stop("Data values must be 0 or 1")
  }
  
  # Create summary values of Data
  z = sum( Data ) # number of 1's in Data
  N = length( Data ) 

  # Compute the Bernoulli likelihood at each value of Theta:
  pDataGivenTheta = Theta^z * (1-Theta)^(N-z)
  # Compute the evidence and the posterior via Bayes' rule:
  pData = sum( pDataGivenTheta * pTheta )
  pThetaGivenData = pDataGivenTheta * pTheta / pData
  
  # Plot the results.
  layout( matrix( c( 1,2,3 ) ,nrow=3 ,ncol=1 ,byrow=FALSE ) ) # 3x1 panels
  par( mar=c(3,3,1,0) , mgp=c(2,0.7,0) , mai=c(0.5,0.5,0.3,0.1) ) # margins
  cexAxis = 1.33
  cexLab = 1.75
  # convert plotType to notation used by plot:
  if ( plotType=="Points" ) { plotType="p" }
  if ( plotType=="Bars" ) { plotType="h" }
  dotsize = 5 # how big to make the plotted dots
  barsize = 5 # how wide to make the bar lines    
  # If the comb has a zillion teeth, it's too many to plot, so plot only a
  # thinned out subset of the teeth.
  nteeth = length(Theta)
  if ( nteeth > nToPlot ) {
    thinIdx = round( seq( 1, nteeth , length=nteeth ) )
  } else { 
    thinIdx = 1:nteeth 
  }

  # Plot the prior.
  yLim = c(0,1.1*max(c(pTheta,pThetaGivenData)))
  plot( Theta[thinIdx] , pTheta[thinIdx] , type=plotType , 
        pch="." , cex=dotsize , lwd=barsize ,
        xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis ,
        xlab=bquote(theta) , ylab=bquote(p(theta)) , cex.lab=cexLab ,
        main="Prior" , cex.main=1.5 , col="skyblue" )
  if ( showCentTend != "None" ) {
    if ( showCentTend == "Mean" ) {
      meanTheta = sum( Theta * pTheta ) 
      if ( meanTheta > .5 ) {
         textx = 0 ; textadj = c(0,1)
      } else {
        textx = 1 ; textadj = c(1,1)
      }
      text( textx , yLim[2] ,
            bquote( "mean=" * .(signif(meanTheta,3)) ) ,
            cex=2.0 , adj=textadj )
    }
    if ( showCentTend == "Mode" ) {
      modeTheta = Theta[ which.max( pTheta ) ]
      if ( modeTheta > .5 ) {
        textx = 0 ; textadj = c(0,1)
      } else {
        textx = 1 ; textadj = c(1,1)
      }
      text( textx , yLim[2] ,
            bquote( "mode=" * .(signif(modeTheta,3)) ) ,
            cex=2.0 , adj=textadj )
    }
  }
  
  # Mark the highest density interval. HDI points are not thinned in the plot.
  if ( showHDI ) {
    HDIinfo = HDIofGrid( pTheta , credMass=HDImass )
    points( Theta[ HDIinfo$indices ] , 
            rep( HDIinfo$height , length( HDIinfo$indices ) ) , 
            pch="-" , cex=1.0 )
    text( mean( Theta[ HDIinfo$indices ] ) , HDIinfo$height ,
          bquote( .(100*signif(HDIinfo$mass,3)) * "% HDI" ) ,
          adj=c(0.5,-1.5) , cex=1.5 )
    # Mark the left and right ends of the waterline. 
    # Find indices at ends of sub-intervals:
    inLim = HDIinfo$indices[1] # first point
    for ( idx in 2:(length(HDIinfo$indices)-1) ) {
      if ( ( HDIinfo$indices[idx] != HDIinfo$indices[idx-1]+1 ) | # jumps on left, OR
             ( HDIinfo$indices[idx] != HDIinfo$indices[idx+1]-1 ) ) { # jumps on right
        inLim = c(inLim,HDIinfo$indices[idx]) # include idx
      }
    }
    inLim = c(inLim,HDIinfo$indices[length(HDIinfo$indices)]) # last point
    # Mark vertical lines at ends of sub-intervals:
    for ( idx in inLim ) {
      lines( c(Theta[idx],Theta[idx]) , c(-0.5,HDIinfo$height) , type="l" , lty=2 , 
             lwd=1.5 )
      text( Theta[idx] , HDIinfo$height , bquote(.(round(Theta[idx],3))) ,
            adj=c(0.5,-0.1) , cex=1.2 )
    }
  }
  
  # Plot the likelihood: p(Data|Theta)
  plot( Theta[thinIdx] , pDataGivenTheta[thinIdx] , type=plotType , 
        pch="." , cex=dotsize , lwd=barsize ,
        xlim=c(0,1) , ylim=c(0,1.1*max(pDataGivenTheta)) , cex.axis=cexAxis ,
        xlab=bquote(theta) , ylab=bquote( "p(D|" * theta * ")" ) , cex.lab=cexLab ,
        main="Likelihood" , cex.main=1.5 , col="skyblue" )
  if ( z > .5*N ) { textx = 0 ; textadj = c(0,1) }
  else { textx = 1 ; textadj = c(1,1) }
  text( textx ,1.0*max(pDataGivenTheta) ,cex=2.0
  	,bquote( "Data: z=" * .(z) * ",N=" * .(N) ) ,adj=textadj )
  if ( showCentTend != "None" ) {
    if ( showCentTend == "Mean" ) {
      meanTheta = sum( Theta * pDataGivenTheta ) 
      if ( meanTheta > .5 ) {
        textx = 0 ; textadj = c(0,1)
      } else {
        textx = 1 ; textadj = c(1,1)
      }
      text( textx , 0.7*max(pDataGivenTheta) ,
            bquote( "mean=" * .(signif(meanTheta,3)) ) ,
            cex=2.0 , adj=textadj )
    }
    if ( showCentTend == "Mode" ) {
      modeTheta = Theta[ which.max( pDataGivenTheta ) ]
      if ( modeTheta > .5 ) {
        textx = 0 ; textadj = c(0,1)
      } else {
        textx = 1 ; textadj = c(1,1)
      }
      text( textx , 0.7*max(pDataGivenTheta) ,
            bquote( "mode=" * .(signif(modeTheta,3)) ) ,
            cex=2.0 , adj=textadj )
    }
  }
  
  # Plot the posterior.
  yLim = c(0,1.1*max(c(pTheta,pThetaGivenData)))
  plot( Theta[thinIdx] , pThetaGivenData[thinIdx] , type=plotType , 
        pch="." , cex=dotsize , lwd=barsize ,
        xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis ,
        xlab=bquote(theta) , ylab=bquote( "p(" * theta * "|D)" ) , cex.lab=cexLab ,
        main="Posterior" , cex.main=1.5 , col="skyblue" )
  if ( showCentTend != "None" ) {
    if ( showCentTend == "Mean" ) {
      meanTheta = sum( Theta * pThetaGivenData ) 
      if ( meanTheta > .5 ) {
        textx = 0 ; textadj = c(0,1)
      } else {
        textx = 1 ; textadj = c(1,1)
      }
      text( textx , yLim[2] ,
            bquote( "mean=" * .(signif(meanTheta,3)) ) ,
            cex=2.0 , adj=textadj )
    }
    if ( showCentTend == "Mode" ) {
      modeTheta = Theta[ which.max( pThetaGivenData ) ]
      if ( modeTheta > .5 ) {
        textx = 0 ; textadj = c(0,1)
      } else {
        textx = 1 ; textadj = c(1,1)
      }
      text( textx , yLim[2] ,
            bquote( "mode=" * .(signif(modeTheta,3)) ) ,
            cex=2.0 , adj=textadj )
    }
  }

  
  # Plot marginal likelihood pData:
  if ( showpD ) {
    meanTheta = sum( Theta * pThetaGivenData ) 
    if ( meanTheta > .5 ) {
      textx = 0 ; textadj = c(0,1)
    } else {
      textx = 1 ; textadj = c(1,1)
    }
    text( textx , 0.75*max(pThetaGivenData) , cex=2.0 ,
    	    bquote( "p(D)=" * .(signif(pData,3)) ) ,adj=textadj )
  }
  
  # Mark the highest density interval. HDI points are not thinned in the plot.
  if ( showHDI ) {
    HDIinfo = HDIofGrid( pThetaGivenData , credMass=HDImass )
    points( Theta[ HDIinfo$indices ] , 
            rep( HDIinfo$height , length( HDIinfo$indices ) ) , 
            pch="-" , cex=1.0 )
    text( mean( Theta[ HDIinfo$indices ] ) , HDIinfo$height ,
             bquote( .(100*signif(HDIinfo$mass,3)) * "% HDI" ) ,
             adj=c(0.5,-1.5) , cex=1.5 )
    # Mark the left and right ends of the waterline. 
    # Find indices at ends of sub-intervals:
    inLim = HDIinfo$indices[1] # first point
    for ( idx in 2:(length(HDIinfo$indices)-1) ) {
      if ( ( HDIinfo$indices[idx] != HDIinfo$indices[idx-1]+1 ) | # jumps on left, OR
        ( HDIinfo$indices[idx] != HDIinfo$indices[idx+1]-1 ) ) { # jumps on right
        inLim = c(inLim,HDIinfo$indices[idx]) # include idx
      }
    }
    inLim = c(inLim,HDIinfo$indices[length(HDIinfo$indices)]) # last point
    # Mark vertical lines at ends of sub-intervals:
    for ( idx in inLim ) {
      lines( c(Theta[idx],Theta[idx]) , c(-0.5,HDIinfo$height) , type="l" , lty=2 , 
             lwd=1.5 )
      text( Theta[idx] , HDIinfo$height , bquote(.(round(Theta[idx],3))) ,
            adj=c(0.5,-0.1) , cex=1.2 )
    }
  }
  
  # return( pThetaGivenData )
} # end of function
```

You plot using Kruschke's method, like so.

```{r, fig.width = 5, fig.height = 6, warning = F, message = F}
Data <- c(rep(0, 13), rep(1, 14))

BernGrid(theta, p_theta, Data, plotType = "Bars",
         showCentTend = "None", showHDI = FALSE, showpD = FALSE)
```

The method works fine. But, I'm not a fan. It's clear Kruschke put a lot of thought into the `BernGrid()` function. However, its inner workings are too opaque, for me, which leads to our next section...

#### Figure 6.5 in **tidyverse** style.

Here we'll be plotting with **ggplot2**. But let's first get the data into a tibble.

```{r}
# we need these to compute the likelihood
n <- 27
z <- 14

trial_data <- c(rep(0, times = n - z), rep(1, times = z))        # (i.e., Data)

d <-
  tibble(theta = seq(from = 0, to = 1, length.out = 1000),       # (i.e., Theta)
         Prior = c(rep(1, 200),                                  # (i.e., pTheta)
                   seq(1, 100, length = 50), 
                   seq(100, 1, length = 50), 
                   rep(1, 200)) %>% 
           rep(., times = 2)) %>% 
  mutate(Prior      = Prior / sum(Prior),
         Likelihood = bernoulli_likelihood(theta = theta,        # (i.e., pDataGivenTheta)
                                           data  = trial_data)) %>%
  mutate(evidence = sum(Likelihood * Prior)) %>%                 # (i.e., pData)
  mutate(Posterior = Likelihood * Prior / evidence)              # (i.e., pThetaGivenData)
  
glimpse(d)
```

With our nice tibble in hand, we'll plot the prior, likelihood, and posterior one at a time.

```{r, fig.width = 5, fig.height = 2}
# prior
(p1 <-
  d %>% 
  ggplot(aes(x = theta, y = Prior)) +
  geom_area(fill = "steelblue") +
  labs(title = "Prior",
       x = expression(theta),
       y = expression(p(theta))) +
  theme_cowplot()
 )

# likelihood
(p2 <-
  d %>% 
  ggplot(aes(x = theta, y = Likelihood)) +
  geom_area(fill = "steelblue") +
  labs(title = "Likelihood",
       x = expression(theta),
       y = expression(p(D*"|"*theta))) +
  theme_cowplot()
 )

# posterior
(p3 <-
  d %>% 
  ggplot(aes(x = theta, y = Posterior)) +
  geom_area(fill = "steelblue") +
  labs(title = "Posterior",
       x = expression(theta),
       y = expression(p(theta*"|"*D))) +
  theme_cowplot()
 )
```

Note how we saved each the plots as objects. There are many ways to combine multiple ggplots, such as stacking them one atop another like they're presented in Figure 6.5. One of the earliest methods I learned was the good old [`multiplot()` function](http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/). For a long time I relied on `grid.arrange()` from the [**gridExtra** package](https://CRAN.R-project.org/package=gridExtra) [@R-gridExtra]. But it's hard to beat the elegant syntax from [Thomas Lin Pedersen](https://twitter.com/thomasp85)'s [-@R-patchwork] [**patchwork** package](https://patchwork.data-imaginist.com/).

```{r, fig.width = 5, fig.height = 6, warning = F, message = F}
library(patchwork)

p1 / p2 / p3
```

We could have taken this same approach to combine all our subplots from the three columns and three rows of Figure 6.4. You can learn more about how to use **patchwork** this way [here](https://patchwork.data-imaginist.com/articles/patchwork.html). We'll have many more opportunities to practice as we progress through the chapters.

## Summary

> The main point of this chapter was to demonstrate how Bayesian inference works when Bayes' rule can be solved analytically, using mathematics alone, without numerical approximation...
>
> Unfortunately, there are two severe limitations with this approach... Thus, although it is interesting and educational to see how Bayes' rule can be solved analytically, we will have to abandon exact mathematical solutions when doing complex applications. We will instead use Markov chain Monte Carlo (MCMC) methods. (p. 139)

And if you're using this project, I imagine that's exactly what you're looking for. We want to use the power of a particular kind of MCMC, Hamiltonian Monte Carlo, through the interface of the [**brms** package](https://github.com/paul-buerkner/brms). Get excited. It's coming.

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
rm(bernoulli_likelihood, theta, a, b, length, d, betaABfromMeanKappa, betaABfromModeKappa, betaABfromMeanSD, beta_param, hdi_of_icdf, n, z, proportion_heads, prior_mean, posterior_mean, trial_data, levels, line, text, prior_hdi, post_hdi, p_theta, BernGrid, Data, p1, p2, p3)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:06.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

# Markov Chain Monte Carlo

> This chapter introduces the methods we will use for producing accurate approximations to Bayesian posterior distributions for realistic applications. The class of methods is called Markov chain Monte Carlo (MCMC), for reasons that will be explained later in the chapter. It is MCMC algorithms and software, along with fast computer hardware, that allow us to do Bayesian data analysis for realistic applications that would have been effectively impossible 30 years ago. [@kruschkeDoingBayesianData2015, p. 144]

Statistician David Draper covered some of the history of MCMC in his lecture, [*Bayesian Statistical Reasoning*](https://www.youtube.com/watch?v=072Q18nX91I&frags=pl%2Cwn).

## Approximating a distribution with a large sample

> The concept of representing a distribution by a large representative sample is foundational for the approach we take to Bayesian analysis of complex models. The idea is applied intuitively and routinely in everyday life and in science. For example, polls and surveys are founded on this concept: By randomly sampling a subset of people from a population, we estimate the underlying tendencies in the entire population. The larger the sample, the better the estimation. What is new in the present application is that the population from which we are sampling is a mathematically defined distribution, such as a posterior probability distribution. (p. 145)

Like in Chapters 4 and 6, we need to define our `hdi_of_icdf()` function.

```{r}
hdi_of_icdf <- function(name, width = .95, tol = 1e-8, ... ) {
  
  incredible_mass <- 1.0 - width
  interval_width <- function(low_tail_prob, name, width, ...) {
    name(width + low_tail_prob, ...) - name(low_tail_prob, ...)
  }
  opt_info <- optimize(interval_width, c(0, incredible_mass), 
                       name = name, width = width, 
                       tol = tol, ...)
  hdi_lower_tail_prob <- opt_info$minimum
  return(c(name(hdi_lower_tail_prob, ...),
           name(width + hdi_lower_tail_prob, ...)))
  
}
```

Our `hdi_of_icdf()` function will compute the analytic 95% highest density intervals (HDIs) for the distribution under consideration in Figure 7.1, $\operatorname{Beta}(\theta | 15, 7)$.

```{r}
h <-
  hdi_of_icdf(name = qbeta,
              shape1 = 15,
              shape2 = 7)

h
```

Using an equation from [Chapter 6][Specifying a beta prior.], $\omega = (a − 1) / (a + b − 2)$, we can compute the corresponding mode.

```{r}
(omega <- (15 - 1) / (15 + 7 - 2))
```

To get the density in the upper left panel of Figure 7.1, we'll make use of the `dbeta()` function and of our `h[1:2]` and `omega` values.

```{r, fig.width = 4, fig.height = 2.5, warning = F, message = F}
library(tidyverse)
library(cowplot)

tibble(theta = seq(from = 0, to = 1, length.out = 100)) %>% 
  
  ggplot() +
  geom_area(aes(x = theta, y = dbeta(theta, shape1 = 15, shape2 = 7)),
            fill = "steelblue") +
  geom_segment(aes(x = h[1], xend = h[2], y = 0, yend = 0),
               size = .75) +
  geom_point(aes(x = omega, y = 0),
             size = 1.5, shape = 19) +
  annotate(geom = "text", x = .675, y = .4, 
           label = "95% HDI", color = "white") +
  scale_x_continuous(expression(theta), 
                     breaks = c(0, h, omega, 1),
                     labels = c("0", h %>% round(2), omega, "1")) +
  ggtitle("Exact distribution") +
  ylab(expression(p(theta))) +
  theme_cowplot()
```

Note how we're continuing to use `theme_cowplot()`, which we introduced in the last chapter. The remaining panels in Figure 7.1 require we simulate the data.

```{r}
set.seed(7)

d <-
  tibble(n = c(500, 5000, 50000)) %>% 
  mutate(theta = map(n, ~rbeta(., shape1 = 15, shape2 = 7))) %>% 
  unnest(theta) %>% 
  mutate(key = str_c("Sample N = ", n))

head(d)
```

With the data in hand, we're ready to plot the remaining panels for Figure 7.1. This time, we'll use the handy `stat_pointinterval()` function from the [**tidybayes** package](https://github.com/mjskay/tidybayes) to mark off the mode and 95% HDIs.

```{r, fig.width = 10, fig.height = 2.75, warning = F, message = F}
library(tidybayes)

d %>% 
  ggplot(aes(x = theta, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95, breaks = 30) +
  scale_x_continuous(expression(theta), limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme_cowplot() +
  facet_wrap(~ key, ncol = 3, scales = "free")
```

If we want the exact values for the mode and 95% HDIs, we can use the `tidybayes::mode_hdi()` function.

```{r}
d %>% 
  group_by(key) %>% 
  mode_hdi(theta)
```

If you wanted a better sense of the phenomena, you could do a simulation. We'll make a custom simulation function to compute the modes from many random draws from our $\operatorname{Beta}(\theta | 15, 7)$ distribution, with varying $N$ values.

```{r}
my_mode_simulation <- function(seed) {
  
  set.seed(seed)
  
  tibble(n = c(500, 5000, 50000)) %>% 
    mutate(theta = map(n, ~rbeta(., shape1 = 15, shape2 = 7))) %>% 
    unnest(theta) %>% 
    mutate(key = str_c("Sample N = ", n)) %>% 
    group_by(key) %>% 
    mode_hdi(theta)
  
}
```

Here we put our `my_mode_simulation()` function to work.

```{r sim, fig.width = 8, fig.height = 2.75, warning = F, message = F, cache = T}
# we need an index of the values we set our seed with in our `my_mode_simulation()` function
sim <-
  tibble(seed = 1:1e3) %>% 
  group_by(seed) %>% 
  # inserting our subsamples
  mutate(modes = map(seed, my_mode_simulation)) %>% 
   # unnesting allows us to access our model results
  unnest(modes) 

sim %>% 
  ggplot(aes(x = theta, y = key)) +
  geom_vline(xintercept = .7, color = "white") +
  stat_histinterval(.width = c(.5, .95), breaks = 20, fill = "steelblue") +
  labs(title = expression("Variability of the mode for simulations of "*beta(theta*'|'*15*', '*7)*", the true mode of which is .7"),
       subtitle = "For each sample size, the dot is the median, the inner thick line is the percentile-based 50% interval,\nand the outer thin line the percentile-based 95% interval. Although the central tendency\napproximates the true value for all three conditions, the variability of the mode estimate is inversely\nrelated to the sample size.",
       x = "mode", 
       y = NULL) +
  coord_cartesian(xlim = c(.6, .8),
                  ylim = c(1.25, 3.5)) +
  theme_cowplot(font_size = 11.5) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())
```

## A simple case of the Metropolis algorithm

> Our goal in Bayesian inference is to get an accurate representation of the posterior distribution. One way to do that is to sample a large number of representative points from the posterior. The question then becomes this: How can we sample a large number of representative values from a distribution? (p. 146).

The answer, my friends, is MCMC.

### A politician stumbles upon the Metropolis algorithm.

I’m not going to walk out Kruschke’s politician example in any detail, here. But if we denote $P_\text{proposed}$ as the population of the proposed island and $P_\text{current}$ as the population of the current island, then

$$p_\text{move} = \frac{P_\text{proposed}}{P_\text{current}}.$$

"What's amazing about this heuristic is that it works: In the long run, the probability that the politician is on any one of the islands exactly matches the relative population of the island" (p. 147)!

### A random walk.

The code below will allow us to reproduce Kruschke's random walk. To give credit where it's due, this is a mild amendment to the code from Chapter 8 of McElreath's [-@mcelreathStatisticalRethinkingBayesian2015] text, [*Statistical rethinking: A Bayesian course with examples in R and Stan*](https://xcelab.net/rm/statistical-rethinking/).

```{r random_walk, cache = T}
set.seed(7)

num_days  <- 5e4
positions <- rep(0, num_days)
current   <- 4
for (i in 1:num_days) {
  # record current position
  positions[i] <- current
  # flip coin to generate proposal
  proposal <- current + sample(c(-1, 1), size = 1)
  # now make sure he loops around from 7 back to 1
  if (proposal < 1) proposal <- 7
  if (proposal > 7) proposal <- 1
  # move?
  prob_accept_the_proposal <- proposal/current
  current <- ifelse(runif(1) < prob_accept_the_proposal, proposal, current)
}
```

If you missed it, `positions` is the main product of our simulation. Here we'll put `positions` in a tibble and reproduce the top portion of Figure 7.2.

```{r, fig.width = 6, fig.height = 2}
tibble(theta = positions) %>% 
  
  ggplot(aes(x = theta)) +
  geom_bar(fill = "steelblue") +
  scale_x_continuous(expression(theta), breaks = 1:7) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_cowplot()
```

Did you notice that `scale_y_continuous()` line in the code? Claus Wilke, the author of the **cowplot** package, has a lot of thoughts on data visualization. He even wrote a [-@wilkeFundamentalsDataVisualization2019] book on it: [*Fundamentals of data visualization*](https://clauswilke.com/dataviz/). In his [-@Wilke2020Themes] [*Themes*](https://wilkelab.org/cowplot/articles/themes.html) vignette, Wilke recommended against allowing for space between the bottoms of the bars in a bar plot and the $x$-axis line. The **ggplot2** default is to allow for such a space. Here we followed Wilke and suppressed that space with `expand = expansion(mult = c(0, 0.05))`. You can learn more about the `ggplot2::expansion()` function [here](https://ggplot2.tidyverse.org/reference/expansion.html).

Here's the middle portion of Figure 7.2.

```{r, fig.width = 6, fig.height = 2.5}
tibble(t     = 1:5e4,
       theta = positions) %>% 
  slice(1:500) %>% 
  
  ggplot(aes(x = theta, y = t)) +
  geom_path(size = 1/4, color = "steelblue") +
  geom_point(size = 1/2, alpha = 1/2, color = "steelblue") +
  scale_x_continuous(expression(theta), breaks = 1:7) +
  scale_y_log10("Time Step", breaks = c(1, 2, 5, 20, 100, 500)) +
  theme_cowplot()
```

And now we make the bottom.

```{r, fig.width = 6, fig.height = 2}
tibble(x = 1:7,
       y = 1:7) %>% 
  
  ggplot(aes(x = x, y = y)) +
  geom_col(width = .2, fill = "steelblue") +
  scale_x_continuous(expression(theta), breaks = 1:7) +
  scale_y_continuous(expression(p(theta)), expand = expansion(mult = c(0, 0.05))) +
  theme_cowplot()
```

> Notice that the sampled relative frequencies closely mimic the actual relative populations in the bottom panel! In fact, a sequence generated this way will converge, as the sequence gets longer, to an arbitrarily close approximation of the actual relative probabilities. (p. 149)

### General properties of a random walk.

> The tajectory shown in Figure 7.2 is just one possible sequence of positions when the movement heuristic is applied. At each time step, the direction of the proposed move is random, and if the relative probability of the proposed position is less than that of the current position, then acceptance of the proposed move is also random. Because of the randomness, if the process were started over again, then the specific trajectory would almost certainly be different. Regardless of the specific trajectory, in the long run the relative frequency of visits mimics the target distribution.
>
> Figure 7.3 shows the probability of being in each position as a function of time. (p. 149)

I was initially stumped on how to reproduce the simulation depicted in Figure 7.3. However, fellow enthusiast [Cardy Moten III](https://github.com/cmoten) kindly [shared a solution](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/14) which was itself based on Kruschke's blog post from 2012, [*Metropolis algorithm: Discrete position probabilities*](https://doingbayesiandataanalysis.blogspot.com/2012/08/metropolis-algorithm-discrete-position.html). Here's a mild reworking of their solutions. First, we simulate.

```{r}
nslots   <- 7
p_target <- 1:7
p_target <- p_target / sum(p_target)

# construct the transition matrix
proposal_matrix <- matrix(0, nrow = nslots, ncol = nslots)

for(from_idx in 1:nslots) {
  for(to_idx in 1:nslots) {
    if(to_idx == from_idx - 1) {proposal_matrix[from_idx, to_idx] <- 0.5}
    if(to_idx == from_idx + 1) {proposal_matrix[from_idx, to_idx] <- 0.5}
  }
}

# construct the acceptance matrix
acceptance_matrix <- matrix(0, nrow = nslots, ncol = nslots)

for(from_idx in 1:nslots) {
  for(to_idx in 1:nslots) {
    acceptance_matrix[from_idx, to_idx] <- min(p_target[to_idx] / p_target[from_idx], 1)
  }
}

# compute the matrix of move probabilities
move_matrix <- proposal_matrix * acceptance_matrix

# compute the transition matrix, including the probability of staying in place
transition_matrix <- move_matrix
for (diag_idx in 1:nslots) {
  transition_matrix[diag_idx, diag_idx] = 1.0 - sum(move_matrix[diag_idx, ])
}

# specify starting position vector:
position_vec <- rep(0, nslots)
position_vec[round(nslots / 2)] <- 1.0

p <- list()
data <- 
  tibble(position = 1:nslots, 
         prob     = position_vec)

# loop through the requisite time indexes
# update the data and transition vector
for(time_idx in 1:99) {
  
  p[[time_idx]] <- data
  
  # update the position vec
  position_vec <- position_vec %*% transition_matrix
  
  # update the data
  data <- NULL 
  data <- 
    tibble(position = 1:nslots, 
           prob     = t(position_vec))
}
```

Now we wrangle and plot.

```{r, fig.width = 10, fig.height = 8}
p %>% 
  as_tibble_col() %>% 
  mutate(facet = str_c("italic(t)==", 1:99)) %>% 
  slice(c(1:14, 99)) %>% 
  unnest(value) %>% 
  bind_rows(
    tibble(position = 1:nslots, 
           prob     = p_target, 
           facet    = "target")
  ) %>% 
  mutate(facet = factor(facet, levels = c(str_c("italic(t)==", c(1:14, 99)), "target"))) %>% 
  
  # plot!
  ggplot(aes(x = position, y = prob, fill = facet == "target")) +
  geom_col(width = .2) +
  scale_fill_manual(values = c("steelblue", "goldenrod2"), breaks = NULL) +
  scale_x_continuous(expression(theta), breaks = 1:7) +
  scale_y_continuous(expression(italic(p)(theta)), expand = expansion(mult = c(0, 0.05))) +
  theme_cowplot() +
  facet_wrap(~ facet, scales = "free_y", labeller = label_parsed)
```

### Why we care.

Through the simple magic of the random walk procedure,

>  we are able to do *indirectly* something we could not necessarily do directly: We can generate random samples from the target distribution. Moreover, we can generate those random samples from the target distribution even when the target distribution is not normalized.
>
> This technique is profoundly useful when the target distribution $P(\theta)$ is a posterior proportional to $p(D | \theta) p(\theta)$. Merely by evaluating $p(D | \theta) p(\theta)$, without normalizing it by $p(D)$, we can generate random representative values from the posterior distribution. This result is wonderful because the method obviates direct computation of the evidence $p(D)$, which, as you'll recall, is one of the most difficult aspects of Bayesian inference. By using MCMC techniques, we can do Bayesian inference in rich and complex models. It has only been with the development of MCMC algorithms and software that Bayesian inference is applicable to complex data analysis, and it has only been with the production of fast and cheap computer hardware that Bayesian inference is accessible to a wide audience. (p. 152, *emphasis* in the original)

## The Metropolis algorithm more generally

"The procedure described in the previous section was just a special case of a more general procedure known as the Metropolis algorithm, named after the first author of a famous article [@metropolisEquationStateCalculations1953]" (p. 156).

Here's how to generate a proposed jump from a zero-mean normal distribution with a standard deviation of 0.2.

```{r}
rnorm(1, mean = 0, sd = 0.2)
```

To get a sense of what draws from `rnorm()` looks like in the long run, we might plot.

```{r, fig.width = 5, fig.height = 3}
mu    <- 0
sigma <- 0.2

# how many proposals would you like?
n  <- 500

set.seed(7)
tibble(proposed_jump = rnorm(n, mean = mu, sd = sigma)) %>% 
  
  ggplot(aes(x = proposed_jump, y = 0)) +
  geom_jitter(width = 0, height = .1, 
              size = 1/2, alpha = 1/2, color = "steelblue") +
  # this is the idealized distribution
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                color = "steelblue") +
  scale_x_continuous(breaks = seq(from = -0.6, to = 0.6, length.out = 7)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Jump proposals",
       subtitle = "The blue line shows the data generating distribution.") +
  theme_cowplot()
```

Anyway,

> having generated a proposed new position, the algorithm then decides whether or not to accept the proposal. The decision rule is exactly what was already specified in Equation 7.1. In detail, this is accomplished by computing the ratio $p_\text{move} = P(\theta_\text{proposed}) / P(\theta_\text{current})$. Then a random number from the uniform interval $[0, 1]$ is generated; in R, this can be accomplished with the command `runif(1)`. If the random number is between 0 and pmove, then the move is accepted. (p. 157)

We'll see what that might look like in the next section. In the meantime, here's how to use `runif()`.

```{r}
runif(1)
```

Just for kicks, here's what that looks like in bulk.

```{r, fig.width = 5, fig.height = 2}
# how many proposals would you like?
n  <- 500

set.seed(7)
tibble(draw = runif(n)) %>% 
  
  ggplot(aes(x = draw, y = 0)) +
  geom_jitter(width = 0, height = 1/4, 
              size = 1/2, alpha = 1/2, color = "steelblue") +
  stat_function(fun = dunif,
                color = "steelblue") +
  scale_y_continuous(NULL, breaks = NULL, limits = c(-1/3, 5/3)) +
  labs(title = "Uniform draws",
       subtitle = "The blue line shows the data generating distribution.") +
  theme_cowplot()
```

We do not see a concentration towards the mean, this time. The draws are uniformly distributed across the parameter space.

### Metropolis algorithm applied to Bernoulli likelihood and beta prior.

You can find Kruschke's code in the `BernMetrop.R` file. I'm going to break it up a little.

```{r}
# specify the data, to be used in the likelihood function.
my_data <- c(rep(0, 6), rep(1, 14))

# define the Bernoulli likelihood function, p(D|theta).
# the argument theta could be a vector, not just a scalar
likelihood <- function(theta, data) {
  z <- sum(data)
  n <- length(data)
  p_data_given_theta <- theta^z * (1 - theta)^(n - z)
  # the theta values passed into this function are generated at random,
  # and therefore might be inadvertently greater than 1 or less than 0.
  # the likelihood for theta > 1 or for theta < 0 is zero
  p_data_given_theta[theta > 1 | theta < 0] <- 0
  return(p_data_given_theta)
}

# define the prior density function. 
prior_d <- function(theta) {
  p_theta <- dbeta(theta, 1, 1)
  # the theta values passed into this function are generated at random,
  # and therefore might be inadvertently greater than 1 or less than 0.
  # the prior for theta > 1 or for theta < 0 is zero
  p_theta[theta > 1 | theta < 0] = 0
  return(p_theta)
}

# define the relative probability of the target distribution, 
# as a function of vector theta. for our application, this
# target distribution is the unnormalized posterior distribution
target_rel_prob <- function(theta, data) {
  target_rel_prob <- likelihood(theta, data) * prior_d(theta)
  return(target_rel_prob)
}

# specify the length of the trajectory, i.e., the number of jumps to try:
traj_length <- 50000 # this is just an arbitrary large number

# initialize the vector that will store the results
trajectory <- rep(0, traj_length)

# specify where to start the trajectory:
trajectory[1] <- 0.01 # another arbitrary value

# specify the burn-in period
burn_in <- ceiling(0.0 * traj_length) # arbitrary number, less than `traj_length`

# initialize accepted, rejected counters, just to monitor performance:
n_accepted <- 0
n_rejected <- 0
```

That first part follows what Kruschke put in his script. I'm going to bundel the next large potion in a fucntion, `my_metropolis()` which will make it easier to plug the code into the `purrr::map()` function.

```{r}
my_metropolis <- function(proposal_sd) {
  
  # now generate the random walk. the 't' index is time or trial in the walk.
  # specify seed to reproduce same random walk
  set.seed(47405)
  
  
  ## I'm taking this section out and will replace it
  
  # # specify standard deviation of proposal distribution
  # proposal_sd <- c(0.02, 0.2, 2.0)[2]
  
  ## end of the section I took out
  
  
  for (t in 1:(traj_length - 1)) {
    current_position <- trajectory[t]
    # use the proposal distribution to generate a proposed jump
    proposed_jump <- rnorm(1, mean = 0, sd = proposal_sd)
    # compute the probability of accepting the proposed jump
    prob_accept <- min(1,
                       target_rel_prob(current_position + proposed_jump, my_data)
                       / target_rel_prob(current_position, my_data))
    # generate a random uniform value from the interval [0, 1] to
    # decide whether or not to accept the proposed jump
    if (runif(1) < prob_accept) {
      # accept the proposed jump
      trajectory[t + 1] <- current_position + proposed_jump
      # increment the accepted counter, just to monitor performance
      if (t > burn_in) {n_accepted <- n_accepted + 1}
    } else {
      # reject the proposed jump, stay at current position
      trajectory[t + 1] <- current_position
      # increment the rejected counter, just to monitor performance
      if (t > burn_in) {n_rejected <- n_rejected + 1}
    }
  }
  
  # extract the post-burn_in portion of the trajectory
  accepted_traj <- trajectory[(burn_in + 1) : length(trajectory)]
  
  tibble(accepted_traj = accepted_traj,
         n_accepted    = n_accepted, 
         n_rejected    = n_rejected)
  # end of Metropolis algorithm
  
}
```

Now we have `my_metropolis()`, we can run the analysis based on the three `proposal_sd` values, nesting the results in a tibble.

```{r metropolis_sim, cache = T}
d <-
  tibble(proposal_sd = c(0.02, 0.2, 2.0)) %>% 
  mutate(accepted_traj = map(proposal_sd, my_metropolis)) %>% 
  unnest(accepted_traj)

glimpse(d)
```

Now we have `d` in hand, here's the top portion of Figure 7.4.

```{r, fig.width = 10, fig.height = 2.75}
d <-
  d %>% 
  mutate(proposal_sd = str_c("Proposal SD = ", proposal_sd),
         iter        = rep(1:50000, times = 3))
  
d %>% 
  ggplot(aes(x = accepted_traj, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = "steelblue", slab_color = "white", outline_bars = T,
                    breaks = 40, normalize = "panels") +
  scale_x_continuous(expression(theta), breaks = 0:5 * 0.2) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme_cowplot() +
  panel_border() +
  facet_wrap(~ proposal_sd, ncol = 3)
```

The modes are the points and the lines depict the 95% HDIs. Also, did you notice our use of the `cowplot::panel_border()` function? The settings from `theme_cowplot()` can make it difficult to differentiate among subplots when faceting. By throwing in a call to `panel_border()` after `theme_cowplot()`, we added in lightweight panel borders.

Here's the middle of Figure 7.4.

```{r, fig.width = 10, fig.height = 2.75, warning = F}
d %>% 
  ggplot(aes(x = accepted_traj, y = iter)) +
  geom_path(size = 1/4, color = "steelblue") +
  geom_point(size = 1/2, alpha = 1/2, color = "steelblue") +
  scale_x_continuous(expression(theta), breaks = 0:5 * 0.2, limits = c(0, 1)) +
  scale_y_continuous("Step in Chain", limits = c(49900, 50000)) +
  ggtitle("End of Chain") +
  theme_cowplot() +
  panel_border() +
  facet_wrap(~ proposal_sd, ncol = 3)
```

The bottom:

```{r, fig.width = 10, fig.height = 2.75, warning = F}
d %>% 
  ggplot(aes(x = accepted_traj, y = iter)) +
  geom_path(size = 1/4, color = "steelblue") +
  geom_point(size = 1/2, alpha = 1/2, color = "steelblue") +
  scale_x_continuous(expression(theta), breaks = 0:5 * 0.2, limits = c(0, 1)) +
  scale_y_continuous("Step in Chain", limits = c(1, 100)) +
  ggtitle("End of Chain") +
  theme_cowplot() +
  panel_border() +
  facet_wrap(~ proposal_sd, ncol = 3)
```

> Regardless of the which proposal distribution in Figure 7.4 is used, the Metropolis algorithm will eventually produce an accurate representation of the posterior distribution, as is suggested by the histograms in the upper row of Figure 7.4. What differs is the efficiency of achieving a good approximation. (p. 160)

### Summary of Metropolis algorithm.

> The motivation for methods like the Metropolis algorithm is that they provide a high-resolution picture of the posterior distribution, even though in complex models we cannot explicitly solve the mathematical integral in Bayes' rule. The idea is that we get a handle on the posterior distribution by generating a large sample of representative values. The larger the sample, the more accurate is our approximation. As emphasized previously, this is a sample of representative credible parameter values from the posterior distribution; it is not a resampling of data (there is a fixed data set).
>
> The cleverness of the method is that representative parameter values can be randomly sampled from complicated posterior distributions without solving the integral in Bayes' rule, and by using only simple proposal distributions for which efficient random number generators already exist. (p. 161)

## Toward Gibbs sampling: Estimating two coin biases 

"The Metropolis method is very useful, but it can be inefficient. Other methods can be more efficient in some situations" (p. 162).

### Prior, likelihood and posterior for two biases.

> We are considering situations in which there are two underlying biases, namely $\theta_1$ and $\theta_2$, for the two coins. We are trying to determine what we should believe about these biases after we have observed some data from the two coins. Recall that [Kruschke used] the term "bias" as the name of the parameter $\theta$, and not to indicate that the value of $\theta$ deviates from 0.5....
>
> What we have to do next is specify a particular mathematical form for the prior distribution. We will work through the mathematics of a particular case for two reasons: First, it will allow us to explore graphical displays of two-dimensional parameter spaces, which will inform our intuitions about Bayes' rule and sampling from the posterior distribution. Second, the mathematics will set the stage for a specific example of Gibbs sampling. Later in the book when we do applied Bayesian analysis, we will *not* be doing any of this sort of mathematics. We are doing the math now, for simple cases, to understand how the methods work so we can properly interpret their outputs in realistically complex cases. (pp. 163--165, *emphasis* in the original)

### The posterior via exact formal analysis.

The plots in the left column of Figure 7.5 are outside of my skill set. I believe they are referred to as wireframe plots and it's my understanding that **ggplot2** does not support wireframe plots at this time. However, I can reproduce versions of the right hand column. For our initial attempt for the upper right corner, we'll simulate.

```{r betas, cache = T, fig.height = 3}
set.seed(7)

betas <-
  tibble(theta_1 = rbeta(1e5, shape1 = 2, shape2 = 2),
         theta_2 = rbeta(1e5, shape1 = 2, shape2 = 2))

betas %>% 
  ggplot(aes(x = theta_1, y = theta_2)) +
  stat_density_2d() +
  labs(x = expression(theta[1]),
       y = expression(theta[2])) +
  coord_equal() +
  theme_cowplot()
```

Instead of the contour lines, one might use color to depict the density variable.

```{r, fig.height = 3}
betas %>% 
  ggplot(aes(x = theta_1, y = theta_2, fill = stat(density))) +
  stat_density_2d(geom = "raster", contour = F) +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta[1]),
       y = expression(theta[2])) +
  coord_equal() +
  theme_cowplot()
```

Remember how we talked about suppressing the unsightly white space between the bottom of bar-plot bars and the $x$-axis? Well, look at all that unsightly white space between the axes and the boundaries of the parameter space in our bivariate Beta plot. We can further flex our `expansion()` skills to get rid of those in the next plot. Speaking of which, we might make a more precise version of that plot with careful use of `dbeta()`. This approach is also more in line with the title of this subsection: *The posterior via exact formal analysis*.

```{r, fig.height = 3}
theta_sequence <- seq(from = 0, to = 1, by = .01)

tibble(theta_1 = theta_sequence,
       theta_2 = theta_sequence) %>%
  
  mutate(prior_1 = dbeta(x = theta_1, shape1 = 2, shape2 = 2),
         prior_2 = dbeta(x = theta_2, shape1 = 2, shape2 = 2)) %>% 
    
  expand(nesting(theta_1, prior_1), nesting(theta_2, prior_2)) %>%
  
  ggplot(aes(x = theta_1, y = theta_2, fill = prior_1 * prior_2)) +
  geom_tile() +
  scale_fill_viridis_c("joint prior density", option = "A") +
  scale_x_continuous(expression(theta[1]), expand = expansion(mult = 0)) +
  scale_y_continuous(expression(theta[2]), expand = expansion(mult = 0)) +
  coord_equal() +
  theme_cowplot()
```

Look at that--no more unsightly white space! We'll need the `bernoulli_likelihood()` function from back in Chapter 6 for the middle right of Figure 7.5.

```{r}
bernoulli_likelihood <- function(theta, data) {
  
  # theta = success probability parameter ranging from 0 to 1
  # data = the vector of data (i.e., a series of 0's and 1's)
  n <- length(data)
  z <- sum(data)
  
  return(theta^z * (1 - theta)^(n - sum(data)))
  
}
```

With our trusty `bernoulli_likelihood()` function in hand, we're almost ready to compute and plot the likelihood. We just need to define our data.

```{r}
# set the parameters
# coin 1
n1 <- 8
z1 <- 6

# coin 2
n2 <- 7
z2 <- 2

# use the parameters to make the data
theta_1_data <- rep(0:1, times = c(n1 - z1, z1))
theta_2_data <- rep(0:1, times = c(n2 - z2, z2))

# take a look
theta_1_data
theta_2_data
```

Note how these data sequences are of different sample sizes $(N_1 = 8; N_2 = 7 )$. Though it doesn't matter much for the formal analysis approach, this will be important when we fit the model with **brms**. But for right now, we're finally ready to make a version of the middle right panel of Figure 7.5.

```{r, fig.width = 5.5, fig.height = 3}
tibble(theta_1 = theta_sequence,
       theta_2 = theta_sequence) %>%
  mutate(likelihood_1 = bernoulli_likelihood(theta = theta_sequence,
                                             data  = theta_1_data),
         likelihood_2 = bernoulli_likelihood(theta = theta_sequence,
                                             data  = theta_2_data)) %>% 
  expand(nesting(theta_1, likelihood_1), nesting(theta_2, likelihood_2)) %>%
  
  ggplot(aes(x = theta_1, y = theta_2, fill = likelihood_1 * likelihood_2)) +
  geom_tile() +
  scale_fill_viridis_c("joint likelihood", option = "A") +
  scale_x_continuous(expression(theta[1]), expand = expansion(mult = 0)) +
  scale_y_continuous(expression(theta[2]), expand = expansion(mult = 0)) +
  coord_equal() +
  theme_cowplot()
```

Here's the two-dimensional posterior, the lower right panel of Figure 7.5.

```{r, fig.width = 4.5, fig.height = 3}
# this is a redo from two plots up, but saved as `d_prior`
d_prior <-
  tibble(theta_1 = theta_sequence,
         theta_2 = theta_sequence) %>%
  mutate(prior_1 = dbeta(x = theta_1, shape1 = 2, shape2 = 2),
         prior_2 = dbeta(x = theta_2, shape1 = 2, shape2 = 2)) %>% 
  expand(nesting(theta_1, prior_1), nesting(theta_2, prior_2))

# this is a redo from one plot up, but saved as `d_likelihood`
d_likelihood <-
  tibble(theta_1 = theta_sequence,
         theta_2 = theta_sequence) %>%
  mutate(likelihood_1 = bernoulli_likelihood(theta = theta_sequence,
                                             data  = theta_1_data),
         likelihood_2 = bernoulli_likelihood(theta = theta_sequence,
                                             data  = theta_2_data)) %>% 
  expand(nesting(theta_1, likelihood_1), nesting(theta_2, likelihood_2))

# here we combine `d_prior` and `d_likelihood`
d_prior %>% 
  left_join(d_likelihood, by = c("theta_1", "theta_2")) %>% 
  # we need the marginal likelihood, the denominator in Bayes' rule
  mutate(marginal_likelihood = sum(prior_1 * prior_2 * likelihood_1 * likelihood_2)) %>% 
  # finally, the two-dimensional posterior
  mutate(posterior = (prior_1 * prior_2 * likelihood_1 * likelihood_2) / marginal_likelihood) %>% 
  
  # plot!
  ggplot(aes(x = theta_1, y = theta_2, fill = posterior)) +
  geom_tile() +
  scale_fill_viridis_c(expression(italic(p)(theta[1]*', '*theta[2]*'|'*D)), option = "A") +
  scale_x_continuous(expression(theta[1]), expand = expansion(mult = 0)) +
  scale_y_continuous(expression(theta[2]), expand = expansion(mult = 0)) +
  coord_equal() +
  theme_cowplot()
```

That last plot, my friends, is a depiction of

$$p(\theta_1, \theta_2 | D) = \frac{p(D | \theta_1, \theta_2) p(\theta_1, \theta_2)}{p(D)}.$$

### The posterior via the Metropolis algorithm.

I initially skipped over this section because the purpose of this book is to explore Kruschke's material with **brms**, which does not use the Metropolis algorithm (which really is primarily of historic interest, at this point). However, fellow enthusiast [Omid Ghasemi](https://github.com/OmidGhasemi21) worked it through and kindly shared his solution. The workflow, below, is based heavily on his, with a few small adjustments.

To start off, we'll refresh our two data sources and define a few custom functions.

```{r}
# we've already defined these, but here they are again
theta_1_data <- rep(0:1, times = c(n1 - z1, z1))
theta_2_data <- rep(0:1, times = c(n2 - z2, z2))

# define the bivariate Bernoulli likelihood
bivariate_bernoulli_likelihood <- function(theta1, data1, theta2, data2) {
  
  z1 <- sum(data1)
  n1 <- length(data1)
  z2 <- sum(data2)
  n2 <- length(data2)
  p_data_given_theta <- (theta1^z1 * (1 - theta1)^(n1 - z1)) * (theta2^z2 * (1 - theta2)^(n2 - z2))
  p_data_given_theta[theta1 > 1 | theta1 < 0] <- 0
  p_data_given_theta[theta2 > 1 | theta2 < 0] <- 0
  
  return(p_data_given_theta)
  
}

# we need to update the prior density function from above
prior_d <- function(theta1, theta2) {
  
  p_theta <- dbeta(theta1, 1, 1) * dbeta(theta2, 1, 1)
  p_theta[theta1 > 1 | theta1 < 0] = 0
  p_theta[theta2 > 1 | theta2 < 0] = 0
  
  return(p_theta)
  
}

# we also need to update how we define the relative probability of the target distribution
target_rel_prob <- function(theta1, data1, theta2, data2) {
  
  l <- bivariate_bernoulli_likelihood(theta1, data1, theta2, data2)
  p <- prior_d(theta1, theta2)
  
  target_rel_prob <- l * p
  
  return(target_rel_prob)
}
```

The next bit defines how we'll apply the Metropolis algorithm to our bivariate binomial data. Although the guts contain a lot of moving parts, there are only two parameters at the top level. The `traj_length` argument is set to 50,000, which will be our default number of MCMC draws. Of greater interest is the `proposal_sd` argument. From the text, we read:

> Recall that the Metropolis algorithm is a random walk through the parameter space that starts at some arbitrary point. We propose a jump to a new point in parameter space, with the proposed jump randomly generated from a proposal distribution from which it is easy to generate values. For our present purposes, the proposal distribution is a *bivariate* normal. (p. 168, *emphasis* in the original)

For this exercise, the bivariate normal proposal distribution is centered at zero with an adjustable standard deviation. In the text, Kruschke compared the results for $\operatorname{Normal}(0, 0.02)$ and $\operatorname{Normal}(0, 0.2)$. For our `my_bivariate_metropolis()` function, the `proposal_sd` argument controls that $\sigma$ parameter.

```{r}
my_bivariate_metropolis <- function(proposal_sd = 0.02,
                                    # specify the length of the trajectory (i.e., the number of jumps to try)
                                    traj_length = 50000) {
  
  # initialize the vector that will store the results
  trajectory1 <- rep(0, traj_length)
  trajectory2 <- rep(0, traj_length)
  
  # specify where to start the trajectory:
  trajectory1[1] <- 0.5 # another arbitrary value
  trajectory2[1] <- 0.5 # another arbitrary value
  
  # specify the burn-in period
  burn_in <- ceiling(0.0 * traj_length) # arbitrary number, less than `traj_length`
  
  # initialize accepted, rejected counters, just to monitor performance:
  n_accepted <- 0
  n_rejected <- 0
  
  for (t in 1:(traj_length - 1)) {
    current_position1 <- trajectory1[t]
    current_position2 <- trajectory2[t]
    
    # use the proposal distribution to generate a proposed jump
    proposed_jump1 <- rnorm(1, mean = 0, sd = proposal_sd)
    proposed_jump2 <- rnorm(1, mean = 0, sd = proposal_sd)
    
    # compute the probability of accepting the proposed jump
    prob_accept <- min(1,
                       target_rel_prob(current_position1 + proposed_jump1, theta_1_data,
                                       current_position2 + proposed_jump2, theta_2_data)
                       / target_rel_prob(current_position1, theta_1_data, current_position2, theta_2_data))
    
    # generate a random uniform value from the interval [0, 1] to
    # decide whether or not to accept the proposed jump
    if (runif(1) < prob_accept) {
      # accept the proposed jump
      trajectory1[t + 1] <- current_position1  + proposed_jump1
      trajectory2[t + 1] <- current_position2  + proposed_jump2
      # increment the accepted counter, just to monitor performance
      if (t > burn_in) {n_accepted <- n_accepted + 1}
    } else {
      # reject the proposed jump, stay at current position
      trajectory1[t + 1] <- current_position1
      trajectory2[t + 1] <- current_position2
      # increment the rejected counter, just to monitor performance
      if (t > burn_in) {n_rejected <- n_rejected + 1}
    }
  }
  
  # extract the post-burn_in portion of the trajectory
  accepted_traj1 <- trajectory1[(burn_in + 1) : length(trajectory1)]
  accepted_traj2 <- trajectory2[(burn_in + 1) : length(trajectory2)]
  
  # collect the results
  metrop_2d_data <- 
    tibble(iter           = rep(1:traj_length),
           accepted_traj1 = accepted_traj1,
           accepted_traj2 = accepted_traj2,
           n_accepted     = n_accepted, 
           n_rejected     = n_rejected)
  
  return(metrop_2d_data)
  
}
```

Now we've defined `my_bivariate_metropolis()` let's apply it to our data with `proposal_sd == 0.02` and `proposal_sd == 0.2`. We'll save the results as `mh`.

```{r}
mh <-
  tibble(proposal_sd = c(0.02, 0.2)) %>% 
  mutate(mh = map(proposal_sd, my_bivariate_metropolis)) %>% 
  unnest(mh)

mh
```

If you look at the top of Figure 7.6, you'll see Kruschke summarized his results with the acceptance rate, $N_\text{acc} / N_\text{pro}$. Here are ours.

```{r}
mh %>% 
  group_by(proposal_sd) %>% 
  slice(1) %>% 
  summarise(acceptance_rate = n_accepted / (n_accepted + n_rejected))
```

We can compute our effective sample sizes using the `effectiveSize()` function from the [**coda** package](https://CRAN.R-project.org/package=coda) [@R-coda; @plummerCODA2006].

```{r, warning = F, message = F}
library(coda)

mh %>% 
  group_by(proposal_sd) %>% 
  summarise(ess_theta_1 = effectiveSize(accepted_traj1),
            ess_theta_2 = effectiveSize(accepted_traj2))
```

We really won't use the **coda** package in this ebook beyond this chapter and the next. But do note it has a lot to offer and Kruschke used it a bit in his code. Anyway, now we make our version of Figure 7.6.

```{r, fig.height = 5, warning = F}
mh %>% 
  filter(iter < 1000) %>%
  
  ggplot(aes(x = accepted_traj1, y = accepted_traj2)) +
  geom_path(size = 1/8, alpha = 1/2, color = "steelblue") +
  geom_point(alpha = 1/4, color = "steelblue") +
  scale_x_continuous(expression(theta[1]), breaks = 0:5 / 5, expand = c(0, 0), limits = c(0, 1)) +
  scale_y_continuous(expression(theta[2]), breaks = 0:5 / 5, expand = c(0, 0), limits = c(0, 1)) +
  coord_equal() +
  theme_cowplot() +
  panel_border() +
  theme(panel.spacing.x = unit(0.75, "cm")) +
  facet_wrap(~ proposal_sd, labeller = label_both)
```

> In the limit of infinite random walks, the Metropolis algorithm yields arbitrarily accurate representations of the underlying posterior distribution. The left and right panels of Figure 7.6 would eventually converge to an identical and highly accurate approximation to the posterior distribution. But in the real world of finite random walks, we care about how efficiently the algorithm generates an accurate representative sample. We prefer to use the proposal distribution from the right panel of Figure 7.6 because it will, typically, produce a more accurate approximation of the posterior than the proposal distribution from left panel, for the same number of proposed jumps. (p. 170)

### ~~Gibbs~~ Hamiltonian Monte Carlo sampling.

Figure 7.7 is still out of my skill set. But let's fit the model with our primary package, **brms**. First we need to load **brms**.

```{r, warning = F, message = F}
library(brms)
```

These, recall, are the data.

```{r}
theta_1_data
theta_2_data
```

Kruschke said he was starting us out simply. From a regression perspective, we are getting ready to fit an intercepts-only multivariate Bernoulli model, which isn't the simplest of things to code into **brms**. Plus, this particular pair of data sets presents a complication we won't usually have to contend with in this book: The data vectors are different lengths. Remember how we pointed that out in [Section 7.4.2][The posterior via exact formal analysis.]? The issue is that whereas **brms** has extensive multivariate capacities [@Bürkner2022Multivariate], they're usually designed for data with equal sample sizes (i.e., when the rows in the two columns of a data frame are of the same number). Since these are Bernoulli data, we have two options at our disposal:

* employ the `resp_subset()` helper function or
* fit an aggregated binomial[^2] model.

Since each has its strengths and weaknesses, we'll split this section up and fit the model both ways.

#### Uneven multivariate Benoulli via the `resp_subset()` approach.

Though **brms** can receive data from a few different formats, our approach throughout this text will usually be with data frames or tibbles. Here's how we might combine our two data vectors, `theta_1_data` and `theta_2_data`, into a single tibble called `d`.

```{r}
d <-
  tibble(y1 = theta_1_data,
         y2 = c(theta_2_data, NA))

# what is this?
d
```

Because the second data vector was one unit shorter than the first, we had to compensate by adding an eighth cell, which we coded as `NA`, the universal indicator for missing values within the **R** ecosystem.

We'll still need one more data column, though. Using the `if_else()` function, we will make a `subset` column with will be coded `TRUE` for all non-`NA` values in the `y2` columns, and `FALSE` whenever `is.na(y2)`.

```{r}
d <-
  d %>% 
  mutate(subset = if_else(is.na(y2), FALSE, TRUE))

d
```

**brms** includes a handful of helper functions that let users incorporate additional information about the criterion variable(s) into the model. For a full listing of these helper functions, check out the `addition-terms` section of the [**brms** reference manual](https://CRAN.R-project.org/package=brms/brms.pdf) [@brms2022RM]. Though we won't be using a lot of these in this ebook, it just so turns out that two of them will come in handy for our multivariate Bernoulli data. In this case, we want the `resp_subset()` helper function, which, in practice, we will just call as `subset()`. From the **brms** reference manual, we read:

> For multivariate models, `subset` may be used in the `aterms` part, to use different subsets of the data in different univariate models. For instance, if `sub` is a logical variable and `y` is the response of one of the univariate models, we may write `y | subset(sub) ~ predictors` so that `y` is predicted only for those observations for which `sub` evaluates to `TRUE`. (p. 42)

In our case, the `subset` variable in the data tibble will be the logical variable for our criterion `y2`, which would leave us with the formula `y2 | subset(subset) ~ 1`. Note the use of the `|` operator, which is what you always use when adding additional information with a help function of this kind. Since we will be using the data in all eight rows of the `y1` column, that corresponding formula would just be `y1 ~ 1`. In both cases, the `~ 1` portions of the formulas indicates these are intercept-only models. There are not predictor variables in the model.

Next, we need to talk about how to combine these two formulas within the multivariate syntax. If you look through Bürkner's [-@Bürkner2022Multivariate] vignette, [*Estimating multivariate models with brms*](https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html), you'll see there are several ways to define a multivariate model with **brms**. In this case, I think it'll be easiest to define each model as a separate object, which we'll call `model_1` and `model_2`.

```{r}
model_1 <- bf(y1 ~ 1)
model_2 <- bf(y2 | subset(subset) ~ 1)
```

Note how the formula syntax for each was wrapped within the `bf()` function. That's shorthand for `brmsformula()`. If you wanted to, you could have defined these as `model_1 <- brmsformula(y1 ~ 1)`, and so on.

Another issue we need to contend with is Kruschke's $\operatorname{Beta}(2, 2)$ prior. By default, **brms** assumes an unbounded parameter space for the standard intercept priors. But we know that the beta distribution imposes boundaries within the range of $[0, 1]$. This is technically okay with a standard **brms** intercept prior, but it can lead to computational difficulties. When possible, it's better to formally tell **brms** when you are using bounded priors. The difficulty is the the current version of **brms** does not allow users to set lower or upper boundaries in priors of `class = Intercept`. The way to get around that is with the `0 + Intercept` syntax. Here, we remove the default intercept with the `0` part. Then we redefine the intercept with the `+ Intercept` part. The reason you would do such a thing is now our new `Intercept` parameter takes a prior of `class = b`, which is the generic class of priors for predictor variables which CAN accept upper and/or lower boundaries. As a consequence, we will want to redefine our two model objects to use the `0 + Intercept` syntax.

```{r}
model_1 <- bf(y1 ~ 0 + Intercept)
model_2 <- bf(y2 | subset(subset) ~ 0 + Intercept)
```

Here, then, is how we'll define our $\operatorname{Beta}(2, 2)$ prior within the `prior()` function.

```{r, eval = F}
prior(beta(2, 2), class = b, lb = 0, ub = 1, resp = y1)
```

We set the lower boundary with the `lb` argument and then set the upper boundary with the `ub` argument. Also notice our use of the `resp` argument, which told **brms** this prior was connected to the `y1` criterion. For the other criterion, we'd set that to `resp = y2`.

Okay, here's how to put all of this together to fit the model.

```{r fit7.1a, message = F}
fit7.1a <-
  brm(data = d, 
      family = bernoulli(link = identity),
      model_1 + model_2,
      prior = c(prior(beta(2, 2), class = b, lb = 0, ub = 1, resp = y1),
                prior(beta(2, 2), class = b, lb = 0, ub = 1, resp = y2)),
      iter = 3000, warmup = 500, cores = 3, chains = 3,
      seed = 7,
      file = "fits/fit07.01a")
```

Notice how we combined our two model objects with the `+` operator (`model_1 + model_2`). Here is a summary of the results.

```{r}
print(fit7.1a)
```

Here we'll use the `as_draws_df()` function to collect out posterior draws and save them as a data frame, which we'll name `draws.a`.

```{r}
draws.a <- as_draws_df(fit7.1a)

# what is this?
head(draws.a)
```

With `draws.a` in hand, we're ready to make our version of Figure 7.8. To reduce the overplotting, we're only looking at the first 500 post-warmup draws.

```{r, fig.height = 5, warning = F}
p1 <-
  draws.a %>% 
  filter(.draw < 501) %>% 
  
  ggplot(aes(x = b_y1_Intercept, y = b_y2_Intercept)) +
  geom_point(alpha = 1/4, color = "steelblue") +
  geom_path(size = 1/10, alpha = 1/2, color = "steelblue") +
  scale_x_continuous(expression(theta[1]), breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) +
  scale_y_continuous(expression(theta[2]), breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) +
  labs(subtitle = "fit7.1a (resp_subset() method)") +
  coord_equal() +
  theme_cowplot()

p1
```

Finally, after all that terrible technical talk, our efforts payed off! I promise, it won't always be this difficult to fit a model in **brms**. This is just one of those unfortunate cases where a textbook author's seemingly simple example required a lot of non-default settings and approaches when applied to a different software package from the one they highlighted in their textbook.

Now let's try out the second approach.

#### Uneven multivariate Benoulli via the aggregated binomial approach.

Instead of thinking of our data as two vectors of 0's and 1's, we can use the aggregate form to summarize them more compactly.

```{r}
d <-
  tibble(n1 = n1,
         z1 = z1, 
         n2 = n2,
         z2 = z2)

# what does this look like?
d
```

The `resp_trials()` function is the second helper that will let us incorporate additional information about the criterion variable(s) into the model. In actual practice, we'll use it as `trials()`. The basic idea is that for the first coin, we have `z1 == 6` heads out of `n1 == 8` trials. We can express that as an intercept-only model as `z1 | trials(n1) ~ 1`. Since we're still modeling a probability, and therefore want to continue using Kruschke's $\operatorname{Beta}(2, 2)$ prior, we'll want to bring in the `0 + Intercept` syntax. Thus, we'll define our two submodels like this.

```{r}
model_1 <- bf(z1 | trials(n1) ~ 0 + Intercept)
model_2 <- bf(z2 | trials(n2) ~ 0 + Intercept)
```

Notice that when our data are in an aggregated format, we expressed both series of coin flips in one row of the data frame. Therefore, even though the first coin had more trials than the second coin (`n1 > n2`), we don't need to invoke the `subset()` helper. We just need to use `trials()` to tell **brms** how long each sequence of coin flips was.

The other new thing is that instead of directly using the Bernoulli likelihood function, we'll instead be setting `family = binomial(link = "identity")`. Recall that the Bernoulli function is a special case of the binomial function for which $N = 1$, for which each data point is a discrete trial. When each data point is an aggregate of multiple trials, we use the binomial likelihood, instead.

Here's how to fit the model.
      
```{r fit7.1b, message = F}
fit7.1b <-
  brm(data = d, 
      family = binomial(link = "identity"),
      model_1 + model_2,
      prior = c(prior(beta(2, 2), class = b, lb = 0, ub = 1, resp = z1),
                prior(beta(2, 2), class = b, lb = 0, ub = 1, resp = z2)),
      iter = 3000, warmup = 500, cores = 3, chains = 3,
      seed = 7,
      file = "fits/fit07.01b")
```

The model summary for our `fit7.1b` aggregated binomial approach is very similar to the one from our previous `fit7.1a` approach using the `subset()` helper.

```{r}
print(fit7.1b)
```

Here's the updated version of Figure 7.8.

```{r, fig.height = 4, warning = F, message = F}
draws.b <- as_draws_df(fit7.1b)

p2 <-
  draws.b %>% 
  filter(.draw < 501) %>% 
  
  ggplot(aes(x = b_z1_Intercept, y = b_z2_Intercept)) +
  geom_point(alpha = 1/4, color = "steelblue") +
  geom_path(size = 1/10, alpha = 1/2, color = "steelblue") +
  scale_x_continuous(expression(theta[1]), breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) +
  scale_y_continuous(expression(theta[2]), breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) +
  labs(subtitle = "fit7.1b (aggregated binomial method)") +
  coord_equal() +
  theme_cowplot()

# combine
library(patchwork)

p1 + p2
```

Just for kicks and giggles, we might also compare the two model types by plotting the marginal posterior densities. This will give is a better sense of how each of the marginal $\theta$ densities are shaped like the beta distribution.

```{r, fig.height = 2.25, fig.width = 6, warning = F}
# combine the posterior samples from the two models
draws <-
  bind_rows(
    draws.a %>% transmute(`theta[1]` = b_y1_Intercept, `theta[2]` = b_y2_Intercept),
    draws.b %>% transmute(`theta[1]` = b_z1_Intercept, `theta[2]` = b_z2_Intercept)
  ) %>% 
  mutate(fit = rep(c("fit7.1a", "fit7.1b"), each = n() / 2))

# wrangle
draws %>% 
  pivot_longer(-fit) %>% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95, fill = "steelblue") +
  scale_x_continuous("posterior", breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme_cowplot() +
  theme(panel.spacing.x = unit(0.75, "cm")) +
  panel_border() +
  facet_grid(fit ~ name, labeller = label_parsed)
```

I hope this makes clear that the two estimation methods returned results that are within simulation variance of one another.

### Is there a difference between biases?

In his Figure 7.9, Kruschke compared the marginal posterior for $\theta_1 - \theta_2$, as computed by two methods from the Metropolis algorithm and another two methods from the Gibbs sampler. Here we'll focus, instead, on the two methods we explored using **brms**-based Hamiltonian Monte Carlo (HMC).

```{r, fig.height = 2.5, fig.width = 3.25}
draws %>% 
  mutate(dif = `theta[1]` - `theta[2]`) %>% 

  ggplot(aes(x = dif, y = fit)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = "steelblue2", slab_color = "steelblue4", outline_bars = T,
                    breaks = 40, normalize = "panels") +
  geom_vline(xintercept = 0, linetype = 3) +
  labs(x = expression(theta[1]-theta[2]),
       y = NULL) +
  coord_cartesian(ylim = c(1.5, 2.4)) +
  theme_cowplot() 
```

Here are the exact estimates of the mode and 95% HDIs for our difference distribution, $\theta_1 - \theta_2$.

```{r}
draws %>% 
  mutate(dif = `theta[1]` - `theta[2]`) %>% 
  group_by(fit) %>% 
  mode_hdi(dif) %>% 
  mutate_if(is.double, round, digits = 3)
```

I wouldn't put too much emphasis on the seemingly large differences in the two modes. Among the three primary measures of central tendency, modes are particularly sensitive to things like sample variance. Here's what happens if we compare the two methods with the mean, instead.

```{r}
draws %>% 
  mutate(dif = `theta[1]` - `theta[2]`) %>% 
  group_by(fit) %>% 
  summarise(mean_of_the_difference_score = mean(dif) %>% round(digits = 3))
```

Now the difference between the two methods seems trivial.

### Terminology: MCMC.

> Any simulation that samples a lot of random values from a distribution is called a Monte Carlo simulation, named after the dice and spinners and shufflings of the famous casino locale. The appellation "Monte Carlo" is attributed [@eckhardtStanUlamJohn1987] to the mathematicians [Stanislaw Ulam](https://en.wikipedia.org/wiki/Stanislaw_Ulam) (1909--1984) and [John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann) (1903--1957). (p. 177)

In case you didn't know, **brms** is a user-friendly interface for the [Stan probabilistic programing language](https://mc-stan.org/) [Stan; @carpenterStanProbabilisticProgramming2017] and Stan is named after Stanislaw Ulam.

## MCMC representativeness, accuracy, and efficiency

> We have three main goals in generating an MCMC sample from the posterior distribution:
>
> 1. The values in the chain must be *representative* of the posterior distribution. They should not be unduly influenced by the arbitrary initial value of the chain, and they should fully explore the range of the posterior distribution without getting stuck.
> 2. The chain should be of sufficient size so that estimates are *accurate* and *stable*. In particular, the estimates of the central tendency (such as median or mode), and the limits of the 95% HDI, should not be much different if the MCMC analysis is run again (using different seed states for the pseudorandom number generators).
> 3. The chain should be generated *efficiently*, with as few steps as possible, so not to exceed our patience or computing power. (p. 178, *emphasis* in the original)

### MCMC representativeness.

Kruschke defined our new data in the note for Figure 7.10.

```{r}
z <- 35
n <- 50

d <- tibble(y = rep(0:1, times = c(n - z, z)))
```

Here we fit the model. Note how since we're just univariate, it's easy to switch back to directly modeling with the Bernoulli likelihood.

```{r fit7.2}
fit7.2 <-
  brm(data = d, 
      family = bernoulli(link = identity),
      y ~ 1,
      prior(beta(2, 2), class = Intercept),
      iter = 10000, warmup = 500, cores = 3, chains = 3,
      control = list(adapt_delta = 0.9),
      seed = 7,
      file = "fits/fit07.02")
```

On page 179, Kruschke discussed *burn-in* steps within the Gibbs framework:

> The preliminary steps, during which the chain moves from its unrepresentative initial value to the modal region of the posterior, is called the *burn-in* period. For realistic applications, it is routine to apply a burn-in period of several hundred to several thousand steps.

For each HMC chain, the first $n$ iterations are called "warmups." In this example, $n = 500$ (i.e., `warmup = 500`). Within the Stan-HMC paradigm, [warmups are somewhat analogous to but not synonymous with burn-in iterations](https://andrewgelman.com/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/) as done by the Gibbs sampling in JAGS. But HMC warmups are like Gibbs burn-ins in that both are discarded and not used to describe the posterior. For more on warmup, check out McElreath's lecture, [starting here](https://www.youtube.com/watch?v=13mEekRdOcQ&t=75s&frags=pl%2Cwn) or, for more detail, the [*HMC Algorithm Parameters* section (15.2)](https://mc-stan.org/docs/2_29/reference-manual/hmc-algorithm-parameters.html) of the *Stan reference manual*, version 2.29 [@standevelopmentteamStanReferenceManual2022].

It appears that the upshot of all this is some of the packages in the Stan ecosystem don't make it easy to extract the warmup values. For example, the `brms::plot()` function excludes them from the trace plot without the option to include them.

```{r, fig.width = 8, fig.height = 1.25}
plot(fit7.2, widths = c(2, 3))
```

Notice how the $x$-axis on the trace plot ranges from 0 to 9,500. Now recall that our model code included `iter = 10000, warmup = 500`. Those 9,500 iterations in the trace plot are excluding the first 500 warmup iterations. This code is a little janky, but if you really want those warmup iterations, you can extract them from the `fit7.2` object like this.

```{r}
warmups <-
  c(fit7.2$fit@sim$samples[[1]]$b_Intercept[1:500], 
    fit7.2$fit@sim$samples[[2]]$b_Intercept[1:500], 
    fit7.2$fit@sim$samples[[3]]$b_Intercept[1:500]) %>% 
  # since these come from lists, here we'll convert them to a data frame
  as.data.frame() %>% 
  rename(b_Intercept = ".") %>% 
  # we'll need to recapture the iteration and chain information
  mutate(iter  = rep(1:500, times = 3),
         chain = factor(rep(1:3, each = 500), 
                        levels = c("1", "2", "3")))

warmups %>% 
  head()
```

The [**bayesplot** package](https://github.com/stan-dev/bayesplot) [@R-bayesplot; @gabry2019visualization] makes it easier to reproduce some of the plots in Figure 7.10.

```{r, message = F, warning = F}
library(bayesplot)
```

We'll reproduce the upper left panel with `mcmc_trace()`.

```{r, fig.width = 6, fig.height = 2, warning = F}
mcmc_trace(warmups, pars = "b_Intercept")
```

As an alternative, we can also extract the warmup draws from a `brm()` fit with the [**ggmcmc** package](https://cran.rstudio.com/package=ggmcmc) [@R-ggmcmc; @fernandezGGMCMCAnalysisofMCMC2016]. 

```{r, message = F, warning = F}
library(ggmcmc)
```

The **ggmcmc** package has a variety of convenience functions for working with MCMC chains. The `ggs()` function extracts the posterior draws, including `warmup`, and arranges them in a tidy tibble. With those in hand, we can now make a trace plot with warmup draws.

```{r, fig.width = 6, fig.height = 2, warning = F}
ggs(fit7.2) %>%
  filter(Iteration < 501 &
           Parameter == "b_Intercept") %>% 
  mutate(chain = factor(Chain)) %>% 
  
  ggplot(aes(x = Iteration, y = value, color = chain)) +
  geom_line() +
  scale_colour_brewer(direction = -1) +
  labs(title = "My custom trace plots with warmups via ggmcmc::ggs()",
       x = NULL, y = NULL) +
  theme_cowplot(font_size = 12)
```

You can make the same basic plot by pulling the posterior draws with `as_draws_df()`, as long as you include the `inc_warmup = TRUE` argument. Then you just have to be careful to `filter()` by the correct values in the `.iteration` meta-column.

```{r, fig.width = 6, fig.height = 2, warning = F}
draws <- as_draws_df(fit7.2, inc_warmup = T)

draws %>% 
  filter(.iteration <= 500) %>% 

  ggplot(aes(x = .iteration, y = b_Intercept, color = factor(.chain))) +
  geom_line() +
  scale_colour_brewer("chain", direction = -1) +
  labs(title = "My custom trace plots with warmups via as_draws_df()",
       x = NULL, y = NULL) +
  theme_cowplot(font_size = 12)
```

Anyway, it appears our HMC warmup draws found the posterior quite quickly. Here's the autocorrelation plot.

```{r, fig.width = 4, fig.height = 4}
mcmc_acf(warmups, pars = "b_Intercept", lags = 25)
```

Our autocorrelation plots indicate substantially lower autocorrelations yielded by HMC as implemented by Stan than what Kruschke generated with the MH algorithm. This is one of the reasons folks using HMC tend to use fewer iterations than those using MH or Gibbs.

If you were unhappy with the way `mcmc_acf()` defaults to faceting the plot by chain, you could always extract the data from the function and use them to make the plot the way you prefer. E.g., 

```{r, fig.width = 4, fig.height = 2, message = F, warning = F}
mcmc_acf(warmups)$data %>% 
  as_tibble() %>% 
  filter(Parameter == "b_Intercept") %>% 
  
  ggplot(aes(x = Lag, y = AC,
             color = Chain %>% as.factor())) +
  geom_hline(yintercept = 0, color = "white") +
  geom_point(size = 2/3) +
  geom_line() +
  scale_colour_brewer(direction = -1) +
  ylab("Autocorrelation") +
  theme_cowplot() +
  theme(legend.position = "none")
```

Here are the overlaid densities.

```{r, fig.width = 4, fig.height = 2}
mcmc_dens_overlay(warmups, pars = c("b_Intercept"))
```

The densities aren't great, but they still appear nicer than those in for the burn-in iterations in the text. With our warmups in their current state, I'm not aware how we might conveniently make a shrink factor plot, as seen in the lower left of Figure 7.10. So it goes...

Figure 7.11 examined the post-burn-in iterations. We'll follow suit with our post-warmup iterations. Note that for use with the `bayesplot::mcmc_` functions, we'll generally want our posterior draws in a data frame format. We an do this by extracting them with the `brms::as_draws_df()` function.

```{r, fig.width = 6, fig.height = 2}
draws <- as_draws_df(fit7.2)

draws %>% 
  mutate(chain = .chain) %>% 
  mcmc_trace(pars = "b_Intercept")
```


The autocorrelation plots:

```{r, fig.width = 4, fig.height = 4}
draws %>% 
  mutate(chain = .chain) %>% 
  mcmc_acf(pars = "b_Intercept", lags = 40)
```

As with the warmups, above, the post-warmup autocorrelation plots indicate substantially lower autocorrelations yielded by HMC as implemented by Stan than what Kruschke generated with the MH algorithm. This is one of the reasons folks using HMC tend to use fewer iterations than those using MH or Gibbs.

Here are the overlaid densities.

```{r, fig.width = 4, fig.height = 2}
draws %>% 
  mutate(chain = .chain) %>% 
  mcmc_dens_overlay(pars = "b_Intercept")
```

Now that we're focusing on the post-warmup iterations, we can make a shrink factor plot. We'll do so with the `coda::gelman.plot()` function. But you can't just dump your `brm()` fit object into `gelman.plot()`. It's the wrong object type. However, **brms** offers the `as.mcmc()` function which will convert `brm()` objects for use in functions from the **coda** package.

```{r, message = F, warning = F}
fit7.2_c <- as.mcmc(fit7.2)

fit7.2_c %>% glimpse()
```

With our freshly-converted `fit2_c` object in hand, we're ready to plot.

```{r, fig.width = 4, fig.height = 3.25}
gelman.plot(fit7.2_c[, "b_Intercept", ])
```

Looks great. As Kruschke explained on page 181, that plot is based on the potential scale reduction factor, or $\widehat R$ as it's typically referred to in the Stan ecosystem. Happily, **brms** reports the $\widehat R$ values for the major model parameters using `print()` or `summary()`.

```{r}
print(fit7.2)
```

Instead of a running value, you get a single statistic in the 'Rhat' column.

On page 181, Kruschke discussed how his overlaid density plots include the HDIs, by chain. The convenience functions from **brms** and **bayesplot** don't easily get us there. But we can get those easy enough with a little help `tidybayes::stat_halfeye()`. Note that for use with the various **tidybayes** functions, we'll want the posterior draws in the format returned by the `brms::as_draws_df()` function, but this time without the warmups included.

```{r, fig.width = 5, fig.height = 2.75}
draws <- as_draws_df(fit7.2) 

draws %>% 
  mutate(chain = factor(.chain)) %>% 
  
  ggplot(aes(x = b_Intercept, y = chain, fill = chain)) +
  stat_halfeye(point_interval = mode_hdi,
                .width = .95) +
  scale_fill_brewer(direction = -1) +
  scale_y_discrete(expand = expansion(mult = 0.035)) +
  theme_cowplot() +
  theme(legend.position = "none")
```

If you would like your chain-wise posterior densities to overlap, one way would be to play around with the `height` and `alpha` parameters within the `stat_halfeye()` function.

```{r, fig.width = 5, fig.height = 2}
draws %>% 
  mutate(chain = factor(.chain)) %>% 
  
  ggplot(aes(x = b_Intercept, y = chain, fill = chain)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95, 
               height = 9, alpha = 3/4) +
  scale_fill_brewer() +
  scale_y_discrete(expand = expansion(mult = 0.2)) +
  theme_cowplot() +
  theme(legend.position = "none")
```

### MCMC accuracy.

We want

> measures of chain length and accuracy that take into account the clumpiness of the chain. And for that, we need a measure of clumpiness. We will measure clumpiness as *autocorrelation*, which is simply the correlation of the chain values with the chain values $k$ steps ahead. There is a different autocorrelation for each choice of $k$. (p. 182, *emphasis* in the original)

We made a couple autocorrelation plots in the last section, but now it's time to get a better sense of what they mean. Just a little further in the text, Kruschke wrote: "The number of steps between the chain and its superimposed copy is called the *lag*" (p. 182, *emphasis* in the original). In case it's not clear, *lag* is a general term and can be applied to contexts outside of MCMC chains. You find it used sometimes in the longitudinal statistical literature, particularly for what are called timeseries models. Sadly, we won't be fitting those in this book. If you're curious, McElreath discussed them briefly in Chapter 16 of his [-@mcelreathStatisticalRethinkingBayesian2020] text.

We, however, will have to contend with a technical quirk within the **tidyverse**. The two **dplyr** functions relevant to lags are called `lag()` and `lead()`. Here's a little example to see how they work.

```{r}
tibble(step = 1:5) %>% 
  mutate(lag  = lag(step, n = 1),
         lead = lead(step, n = 1))
```

The original values are `1:5` in the `step` column. When you plug those into `lag(n = 1)`, you get back the value from the *previous row*. The opposite happens when you plug `step` into `lead(n = 1)`; there you get back the value from the *next row*. Returning to the block quote above, Kruschke wrote that autocorrelations are "the correlation of the chain values with the chain values $k$ *steps ahead*" (p. 182, *emphasis* added). Within the context of the `lag()` and `lead()` functions, their `n` arguments are what Kruschke called $k$, which is no big deal. Confusingly, though, since Kruschke wanted to focus on MCMC chains values that were "$k$ steps ahead," that means we'll have to use the `lead()` function, not `lag()`. Please don't fret about the semantics, here. Both Kruschke and the **dplyr** package are correct. We're lagging. But in this specific case, we'll be lagging our data with the `lead()` function. You can learn more about `lag()` and `lead()` [here](https://dplyr.tidyverse.org/reference/lead-lag.html).

On to the plot. If you read closely in the text (pp. 182--183), Kruschke didn't elaborate on which model he was showcasing in Figure 7.12. We just get the vague explanation that the "upper panels show an MCMC chain of 70 steps, superimposed with the same chain translated a certain number of steps ahead" (p. 182). To make our lives simple, let's just use the model we've been working with, `fit7.2`. Our sole parameter is the intercept which, as it turns out, will not be on the same scale as you see in the $y$-axis of Figure 7.12. So it goes... But anyways, our first step towards making our variant of the plot is to wrangle out posterior draws a bit. We'll call the wrangled data frame `lagged_draws`.

```{r, warning = F}
lagged_draws <- 
  draws %>% 
  filter(.chain == 1 & .draw < 71) %>% 
  select(b_Intercept, .draw) %>% 
  # sometimes the unlagged data are called lag_0
  rename(lag_0 = b_Intercept) %>% 
  # lags for three different levels of k
  mutate(lag_1  = lead(lag_0, n = 1), 
         lag_5  = lead(lag_0, n = 5),
         lag_10 = lead(lag_0, n = 10)) %>% 
  pivot_longer(-.draw, names_to = "key") 
```

Now here's our version of the top row.

```{r, fig.width = 10, fig.height = 3, warning = F, message = F}
p1 <- 
  lagged_draws %>% 
  filter(key %in% c("lag_0", "lag_1")) %>% 
  
  ggplot(aes(x = .draw, y = value, color = key)) +
  geom_point(aes(alpha = .draw == 50, shape = .draw == 50)) +
  geom_line(size = 1/3, alpha = 1/2) +
  annotate(geom = "text",
           x = 32, y = c(.825, .79),
           label = c("Original", "Lagged"),
           color = c("black", "steelblue")) +
  scale_color_manual(values = c("black", "steelblue")) +
  scale_y_continuous(breaks = 6:8 / 10, limits = c(.53, .84)) +
  labs(title = "Lag 1",
       x = "Index 1:70",
       y = expression(theta))

p2 <-
  lagged_draws %>% 
  filter(key %in% c("lag_0", "lag_5")) %>% 
  
  ggplot(aes(x = .draw, y = value, color = key)) +
  geom_point(aes(alpha = .draw == 50, shape = .draw == 50)) +
  geom_line(size = 1/3, alpha = 1/2) +
  scale_color_manual(values = c("black", "steelblue")) +
  scale_y_continuous(NULL, labels = NULL, 
                     breaks = 6:8 / 10, limits = c(.53, .84)) +
  labs(title = "Lag 5",
       x = "Index 1:70")
  
p3 <-
  lagged_draws %>% 
  filter(key %in% c("lag_0", "lag_10")) %>% 
  
  ggplot(aes(x = .draw, y = value, color = key)) +
  geom_point(aes(alpha = .draw == 50, shape = .draw == 50)) +
  geom_line(size = 1/3, alpha = 1/2) +
  scale_color_manual(values = c("black", "steelblue")) +
  scale_y_continuous(NULL, labels = NULL, 
                     breaks = 6:8 / 10, limits = c(.53, .84)) +
  labs(title = "Lag 10",
       x = "Index 1:70")

# combine
library(patchwork)

(p1 + p2 + p3) &
  scale_alpha_manual(values = c(1/2, 1)) &
  scale_shape_manual(values = c(1, 19)) &
  theme_cowplot() &
  theme(legend.position = "none") 
```

Here's the middle row for Figure 7.12.

```{r, fig.width = 10, fig.height = 3, warning = F, message = F}
p1 <-
  lagged_draws %>% 
  pivot_wider(names_from = key, values_from = value) %>% 
  
  ggplot(aes(x = lag_1, y = lag_0)) +
  stat_smooth(method = "lm") +
  geom_point(aes(alpha = .draw == 50, shape = .draw == 50))

p2 <-
  lagged_draws %>% 
  pivot_wider(names_from = key, values_from = value) %>% 
  
  ggplot(aes(x = lag_5, y = lag_0)) +
  stat_smooth(method = "lm") +
  geom_point(aes(alpha = .draw == 50, shape = .draw == 50)) +
  scale_y_continuous(NULL, labels = NULL)

p3 <-
  lagged_draws %>% 
  pivot_wider(names_from = key, values_from = value) %>% 
  
  ggplot(aes(x = lag_10, y = lag_0)) +
  stat_smooth(method = "lm") +
  geom_point(aes(alpha = .draw == 50, shape = .draw == 50)) +
  scale_y_continuous(NULL, labels = NULL)

# combine
(p1 + p2 + p3) & 
  scale_alpha_manual(values = c(1/2, 1)) &
  scale_shape_manual(values = c(1, 19)) &
  theme_cowplot() &
  theme(legend.position = "none")
```

For kicks and giggles, we used `stat_smooth()` to add an OLS regression line with its 95% frequentist confidence intervals to each plot.

If you want the Pearson's correlations among the lags, the `lowerCor()` function from the [**psych** package](https://CRAN.R-project.org/package=psych) [@R-psych] can be handy.

```{r, warning = F, message = F}
library(psych)

lagged_draws %>% 
  pivot_wider(names_from = key, values_from = value) %>% 
  select(-.draw) %>% 
  lowerCor(digits = 3, use = "pairwise.complete.obs")
```

For our version of the bottom of Figure 7.12, we'll use the `bayesplot::mcmc_acf_bar()` function to get the autocorrelation bar plot, by chain.

```{r, fig.width = 4, fig.height = 5}
draws %>% 
  mutate(chain = .chain) %>% 
  mcmc_acf_bar(pars = "b_Intercept",
               lags = 20)
```

All three rows of our versions for Figure 7.12 indicate in their own way how much lower our autocorrelations were than the ones in the text.

If you're curious of the effective sample sizes for the parameters in your **brms** models, just look at the model summary using either `summary()` or `print()`.

```{r}
print(fit7.2)
```

Look at the last two columns in the `Intercept` summary. Earlier versions of **brms** had one column named `Eff.Sample`, which reported the effect sample size as discussed by Kruschke. Starting with version 2.10.0, **brms** now returns `Bulk_ESS` and `Tail_ESS`, instead. These originate from a [-@vehtariRanknormalizationFoldingLocalization2021] [paper](https://doi.org/10.1214/20-BA1221) by Stan-team all-stars Vehtari, Gelman, Simpson, Carpenter, and Bürkner. From their paper, we read:

> When reporting quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3, we show that convergence of Markov chains is not uniform across the parameter space, that is, convergence might be different in the bulk of the distribution (e.g., for the mean or median) than in the tails (e.g., for extreme quantiles). We propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as bulk-ESS), which mainly assesses how well the centre of the distribution is resolved. Instead, these "tail-ESS" measures allow the user to estimate the MCSE for interval estimates. (pp. 672--673)

For more technical details, see the paper. The `Bulk_ESS` column in current versions of **brms** is what was previously referred to as `Eff.Sample`. This is what corresponds to what Kruschke meant when referring to effective sample size. Now rather than focusing solely on 'the center of the' posterior distribution' as indexed by `Bulk_ESS`, we also gauge the effective sample size in the posterior intervals using `Tail_ESS`.

Anyway, I'm not sure how to reproduce Kruschke's MCMC ESS simulation studies. My confusion comes from at least two levels. If you read in the text, Kruschke described his simulation as based on "MCMC chains from the normal distribution" (p. 184). Though I do know how to initialize HMC chains for a model on data from the normal distribution, I have no idea how one would initialize chains from the standard normal distribution, itself. Second, if you view Kruschke's simulation as based on a model which one could feasibly fit with **brms**, I don't know how one would specify "an ESS of 10,000" for each iteration of the simulation. This is because **brms** is set up to fit models with a fixed number of iterations, for which the ESS values will vary. Kruschke's simulation seems to be set in reverse. For more details on Kruschke's simulation, you'll just have to read through the text. Anyway, if you know how to fit such a simulation using **brms**, please share your code in my [GitHub issue #15](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/15).

If you're interested in the Monte Carlo standard error (MCSE) for your **brms** parameters, the easiest way is to tack `$fit` onto your fit object.

```{r}
fit7.2$fit
```

This returns an [rstan-like summary](https://CRAN.R-project.org/package=rstan/vignettes/stanfit-objects.html) [@standevelopmentteamAccessingContentsStanfit2022]. The 'se_mean' column is the MCSE.

### MCMC efficiency.

Kruschke wrote: "It is often the case in realistic applications that there is strong autocorrelation for some parameters, and therefore, an extremely long chain is required to achieve an adequate ESS or MCSE" (p. 187). As we'll see, this is generally less of a problem for HMC than for MH or Gibbs. But it does still crop up, particularly in complicated models. As he wrote on the following page, "one sampling method that can be relatively efficient is Hamiltonian Monte Carlo." Indeed.

## Summary

> Let's regain perspective on the forest of Bayesian inference after focusing on the trees of MCMC. Recall that the overarching goal of Bayesian analysis is identifying the credibility of parameter values in a descriptive model of data. Bayes' rule provides an exact mathematical formulation for the posterior distribution on the parameter values. But the exact form requires evaluation of an integral that might be intractable for realistically complex models. Therefore, we approximate the posterior distribution, to arbitrarily high accuracy, using MCMC methods. Because of recent developments in MCMC algorithms, software that cleverly applies them in complex models, and hardware that runs them incredibly quickly, we can now use MCMC methods to analyze realistically complex models that would have been impossible only a few decades ago. (pp. 188--189)

## Session info {-}

```{r}
sessionInfo()
```

```{r, message = F, warning = F, echo = F}
# here we'll remove our objects
rm(hdi_of_icdf, h, omega, d, my_mode_simulation, sim, num_days, positions, current, nslots, p_target, proposal_matrix, acceptance_matrix, from_idx, to_idx, diag_idx, time_idx, move_matrix, transition_matrix, position_vec, p, data, i, proposal, prob_accept_the_proposal, mu, sigma, proposed_jump, my_data, likelihood, prior_d, target_rel_prob, traj_length, trajectory, burn_in, n_accepted, n_rejected, my_metropolis, betas, theta_sequence, bernoulli_likelihood, n1, z1, n2, z2, theta_1_data, theta_2_data, d_prior, d_likelihood, bivariate_bernoulli_likelihood, my_bivariate_metropolis, mh, model_1, model_2, fit7.1a, fit7.1b, draws.a, draws.b, draws, z, n, fit7.2, warmups, fit7.2_c, lagged_draws, p1, p2, p3)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

## Footnote {-}

[^2]: We will get a proper introduction to the binomial probability distribution in [Section 11.1.2][With intention to fix $N$.].


<!--chapter:end:07.Rmd-->


# ~~JAGS~~ brms

```{r, echo = FALSE, cachse = FALSE}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

We, of course, will be using **brms** in place of JAGS.

## ~~JAGS~~ brms and its relation to R

In the opening paragraph in his [GitHub repository for **brms**](https://github.com/paul-buerkner/brms), Bürkner explained:

> The **brms** package provides an interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan, which is a C++ package for performing full Bayesian inference (see [http://mc-stan.org/](http://mc-stan.org/)). The formula syntax is very similar to that of the package lme4 to provide a familiar and simple interface for performing regression analyses. A wide range of response distributions are supported, allowing users to fit -- among others -- linear, robust linear, count data, survival, response times, ordinal, zero-inflated, and even self-defined mixture models all in a multilevel context. Further modeling options include non-linear and smooth terms, auto-correlation structures, censored data, missing value imputation, and quite a few more. In addition, all parameters of the response distribution can be predicted in order to perform distributional regression. Multivariate models (i.e., models with multiple response variables) can be fit, as well. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. Model fit can easily be assessed and compared with posterior predictive checks, cross-validation, and Bayes factors. (**emphasis** in the original)

Bürkner's **brms** repository includes many helpful links, such as to where [**brms** lives on CRAN](https://CRAN.R-project.org/package=brms), a [list of blog posts](https://paul-buerkner.github.io/blog/brms-blogposts/) highlighting **brms**, and [a forum](https://discourse.mc-stan.org) where users can ask questions about **brms** in specific or about Stan in general.

You can install the current official version of **brms** in the same way you would any other **R** package (i.e., `install.packages("brms", dependencies = T)`). If you want the current developmental version, you could download it from GitHub by executing the following.

```{r, eval = F}
if (!requireNamespace("devtools")) {
  install.packages("devtools")
}
devtools::install_github("paul-buerkner/brms")
```

## A complete example

We express the likelihood for our coin toss example as

$$y_{i} \sim \operatorname{Bernoulli}(\theta).$$

Our prior will be

$$\theta \sim \operatorname{Beta}(\alpha, \beta).$$

Kruschke pictured the relationships among the data, the likelihood, and the prior in the model diagram in Figure 8.2 (p. 196). If you're tricky, you can make those in **R**. I'm not going to walk my method out in great detail in this ebook. If you want a step-by-step tutorial, check my blog post, [*Make model diagrams, Kruschke style*](https://solomonkurz.netlify.app/post/make-model-diagrams-kruschke-style/). Here's the code for our version of Figure 8.2.

```{r, warning = F, message = F}
library(tidyverse)
library(patchwork)

# plot of a beta density
p1 <-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = "grey67") +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "beta",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(A)*', '*italic(B)", 
           size = 7, family = "Times", parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

## an annotated arrow
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")
p2 <-
  tibble(x    = .5,
         y    = 1,
         xend = .5,
         yend = 0) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = "text",
           x = .375, y = 1/3,
           label = "'~'",
           size = 10, family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# bar plot of Bernoulli data
p3 <-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = "grey67", width = .4) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "Bernoulli",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .94,
           label = "theta", 
           size = 7, family = "Times", parse = T) +
  xlim(-.75, 1.75) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p4 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p5 <-
  tibble(x     = 1,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = "Times") +
  xlim(0, 2) +
  theme_void()
```

```{r fig.height = 3.5, fig.width = 2}
layout <- c(
  area(t = 1, b = 2, l = 1, r = 1),
  area(t = 3, b = 3, l = 1, r = 1),
  area(t = 4, b = 5, l = 1, r = 1),
  area(t = 6, b = 6, l = 1, r = 1),
  area(t = 7, b = 7, l = 1, r = 1)
)

(p1 + p2 + p3 + p4 + p5) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

> Diagrams like Figure 8.2 should be scanned from the bottom up. This is because models of data always start with the data, then conceive of a likelihood function that describes the data values in terms of meaningful parameters, and finally determine a prior distribution over the parameters. (p. 196)

### Load data.

"Logically, models of data start with the data. We must know their basic scale and structure to conceive of a descriptive model" (p. 197).

Here we load the data with the `readr::read_csv()` function, the **tidyverse** version of base **R** `read.csv()`.

```{r, message = F}
my_data <- read_csv("data.R/z15N50.csv")
```

Unlike what Kruschke wrote about JAGS, the **brms** package does not require us to convert the data into a list. It can handle data in lists or data frames, of which [tibbles are a special case](https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html). Here are what the data look like.

```{r}
glimpse(my_data)
```

We might visualize them in a bar plot.

```{r, fig.width = 3, fig.height = 3, warning = F, message = F}
library(cowplot)

my_data %>% 
  mutate(y = y %>% as.character()) %>% 
  
  ggplot(aes(x = y)) +
  geom_bar() +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_minimal_hgrid()
```

For the past couple chapters, we've been using the `cowplot::theme_cowplot()` theme for our plots. The **cowplot** package, however, includes a few more. The `theme_minimal_grid()` is similar, but subtracts the axis lines and adds in minimalistic grid lines. In his [-@Wilke2020Themes] [*Themes* vignette](https://wilkelab.org/cowplot/articles/themes.html), however, Wilke showed vertical grid lines are often unsightly in bar plots. To avoid offending Wilke with our grid lines, we used the `theme_minimal_hgrid()` theme, which only added horizontal grid lines.

Anyway, if you wanted to compute "Ntotal", the number of rows in our tibble, one way is with `count()`.

```{r}
my_data %>% 
  count()
```

However, we're not going to do anything with an "Ntotal" value. For **brms**, the data are fine in their current data frame form. No need for a `dataList`.

### Specify model.

Let's open **brms**.

```{r, warning = F, message = F}
library(brms)
```

The **brms** package does not have code blocks following the JAGS format or the sequence in Kruschke's diagrams. Rather, its syntax is modeled in part after the popular frequentist mixed-effects package, [**lme4**](https://cran.r-project.org/web/packages/lme4/index.html). To learn more about how **brms** compares to **lme4**, see Bürkner's [-@burknerBrmsPackageBayesian2017] overview, [*brms: An R package for Bayesian multilevel models
using Stan*](https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf). 

The primary function in **brms** is `brm()`. Into this one function we will specify the data, the model, the likelihood function, the prior(s), and any technical settings such as the number of MCMC chains, iterations, and so forth. You can order the arguments in any way you like. My typical practice is to start with `data`, `family` (i.e., the likelihood function), the model `formula`, and my `prior`s. If there are any technical specifications such as the number of MCMC iterations I'd like to change from their default values, I usually do that last.

Here's how to fit the model.

```{r fit8.1}
fit8.1 <-
  brm(data = my_data, 
      family = bernoulli(link = identity),
      formula = y ~ 1,
      prior(beta(2, 2), class = Intercept),
      iter = 500 + 3334, warmup = 500, chains = 3,
      seed = 8,
      file = "fits/fit08.01")
```

Also note our use of the `file` argument. This automatically saved my fit object as an external file. You don't have to do that and you can avoid doing so by omitting the `file` argument. For a more detailed explanation of the `brms::brm()` function, spend some time with the `brm` section of the [**brms** reference manual](https://CRAN.R-project.org/package=brms) [@R-brms].

### Initialize chains.

In Stan, and in **brms** by extension, the initial values have default settings. In the [Initialization section of the Program Execution chapter](https://mc-stan.org/docs/2_29/reference-manual/initialization.html) in the *Stan reference manual, Version 2.29* [@standevelopmentteamStanReferenceManual2022] we read:

> If there are no user-supplied initial values, the default initialization strategy is to initialize the unconstrained parameters directly with values drawn uniformly from the interval $(−2, 2)$. The bounds of this initialization can be changed but it is always symmetric around $0$. The value of $0$ is special in that it represents the median of the initialization. An unconstrained value of $0$ corresponds to different parameter values depending on the constraints declared on the parameters.

In general, I do not recommend setting custom initial values in **brms** or Stan. Under the hood, Stan will transform the parameters to the unconstrained space in models where they are bounded. In our Bernoulli model, $\theta$ is bounded at 0 and 1. A little further down in the same section, we read:

> For parameters bounded above and below, the initial value of $0$ on the unconstrained scale corresponds to a value at the midpoint of the constraint interval. For probability parameters, bounded below by $0$ and above by $1$, the transform is the inverse logit, so that an initial unconstrained value of $0$ corresponds to a constrained value of $0.5$, $-2$ corresponds to $0.12$ and $2$ to $0.88$. Bounds other than $0$ and $1$ are just scaled and translated.

If you want to play around with this, have at it. In my experience, it sometimes helps to set these manually to zero, which you can do that by specifying `inits = 0` within `brm()`. But if you really want to experiment, you might check out my blog post, [*Don't forget your inits*](https://solomonkurz.netlify.app/post/2021-06-05-don-t-forget-your-inits/).

### Generate chains.

By default, **brms** will use 4 chains of 2,000 iterations each. The type of MCMC **brms** uses is Hamiltonian Monte Carlo (HMC). You can learn more about HMC at the Stan website, [https://mc-stan.org](https://mc-stan.org), which includes resources such as the [*Stan user's guide*](https://mc-stan.org/docs/2_29/stan-users-guide/index.html) [@standevelopmentteamStanUserGuide2022], the [*Stan reference manual*](https://mc-stan.org/docs/2_29/reference-manual/) [@standevelopmentteamStanReferenceManual2022], and a list of [tutorials](https://mc-stan.org/users/documentation/tutorials). McElreath has a [nice intro lecture](https://www.youtube.com/watch?v=BWEtS3HuU5A) on MCMC in general and HMC in particular. Michael Bentacourt has some good lectures on Stan and HMC, such as [here](https://youtu.be/pHsuIaPbNbY) and [here](https://www.youtube.com/watch?v=jUSZboSq1zg). And, of course, we will cover HMC with Kruschke in [Chapter 14][Stan].

Within each HMC chain, the first $n$ iterations are warmups. Within the Stan-HMC paradigm, [warmups are somewhat analogous to but not synonymous with burn-in iterations](https://andrewgelman.com/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/) as done by the Gibbs sampling in JAGS. But HMC warmups are like Gibbs burn-ins in that both are discarded and not used to describe the posterior. As such, the **brms** default settings yield 1,000 post-warmup iterations for each of the 4 HMC chains. However, we specified `iter = 500 + 3334, warmup = 500, chains = 3`. Thus instead of defaults, we have 3 HMC chains. Each chain has 500 + 3,334 = 3,834 total iterations, of which 500 were discarded `warmup` iterations.

To learn more about the warmup stage in Stan, check out the [HMC Algorithm Parameters section of the MCMC Sampling chapter](https://mc-stan.org/docs/2_26/reference-manual/hmc-algorithm-parameters.html) of the *Stan reference manual*.

### Examine chains.

The `brms::plot()` function returns a density and trace plot for each model parameter.

```{r, fig.width = 10, fig.height = 1.5}
plot(fit8.1)
```

Note how the `brms::plot()` function simply took our model fit object as input. Other post-processing functions will require us to pass the posterior draws in the form of a data frame. To get ready for them, here we'll save them as a data frame with the `as_draws_df()` function.

```{r}
draws <- as_draws_df(fit8.1) 
```

If you want to display each chain as its own density, you can use the handy `mcmc_dens_overlay()` function from the [**bayesplot** package](https://CRAN.R-project.org/package=bayesplot).

```{r, warning = F, message = F}
library(bayesplot)
```

Now we're ready to use our `draws` object within the `mcmc_dens_overlay()` function to return the overlaid densities.

```{r, fig.width = 4, fig.height = 2, message = F, warning = F}
draws %>% 
  mutate(chain = .chain) %>% 
  mcmc_dens_overlay(pars = vars(b_Intercept)) +
  theme_minimal_hgrid()
```

The `bayesplot::mcmc_acf()` function will give us the autocorrelation plots.

```{r, fig.width = 4, fig.height = 4}
draws %>% 
  mutate(chain = .chain) %>% 
  mcmc_acf(pars = vars(b_Intercept), lags = 35) +
  theme_minimal_hgrid()
```

With **brms** functions, we get a sole $\widehat R$ value for each parameter rather than a running vector.

```{r}
rhat(fit8.1)["b_Intercept"]
```

We'll have to employ `brms::as.mcmc()` and `coda::gelman.plot()` to make our running $\widehat R$ plot.

```{r, fig.width = 4, fig.height = 3}
fit8.1_c <- as.mcmc(fit8.1)

coda::gelman.plot(fit8.1_c[, "b_Intercept", ])
```

For whatever reason, many of the package developers within the Stan/**brms** ecosystem don't seem interested in shrink factor plots, like this.

#### ~~The `plotPost` function~~ How to plot your brms posterior distributions.

We'll get into plotting in just a moment. But before we do, here's a summary of the model.

```{r}
print(fit8.1)
```

To summarize a posterior in terms of central tendency, **brms** defaults to the mean value (i.e., the value in the 'Estimate' column of the `print()` output). In many of the other convenience functions, you can also request the median instead. For example, we can use the `robust = T` argument to get the 'Estimate' in terms of the median.

```{r}
posterior_summary(fit8.1, robust = T)
```

Across functions, the intervals default to 95%. With `print()` and `summary()` you can adjust the level with a `prob` argument. For example, here we'll use 50% intervals.

```{r}
print(fit8.1, prob = .5)
```

But in many other **brms** convenience functions, you can use the `probs` argument to request specific percentile summaries.

```{r}
posterior_summary(fit8.1, probs = c(.025, .25, .75, .975))
```

Regardless of what `prob` or `probs` levels you use, **brms** functions always return percentile-based estimates. All this central tendency and interval talk will be important in a moment...

When plotting the posterior distribution of a parameter estimated with **brms**, you typically do so working with the results of an object returned by one of the `as_draws_` functions. Recall we already saved those results as `draws`. Here's a look at `draws`.

```{r}
head(draws)
```

With `draws` in hand, we can use **ggplot2** to do the typical distributional plots, such as with `geom_histogram()`.

```{r, fig.width = 4, fig.height = 2.5, warning = F, message = F}
draws %>% 
  ggplot(aes(x = b_Intercept)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Theta via ggplot2::geom_histogram()",
       x = expression(theta)) +
  theme_minimal_hgrid() +
  theme(plot.title.position = "plot")
```

The `bayesplot::mcmc_areas()` function offers a nice way to depict the posterior densities, along with their percentile-based 50% and 95% ranges.

```{r, fig.width = 4, fig.height = 2.5, message = F, warning = F}
mcmc_areas(
  draws, 
  pars = vars(b_Intercept),
  prob = 0.5,
  prob_outer = 0.95,
  point_est = "mean"
) +
  scale_y_discrete(expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Theta via bayesplot::mcmc_areas()",
       x = expression(theta)) +
  theme_minimal_hgrid() +
  theme(plot.title.position = "plot")
```

**brms** doesn't have a convenient way to compute the posterior mode or HDIs. Base **R** is no help, either. But Matthew Kay's **tidybayes** package makes it easy to compute posterior modes and HDIs with handy functions like `stat_halfeye()` and `stat_histinterval()`.

```{r, fig.width = 4, fig.height = 2.5, warning = F, message = F}
library(tidybayes)

draws %>% 
  ggplot(aes(x = b_Intercept, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = expression(theta*" via tidybayes::stat_halfeye()"),
       x = expression(theta)) +
  theme_minimal_hgrid()

draws %>% 
  ggplot(aes(x = b_Intercept, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = expression(theta*" via tidybayes::stat_histinterval()"),
       x = expression(theta)) +
  theme_minimal_hgrid()
```

The `tidybayes::stat_halfeye()` function returns a density with a measure of the posterior's central tendency in a dot and one or multiple interval bands as horizontal lines at the base of the density. The `stat_histinterval()` function returns much the same, but replaces the density with a histogram. For both functions, the `point_interval = mode_hdi` argument allowed us to request the mode to be our measure of central tendency and the highest posterior density intervals to be our type intervals. With `.width = c(.95, .5)`, we requested our HDIs be at both the 95% and 50% levels.

If we wanted to be more congruent with Kruschke's plotting sensibilities, we could further modify `tidybayes::stat_histinterval()`.

```{r, fig.width = 4, fig.height = 2.5, warning = F, message = F}
# this is unnecessary, but makes for nicer x-axis breaks
my_breaks <-
  mode_hdi(draws$b_Intercept)[, 1:3] %>% 
  pivot_longer(everything(), values_to = "breaks") %>% 
  mutate(labels = breaks %>% round(digits = 3))

# here's the main plot code
draws %>% 
  ggplot(aes(x = b_Intercept, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = "steelblue", slab_color = "white",
                    breaks = 40, slab_size = .25, outline_bars = T) +
  scale_x_continuous(breaks = my_breaks$breaks,
                     labels = my_breaks$labels) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "tidybayes::stat_histinterval() all gussied up",
       x = expression(theta)) +
  theme_minimal_hgrid() +
  theme(title = element_text(size = 10.5))
```

With the `point_interval` argument within `stat_histinterval()` and related functions, we can request different combinations of measures of central tendency (i.e., mean, median, mode) and interval types (i.e., percentile-based and HDIs). Although all of these are legitimate ways to summarize a posterior, they can yield somewhat different results. For example, here we'll contrast our mode + HDI summary with a median + percentile-based interval summary using `tidybayes::stat_pointinterval()`.

```{r, fig.width = 4, fig.height = 1.5}
draws %>% 
  ggplot(aes(x = b_Intercept)) +
  stat_pointinterval(aes(y = 1), point_interval = median_qi, .width = c(.95, .5)) +
  stat_pointinterval(aes(y = 2), point_interval = mode_hdi,  .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = 1:2,
                     labels = c("median_qi", "mode_hdi")) +
  coord_cartesian(ylim = c(0, 3)) +
  labs(title = "Theta via tidybayes::stat_pointinterval()",
       x = expression(theta)) +
  theme_minimal_vgrid() +
  theme(axis.line.y.left = element_blank(),
        axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        plot.title.position = "plot",
        title = element_text(size = 10.5))
```

Similar, yet distinct. To get a sense of the full variety of ways **tidybayes** allows users to summarize and plot the results of a Bayesian model, check out Kay's [-@kaySlabIntervalStats2022] vignette, [*Slab + interval stats and geoms*](https://mjskay.github.io/ggdist/articles/slabinterval.html). Also, did you notice how we switched to `theme_minimal_vgrid()`? That's how we added the vertical grid lines in the absence of horizontal grid lines.

## Simplified scripts for frequently used analyses

A lot has happened in **R** for Bayesian analysis since Kruschke wrote his [-@kruschkeDoingBayesianData2015] text. In addition to our use of the **tidyverse**, the **brms**, **bayesplot**, and **tidybayes** packages offer an array of useful convenience functions. We can and occasionally will write our own. But really, the rich **R** ecosystem already has us pretty much covered.

## Example: Difference of biases

Here are our new data.

```{r, message = F}
my_data <- read_csv("data.R/z6N8z2N7.csv")

glimpse(my_data)
```

They look like this.

```{r, fig.width = 5, fig.height = 3, warning = F, message = F}
library(ggthemes)

my_data %>% 
  mutate(y = y %>% as.character()) %>% 
  
  ggplot(aes(x = y, fill = s)) +
  geom_bar(show.legend = F) +
  scale_fill_colorblind() +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_minimal_hgrid() +
  facet_wrap(~ s)
```

Note our use of the `scale_fill_colorblind()` function from the [**ggthemes** package](https://CRAN.R-project.org/package=ggthemes) [@R-ggthemes]. When you want to use color to emphasize different factor levels, such as `s`, it's a good idea to make sure those colors can be distinguished by folks who are colorblind. The `scale_fill_colorblind()` function provides a discrete palette of eight colorblind safe colors. If you'd prefer to use a different palette, but wand to make sure its accessible to folks with colorblindness, check out the [**colorblindr** package](https://github.com/clauswilke/colorblindr) [@R-colorblindr].

Here's our **ggplot2** version of the model diagram in Figure 8.5.

```{r, fig.width = 2, fig.height = 3.5}
# plot of a beta density
p1 <-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = "steelblue") +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "beta",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(A)*', '*italic(B)", 
           size = 7, family = "Times", parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# an annotated arrow
p2 <-
  tibble(x     = c(.35, .65),
         y     = 1/3,
         label = c("'~'", "italic(s)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# bar plot of Bernoulli data
p3 <-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = "steelblue", width = .4) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "Bernoulli",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .92,
           label = "theta[italic(s)]", 
           size = 7, family = "Times", parse = T) +
  xlim(-.75, 1.75) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p4 <-
  tibble(x     = c(.35, .65),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)*'|'*italic(s)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p5 <-
  tibble(x     = 1,
         y     = .5,
         label = "italic(y)[italic(i)*'|'*italic(s)]") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = "Times") +
  xlim(0, 2) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 1, r = 1),
  area(t = 3, b = 3, l = 1, r = 1),
  area(t = 4, b = 5, l = 1, r = 1),
  area(t = 6, b = 6, l = 1, r = 1),
  area(t = 7, b = 7, l = 1, r = 1)
)

# combine and plot!
(p1 + p2 + p3 + p4 + p5) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Here is how we might fit the model with `brms::brm()`.

```{r fit8.2}
fit8.2 <-
  brm(data = my_data, 
      family = bernoulli(identity),
      y ~ 0 + s,
      prior(beta(2, 2), class = b, lb = 0, ub = 1),
      iter = 2000, warmup = 500, cores = 4, chains = 4,
      seed = 8,
      file = "fits/fit08.02")
```

More typically, we'd parameterize the model as `y ~ 1 + s`. This form would yield an intercept and a slope. Behind the scenes, **brms** would treat the nominal `s` variable as an 0-1 coded dummy variable. One of the nominal levels would become the reverence category, depicted by the `Intercept`, and the difference between that and the other category would be the `s` slope. However, with our `y ~ 0 + s` syntax, we've suppressed the typical model intercept. The consequence is that each level of the nominal variable `s` gets its own intercept or [i] index, if you will. This is analogous to Kruschke's `y[i] ∼ dbern(theta[s[i]])` code.

Also, notice how we set `lb = 0` and `ub = 1` within the `prior()` function. These told **brms** that the lower and upper boundaries for our two intercept parameters are 0 and 1, respectively. The current version of **brms** will allow you to fit the model with out explicitly setting these. However, it will throw a warning. This is because there are no natural boundaries for the parameters of `class = b` on a model like this. Or at least, there aren't any that **brms** has been made aware of. However, those beta priors presume the lower and upper boundaries of 0 and 1. As we'll see later on in this ebook, we won't usually fit models using the identity link and the Bernoulli likelihood for data like these. But that's for another section...

All that aside, here are the chains.

```{r, fig.width = 10, fig.height = 3}
plot(fit8.2, widths = c(2, 3))
```

The model `summary()` is as follows:

```{r}
summary(fit8.2)
```

The `brms::pairs()` function gets us the bulk of Figure 8.6.

```{r, fig.width = 4.5, fig.height = 4}
pairs(fit8.2,
      off_diag_args = list(size = 1/3, alpha = 1/3))
```

But to get at that difference-score distribution, we'll have extract the posterior draws with `as_draws_df()`, make difference score with `mutate()`, and manually plot with **ggplot2**.

```{r}
draws <- as_draws_df(fit8.2)

draws <-
  draws %>% 
  rename(theta_Reginald = b_sReginald,
         theta_Tony     = b_sTony) %>% 
  mutate(`theta_Reginald - theta_Tony` = theta_Reginald - theta_Tony)

glimpse(draws)
```

```{r, fig.width = 10, fig.height = 3, message = F, warning = F}
long_draws <-
  draws %>% 
  select(starts_with("theta")) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("theta_Reginald", "theta_Tony", "theta_Reginald - theta_Tony"))) 
  
long_draws %>% 
  ggplot(aes(x = value, y = 0, fill = name)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    slab_color = "white", outline_bars = T,
                    normalize = "panels") +
  scale_fill_manual(values = colorblind_pal()(8)[2:4], breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme_minimal_hgrid() +
  facet_wrap(~ name, scales = "free")
```

Note how this time we used the `colorblind_pal()` function within `scale_fill_manual()` to manually select three of the of the `scale_fill_colorblind()` colors.

Anyway, here's a way to get the numeric summaries out of `post`.

```{r}
long_draws %>% 
  group_by(name) %>% 
  mode_hdi()
```

In this context, the `mode_hdi()` summary yields:

* `key` (i.e., the name we used to denote the parameters)
* `value` (i.e., the value of the measure of central tendency)
* `.lower` (i.e., the lower level of the 95% HDI)
* `.upper` (i.e., the upper level...)
* `.width` (i.e., what interval we used)
* `.point` (i.e., the type of measure of central tendency)
* `.interval` (i.e., the type of interval)

## Sampling from the prior distribution in ~~JAGS~~ brms

There are a few ways to sample from the prior distribution with **brms**. Here we'll do so by setting `sample_prior = "only"`. As a consequence, `brm()` will ignore the likelihood and return draws based solely on the model priors.

```{r fit8.3}
fit8.3 <-
  brm(data = my_data, 
      family = bernoulli(identity),
      y ~ 0 + s,
      prior = c(prior(beta(2, 2), class = b, coef = sReginald),
                prior(beta(2, 2), class = b, coef = sTony),
                # this just sets the lower and upper bounds
                prior(beta(2, 2), class = b, lb = 0, ub = 1)),
      iter = 2000, warmup = 500, cores = 4, chains = 4,
      sample_prior = "only",
      seed = 8,
      file = "fits/fit08.03")
```

Because we set `sample_prior = "only"`, the `as_draws_df()` function will now return draws from the priors.

```{r}
draws <- as_draws_df(fit8.3) %>% select(-lp__)

head(draws)
```

With our prior draws in hand, we're almost ready to make the prior histograms of Figure 8.7. But first we'll want to determine the $z/N$ values in order to mark them off in the plots. [You'll note Kruschke did so with gray plus marks in his.]

```{r, message = F}
my_data %>% 
  group_by(s) %>% 
  summarise(z = sum(y),
            N = n()) %>% 
  mutate(`z/N` = z / N)

levels <- c("theta_Reginald", "theta_Tony", "theta_Reginald - theta_Tony")

d_line <-
  tibble(value = c(.75, .286, .75 - .286),
         name  =  factor(c("theta_Reginald", "theta_Tony", "theta_Reginald - theta_Tony"), 
                         levels = levels))
```

Behold the histograms of Figure 8.7.

```{r, fig.width = 10, fig.height = 3, message = F, warning = F}
draws %>% 
  rename(theta_Reginald = b_sReginald,
         theta_Tony     = b_sTony) %>% 
  mutate(`theta_Reginald - theta_Tony` = theta_Reginald - theta_Tony) %>% 
  pivot_longer(contains("theta")) %>% 
  mutate(name = factor(name, levels = levels)) %>%
  
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = colorblind_pal()(8)[5], normalize = "panels") +
  geom_vline(data = d_line, 
             aes(xintercept = value), 
             linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = expression("The dashed vertical lines mark off "*italic(z[s])/italic(N[s]))) +
  theme_cowplot() +
  facet_wrap(~ name, scales = "free")
```

Here's how you might make the scatter plot.

```{r, fig.width = 3, fig.height = 3}
draws %>% 
  rename(theta_Reginald = b_sReginald,
         theta_Tony     = b_sTony) %>% 
  
  ggplot(aes(x = theta_Reginald, y = theta_Tony)) +
  geom_point(alpha = 1/4, color = colorblind_pal()(8)[6]) +
  coord_equal() +
  theme_minimal_grid()
```

Or you could always use a two-dimensional density plot with `stat_density_2d()`.

```{r, fig.width = 3, fig.height = 3, warning = F}
draws %>% 
  rename(theta_Reginald = b_sReginald,
         theta_Tony     = b_sTony) %>% 
  
  ggplot(aes(x = theta_Reginald, y = theta_Tony)) +
  stat_density_2d(aes(fill = stat(density)), 
                  geom = "raster", contour = F) +
  scale_fill_viridis_c(option = "B", breaks = NULL) +
  scale_x_continuous(expression(theta[1]), 
                     expand = c(0, 0), limits = c(0, 1),
                     breaks = 0:4 / 4, labels = c("0", ".25", ".5", ".75", "1")) +
  scale_y_continuous(expression(theta[2]), 
                     expand = c(0, 0), limits = c(0, 1),
                     breaks = 0:4 / 4, labels = c("0", ".25", ".5", ".75", "1")) +
  coord_equal() +
  theme_minimal_grid()
```

The **viridis** color palettes in functions like `scale_fill_viridis_c()` and `scale_fill_viridis_d()` are designed to be colorblind safe, too [@R-viridis].

## Probability distributions available in ~~JAGS~~  brms

> [**brms**] has a large collection of frequently used probability distributions that are built-in. These distributions include the beta, gamma, normal, Bernoulli, and binomial along with many others. A complete list of distributions, and their [**brms**] names, can be found in [Bürkner's [-@Bürkner2022Parameterization] vignette [*Parameterization of response distributions in brms*](https://CRAN.R-project.org/package=brms/vignettes/brms_families.html)]. [@kruschkeDoingBayesianData2015, pp. 213--214, **emphasis** added]

### Defining new likelihood functions.

In addition to all the likelihood functions listed in above mentioned vignette, you can also make your own likelihood functions. Bürkner explained the method in his [-@Bürkner2022Define] vignette, [*Define custom response distributions with brms*](https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html).

## Faster sampling with parallel processing in ~~runjags~~ `brms::brm()`

We don't need to open another package to sample in parallel in **brms**. In fact, we've already been doing that. Take another look at the code use used for the last model, `fit8.2`.

```{r, eval = F}
fit8.2 <-
  brm(data = my_data, 
      family = bernoulli(identity),
      y ~ 0 + s,
      prior(beta(2, 2), class = b, lb = 0, ub = 1),
      iter = 2000, warmup = 500, cores = 4, chains = 4,
      seed = 8,
      file = "fits/fit08.02")
```

See the `cores = 4, chains = 4` arguments? With that bit of code, we told `brms::brm()` we wanted 4 chains, which we ran in parallel across 4 cores.

## Tips for expanding ~~JAGS~~ brms models

I'm in complete agreement with Kruschke, here:

> Often, the process of programming a model is done is stages, starting with a simple model and then incrementally incorporating complexifications. At each step, the model is checked for accuracy and efficiency. This procedure of incremental building is useful for creating a desired complex model from scratch, for expanding a previously created model for a new application, and for expanding a model that has been found to be inadequate in a posterior predictive check. (p. 218)

## Session info {-}

```{r}
sessionInfo()
```

```{r, message = F, warning = F, echo = F}
# Here we'll remove our objects
rm(p1, my_arrow, p2, p3, p4, p5, layout, my_data, fit8.1, draws, fit8.1_c, estimate_mode, my_breaks, fit8.2, long_draws, fit8.3, d_line, levels)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:08.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

# Hierarchical Models

As Kruschke put it, "There are many realistic situations that involve meaningful hierarchical structure. Bayesian modeling software makes it straightforward to specify and analyze complex hierarchical models" [-@kruschkeDoingBayesianData2015, p. 221]. IMO, **brms** makes it even easier than JAGS. Further down, we read:

> The parameters at different levels in a hierarchical model are all merely parameters that coexist in a joint parameter space. We simply apply Bayes' rule to the joint parameter space, as we did for example when estimating two coin biases back in Figure 7.5, p. 167. To say it a little more formally with our parameters $\theta$ and $\omega$, Bayes' rule applies to the joint parameter space: $p(\theta, \omega | D) \propto p(D | \theta, \omega) p(\theta, \omega)$. What is special to hierarchical models is that the terms on the right-hand side can be factored into a chain of dependencies, like this:
>
> \begin{align*}
> p(\theta, \omega | D) & \propto p(D | \theta, \omega) \; p(\theta, \omega) \\
>                       & = p(D | \theta) \; p(\theta | \omega) \; p(\omega)
> \end{align*}
>
> The refactoring in the second line means that the data depend only on the value of $\theta$, in the sense that when the value $\theta$ is set then the data are independent of all other parameter values. Moreover, the value of $\theta$ depends on the value of $\omega$ and the value of $\theta$ is conditionally independent of all other parameters. Any model that can be factored into a chain of dependencies like [this] is a hierarchical model. (pp. 222--223)

## A single coin from a single mint

Recall from the last chapter that our likelihood is the Bernoulli distribution,

$$y_i \sim \operatorname{Bernoulli}(\theta).$$

We'll use the beta density for our prior distribution for $\theta$,

$$\theta \sim \operatorname{beta}(\alpha, \beta).$$

And we can re-express $\alpha$ and $\beta$ in terms of the mode $\omega$ and concentration $\kappa$, such that

$$\alpha = \omega(\kappa - 2) + 1 \;\;\; \textrm{and} \;\;\; \beta = (1 - \omega)(\kappa - 2) + 1.$$

As a consequence, we can re-express $\theta$ as

$$\theta \sim \operatorname{beta}(\omega(\kappa - 2) + 1, (1 - \omega)(\kappa - 2) + 1).$$

On page 224, Kruschke wrote: "The value of $\kappa$ governs how near $\theta$ is to $\omega$, with larger values of $\kappa$ generating values of $\theta$ more concentrated near $\omega$." To give a sense of that, we'll simulate 20 beta distributions, all with $\omega = .25$ but with $\theta$ increasing from 10 to 200, by 10. We'll then plot them with a little help from the [**ggridges** package](https://CRAN.R-project.org/package=ggridges) [@R-ggridges].

```{r, fig.width = 5, fig.height = 5, message = F, warning = F}
library(tidyverse)
library(cowplot)
library(ggridges)

beta_by_k <- function(k) {
  
  w <- .25
  
  tibble(x = seq(from = 0, to = 1, length.out = 1000)) %>% 
    mutate(theta = dbeta(x = x,
                         shape1 = w * (k - 2) + 1,
                         shape2 = (1 - w) * (k - 2) + 1))
  
}

tibble(k = seq(from = 10, to = 200, by = 10)) %>% 
  mutate(theta = map(k, beta_by_k)) %>% 
  unnest(theta) %>%
  
  ggplot(aes(x = x, y = k,
             height = theta,
             group = k, fill = k)) +
  geom_vline(xintercept = .25, color = "grey85", size = 1/2) +
  geom_ridgeline(size = 1/5, color = "grey92", scale = 2) +
  scale_fill_viridis_c(expression(kappa), option = "A") +
  scale_y_continuous(expression(kappa), breaks = seq(from = 10, to = 200, by = 10)) +
  xlab(expression(theta)) +
  theme_minimal_hgrid()
```

Holding $\omega$ constant, the density gets more concentrated around $\omega$ as $\kappa$ increases. But back to the text: "Now we make the essential expansion of our scenario into the realm of hierarchical models. Instead of thinking of $\omega$ as fixed by prior knowledge, we think of it as another parameter to be estimated" (p. 224). In the hierarchical model diagram of Figure 9.1, Kruschke depicted how we might treat $\omega$ as a parameter controlled by the prior distribution, $\operatorname{beta}(A_\omega, B_\omega)$. Here's our version of the diagram.

```{r, fig.width = 3.8, fig.height = 5, message = F}
library(ggforce)
library(patchwork)

p1 <-
  tibble(x = seq(from = .01, to = .99, by = .01),
       d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = "grey67") +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "beta",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(A[omega])*', '*italic(B[omega])", 
           size = 7, family = "Times", parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

p2 <-
  tibble(x = c(.5, .475, .26, .08, .06,
               .5, .55, .85, 1.15, 1.2),
         y = c(1, .7, .6, .5, .2,
               1, .7, .6, .5, .2),
         line = rep(letters[2:1], each = 5)) %>% 
  
  ggplot(aes(x = x, y = y)) +
  geom_bspline(aes(color = line),
               size = 2/3, show.legend = F) + 
  annotate(geom = "text",
           x = 0, y = .125,
           label = "omega(italic(K)-2)+1*', '*(1-omega)(italic(K)-2)+1",
           size = 7, parse = T, family = "Times", hjust = 0) +
  annotate(geom = "text",
           x = 1/3, y = .7,
           label = "'~'",
           size = 10, parse = T, family = "Times") +
  scale_color_manual(values = c("grey75", "black")) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) +
  ylim(0, 1) +
  theme_void()

p3 <-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = "grey67") +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "beta",
           size = 7) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

## an annotated arrow
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")

p4 <-
  tibble(x    = .5,
         y    = 1,
         xend = .5,
         yend = 0) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = "text",
           x = .375, y = 1/3,
           label = "'~'",
           size = 10, family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# bar plot of Bernoulli data
p5 <-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = "grey67", width = .4) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "Bernoulli",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .94,
           label = "theta", 
           size = 7, family = "Times", parse = T) +
  xlim(-.75, 1.75) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p6 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p7 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 1, r = 1),
  area(t = 4, b = 5, l = 1, r = 1),
  area(t = 3, b = 4, l = 1, r = 2),
  area(t = 6, b = 6, l = 1, r = 1),
  area(t = 7, b = 8, l = 1, r = 1),
  area(t = 9, b = 9, l = 1, r = 1),
  area(t = 10, b = 10, l = 1, r = 1)
)

# combine and plot!
(p1 + p3 + p2 + p4 + p5 + p6 + p7) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Note that whereas this model includes a hierarchical prior for $\omega$, the hyperparameter $K$ is fixed across cases.

### Posterior via grid approximation.

> When the parameters extend over a finite domain, and there are not too many of them, then we can approximate the posterior via grid approximation. In our present situation, we have the parameters $\theta$ and $\omega$ that both have finite domains, namely the interval $[0, 1]$. Therefore, a grid approximation is tractable and the distributions can be readily graphed. (p. 226)

Given $\alpha$ and $\beta$, we can compute the corresponding mode $\omega$. To foreshadow, consider $\text{beta}(2, 2)$.

```{r}
alpha <- 2
beta  <- 2

(alpha - 1) / (alpha + beta - 2)
```

That is, the mode of $\operatorname{beta}(2, 2)$ is $.5$.

We won't be able to make the wireframe plots on the left of Figure 9.2, but we can make the others. We'll make the initial data following Kruschke's (p. 226) formulas.

$$p(\theta, \omega) = p(\theta | \omega) \; p(\omega) = \operatorname{beta} \big (\theta | \omega (100 - 2) + 1, (1 - \omega) (100 - 2) + 1 \big ) \; \operatorname{beta}(\omega | 2, 2)$$

First, we'll make a custom function, `make_prior()` based on the formulas.

```{r}
make_prior <- function(theta, omega, alpha, beta, kappa) {
  
  # p(theta | omega)
  t <- dbeta(x = theta,
             shape1 =      omega  * (kappa - 2) + 1,
             shape2 = (1 - omega) * (kappa - 2) + 1)
  # p(omega)
  o <- dbeta(x = omega,
             shape1 = alpha,
             shape2 = beta)
  
  # p(theta, omega) = p(theta | omega) * p(omega)
  return(t * o)
  
}
```

Next we'll define the parameter space as a tightly-spaced sequence of values ranging from 0 to 1.

```{r}
parameter_space <- seq(from = 0, to = 1, by = .01)
```

Now we'll use `parameter_space` to define the ranges for the two variables, `theta` and `omega`, which we'll save in a tibble. We'll then sequentially feed those `theta` and `omega` values into our `make_prior()` while manually specifying the desired values for `alpha`, `beta`, and `kappa`.

```{r, fig.height = 3}
d <-
  # here we define the grid for our grid approximation
  crossing(theta = parameter_space,
           omega = parameter_space) %>%
  # compute the joint prior
  mutate(prior = make_prior(theta, omega, alpha = 2, beta = 2, kappa = 100)) %>% 
  # convert the prior from the density metric to the probability metric
  mutate(prior = prior / sum(prior))

head(d)
```

Now we're ready to plot the top middle panel of Figure 9.2.

```{r, fig.height = 3, warning = F, message = F}
d %>% 
  ggplot(aes(x = theta, y = omega, fill = prior)) +
  geom_raster(interpolate = T) +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta),
       y = expression(omega)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) +
  coord_equal() +
  theme_minimal_grid() +
  theme(legend.position = "none")
```

You could also make this with `geom_tile()`, but `geom_raster()` with the `interpolate = T` argument smooths the color transitions. Since we are going to be making a lot of plots like this in this chapter, we should consider streamlining our plotting code. In [Chapter 19](https://ggplot2-book.org/programming.html) of Wichkam's [-@wickhamGgplot2ElegantGraphics2016] *ggplot2: Elegant graphics for data analysis*, we learn how to make a custom geom. Here we'll use those skills to wrap the bulk of the plot code from above into a single geom we'll call `geom_2dd()`, for 2D-density plots.

```{r}
geom_2dd <- function(...) {
  
  list(
    geom_raster(interpolate = T),
    scale_fill_viridis_c(option = "A"),
    scale_x_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5),
    scale_y_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5),
    coord_equal(),
    theme_minimal_grid(...),
    theme(legend.position = "none")
  )
  
}
```

Try it out.

```{r, fig.height = 3, warning = F}
d %>% 
  ggplot(aes(x = theta, y = omega, fill = prior)) +
  geom_2dd() +
  labs(x = expression(theta),
       y = expression(omega))
```

If we collapse "the joint prior across $\theta$" (i.e., `group_by(omega)` and then `sum(prior)`), we plot the marginal distribution for $p(\omega)$ as seen in the top right panel.

```{r, fig.width = 3.5, fig.height = 3, warning = F, message = F}
library(viridis)

a_purple <- viridis_pal(option = "A")(9)[4]

d %>%
  group_by(omega) %>% 
  summarise(prior = sum(prior)) %>% 
  
  ggplot(aes(x = omega, y = prior)) +
  geom_area(fill = a_purple) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.035)) +
  labs(x = expression(omega),
       y = expression(Marginal~p(omega))) +
  coord_flip() +
  theme_cowplot() +
  panel_border() +
  theme(axis.line = element_blank())
```

Note how we loaded the [**viridis** package](https://github.com/sjmgarnier/viridis) [@R-viridis]. That gave us access to the `viridis_pal()` function, which will allow us to discretize the **viridis** palettes and save the color names as objects. In our case, we discretized the `"A"` palette into nine colors and saved the fourth as `a_purple`. Here's the color name.

```{r}
a_purple
```

We'll use that color in many of the plots to follow. It'll be something of a signature color for this chapter.

Anyway, since we are going to be making a lot of plots like this in this chapter, we'll make another custom geom called `geom_marginal()`.

```{r}
geom_marginal <- function(ul, ...) {
  
  list(
    geom_area(fill = viridis_pal(option = "A")(9)[4]),
    scale_x_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5),
    scale_y_continuous(expand = c(0, 0), limits = c(0, ul)),
    theme_cowplot(...),
    panel_border(),
    theme(axis.line = element_blank())
  )
  
}
```

Try it out.

```{r, fig.width = 3.5, fig.height = 3, message = F}
d %>%
  group_by(omega) %>% 
  summarise(prior = sum(prior)) %>% 
  
  ggplot(aes(x = omega, y = prior)) +
  geom_marginal(ul = 0.035) +
  labs(x = expression(omega),
       y = expression(Marginal~p(omega))) +
  coord_flip()
```

We'll follow a similar procedure to get the marginal probability distribution for `theta`.

```{r, fig.width = 3.5, fig.height = 3, message = F}
d %>%
  group_by(theta) %>% 
  summarise(prior = sum(prior)) %>% 
  
  ggplot(aes(x = theta, y = prior)) +
  geom_marginal(ul = 0.035) +
  labs(x = expression(theta),
       y = expression(Marginal~p(theta)))
```

We'll use the `filter()` function to take the two slices from the posterior grid. Since we're taking slices, we're no longer working with the joint probability distribution. As such, our two marginal prior distributions for `theta` no longer sum to 1, which means they're no longer in a probability metric. No worries. After we group by `omega`, we can simply divide `prior` by the `sum()` of `prior` which renormalizes the two slices "so that they are individually proper probability densities that sum to 1.0 over $\theta$" (p. 226).

```{r, fig.width = 3.5, fig.height = 3}
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  group_by(omega) %>% 
  mutate(prior = prior / sum(prior)) %>% 
  mutate(label = factor(str_c("omega==", omega), 
                        levels = c("omega==0.75", "omega==0.25"))) %>% 
  
  ggplot(aes(x = theta, y = prior)) +
  geom_marginal(ul = 0.095) +
  labs(x = expression(theta),
       y = expression(p(theta*"|"*omega))) +
  facet_wrap(~ label, ncol = 1, labeller = label_parsed)
```

As Kruschke pointed out at the top of page 228, these are indeed beta densities. Here's proof.

```{r, fig.width = 3.5, fig.height = 1.75}
# we'll want this for the annotation
text <-
  tibble(theta = c(.75, .25),
         y     = 10,
         label = c("beta(74.5, 25.5)", "beta(25.5, 74.5)"),
         omega = letters[1:2])

# here's the primary data for the plot
tibble(theta = rep(parameter_space, times = 2),
       alpha = rep(c(74.5, 25.5), each = 101),
       beta  = rep(c(25.5, 74.5), each = 101),
       omega = rep(letters[1:2],  each = 101)) %>%
  
  # the plot
  ggplot(aes(x = theta, fill = omega)) +
  geom_area(aes(y = dbeta(x = theta, shape1 = alpha, shape2 = beta))) +
  geom_text(data = text,
            aes(y = y, label = label, color = omega)) +
  scale_fill_viridis_d(option = "A", begin = 2/9, end = 6/9) +
  scale_color_viridis_d(option = "A", begin = 2/9, end = 6/9) +
  scale_x_continuous(expression(theta), expand = c(0, 0), limits = c(0, 1), 
                     breaks = 0:5 / 5) +
  scale_y_continuous("density", expand = c(0, 0), limits = c(0, 11)) +
  theme_cowplot() +
  panel_border() +
  theme(axis.line = element_blank(),
        legend.position = "none")
```

But back on track, we need the Bernoulli likelihood function for the lower three rows of Figure 9.2.

```{r}
bernoulli_likelihood <- function(theta, data) {
  n <- length(data)
  z <- sum(data)
  
  return(theta^z * (1 - theta)^(n - sum(data)))
}
```

Time to feed `theta` and our data into the `bernoulli_likelihood()` function, which will allow us to make the 2-dimensional density plot in the middle of Figure 9.2.

```{r, fig.height = 3, warning = F}
# define the data
n <- 12
z <- 9

trial_data <- rep(0:1, times = c(n - z, z))

# compute the likelihood
d <-
  d %>% 
  mutate(likelihood = bernoulli_likelihood(theta = theta, 
                                           data  = trial_data))

# plot
d %>%
  ggplot(aes(x = theta, y = omega, fill = likelihood)) +
  geom_2dd() +
  labs(x = expression(theta),
       y = expression(omega))
```

Note how this plot demonstrates how the likelihood is solely dependent on $\theta$; it's orthogonal to $\omega$. This is the visual consequence of Kruschke's Formula 9.6,

\begin{align*}
p (\theta, \omega | y) & = \frac{p (y | \theta, \omega) \; p (\theta, \omega)}{p (y)} \\
                       & = \frac{p (y | \theta) \; p (\theta | \omega) \; p (\omega)}{p (y)}.
\end{align*}

That is, in the second line of the equation, the probability of $y$ was only conditional on $\theta$. But the reason we call this a hierarchical model is because the probability of $\theta$ itself is conditioned on $\omega$. The prior itself had a prior.

From Formula 9.1, the posterior $p(\theta, \omega | D)$ is proportional to $p(D | \theta) \; p(\theta | \omega) \; p(\omega)$. Divide that by the normalizing constant and we'll have it in a proper probability metric. Recall that we've already saved the results of $p(\theta | \omega) \; p(\omega)$ in the `prior` column. So we just need to multiply `prior` by `likelihood` and divide by their sum.

Our first depiction will be the middle panel of the second row from the bottom.

```{r, fig.height = 3, warning = F}
d <-
  d %>% 
  mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) 

d %>% 
  ggplot(aes(x = theta, y = omega, fill = posterior)) +
  geom_2dd() +
  labs(x = expression(theta),
       y = expression(omega))
```

Although the likelihood was orthogonal to $\omega$, conditioning the prior for $\theta$ on $\omega$ resulted in a posterior that was conditioned on both $\theta$ and $\omega$.

Making the marginal plots for `posterior` is much like when making them for `prior`, above.

```{r, fig.width = 3.5, fig.height = 3, message = F}
# for omega
d %>%
  group_by(omega) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = omega, y = posterior)) +
  geom_marginal(ul = 0.035) +
  labs(x = expression(omega),
       y = expression(Marginal~p(omega*"|"*D))) +
  coord_flip()

# for theta
d %>%
  group_by(theta) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = theta, y = posterior)) +
  geom_marginal(ul = 0.035) +
  labs(x = expression(theta),
       y = expression(Marginal~p(theta*"|"*D))) +
  coord_cartesian()
```

Note that after we slice with `filter()`, the next two wrangling lines renormalize those posterior slices into probability metrics. That is, when we take a slice through the joint posterior at a particular value of $\omega$, and renormalize by dividing the sum of discrete probability masses in that slice, we get the conditional distribution $p(\theta | \omega, D)$.

```{r, fig.width = 3.5, fig.height = 3}
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  group_by(omega) %>% 
  mutate(posterior = posterior / sum(posterior)) %>% 
  mutate(label = factor(str_c("omega==", omega),
                        levels = c("omega==0.75", "omega==0.25"))) %>% 
  
  ggplot(aes(x = theta, y = posterior)) +
  geom_marginal(ul = 0.1) +
  labs(x = expression(theta),
       y = expression(p(theta*"|"*omega))) +
  facet_wrap(~ label, ncol = 1, labeller = label_parsed, scales = "free")
```

In the next example depicted in Figure 9.3, we consider what happens when we combine the same data of 9 heads out of 12 trials to the same Bernoulli likelihood $p(y | \theta)$, but his time with a much lower $K$ values expressing greater uncertainty in the $\operatorname{beta} \big (\theta | \omega (6 - 2) + 1, (1 - \omega) (6 - 2) + 1 \big )$ portion of the joint prior and with a more certain hyperprior for $\omega$, $\operatorname{beta}(\omega | 20, 20)$.

To repeat the process for Figure 9.3, we'll first compute the new joint prior.

```{r}
d <-
  crossing(theta = parameter_space,
           omega = parameter_space) %>%
  mutate(prior = make_prior(theta, omega, alpha = 20, beta = 20, kappa = 6)) %>% 
  mutate(prior = prior / sum(prior))
```

Here's the initial data and the 2-dimensional density plot for the prior, the middle plot in the top row of Figure 9.3.

```{r, fig.height = 3, warning = F}
d %>% 
  ggplot(aes(x = theta, y = omega, fill = prior)) +
  geom_2dd() +
  labs(x = expression(theta),
       y = expression(omega))
```

That higher certainty in $\omega$ resulted in a two-dimensional density plot where the values on the $y$-axis were concentrated near .5. This will have down-the-road consequences for the posterior. But before we get there, we'll average over `omega` and `theta` to plot their marginal prior distributions.

```{r, fig.width = 3.5, fig.height = 3, message = F}
# for omega
d %>%
  group_by(omega) %>% 
  summarise(prior = sum(prior)) %>% 
  
  ggplot(aes(x = omega, y = prior)) +
  geom_marginal(ul = 0.052) +
  labs(x = expression(omega),
       y = expression(Marginal~p(omega))) +
  coord_flip()

# for theta
d %>%
  group_by(theta) %>% 
  summarise(prior = sum(prior)) %>% 
  
  ggplot(aes(x = theta, y = prior)) +
  geom_marginal(ul = 0.039) +
  labs(x = expression(theta),
       y = expression(Marginal~p(theta)))
```

Here are the two short plots in the right panel of the second row from the top of Figure 9.3.

```{r, fig.width = 3.5, fig.height = 3}
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  group_by(omega) %>% 
  mutate(prior = prior / sum(prior)) %>% 
  mutate(label = factor(str_c("omega == ", omega), 
                        levels = c("omega == 0.75", "omega == 0.25"))) %>%
  
  ggplot(aes(x = theta, y = prior)) +
  geom_marginal(ul = 0.039) +
  labs(x = expression(theta),
       y = expression(p(theta*"|"*omega))) +
  facet_wrap(~ label, ncol = 1, labeller = label_parsed)
```

Now we're ready for the likelihood.

```{r, fig.height = 3, warning = F, message = F}
# compute
d <-
  d %>% 
  mutate(likelihood = bernoulli_likelihood(theta = theta, 
                                           data  = trial_data))

# plot
d %>%
  ggplot(aes(x = theta, y = omega, fill = likelihood)) +
  geom_2dd() +
  labs(x = expression(theta),
       y = expression(omega))
```

Now on to the posterior. Our first depiction will be the middle panel of the second row from the bottom of Figure 9.3. This will be $p(\theta, \omega | y)$.

```{r, fig.height = 3, warning = F}
# compute the posterior
d <-
  d %>% 
  mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) 

# plot
d %>% 
  ggplot(aes(x = theta, y = omega, fill = posterior)) +
  geom_2dd() +
  labs(x = expression(theta),
       y = expression(omega))
```

Here are the marginal plots for the two dimensions in our `posterior`.

```{r, fig.width = 3.5, fig.height = 3, message = F}
# for omega
d %>%
  group_by(omega) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = omega, y = posterior)) +
  geom_marginal(ul = 0.052) +
  labs(x = expression(omega),
       y = expression(Marginal~p(omega*"|"*D))) +
  coord_flip()

# for theta
d %>%
  group_by(theta) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = theta, y = posterior)) +
  geom_marginal(ul = 0.039) +
  labs(x = expression(theta),
       y = expression(Marginal~p(theta*"|"*D)))
```

And we'll finish off with the plots of Figure 9.3's lower right panel.

```{r, fig.width = 3.5, fig.height = 3}
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  group_by(omega) %>% 
  mutate(posterior = posterior / sum(posterior)) %>% 
  mutate(label = factor(str_c("omega==", omega),
                        levels = c("omega==0.75", "omega==0.25"))) %>%
  
  ggplot(aes(x = theta, y = posterior)) +
  geom_marginal(ul = 0.039) +
  labs(x = expression(theta),
       y = expression(p(theta*"|"*omega))) +
  facet_wrap(~ label, ncol = 1, labeller = label_parsed, scales = "free")
```

> In summary, Bayesian inference in a hierarchical model is merely Bayesian inference on a joint parameter space, but we look at the joint distribution (e.g., $p(\theta, \omega)$) in terms of its marginal on a subset of parameters (e.g., $p(\omega)$) and its conditional distribution for other parameters (e.g., p$p(\theta | \omega)$. We do this primarily because it is meaningful in the context of particular models. (p. 230)

## Multiple coins from a single mint

> What if we collect data from more than one coin created by the mint? If each coin has its own distinct bias $\theta_s$, then we are estimating a distinct parameter value for each coin, and using all the data to estimate $\omega$. (p. 230)

Kruschke broke down a model of this form with his diagram in Figure 9.4. Here's our version of that figure.

```{r, fig.width = 3.8, fig.height = 5.2, message = F}
p1 <-
  tibble(x = seq(from = .01, to = .99, by = .01),
       d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = a_purple) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "beta",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(A[omega])*', '*italic(B[omega])", 
           size = 7, family = "Times", parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

p2 <-
  tibble(x = c(.5, .475, .26, .08, .06,
               .5, .55, .85, 1.15, 1.2),
         y = c(1, .7, .6, .5, .2,
               1, .7, .6, .5, .2),
         line = rep(letters[2:1], each = 5)) %>% 
  
  ggplot(aes(x = x, y = y)) +
  geom_bspline(aes(color = line),
               size = 2/3, show.legend = F) + 
  annotate(geom = "text",
           x = 0, y = .125,
           label = "omega(italic(K)-2)+1*', '*(1-omega)(italic(K)-2)+1",
           size = 7, parse = T, family = "Times", hjust = 0) +
  annotate(geom = "text",
           x = 1/3, y = .7,
           label = "'~'",
           size = 10, parse = T, family = "Times") +
  scale_color_manual(values = c("grey75", "black")) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) +
  ylim(0, 1) +
  theme_void()

p3 <-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = a_purple) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "beta",
           size = 7) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# an annotated arrow
p4 <-
  tibble(x     = c(.35, .65),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(s)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# bar plot of Bernoulli data
p5 <-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = a_purple, width = .4) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "Bernoulli",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .92,
           label = "theta[italic(s)]", 
           size = 7, family = "Times", parse = T) +
  xlim(-.75, 1.75) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p6 <-
  tibble(x     = c(.35, .65),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)*'|'*italic(s)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p7 <-
  tibble(x     = 1,
         y     = .5,
         label = "italic(y)[italic(i)*'|'*italic(s)]") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = "Times") +
  xlim(0, 2) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 1, r = 1),
  area(t = 4, b = 5, l = 1, r = 1),
  area(t = 3, b = 4, l = 1, r = 2),
  area(t = 6, b = 6, l = 1, r = 1),
  area(t = 7, b = 8, l = 1, r = 1),
  area(t = 9, b = 9, l = 1, r = 1),
  area(t = 10, b = 10, l = 1, r = 1)
)

# plot!
(p1 + p3 + p2 + p4 + p5 + p6 + p7) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

The diagram accounts for multiple coins with the $s$ index.

### Posterior via grid approximation.

Now we have two coins,

> the full prior distribution is a joint distribution over three parameters: $\omega$, $\theta_1$, and $\theta_2$. In a grid approximation, the prior is specified as a three-dimensional (3D) array that holds the prior probability at various grid points in the 3D space. (p. 233)

The biases for both coins, $\theta_1$, and $\theta_2$, have the same prior $\operatorname{beta}\big (\theta _j| \omega (5 - 2) + 1, (1 - \omega)(5 - 2) + 1 \big) $, which, if it's not apparent, is marked by the rather uncertain $K = 5$. As in our first example depicted in Figure 9.2, we have a gentle hyperprior $\operatorname{beta}(\omega | 2, 2)$, which centers the posterior mode for $\omega$ at .5. To express this in plots, we're going to have to update our `make_prior()` function. It was originally designed to handle two dimensions, $\theta$ and $\omega$. But now we have to update it to handle our three dimensions.

```{r}
make_prior <- function(theta1, theta2, omega, alpha, beta, kappa) {
  
  # p(theta_1 | omega)
  t1 <- dbeta(x = theta1,
             shape1 =      omega  * (kappa - 2) + 1,
             shape2 = (1 - omega) * (kappa - 2) + 1)
  
  # p(theta_2 | omega)
  t2 <- dbeta(x = theta2,
             shape1 =      omega  * (kappa - 2) + 1,
             shape2 = (1 - omega) * (kappa - 2) + 1)
  
  # p(omega)
  o <- dbeta(x = omega,
             shape1 = alpha,
             shape2 = beta)
  
  # p(theta1, theta2, omega) = p(theta1 | omega) * p(theta2 | omega) * p(omega)
  return(t1 * t2 * o)
  
}
```

Let's make our new data object, `d`.

```{r}
d <-
  crossing(theta_1 = parameter_space,
           theta_2 = parameter_space,
           omega   = parameter_space) %>%
  mutate(prior = make_prior(theta_1, theta_2, omega, alpha = 2, beta = 2, kappa = 5)) %>% 
  # here we normalize
  mutate(prior = prior / sum(prior))

glimpse(d)
```

Unlike what Kruschke said in the text, we're not using a 3D data array. Rather, we're just using a tibble with which `prior` has been expanded across all possible dimensions of the three indexing variables: `theta_1`, `theta_2`, and `omega`. As you can see from the 'Rows' count, above, this makes for a very long tibble.

"Because the parameter space is 3D, a distribution on it cannot easily be displayed on a 2D page. Instead, Figure 9.5 shows various marginal distributions" (p. 234). The consequence of that is when we marginalize, we'll have to group by the two variables we'd like to retain for the plot. For example, the plots in the left and middle columns of the top row are the same save for their indices. So let's just do the plot for `theta_1`. In order to marginalize over `theta_2`, we'll need to `group_by(theta_1, omega)` and then `summarise(prior = sum(prior))`.

```{r, fig.height = 3, message = F, warning = F}
d %>% 
  group_by(theta_1, omega) %>% 
  summarise(prior = sum(prior)) %>% 

  ggplot(aes(x = theta_1, y = omega, fill = prior)) +
  geom_2dd() +
  labs(x = expression(theta[1]),
       y = expression(omega))
```

But we just have to average over `omega` and `theta_1` to plot their marginal prior distributions.

```{r, fig.width = 3.5, fig.height = 3, message = F}
# for omega
d %>%
  group_by(omega) %>% 
  summarise(prior = sum(prior)) %>% 
  
  ggplot(aes(x = omega, y = prior)) +
  geom_marginal(ul = 0.041) +
  labs(x = expression(omega),
       y = expression(p(omega))) +
  coord_flip()

# for theta
d %>%
  group_by(theta_1) %>% 
  summarise(prior = sum(prior)) %>% 

  ggplot(aes(x = theta_1, y = prior)) +
  geom_marginal(ul = 0.041) +
  labs(x = expression(theta[1]),
       y = expression(p(theta[1])))
```

Before we make the plots in the middle row of Figure 9.5, we need to add the likelihoods. Recall that we're presuming the coin flips contained in $D_1$ and $D_2$ are independent. Kruschke explained in [Section 7.4.1][Prior, likelihood and posterior for two biases.], that

> independence of the data across the two coins means that the data from coin 1 depend only on the bias in coin 1, and the data from coin 2 depend only on the bias in coin 2, which can be expressed formally as $p(y_1 | \theta_1, \theta_2) = p(y_1 | \theta_1)$ and $p(y_2 | \theta_1, \theta_2) = p(y_2 | \theta_2)$. (p. 164)

The likelihood function for our two series of coin flips is then

$$p(D | \theta_1, \theta_2) = \left ( \theta_1^{z_1} (1 - \theta_1) ^ {N_1 - z_1} \right ) \left ( \theta_2^{z_2} (1 - \theta_2) ^ {N_2 - z_2} \right ).$$

The upshot is we can compute the likelihoods for $D_1$ and $D_2$ separately and just multiply them together.

```{r}
# D1: 3 heads, 12 tails
n <- 15
z <- 3

trial_data_1 <- rep(0:1, times = c(n - z, z))

# D2: 4 heads, 1 tail
n <- 5
z <- 4

trial_data_2 <- rep(0:1, times = c(n - z, z))
d <-
  d %>% 
  mutate(likelihood_1 = bernoulli_likelihood(theta = theta_1, data = trial_data_1),
         likelihood_2 = bernoulli_likelihood(theta = theta_2, data = trial_data_2)) %>% 
  mutate(likelihood = likelihood_1 * likelihood_2)

head(d)
```

Now after a little `group_by()` followed by `summarise()` we can plot the two marginal likelihoods, the two plots in the middle row of Figure 9.5.

```{r, fig.height = 3, warning = F, message = F}
# likelihood_1
d %>%
  group_by(theta_1, omega) %>% 
  summarise(likelihood = sum(likelihood)) %>% 
  
  ggplot(aes(x = theta_1, y = omega, fill = likelihood)) +
  geom_2dd() +
  labs(x = expression(theta[1]),
       y = expression(omega))

# likelihood_2
d %>%
  group_by(theta_2, omega) %>% 
  summarise(likelihood = sum(likelihood)) %>% 
  
  ggplot(aes(x = theta_2, y = omega, fill = likelihood)) +
  geom_2dd() +
  labs(x = expression(theta[2]),
       y = expression(omega))
```

The likelihoods look good. Next we compute the posterior in the same way we've done before: multiply the prior and the likelihood and then divide by their sum in order to convert the results to a probability metric.

```{r, fig.height = 3, warning = F, message = F}
# compute
d <-
  d %>% 
  mutate(posterior = (prior * likelihood) / sum(prior * likelihood))

# posterior_1
d %>% 
  group_by(theta_1, omega) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = theta_1, y = omega, fill = posterior)) +
  geom_2dd() +
  labs(x = expression(theta[1]),
       y = expression(omega))

# posterior_2
d %>% 
  group_by(theta_2, omega) %>% 
  summarise(posterior = sum(posterior)) %>%
  
  ggplot(aes(x = theta_2, y = omega, fill = posterior)) +
  geom_2dd() +
  labs(x = expression(theta[2]),
       y = expression(omega))
```

Here's the right plot on the second row from the bottom, the posterior distribution for $\omega$.

```{r, fig.width = 3.5, fig.height = 3, message = F}
# for omega
d %>%
  group_by(omega) %>% 
  summarise(posterior = sum(posterior)) %>%
  
  ggplot(aes(x = omega, y = posterior)) +
  geom_marginal(ul = 0.041) +
  labs(x = expression(omega),
       y = expression(p(omega*"|"*D))) +
  coord_flip()
```

Now here are the marginal posterior plots on the bottom row of Figure 9.5.

```{r, fig.width = 3.5, fig.height = 3, message = F}
# for theta_1
d %>%
  group_by(theta_1) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = theta_1, y = posterior)) +
  geom_marginal(ul = 0.041) +
  labs(x = expression(theta[1]),
       y = expression(p(theta[1]*"|"*D)))

# for theta_2
d %>%
  group_by(theta_2) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = theta_2, y = posterior)) +
  geom_marginal(ul = 0.041) +
  labs(x = expression(theta[2]),
       y = expression(p(theta[2]*"|"*D)))
```

We'll do this dog and pony one more time for Figure 9.6. Keeping the data and likelihood constant, we now set $K = 75$ and but retain both $A_\omega = 2$ and $B_\omega = 2$. First, we make our new data object, `d`.

```{r}
d <-
  crossing(theta_1 = parameter_space,
           theta_2 = parameter_space,
           omega   = parameter_space) %>%
  mutate(prior = make_prior(theta_1, theta_2, omega, alpha = 2, beta = 2, kappa = 75)) %>% 
  mutate(prior = prior / sum(prior))
```

Again, note how the only thing we changed from the last time was increasing `kappa` to 75. Also like last time, the plots in the left and middle columns of the top row are the same save for their indices. But unlike last time, we'll make both in preparation for a grand plotting finale. You'll see.

One more step: for the 2D density plots in this section, we'll omit the `coord_equal()` line from our custom `geom_2dd()` geom. This will help us with the formatting of our final plot.

```{r}
geom_2dd <- function(...) {
  
  list(
    geom_raster(interpolate = T),
    scale_fill_viridis_c(option = "A"),
    scale_x_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5),
    scale_y_continuous(expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5),
    # coord_equal(),
    theme_minimal_grid(...),
    theme(legend.position = "none")
  )
  
}
```

Okay, here are our two 2D prior density plots.

```{r, fig.height = 3, fig.width = 3.25, message = F, warning = F}
p11 <-
  d %>% 
  group_by(theta_1, omega) %>% 
  summarise(prior = sum(prior)) %>% 
  ggplot(aes(x = theta_1, y = omega, fill = prior)) +
  geom_2dd(font_size = 10) +
  annotate(geom = "text", x = .05, y = .925, 
           label = "p(list(theta[1], omega))", 
           parse = T, color = "white", hjust = 0) +
  labs(x = expression(theta[1]),
       y = expression(omega))

p12 <-
  d %>% 
  group_by(theta_2, omega) %>% 
  summarise(prior = sum(prior)) %>% 
  ggplot(aes(x = theta_2, y = omega, fill = prior)) +
  geom_2dd(font_size = 10) +
  annotate(geom = "text", x = .05, y = .925, 
           label = "p(list(theta[2], omega))", 
           parse = T, color = "white", hjust = 0) +
  labs(x = expression(theta[2]),
       y = expression(omega))

p11
p12
```

Now we'll average over `omega` and `theta` to plot their marginal prior distributions.

```{r, fig.width = 3.5, fig.height = 3, message = F}
# for omega
p13 <-
  d %>%
  group_by(omega) %>% 
  summarise(prior = sum(prior)) %>% 
  
  ggplot(aes(x = omega, y = prior)) +
  geom_marginal(ul = 0.041, font_size = 10) +
  labs(x = expression(omega),
       y = expression(p(omega))) +
  coord_flip()

# for theta_1
p21 <-
  d %>%
  group_by(theta_1) %>% 
  summarise(prior = sum(prior)) %>% 

  ggplot(aes(x = theta_1, y = prior)) +
  geom_marginal(ul = 0.041, font_size = 10) +
  labs(x = expression(theta[1]),
       y = expression(p(theta[1])))

# for theta_2
p22 <-
  d %>%
  group_by(theta_2) %>% 
  summarise(prior = sum(prior)) %>% 

  ggplot(aes(x = theta_2, y = prior)) +
  geom_marginal(ul = 0.041, font_size = 10) +
  labs(x = expression(theta[2]),
       y = expression(p(theta[2])))

p13
p21
p22
```

Let's get those likelihoods in there and plot.

```{r, fig.height = 3, fig.width = 3.25, warning = F, message = F}
# D1: 3 heads, 12 tails
n <- 15
z <- 3

trial_data_1 <- rep(0:1, times = c(n - z, z))

# D2: 4 heads, 1 tail
n <- 5
z <- 4

trial_data_2 <- rep(0:1, times = c(n - z, z))

# compute the likelihoods
d <-
  d %>% 
  mutate(likelihood_1 = bernoulli_likelihood(theta = theta_1, data = trial_data_1),
         likelihood_2 = bernoulli_likelihood(theta = theta_2, data = trial_data_2)) %>% 
  mutate(likelihood = likelihood_1 * likelihood_2)

# plot likelihood_1
p31 <-
  d %>%
  group_by(theta_1, omega) %>% 
  summarise(likelihood = sum(likelihood)) %>% 
  
  ggplot(aes(x = theta_1, y = omega, fill = likelihood)) +
  geom_2dd(font_size = 10) +
  labs(x = expression(theta[1]),
       y = expression(omega))

# plot likelihood_2
p32 <-
  d %>%
  group_by(theta_2, omega) %>% 
  summarise(likelihood = sum(likelihood)) %>% 
  
  ggplot(aes(x = theta_2, y = omega, fill = likelihood)) +
  geom_2dd(font_size = 10) +
  labs(x = expression(theta[2]),
       y = expression(omega))

p31
p32
```

Compute the posterior and make the left and middle plots of the second row to the bottom of Figure 9.6.

```{r, fig.height = 3, fig.width = 3.25, warning = F, message = F}
d <-
  d %>% 
  mutate(posterior = (prior * likelihood) / sum(prior * likelihood))

# posterior_1
p41 <-
  d %>% 
  group_by(theta_1, omega) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = theta_1, y = omega, fill = posterior)) +
  geom_2dd(font_size = 10) +
  annotate(geom = "text", x = .05, y = .925, 
           label = expression(p(list(theta[1], omega)*"|"*D)), 
           parse = T, color = "white", hjust = 0) +
  labs(x = expression(theta[1]),
       y = expression(omega))

# posterior_2
p42 <-
  d %>% 
  group_by(theta_2, omega) %>% 
  summarise(posterior = sum(posterior)) %>%
  
  ggplot(aes(x = theta_2, y = omega, fill = posterior)) +
  geom_2dd(font_size = 10) +
  annotate(geom = "text", x = .05, y = .925, 
           label = expression(p(list(theta[2], omega)*"|"*D)), 
           parse = T, color = "white", hjust = 0) +
  labs(x = expression(theta[2]),
       y = expression(omega))

p41
p42
```

Here's the right plot on the same row, the posterior distribution for $\omega$.

```{r, fig.width = 3.5, fig.height = 3, message = F}
# for omega
p43 <-
  d %>%
  group_by(omega) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = omega, y = posterior)) +
  geom_marginal(ul = 0.041, font_size = 10) +
  labs(x = expression(omega),
       y = expression(p(omega*"|"*D))) +
  coord_flip()

p43
```

Finally, here are the marginal posterior plots on the bottom row of Figure 9.6.

```{r, fig.width = 3.5, fig.height = 3, message = F}
# for theta_1
p51 <-
  d %>%
  group_by(theta_1) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = theta_1, y = posterior)) +
  geom_marginal(ul = 0.041, font_size = 10) +
  labs(x = expression(theta[1]),
       y = expression(p(theta[1]*"|"*D)))

# for theta_2
p52 <-
  d %>%
  group_by(theta_2) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = theta_2, y = posterior)) +
  geom_marginal(ul = 0.041, font_size = 10) +
  labs(x = expression(theta[2]),
       y = expression(p(theta[2]*"|"*D)))

p51
p52
```

Did you notice how we saved each of plot from this last batch as objects? For our grand finale for this subsection, we'll be stitching all those subplots together using syntax from the **patchwork** package. But before we do, we need to define three more subplots: the subplots with the annotation.

```{r, fig.width = 3, fig.height = 3}
text <-
  tibble(x = 1,
         y = 10:8,
         label = c("Prior", "list(A[omega]==2, B[omega]==2)", "K==75"),
         size = c(2, 1, 1))

p23 <-
  text %>% 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(aes(size = size), 
            parse = T, hjust = 0, show.legend = F) +
  scale_size_continuous(range = c(3.5, 5.5)) +
  coord_cartesian(xlim = c(1, 2),
                  ylim = c(4, 11)) +
  theme_cowplot(font_size = 10) +
  theme(axis.line = element_blank(),
        axis.text = element_text(color = "white"),
        axis.ticks = element_blank(),
        text = element_text(color = "white"))

text <-
  tibble(x = 1,
         y = 10:8,
         label = c("Likelihood", "D1: 3 heads, 12 tails", "D2: 4 heads, 1 tail"),
         size = c(2, 1, 1))

p33 <-
  text %>% 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(aes(size = size), 
            hjust = 0, show.legend = F) +
  scale_size_continuous(range = c(3.5, 5.5)) +
  coord_cartesian(xlim = c(1, 2),
                  ylim = c(4, 11)) +
  theme_cowplot(font_size = 10) +
  theme(axis.line = element_blank(),
        axis.text = element_text(color = "white"),
        axis.ticks = element_blank(),
        text = element_text(color = "white"))

p53 <-
  ggplot() +
  annotate(geom = "text", x = 1, y = 10, 
           label = "Posterior", size = 6, hjust = 0) +
  coord_cartesian(xlim = c(1, 2),
                  ylim = c(3, 11)) +
  theme_cowplot(font_size = 10) +
  theme(axis.line = element_blank(),
        axis.text = element_text(color = "white"),
        axis.ticks = element_blank(),
        text = element_text(color = "white"))
```

Okay, let's make the full version of Figure 9.6.

```{r, fig.width = 6.5, fig.height = 9, warning = F, message = F}
(p11 / p21 / p31 / p41 / p51) | (p12 / p22 / p32 / p42 / p52) | (p13 / p23 / p33 / p43 / p53)
```

Oh mamma!

> The grid approximation displayed in Figures 9.5 and 9.6 used combs of only [101] points on each parameter ($\omega$, $\theta_1$, and $\theta_2$). This means that the 3D grid had [101^3^ = 1,030,301] points, which is a size that can be handled easily on an ordinary desktop computer of the early 21st century. It is interesting to remind ourselves that the grid approximation displayed in Figures 9.5 and 9.6 would have been on the edge of computability 50 years ago, and would have been impossible 100 years ago. The number of points in a grid approximation can get hefty in a hurry. If we were to expand the example by including a third coin, with its parameter $\theta_3$, then the grid would have [101^4^ = 104,060,401] points, which already strains small computers. Include a fourth coin, and the grid contains over [10 billion] points. Grid approximation is not a viable approach to even modestly large problems, which we encounter next. (p. 235)

In case you didn't catch it, we used different numbers of points to evaluate each parameter. Whereas Kruschke indicated in the text he only used 50, we used 101. That value of 101 came from how we defined our `parameter_space` with the code `seq(from = 0, to = 1, by = .01)`. The reason we used a more densely-packed parameter space was to get smoother-looking 2D density plots.

### A realistic model with MCMC.

In this section, Kruschke freed up the previously fixed value of $K$, now letting $\kappa$ vary hierarchically with a gamma prior. He depicted the model in Figure 9.7. Here's our version of the figure.

```{r, fig.width = 3.85, fig.height = 5, message = F}
# a beta density
p1 <-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = a_purple) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "beta",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(A[omega])*', '*italic(B[omega])", 
           size = 7, family = "Times", parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# a gamma density
p2 <-
  tibble(x = seq(from = 0, to = 5, by = .01),
         d = (dgamma(x, 1.75, .85) / max(dgamma(x, 1.75, .85)))) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = a_purple) +
  annotate(geom = "text",
           x = 2.5, y = .2,
           label = "gamma",
           size = 7) +
  annotate(geom = "text",
           x = 2.5, y = .6,
           label = "list(italic(S)[kappa], italic(R)[kappa])",
           size = 7, family = "Times", parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

p3 <-
  tibble(x = c(.5, .475, .26, .08, .06,
               .5, .55, .85, 1.15, 1.175,
               1.5, 1.4, 1, .25, .2,
               1.5, 1.49, 1.445, 1.4, 1.39),
         y = c(1, .7, .6, .5, .2,
               1, .7, .6, .5, .2,
               1, .7, .6, .5, .2,
               1, .75, .6, .45, .2),
         line = rep(letters[2:1], each = 5) %>% rep(., times = 2),
         plot = rep(1:2, each = 10)) %>% 
  
  ggplot(aes(x = x, y = y, group = interaction(plot, line))) +
  geom_bspline(aes(color = line),
               size = 2/3, show.legend = F) + 
  annotate(geom = "text",
           x = 0, y = .1,
           label = "omega(kappa-2)+1*', '*(1-omega)(kappa-2)+1",
           size = 7, parse = T, family = "Times", hjust = 0) +
  annotate(geom = "text",
           x = c(1/3, 1.15), y = .7,
           label = "'~'",
           size = 10, parse = T, family = "Times") +
  scale_color_manual(values = c("grey75", "black")) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) +
  ylim(0, 1) +
  theme_void()

# another beta density
p4 <-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = a_purple) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "beta",
           size = 7) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# an annotated arrow
p5 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(s)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = 0.5, xend = 0.5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# bar plot of Bernoulli data
p6 <-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = a_purple, width = .4) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "Bernoulli",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .94,
           label = "theta", 
           size = 7, family = "Times", parse = T) +
  xlim(-.75, 1.75) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p7 <-
  tibble(x     = c(.35, .65),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)*'|'*italic(s)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p8 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])['|'][italic(s)]") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 1, r = 1),
  area(t = 1, b = 2, l = 2, r = 2),
  area(t = 4, b = 5, l = 1, r = 1),
  area(t = 3, b = 4, l = 1, r = 2),
  area(t = 6, b = 6, l = 1, r = 1),
  area(t = 7, b = 8, l = 1, r = 1),
  area(t = 9, b = 9, l = 1, r = 1),
  area(t = 10, b = 10, l = 1, r = 1)
)

# plot!
(p1 + p2 + p4 + p3 + p5 + p6 + p7 + p8) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

"Because the value of $\kappa − 2$ must be non-negative, the prior distribution on $\kappa − 2$ must not allow negative values" (p. 237). Gamma is one of the distributions with that property. The gamma distribution is defined by two parameters, its *shape* and *rate*. To get a sense of how those play out, here' a look at the gamma densities of Figure 9.8.

```{r, fig.width = 6, fig.height = 5}
# how many points do you want in your sequence of x values?
length <- 150

# wrangle
tibble(shape = c(.01, 1.56, 1, 6.25),
       rate  = c(.01, .0312, .02, .125)) %>% 
  expand(nesting(shape, rate),
         x = seq(from = 0, to = 200, length.out = length)) %>% 
  mutate(mean = shape * 1 / rate,
         sd   = sqrt(shape * (1 / rate)^2)) %>% 
  mutate(label = str_c("shape = ", shape, ", rate = ", rate, 
                       "\nmean = ", mean, ", sd = ", round(sd, 4))) %>%
  
  # plot
  ggplot(aes(x = x, y = dgamma(x = x, shape = shape, rate = rate))) +
  geom_area(aes(fill = label)) +
  scale_fill_viridis_d(option = "A", end = .9, breaks = NULL) +
  scale_x_continuous(expression(kappa), expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(expression(p(kappa*"|"*s*","*r)), breaks = c(0, .01, .02),
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 150)) +
  theme_cowplot(line_size = 0) +
  panel_border() +
  facet_wrap(~ label)
```

You can find the formulas for the mean and $SD$ for a given gamma distribution [here](https://astrostatistics.psu.edu/su07/R/html/stats/html/GammaDist.html). We used those formulas in the second `mutate()` statement for the data-prep stage of that last figure.

Using $s$ for shape and $r$ for rate, Kruschke's Equations 9.7 and 9.8 are as follows:

$$
s = \frac{\mu^2}{\sigma^2} \;\;\; \text{and} \;\;\; r = \frac{\mu}{\sigma^2} \;\;\; \text{for mean} \;\;\; \mu > 0 \\
s = 1 + \omega r \;\;\; \text{where} \;\;\; r = \frac{\omega + \sqrt{\omega^2 + 4\sigma^2}}{2\sigma^2} \;\;\; \text{for mode} \;\;\; \omega > 0.
$$

With those in hand, we can follow Kruschke's `DBDA2E-utilities.R` file to make a couple convenience functions.

```{r}
gamma_s_and_r_from_mean_sd <- function(mean, sd) {
  if (mean <= 0) stop("mean must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  shape <- mean^2 / sd^2
  rate  <- mean   / sd^2
  return(list(shape = shape, rate = rate))
}

gamma_s_and_r_from_mode_sd <- function(mode, sd) {
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate  <- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape <- 1 + mode * rate
  return(list(shape = shape, rate = rate))
}
```

They're easy to put to use:

```{r}
gamma_s_and_r_from_mean_sd(mean = 10, sd = 100)
gamma_s_and_r_from_mode_sd(mode = 10, sd = 100)
```

Here's a more detailed look at the structure of their output.

```{r}
gamma_param <- gamma_s_and_r_from_mode_sd(mode = 10, sd = 100)

str(gamma_param)
```

### Doing it with ~~JAGS~~ brms.

Unlike JAGS, the **brms** `formula` will not correspond as closely to the schematic in Figure 9.7. You'll see in just a bit.

### Example: Therapeutic touch.

Load the data from the `TherapeuticTouchData.csv` file [see @rosaCloseLookTherapeutic1998].

```{r, warning = F, message = F}
my_data <- read_csv("data.R/TherapeuticTouchData.csv")

glimpse(my_data)
```

Here are what the data look like.

```{r, fig.width = 10, fig.height = 4}
my_data %>% 
  mutate(y = y %>% as.character()) %>% 
  
  ggplot(aes(y = y)) +
  geom_bar(aes(fill = stat(count))) +
  scale_fill_viridis_c(option = "A", end = .7, breaks = NULL) +
  scale_x_continuous(breaks = 0:4 * 2, expand = c(0, NA), limits = c(0, 9)) +
  theme_minimal_vgrid() +
  panel_border() +
  facet_wrap(~ s, ncol = 7)
```

And here's our Figure 9.9.

```{r, fig.width = 4, fig.height = 3.25, message = F, warning = F}
my_data %>% 
  group_by(s) %>% 
  summarize(mean = mean(y)) %>%
  
  ggplot(aes(x = mean)) +
  geom_histogram(color = "white", fill = a_purple,
                 size = .2, binwidth = .1) +
  scale_x_continuous("Proportion Correct", limits = c(0, 1)) +
  scale_y_continuous("# Practitioners", expand = c(0, NA)) +
  theme_minimal_hgrid()
```

Let's open **brms**.

```{r, warning = F, message = F}
library(brms)
```

In applied statistics, the typical way to model a Bernoulli variable is with logistic regression. Instead of going through the pain of setting up a model in **brms** that mirrors the one in the text, I'm going to set up a hierarchical logistic regression model, instead.

Note the `family = bernoulli(link = logit)` argument. In work-a-day regression with vanilla Gaussian variables, the prediction space is unbounded. But when we want to model the probability of a success for a Bernoulli variable (i.e., $\theta$), we need to constrain the model to only produce predictions between 0 and 1. With logistic regression, we use a link function to do just that. The consequence is that instead of modeling the probability, $\theta$, we're modeling the logit probability.

In case you're curious, the logit of $\theta$ follows the formula

$$\operatorname{logit}(\theta) = \log (\theta/[1 - \theta] ).$$

But anyway, we'll be doing logistic regression using the logit link. Kruschke covered this in detail in [Chapter 21][Dichotomous Predicted Variable].

The next new part of our syntax is `(1 | s)`. As in the popular frequentist [**lme4** package](https://CRAN.R-project.org/package=lme4) [@R-lme4; @batesFittingLinearMixedeffects2015], you specify random effects or group-level parameters with the `(|)` syntax in **brms**. On the left side of the `|`, you tell **brms** what parameters you'd like to make random (i.e., vary by group). On the right side of the `|`, you tell **brms** what variable you want to group the parameters by. In our case, we want the intercepts to vary over the grouping variable `s`.

```{r fit9.1}
fit9.1 <-
  brm(data = my_data,
      family = bernoulli(link = logit),
      y ~ 1 + (1 | s),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4,
      seed = 9,
      file = "fits/fit09.01")
```

As it turns out, the $N(0, 1.5)$ prior is flat in the probability space for the intercept in a logistic regression model. We'll explore that a little further down. The $N(0, 1)$ prior for the random effect is actually a half Normal. That's because **brms** defaults to bound $SD$ parameters to zero and above. The half Normal prior for a hierarchical $SD$ parameter in a logistic regression model is weakly regularizing and is conservative in the sense that it presumes some pooling is preferable to no pooling. If you wanted to take a lighter approach, you might use something like a `cauchy(0, 5)`, instead. See the [prior wiki](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) by the Stan team for more ideas on priors.

Here are the trace plots and posterior densities of the main parameters.

```{r, fig.width = 8, fig.height = 2.5}
plot(fit9.1, widths = c(2, 3))
```

The trace plots indicate no problems with convergence. We'll need to extract the posterior draws with `as_draws_df()` and open the **bayesplot** package before we can examine the autocorrelations.

```{r, warning = F, message = F}
draws <- as_draws_df(fit9.1)

library(bayesplot)
```

One of the nice things about **bayesplot** is it returns **ggplot2** objects. As such, we can amend their theme settings to be consistent with our other **ggplot2** plots. Here we'll amend `bayesplot::mcmc_acf()` to the `theme_cowplot()` theme.

```{r, fig.width = 6, fig.height = 4, warning = F, message = F}
draws %>% 
  mutate(chain = .chain) %>% 
  mcmc_acf(pars = vars(b_Intercept, sd_s__Intercept), lags = 10) +
  theme_cowplot()
```

It appears `fit9.1` had very low autocorrelations. Here we'll examine the $N_{eff}/N$ ratio.

```{r, fig.width = 6, fig.height = 3.75}
neff_ratio(fit9.1) %>% 
  mcmc_neff() +
  theme_cowplot(font_size = 12)
```

The $N_{eff}/N$ ratio values for our model parameters were excellent. Here's a numeric summary of the model.

```{r}
print(fit9.1)
```

We'll need `brms::inv_logit_scaled()` to convert the model parameters to predict $\theta$ rather than $\operatorname{logit}(\theta)$. After the conversions, we'll be ready to make the histograms in the lower portion of Figure 9.10.

```{r, fig.width = 8, fig.height = 4, warning = F, message = F}
# load
library(tidybayes)

# wrangle
draws_small <-
  draws %>% 
  # convert the linear model parameters to the probability space with `inv_logit_scaled()`
  mutate(`theta[1]`  = (b_Intercept + `r_s[S01,Intercept]`) %>% inv_logit_scaled(),
         `theta[14]` = (b_Intercept + `r_s[S14,Intercept]`) %>% inv_logit_scaled(),
         `theta[28]` = (b_Intercept + `r_s[S28,Intercept]`) %>% inv_logit_scaled()) %>% 
  # make the difference distributions
  mutate(`theta[1] - theta[14]`  = `theta[1]`  - `theta[14]`,
         `theta[1] - theta[28]`  = `theta[1]`  - `theta[28]`,
         `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) %>% 
  select(starts_with("theta"))

draws_small %>% 
  pivot_longer(everything()) %>% 
  # this line is unnecessary, but will help order the plots 
  mutate(name = factor(name, levels = c("theta[1]", "theta[14]", "theta[28]", 
                                        "theta[1] - theta[14]", "theta[1] - theta[28]", "theta[14] - theta[28]"))) %>% 

  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = a_purple, breaks = 40, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  theme_minimal_hgrid() +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

If you wanted the specific values of the posterior modes and 95% HDIs, you could execute this.

```{r}
draws_small %>% 
  pivot_longer(everything()) %>%
  group_by(name) %>% 
  mode_hdi(value) %>% 
  mutate_if(is.double, round, digits = 3)
```

And here are the Figure 9.10 scatter plots.

```{r, fig.width = 8, fig.height = 2.5, warning = F, message = F}
p1 <-
  draws_small %>% 
  ggplot(aes(x = `theta[1]`, y = `theta[14]`)) +
  geom_abline(linetype = 2) +
  geom_point(color = a_purple, size = 1/8, alpha = 1/8)

p2 <-
  draws_small %>% 
  ggplot(aes(x = `theta[1]`, y = `theta[28]`)) +
  geom_abline(linetype = 2) +
  geom_point(color = a_purple, size = 1/8, alpha = 1/8)

p3 <-
  draws_small %>% 
  ggplot(aes(x = `theta[14]`, y = `theta[28]`)) +
  geom_abline(linetype = 2) +
  geom_point(color = a_purple, size = 1/8, alpha = 1/8)

(p1 + p2 + p3) &
  coord_cartesian(xlim = c(0, 1),
                  ylim = c(0, 1)) & 
  theme_minimal_grid()
```

This is posterior distribution for the population estimate for $\theta$, which roughly corresponds to the upper right histogram of $\omega$ in Figure 9.10.

```{r, fig.width = 3.25, fig.height = 2.5, warning = F, message = F}
# this part makes it easier to set the break points in `scale_x_continuous()` 
labels <-
  draws %>% 
  transmute(theta = b_Intercept %>% inv_logit_scaled()) %>%
  mode_hdi() %>% 
  pivot_longer(theta:.upper) %>% 
  mutate(label = value %>% round(3) %>% as.character)
  
draws %>% 
  mutate(theta = b_Intercept %>% inv_logit_scaled()) %>% 

  ggplot(aes(x = theta, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = a_purple, breaks = 40) +
  scale_x_continuous(expression(theta), 
                     breaks = labels$value,
                     labels = labels$label) +  
  scale_y_continuous(NULL, breaks = NULL) +
  theme_minimal_hgrid()
```

I'm not aware there's a straight conversion to get $\sigma$ in a probability metric. As far as I can tell, you have to first use `coef()` to "extract [the] model coefficients, which are the sum of population-level effects and corresponding group-level effects" [@brms2022RM, p. 58]. With the model coefficient draws in hand, you can index them by posterior iteration, group them by that index, compute the iteration-level $SD$s, and then plot the distribution of the $SD$s.

```{r, fig.width = 3.25, fig.height = 2.5, warning = F, message = F}
# the tibble of the primary data
sigmas <-
  coef(fit9.1, summary = F)$s %>% 
  as_tibble() %>% 
  mutate(iter = 1:n()) %>% 
  group_by(iter) %>% 
  pivot_longer(-iter) %>% 
  mutate(theta = inv_logit_scaled(value)) %>% 
  summarise(sd = sd(theta))

# this, again, is just to customize `scale_x_continuous()`
labels <-
  sigmas %>% 
  mode_hdi(sd) %>% 
  pivot_longer(sd:.upper) %>% 
  mutate(label = value %>% round(3) %>% as.character)
  
# the plot
sigmas %>% 
  ggplot(aes(x = sd, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = a_purple, breaks = 40) +
  scale_x_continuous(expression(paste(sigma, " of ", theta, " in a probability metric")),
                     breaks = labels$value,
                     labels = labels$label) +  
  scale_y_continuous(NULL, breaks = NULL) +
  theme_minimal_hgrid()
```

And now you have a sense of how to do all those by hand, `bayesplot::mcmc_pairs()` offers a fairly quick way to get a good portion of Figure 9.10.

```{r, fig.width = 5.5, fig.height = 5, warning = F, message = F}
color_scheme_set("purple")
bayesplot_theme_set(theme_default() + theme_minimal_grid())

coef(fit9.1, summary = F)$s %>% 
  inv_logit_scaled() %>% 
  data.frame() %>% 
  rename(`theta[1]`  = S01.Intercept, 
         `theta[14]` = S14.Intercept, 
         `theta[28]` = S28.Intercept) %>% 
  select(`theta[1]`, `theta[14]`, `theta[28]`) %>% 
  
  mcmc_pairs(off_diag_args = list(size = 1/8, alpha = 1/8))
```

Did you see how we slipped in the `color_scheme_set()` and `bayesplot_theme_set()` lines at the top? Usually, the plots made with **bayesplot** are easy to modify with **ggplot2** syntax. Plots made with `mcmc_pairs()` function are one notable exception. On the back end, these made by combining multiple **ggplot** into a grid, a down-the-line result of which is they are difficult to modify. Happily, one can make some modifications beforehand by altering the global settings with the `color_scheme_set()` and `bayesplot_theme_set()` functions. You can learn more in the discussion on [issue #128](https://github.com/stan-dev/bayesplot/issues/128) on the **bayesplot** GitHub repo.

Kruschke used a $\operatorname{Beta}(1, 1)$ prior for $\omega$. If you randomly draw from that prior and plot a histogram, you'll see it was flat.

```{r, fig.width = 3.5, fig.height = 3}
set.seed(1)

tibble(prior = rbeta(n = 1e5, 1, 1)) %>% 
  ggplot(aes(x = prior)) +
  geom_histogram(fill = a_purple, color = "white", 
                 size = .2, binwidth = .05, boundary = 0) +
  scale_x_continuous(expression(omega), labels = c("0", ".25", ".5", ".75", "1")) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 1)) +
  theme_minimal_hgrid()
```

You'll note that plot corresponds to the upper right panel of Figure 9.11.

Recall that we used a logistic regression model with a `normal(0, 1.5)` prior on the intercept. If you sample from `normal(0, 1.5)` and then convert the draws using `brms::inv_logit_scaled()`, you'll discover that our `normal(0, 1.5)` prior was virtually flat on the probability scale. Here we'll show the consequence of a variety of zero-mean Gaussian priors for the intercept of a logistic regression model.

```{r prior_sim, cache = T, fig.width = 8, fig.height = 6}
# define a function
r_norm <- function(i, n = 1e4) {
  
  set.seed(1)
  rnorm(n = n, mean = 0, sd = i) %>% 
    inv_logit_scaled()
  
}

# simulate and wrangle
tibble(sd = seq(from = .25, to = 3, by = .25)) %>% 
  group_by(sd) %>% 
  mutate(prior = map(sd, r_norm)) %>% 
  unnest(prior) %>% 
  ungroup() %>% 
  mutate(sd = str_c("sd = ", sd)) %>% 
  
  # plot!
  ggplot(aes(x = prior)) +
  geom_histogram(fill = a_purple, color = "white", size = .2,
                 binwidth = .05, boundary = 0) +
  scale_x_continuous(labels = c("0", ".25", ".5", ".75", "1")) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 1)) +
  theme_minimal_hgrid() +
  panel_border() +
  facet_wrap(~ sd)
```

It appears that as $\sigma$ goes lower than 1.25, the prior becomes increasingly regularizing, pulling the estimate for $\theta$ to a neutral .5. However, as the prior's $\sigma$ gets larger than 1.25, more and more of the probability mass ends up at extreme values.

Next, Kruschke examined the prior distribution. There are a few ways to do this. Like in the last chapter, the one we'll explore involves adding the `sample_prior = "only"` argument to the `brm()` function. When you do so, the results of the model are just the prior. That is, `brm()` leaves out the likelihood. This returns a bunch of draws from the prior predictive distribution.

```{r fit9.1_prior}
fit9.1_prior <-
  brm(data = my_data,
      family = bernoulli(link = logit),
      y ~ 1 + (1 | s),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4,
      seed = 9,
      sample_prior = "only",
      file = "fits/fit09.01_prior")
```

If we feed `fit9.1_prior` into the `as_draws_df()` function, we'll get back a data frame of draws from the prior, but with the same parameter names we'd get from the posterior.

```{r}
prior_draws <-
  as_draws_df(fit9.1_prior)

head(prior_draws)
```

And here we'll take a subset of the columns in `prior_draws`, transform the results to the probability metric, and save.

```{r, warning = F}
prior_draws <-
  prior_draws %>% 
  transmute(`theta[1]`  = b_Intercept + `r_s[S01,Intercept]`,
            `theta[14]` = b_Intercept + `r_s[S14,Intercept]`,
            `theta[28]` = b_Intercept + `r_s[S28,Intercept]`) %>% 
  mutate_all(.funs = inv_logit_scaled)

head(prior_draws)
```

Now we can use our `prior_draws` object to make the diagonal of the lower grid of Figure 9.11.

```{r, fig.width = 8, fig.height = 2.5}
prior_draws %>% 
  pivot_longer(everything()) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(fill = a_purple, color = "white", size = .2,
                 binwidth = .05, boundary = 0) +
  scale_x_continuous(labels = c("0", ".25", ".5", ".75", "1")) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 1)) +
  theme_minimal_hgrid() +
  panel_border() +
  facet_wrap(~ name)
```

With a little subtraction, we can reproduce the plots in the upper triangle.

```{r, fig.width = 8, fig.height = 2.5}
prior_draws %>% 
  mutate(`theta[1] - theta[14]`  = `theta[1]`  - `theta[14]`,
         `theta[1] - theta[28]`  = `theta[1]`  - `theta[28]`,
         `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) %>% 
  pivot_longer(contains("-")) %>% 

  ggplot(aes(x = value)) +
  geom_histogram(fill = a_purple, color = "white", size = .2,
                 binwidth = .05, boundary = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme_minimal_hgrid() +
  panel_border() +
  facet_wrap(~ name)
```

Those plots clarify our hierarchical logistic regression model was a little more regularizing than Kruschke's. The consequence of our priors was more aggressive regularization, greater shrinkage toward zero. The prose in the next section of the text clarifies this isn't necessarily a bad thing.

Finally, here are the plots for the lower triangle in Figure 9.11.

```{r, fig.width = 8, fig.height = 2.5}
p1 <-
  prior_draws %>% 
  ggplot(aes(x = `theta[1]`, y = `theta[14]`)) +
  geom_point(color = a_purple, size = 1/8, alpha = 1/8)

p2 <-
  prior_draws %>% 
  ggplot(aes(x = `theta[1]`, y = `theta[28]`)) +
  geom_point(color = a_purple, size = 1/8, alpha = 1/8)

p3 <-
  prior_draws %>% 
  ggplot(aes(x = `theta[14]`, y = `theta[28]`)) +
  geom_point(color = a_purple, size = 1/8, alpha = 1/8)

(p1 + p2 + p3) &
  geom_abline(linetype = 2) &
  coord_cartesian(xlim = c(0, 1),
                  ylim = c(0, 1)) &
  theme_minimal_grid()
```

In case you were curious, here are the Pearson's correlation coefficients among the priors.

```{r}
cor(prior_draws) %>% round(digits = 2)
```

## Shrinkage in hierarchical models

"In typical hierarchical models, the estimates of low-level parameters are pulled closer together than they would be if there were not a higher-level distribution. This pulling together is called *shrinkage* of the estimates" (p. 245, *emphasis* in the original)

Further,

> shrinkage is a rational implication of hierarchical model structure, and is (usually) desired by the analyst because the shrunken parameter estimates are less affected by random sampling noise than estimates derived without hierarchical structure. Intuitively, shrinkage occurs because the estimate of each low-level parameter is influenced from two sources: (1) the subset of data that are directly dependent on the low-level parameter, and (2) the higher-level parameters on which the low-level parameter depends. The higher- level parameters are affected by all the data, and therefore the estimate of a low-level parameter is affected indirectly by all the data, via their influence on the higher-level parameters. (p. 247)

Recall Formula 9.4 from page 223,

$$\theta \sim \operatorname{beta} \big(\omega(\kappa - 2) + 1 \big ), (1 - \omega)(\kappa - 2) + 1).$$

With that formula, we can express `dbeta()`'s `shape1` and `shape2` in terms of $\omega$ and $\kappa$ and make the shapes in Figure 9.12.

```{r, fig.width = 6, fig.height = 2.5}
omega  <- 0.5
kappa1 <- 2.1
kappa2 <- 15.8

tibble(x = seq(from = 0, to = 1, by = .001)) %>%
  mutate(`kappa==2.1` = dbeta(x = x, 
                              shape1 = omega * (kappa1 - 2) + 1, 
                              shape2 = (1 - omega) * (kappa1 - 2) + 1),
         `kappa==15.8` = dbeta(x = x, 
                               shape1 = omega * (kappa2 - 2) + 1, 
                               shape2 = (1 - omega) * (kappa2 - 2) + 1)) %>% 
  pivot_longer(-x) %>% 
  mutate(name = factor(name, levels = c("kappa==2.1", "kappa==15.8"))) %>% 
  
  ggplot(aes(x = x, y = value)) +
  geom_area(fill = a_purple) +
  scale_y_continuous(expression(dbeta(theta*"|"*omega*", "*kappa)), breaks = NULL) +
  xlab(expression(Data~Proportion~or~theta~value)) +
  theme_minimal_hgrid() +
  panel_border() +
  facet_wrap(~ name, labeller = label_parsed)
```

## Speeding up ~~JAGS~~ brms

Here we'll compare the time it takes to fit `fit1` as either `bernoulli(link = logit)` or `binomial(link = logit)`.

```{r, echo = F, message = F, warning = F}
# save(list = c("stop_time_bernoulli", "start_time_bernoulli", "stop_time_binomial", "start_time_binomial"),
#      file = "fits/09_fit_times.rda")
# 
# rm(stop_time_bernoulli, start_time_bernoulli, stop_time_binomial, start_time_binomial)

load("fits/09_fit_times.rda")
```

```{r, eval = F}
# bernoulli
start_time_bernoulli <- proc.time()

brm(data = my_data,
      family = bernoulli(link = logit),
      y ~ 1 + (1 | s),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4,
      seed = 9)

stop_time_bernoulli <- proc.time()

# binomial
start_time_binomial <- proc.time()

brm(data = my_data,
      family = binomial(link = logit),
      y | trials(1) ~ 1 + (1 | s),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4,
      seed = 9)

stop_time_binomial <- proc.time()
```

See how we're using `proc.time()` to record when we began and finished evaluating our `brm()` code? The last time we covered that was way back in [Section 3.7.5][Measuring processing time]. In that section, we also learned how subtracting the former from the latter yields the total elapsed time.

```{r}
stop_time_bernoulli - start_time_bernoulli
stop_time_binomial - start_time_binomial
```

These times are based on my current laptop (a 2019 MacBook Pro). Your mileage may vary. If you wanted to be rigorous about this, you could do this multiple times in a mini simulation.

As to the issue of parallel processing, we've been doing this all along. Note our `chains = 4, cores = 4` arguments.

Since Kruschke wrote his text, we have other options for speeding up your **brms** models related to within-chain parallelization and the `backend = "cmdstanr"` option. For all the details, see Weber & Bürkner's (-@weberRunningBrmsModels2022) vignette, *Running brms models with within-chain parallelization*.

## Extending the hierarchy: Subjects within categories

> Many data structures invite hierarchical descriptions that may have multiple levels. Software such as ~~JAGS~~ [**brms**] makes it easy to implement hierarchical models, and Bayesian inference makes it easy to interpret the parameter estimates, even for complex nonlinear hierarchical models. Here, we take a look at one type of extended hierarchical model. (p. 251)

As we will address below, our version of Figure 9.13 will look rather different from Kruschke's. It's something of a combination of the sensibilities from Figures 20.2 and 21.10. Even still, the diagram is of three-level model that shares many similarities to Kruschke's and, as we will see, yields very similar results.

```{r, fig.width = 7.75, fig.height = 6.75, warning = F, message = F}
# half-normal density
p1 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = a_purple) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[italic(s)*'|'*italic(c)]", 
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# second half-normal density
p2 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = a_purple) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[italic(c)]", 
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))
  
#  annotated arrow
p3 <-
  tibble(x    = .85,
         y    = 1,
         xend = .5,
         yend = .25) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = "text",
           x = .54, y = .6, label = "'~'", 
           size = 10, family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# normal density
p4 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = a_purple) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# second normal density
p5 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = a_purple,) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("0", "sigma[italic(s)*'|'*italic(c)]"), 
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# third normal density
p6 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = a_purple) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("0", "sigma[italic(c)]"), 
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# three annotated arrows
p7 <-
  tibble(x    = c(.09, .48, .9),
         y    = c(1, 1, 1),
         xend = c(.2, .425, .775),
         yend = c(.2, .2, .2)) %>%
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = "text",
           x = c(.10, .42, .49, .81, .87), y = .6,
           label = c("'~'", "'~'", "italic(s)*'|'*italic(c)", "'~'", "italic(c)"),
           size = c(10, 10, 7, 10, 7), 
           family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p8 <-
  tibble(x = .5,
         y = .5,
         label = "logistic(beta[0]+sum()[italic(s)*'|'*italic(c)]*beta['['*italic(s)*'|'*italic(c)*']']*italic(x)['['*italic(s)*'|'*italic(c)*']'](italic(i))+sum()[italic(c)]*beta['['*italic(c)*']']*italic(x)['['*italic(c)*']'](italic(i)))") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# a second annotated arrow
p9 <-
  tibble(x     = c(.375, .5),
         y     = c(.75, .3),
         label = c("'='", "mu[italic(i)*'|'*italic(sc)]")) %>%
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = .4,
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# binomial density
p10 <-
tibble(x = 0:7) %>% 
  ggplot(aes(x = x, 
             y = (dbinom(x, size = 7, prob = .625)) / max(dbinom(x, size = 7, prob = .625)))) +
  geom_col(fill = a_purple, width = .4) +
  annotate(geom = "text",
           x = 3.5, y = .2,
           label = "binomial",
           size = 7) +
  annotate(geom = "text",
           x = 7, y = .85,
           label = "italic(N)[italic(i)*'|'*italic(s)]",
           size = 7, family = "Times", parse = TRUE) +
  coord_cartesian(xlim = c(-1, 8),
                  ylim = c(0, 1.2)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p11 <-
  tibble(x     = c(.375, .7),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)*'|'*italic(sc)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p12 <-
  tibble(x     = 1,
         y     = .5,
         label = "italic(y)[italic(i)*'|'*italic(sc)]") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = "Times") +
  xlim(0, 2) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 7, r = 9),
  area(t = 1, b = 2, l = 11, r = 13),
  area(t = 4, b = 5, l = 1, r = 3),
  area(t = 4, b = 5, l = 5, r = 7),
  area(t = 4, b = 5, l = 9, r = 11),
  area(t = 3, b = 4, l = 6, r = 8),
  area(t = 3, b = 4, l = 10, r = 12),
  area(t = 7, b = 8, l = 1, r = 11),
  area(t = 6, b = 7, l = 1, r = 11),
  area(t = 11, b = 12, l = 5, r = 7),
  area(t = 9, b = 11, l = 5, r = 7),
  area(t = 13, b = 14, l = 5, r = 7),
  area(t = 15, b = 15, l = 5, r = 7)
)

# combine and plot!
(p1 + p2 + p4 + p5 + p6 + p3 + p3 + p8 + p7 + p10 + p9 + p11 + p12) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Though we will be fitting a hierarchical model with subjects $s$ within categories $c$, the higher-level parameters will not be $\omega$ and $\kappa$. As we'll go over, below, we will use the binomial distribution within a more conventional hierarchical logistic regression paradigm. In this paradigm, we have an overall intercept, often called $\alpha$ or $\beta_0$, which will be our analogue to Kruschke's overall $\omega$. For the two grouping categories, $s$ and $c$, we will have $\sigma$ estimates, which express the variability within those grouping. You'll see when we get there.

### Example: Baseball batting abilities by position.

Here are the batting average data.

```{r, warning = F, message = F}
my_data <- read_csv("data.R/BattingAverage.csv")

glimpse(my_data)
```

In his footnote #6, Kruschke indicated he retrieved the data from [http://www.baseball-reference.com/leagues/MLB/2012-standard-batting.shtml](http://www.baseball-reference.com/leagues/MLB/2012-standard-batting.shtml) on December of 2012. To give a sense of the data, here are the number of occasions by primary position, `PriPos`, with their median at bat, `AtBats`, values.

```{r, message = F}
my_data %>% 
  group_by(PriPos) %>% 
  summarise(n      = n(),
            median = median(AtBats)) %>% 
  arrange(desc(n))
```

As these data are aggregated, we'll fit with an aggregated binomial model. This is still logistic regression. The Bernoulli distribution is a special case of the binomial distribution when the number of trials in each data point is 1 [see @Bürkner2022Parameterization for details]. Since our data are aggregated, the information encoded in `Hits` is a combination of multiple trials, which requires us to jump up to the more general binomial likelihood. Note the `Hits | trials(AtBats)` syntax. With that bit, we instructed **brms** that our criterion, `Hits`, is an aggregate of multiple trials and the number of trials is encoded in `AtBats`.

Also note the `(1 | PriPos) + (1 | PriPos:Player)` syntax. In this model, we have two grouping factors, `PriPos` and `Player`. Thus we have two `(|)` arguments. But since players are themselves nested within positions, we have encoded that nesting with the `(1 | PriPos:Player)` syntax. For more on this style of syntax, see [Kristoffer Magnusson](https://twitter.com/krstoffr)'s handy blog post, [*Using R and lme/lmer to fit different two- and three-level longitudinal models*](https://rpsychologist.com/r-guide-longitudinal-lme-lmer). Since **brms** syntax is based on that from the earlier **lme4** package, the basic syntax rules apply. Bürkner [-@brms2022RM], of course, also covered these topics in the `brmsformula` subsection of the [**brms** reference manual](https://CRAN.R-project.org/package=brms/brms.pdf).

```{r fit9.2}
fit9.2 <-
  brm(data = my_data,
      family = binomial(link = logit),
      Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 3500, warmup = 500, chains = 3, cores = 3,
      control = list(adapt_delta = .99),
      seed = 9,
      file = "fits/fit09.02")
```

The chains look good.

```{r, fig.width = 10, fig.height = 4}
plot(fit9.2, widths = c(2, 3))
```

Note how our `color_scheme_set("purple")` line from back up in the `mcmc_pairs()` has effected the color scheme of `brms::plot()`.

We might examine the autocorrelations within the chains.

```{r, fig.width = 8, fig.height = 4, warning = F, message = F}
draws <- as_draws_df(fit9.2)

draws %>% 
  mutate(chain = .chain) %>% 
  mcmc_acf(pars = vars(b_Intercept:`sd_PriPos:Player__Intercept`), 
           lags = 8) +
  theme_minimal_hgrid()
```

Here's a histogram of the $N_{eff}/N$ ratios.

```{r, fig.width = 6, fig.height = 3}
fit9.2 %>% 
  neff_ratio() %>% 
  mcmc_neff_hist(binwidth = .1) +
  yaxis_text() +
  theme_minimal_hgrid()
```

Happily, most have a very favorable ratio. Here's a numeric summary of the primary model parameters.

```{r}
print(fit9.2)
```

As far as I'm aware, **brms** offers three major ways to get the group-level parameters for a hierarchical model: using one of the `as_draws` functions, `coef()`, or `fitted()`. We'll cover each, beginning with `as_draws`. In order to look at the autocorrelation plots, above, we already saved the results from `as_draws_df(fit9.2)` as `draws`. Let's look at its structure with `head()`.

```{r}
head(draws)
```

In the text, Kruschke described the model as having 968 parameters. Our `draws` data frame has one vector for each, with a couple others tacked onto the end. In the hierarchical logistic regression model, the group-specific parameters for the levels of `PriPos` are additive combinations of the global intercept vector, `b_Intercept` and each position-specific vector, `r_PriPos[i.Base,Intercept]`, where `i` is a fill-in for the position of interest. And recall that since the linear model is of the logit of the criterion, we'll need to use `inv_logit_scaled()` to convert that to the probability space.

```{r, warning = F}
draws_small <-
  draws %>% 
  transmute(`1st Base` = (b_Intercept + `r_PriPos[1st.Base,Intercept]`), 
            Catcher    = (b_Intercept + `r_PriPos[Catcher,Intercept]`), 
            Pitcher    = (b_Intercept + `r_PriPos[Pitcher,Intercept]`)) %>% 
  mutate_all(inv_logit_scaled) %>% 
  # here we compute our difference distributions
  mutate(`Pitcher - Catcher`  = Pitcher - Catcher,
         `Catcher - 1st Base` = Catcher - `1st Base`)

head(draws_small)
```

If you take a glance at Figures 9.14 through 9.16 in the text, we'll be making a lot of histograms of the same basic structure. To streamline our code a bit, we can make a custom histogram plotting function.

```{r}
make_histogram <- function(data, mapping, title, xlim, ...) {
  
  ggplot(data, mapping) +
    geom_histogram(fill = viridis::viridis_pal(option = "A")(9)[4], 
                   color = "white", size = .2,
                   bins = 30) +
    stat_pointinterval(aes(y = 0), 
                       point_interval = mode_hdi, .width = .95) +
    scale_y_continuous(NULL, breaks = NULL) +
    labs(title = title,
         x = expression(theta)) +
    coord_cartesian(xlim = xlim) +
    theme_minimal_hgrid() +
    panel_border()
  
}
```

We'll do the same thing for the correlation plots.

```{r}
make_point <- function(data, mapping, limits, ...) {
  
  ggplot(data, mapping) +
    geom_abline(linetype = 3, color = "grey50") +
    geom_point(color = viridis::viridis_pal(option = "A")(9)[4], 
               size = 1/10, alpha = 1/20) +
    coord_cartesian(xlim = limits,
                    ylim = limits) +
    theme_minimal_grid(line_size = 0) +
    panel_border()
  
}
```

To learn more about wrapping custom plots into custom functions, check out [Chapter 19](https://ggplot2-book.org/programming.html) of Wickham's [-@wickhamGgplot2ElegantGraphics2016] [*ggplot2: Elegant graphics for data analysis*](https://ggplot2-book.org/).

Now we have our `make_histogram()` and `make_point()` functions, we'll use `grid.arrange()` to paste together the left half of Figure 9.14.

```{r, fig.width = 6, fig.height = 4.5}
p1 <-
  make_histogram(data = draws_small,
                 aes(x = Pitcher), 
                 title = "Pitcher", 
                 xlim = c(.1, .25))

p2 <-
  make_histogram(data = draws_small,
                 aes(x = `Pitcher - Catcher`), 
                 title = "Pitcher - Catcher", 
                 xlim = c(-.15, 0))

p3 <-
  make_point(data = draws_small,
             aes(x = Pitcher, y = Catcher),
             limits = c(.12, .25))

p4 <-
  make_histogram(data = draws_small,
                 aes(x = Catcher), 
                 title = "Catcher", 
                 xlim = c(.1, .25))

p1 + p2 + p3 + p4
```

We could follow the same procedure to make the right portion of Figure 9.14. But instead, let's switch gears and explore the second way **brms** affords us for plotting group-level parameters. This time, we'll use `coef()`.

Up in [Section 9.2.4][Example: Therapeutic touch.], we learned that we can use `coef()` to "extract [the] model coefficients, which are the sum of population-level effects and corresponding group-level effects" [@brms2022RM, p. 58]. The grouping level we're interested in is ` PriPos`, so we'll use that to index the information returned by `coef()`. Since `coef()` returns a matrix, we'll use `as_tibble()` to convert it to a tibble.

```{r}
coef_primary_position <-
  coef(fit9.2, summary = F)$PriPos %>% 
  as_tibble()
  
str(coef_primary_position)
```

Keep in mind that `coef()` returns the values in the logit scale when used for logistic regression models. So we'll have to use `brms::inv_logit_scaled()` to convert the estimates to the probability metric. After we're done converting the estimates, we'll then make the difference distributions.

```{r}
coef_small <-
  coef_primary_position %>% 
  select(`1st Base.Intercept`, Catcher.Intercept, Pitcher.Intercept) %>% 
  transmute(`1st Base` = `1st Base.Intercept`, 
            Catcher    = Catcher.Intercept, 
            Pitcher    = Pitcher.Intercept) %>% 
  mutate_all(inv_logit_scaled) %>% 
  # here we make the difference distributions
  mutate(`Pitcher - Catcher`  = Pitcher - Catcher,
         `Catcher - 1st Base` = Catcher - `1st Base`)

head(coef_small)
```

Now we're ready for the right half of Figure 9.14.

```{r, fig.width = 6, fig.height = 4.5}
p1 <-
  make_histogram(data = coef_small,
                 aes(x = Catcher), 
                 title = "Catcher", 
                 xlim = c(.22, .27))

p2 <-
  make_histogram(data = coef_small,
                 aes(x = `Catcher - 1st Base`), 
                 title = "Catcher - 1st Base", 
                 xlim = c(-.04, .01))

p3 <-
  make_point(data = coef_small,
             aes(x = Catcher, y = `1st Base`),
             limits = c(.22, .27))

p4 <-
  make_histogram(data = coef_small,
                 aes(x = `1st Base`), 
                 title = "1st Base", 
                 xlim = c(.22, .27))

p1 + p2 + p3 + p4
```

And if you wanted the posterior modes and HDIs, you'd use `mode_hdi()` after a little wrangling.

```{r}
coef_small %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  mode_hdi(value) %>% 
  mutate_if(is.double, round, digits = 3)
```

While we're at it, we should capitalize on the opportunity to show how these results are the same as those derived from our `as_draws_df()` approach, above.

```{r}
draws_small %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  mode_hdi(value) %>% 
  mutate_if(is.double, round, digits = 3)
```

Success!

For Figures 9.15 and 9.16, Kruschke drilled down further into the posterior. To drill along with him, we'll take the opportunity to showcase `fitted()`, the third way **brms** affords us for plotting group-level parameters.

```{r, fig.width = 3.5, fig.height = 2.5, warning = F}
# this will make life easier. just go with it
name_list <- c("Kyle Blanks", "Bruce Chen", "ShinSoo Choo", "Ichiro Suzuki", 
               "Mike Leake", "Wandy Rodriguez", "Andrew McCutchen", "Brett Jackson")

# we'll define the data we'd like to feed into `fitted()`, here
nd <-
  my_data %>% 
  filter(Player %in% name_list) %>% 
  # these last two lines aren't typically necessary, but they allow us to
  # arrange the rows in the same order we find the names in Figures 9.15 and 9.16
  mutate(Player = factor(Player, levels = name_list)) %>% 
  arrange(Player)

fitted_players <-
  fitted(fit9.2, 
         newdata = nd,
         scale = "linear",
         summary = F) %>% 
  as_tibble() %>% 
  # rename the values as returned by `as_tibble()`
  set_names(name_list) %>% 
  # convert the values from the logit scale to the probability scale
  mutate_all(inv_logit_scaled) %>% 
  # in this last section, we make our difference distributions 
  mutate(`Kyle Blanks - Bruce Chen`         = `Kyle Blanks`      - `Bruce Chen`,
         `ShinSoo Choo - Ichiro Suzuki`     = `ShinSoo Choo`     - `Ichiro Suzuki`,
         `Mike Leake - Wandy Rodriguez`     = `Mike Leake`       - `Wandy Rodriguez`,
         `Andrew McCutchen - Brett Jackson` = `Andrew McCutchen` - `Brett Jackson`)
    
glimpse(fitted_players)
```

Note our use of the `scale = "linear"` argument in the `fitted()` function. By default, `fitted()` returns predictions on the scale of the criterion. But we don't want a list of successes and failures; we want player-level parameters. When you specify `scale = "linear"`, you request `fitted()` return the values in the parameter scale.

Here's the left portion of Figure 9.15.

```{r, fig.width = 6, fig.height = 4.5}
p1 <-
  make_histogram(data = fitted_players,
                 aes(x = `Kyle Blanks`), 
                 title = "Kyle Blanks (1st Base)", 
                 xlim = c(.05, .35))

p2 <-
  make_histogram(data = fitted_players,
                 aes(x = `Kyle Blanks - Bruce Chen`), 
                 title = "Kyle Blanks (1st Base) -\nBruce Chen (Pitcher)", 
                 xlim = c(-.1, .25))

p3 <-
  make_point(data = fitted_players,
             aes(x = `Kyle Blanks`, y = `Bruce Chen`),
             limits = c(.09, .35))

p4 <-
  make_histogram(data = fitted_players,
                 aes(x = `Bruce Chen`), 
                 title = "Bruce Chen (Pitcher)", 
                 xlim = c(.05, .35))

p1 + p2 + p3 + p4
```

Figure 9.15, right:

```{r, fig.width = 6, fig.height = 4.5}
p1 <-
  make_histogram(data = fitted_players,
                 aes(x = `ShinSoo Choo`), 
                 title = "ShinSoo Choo (Right Field)", 
                 xlim = c(.22, .34))

p2 <-
  make_histogram(data = fitted_players,
                 aes(x = `ShinSoo Choo - Ichiro Suzuki`), 
                 title = "ShinSoo Choo (Right Field) -\nIchiro Suzuki (Right Field)", 
                 xlim = c(-.07, .07))

p3 <-
  make_point(data = fitted_players,
             aes(x = `ShinSoo Choo`, y = `Ichiro Suzuki`),
             limits = c(.23, .32))

p4 <-
  make_histogram(data = fitted_players,
                 aes(x = `Ichiro Suzuki`), 
                 title = "Ichiro Suzuki (Right Field)", 
                 xlim = c(.22, .34))

(p1 + p2 + p3 + p4) & 
  theme(title = element_text(size = 11))
```

Figure 9.16, left:

```{r, fig.width = 6, fig.height = 4.5}
p1 <-
  make_histogram(data = fitted_players,
                 aes(x = `Mike Leake`), 
                 title = "Mike Leake (Pitcher)", 
                 xlim = c(.05, .35))

p2 <-
  make_histogram(data = fitted_players,
                 aes(x = `Mike Leake - Wandy Rodriguez`), 
                 title = "Mike Leake (Pitcher) -\nWandy Rodriguez (Pitcher)", 
                 xlim = c(-.05, .25))

p3 <-
  make_point(data = fitted_players,
             aes(x = `Mike Leake`, y = `Wandy Rodriguez`),
             limits = c(.07, .25))

p4 <-
  make_histogram(data = fitted_players,
                 aes(x = `Wandy Rodriguez`), 
                 title = "Wandy Rodriguez (Pitcher)", 
                 xlim = c(.05, .35))

(p1 + p2 + p3 + p4) & 
  theme(title = element_text(size = 11))
```

Figure 9.16, right:

```{r, fig.width = 7, fig.height = 4.5}
p1 <-
  make_histogram(data = fitted_players,
                 aes(x = `Andrew McCutchen`), 
                 title = "Andrew McCutchen (Center Field)", 
                 xlim = c(.15, .35))

p2 <-
  make_histogram(data = fitted_players,
                 aes(x = `Andrew McCutchen - Brett Jackson`), 
                 title = "Andrew McCutchen (Center Field) -\nBrett Jackson (Center Field)", 
                 xlim = c(0, .20))

p3 <-
  make_point(data = fitted_players,
             aes(x = `Andrew McCutchen`, y = `Brett Jackson`),
             limits = c(.15, .35))

p4 <-
  make_histogram(data = fitted_players,
                 aes(x = `Brett Jackson`), 
                 title = "Brett Jackson (Center Field)", 
                 xlim = c(.15, .35))

(p1 + p2 + p3 + p4) & 
  theme(title = element_text(size = 11))
```

If you wanted the posterior modes and HDIs for any of the players and their contrasts, you'd use `mode_hdi()` after a little wrangling.

```{r}
fitted_players %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  mode_hdi(value) %>% 
  mutate_if(is.double, round, digits = 3)
```

To make our version of Figure 9.7, we'll have to switch gears from player-specific effects to those specific to positions averaged over individual players. The `fitted()` approach will probably make this the easiest. To do this, we'll need to specify values for `AtBats`, which will need to be a positive integer . However, since we're asking for fitted values of the linear predictors by setting `scale = "linear"`, any value meeting the positive-integer criterion will return the same results. We'll keep things simple and set `AtBats = 1`.

Another consideration is if we'd like to use `fitted()` to average across one of the hierarchical grouping parameters (i.e., `(1 | PriPos:Player)`), we'll need to employ the `re_formula` argument. With the line `re_formula = ~ (1 | PriPos)`, we'll instruct `fitted()` to return the `PriPos`-specific effects after averaging across levels of `Player`. The rest is quire similar to our method from above.

```{r, message = F}
nd <-
  my_data %>% 
  distinct(PriPos,
           AtBats = 1)

fitted_positions <-
  fitted(fit9.2, 
         newdata = nd,
         re_formula = ~ (1 | PriPos),
         scale = "linear",
         summary = F) %>% 
  as_tibble() %>% 
  set_names(distinct(my_data, PriPos) %>% pull()) %>% 
  mutate_all(inv_logit_scaled)
    
glimpse(fitted_positions)
```

Now we make and save the nine position-specific subplots for Figure 9.17.

```{r}
p1 <-
  fitted_positions %>% 
  pivot_longer(everything(),
               values_to = "theta") %>% 
  # though technically not needed, this line reorders the panels to match the text
  mutate(name = factor(name,
                       levels = c("1st Base", "Catcher", "Pitcher",
                                  "2nd Base", "Center Field", "Right Field",
                                  "3rd Base", "Left Field", "Shortstop"))) %>% 
  
  ggplot(aes(x = theta)) +
  geom_histogram(fill = a_purple, color = "white", 
                 size = .1, binwidth = .0025) +
  stat_pointinterval(aes(y = 0), 
                     point_interval = mode_hdi, .width = .95, size = 1) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(theta)) +
  coord_cartesian(xlim = c(.1, .28)) +
  theme_minimal_hgrid() +
  panel_border() +
  facet_wrap(~ name, nrow = 3, scales = "free_y")
```

In this code block, we'll make the subplot for the overall batting average. Given the size of the model, it's perhaps easiest to pull that information from the model with the `fixef()` function.

```{r}
p2 <-
  fixef(fit9.2, summary = F) %>% 
  as_tibble() %>% 
  transmute(theta = inv_logit_scaled(Intercept),
            name = "Overall") %>% 
  
  ggplot(aes(x = theta)) +
  geom_histogram(fill = a_purple, color = "white", 
                 size = .2, binwidth = .005) +
  stat_pointinterval(aes(y = 0), 
                    point_interval = mode_hdi, .width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(theta)) +
  coord_cartesian(xlim = c(.1, .28)) +
  theme_minimal_hgrid() +
  panel_border() +
  facet_wrap(~ name)
```

Now combine the plots with a little **patchwork** magic.

```{r, fig.width = 8, fig.height = 5}
p3 <- plot_spacer()

p1 + (p2 / p3 / p3) +
  plot_layout(widths = c(3, 1))
```

Do note that, unlike Kruschke's Figure 9.17, our subplots are all based on $\theta$ rather than $\omega$. This is because of our use of a hierarchical aggregated binomial, rather than the approach Kruschke took. Even so, look how similar the results are.

> Finally, we have only looked at a tiny fraction of the relations among the 968 parameters. We could investigate many more comparisons among parameters if we were specifically interested. In traditional statistical testing based on $p$-values (which will be discussed in Chapter 11), we would pay a penalty for even intending to make more comparisons. This is because a $p$ value depends on the space of counter-factual possibilities created from the testing intentions. In a Bayesian analysis, however, decisions are based on the posterior distribution, which is based only on the data (and the prior), not on the testing intention. More discussion of multiple comparisons can be found in [Section 11.4][Multiple comparisons]. (pp. 259--260)

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
# remove our objects
rm(beta_by_k, p1, p2, p3, my_arrow, p4, p5, p6, p7, layout, alpha, beta, make_prior, parameter_space, d, geom_2dd, a_purple, geom_marginal, text, bernoulli_likelihood, n, z, trial_data, trial_data_1, trial_data_2, p11, p12, p13, p21, p22, p23, p31, p32, p33, p41, p42, p43, p51, p52, p53, p8, length, gamma_s_and_r_from_mean_sd, gamma_s_and_r_from_mode_sd, gamma_param, my_data, fit9.1, draws, draws_small, labels, sigmas, r_norm, fit9.1_prior, prior_draws, omega, kappa1, kappa2, stop_time_bernoulli, start_time_bernoulli, stop_time_binomial, start_time_binomial, p9, p10, fit9.2, make_histogram, make_point, coef_primary_position, coef_small, name_list, nd, fitted_players, fitted_positions)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:09.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

# Model Comparison and Hierarchical Modeling

> There are situations in which different models compete to describe the same set of data... 
>
> ...Bayesian inference is reallocation of credibility over possibilities. In model comparison, the focal possibilities are the models, and Bayesian model comparison reallocates credibility across the models, given the data. In this chapter, we explore examples and methods of Bayesian inference about the relative credibilities of models. [@kruschkeDoingBayesianData2015, pp. 265--266]

In the text, the emphasis is on the Bayes Factor paradigm. While we will discuss that, we will also present the alternatives available with information criteria, model averaging, and model stacking.

## General formula and the Bayes factor

So far we have spoken of

* the data, denoted by $D$ or $y$;
* the model parameters, generically denoted by $\theta$;
* the likelihood function, denoted by $p(D | \theta)$; and
* the prior distribution, denoted by $p(\theta)$.

Now we add to that $m$, which is a model index with $m = 1$ standing for the first model, $m = 2$ standing for the second model, and so on. So when we have more than one model in play, we might refer to the likelihood as $p_m(y | \theta_m, m)$ and the prior as $p_m(\theta_m | m)$. It's also the case, then, that each model can be given a prior probability $p(m)$.

"The Bayes factor (BF) is the ratio of the probabilities of the data in models 1 and 2" (p. 268).

This can be expressed simply as

$$\text{BF} = \frac{p(D | m = 1)}{p(D | m = 2)}.$$

Kruschke further explained that

> one convention for converting the magnitude of the BF to a discrete decision about the models is that there is "substantial" evidence for model $m = 1$ when the BF exceeds 3.0 and, equivalently, "substantial" evidence for model $m = 2$ when the BF is less than 1/3 [@jeffreysTheoryProbability1961; @kassBayesFactors1995; @wetzelsStatisticalEvidenceExperimental2011]. (p. 268)

However, as with $p$-values, effect sizes, and so on, BF values exist within continua and might should be evaluated in terms of degree more so than as ordered kinds.

## Example: Two factories of coins

Kruschke considered the coin bias of two factories, each described by the beta distribution. We can organize how to derive the $\alpha$ and $\beta$ parameters from $\omega$ and $\kappa$ with a tibble.

```{r, warning = F, message = F}
library(tidyverse)

d <-
  tibble(factory = 1:2,
         omega   = c(.25, .75),
         kappa   = 12) %>% 
  mutate(alpha =      omega  * (kappa - 2) + 1,
         beta  = (1 - omega) * (kappa - 2) + 1)

d %>% 
  knitr::kable()
```

Thus given $\omega_1 = .25$, $\omega_2 = .75$ and $\kappa = 12$, we can describe the bias of the two coin factories as $\operatorname{beta}(\theta_{[m = 1]} | 3.5, 8.5)$ and $\operatorname{beta}(\theta_{[m = 2]} | 8.5, 3.5)$. With a little wrangling, we can use our `d` tibble to make the densities of Figure 10.2. But before we do, we should discuss plotting.

In the past few chapters, we have explored different plotting conventions using themes from Wilke's [**cowplot** package](https://wilkelab.org/cowplot), such as `theme_cowplot()` and `theme_minimal_grid()`. We also modified some of our plots using principles from Wilke's [-@wilkeFundamentalsDataVisualization2019] text, [*Fundamentals of data visualization*](https://clauswilke.com/dataviz/), and his [-@Wilke2020Themes] [*Themes*](https://wilkelab.org/cowplot/articles/themes.html) vignette. To further build on those principles, each chapter from here onward will have its own color scheme. The scheme in this chapter is based on [Katsushika Hokusai](https://www.katsushikahokusai.org/)'s [-@HokusaiGreatWaveOffKanagawa1820] woodblock print, [*The great wave off Kanagawa*](https://artsandculture.google.com/asset/the-great-wave-off-kanagawa/MgHm0BHMRIT73g). We can get a prearranged color palette based on *The great wave off Kanagawa* from  [Tyler Littlefield](https://twitter.com/tyluRp)'s [**lisa** package](https://github.com/tyluRp/lisa) [@R-lisa].

```{r, warning = F, message = F, fig.width = 3, fig.height = 1}
library(lisa)

lisa_palette("KatsushikaHokusai")
plot(lisa_palette("KatsushikaHokusai"))
```

The `"KatsushikaHokusai"` palette comes out of the box with five colors. However, we can use the `lisa_palette()` function to expand the palette by setting `type = "continuous"` and then increasing the `n` argument to a value larger than five. Here's what happens when you set `n = 9` and `n = 1000`.

```{r, fig.width = 3, fig.height = 1}
plot(lisa_palette("KatsushikaHokusai", n = 9, type = "continuous"))
plot(lisa_palette("KatsushikaHokusai", n = 1000, type = "continuous"))
```

Next, we will use the five base colors from `"KatsushikaHokusai"` to adjust the global theme default for all ggplots in this chapter. We can accomplish this with the `ggplot2::theme_set()` function. First, we start with the default `theme_grey()` as our base and then modify several of the settings with arguments within the `theme()` function.

```{r}
theme_set(
  theme_grey() +
    theme(text = element_text(color = lisa_palette("KatsushikaHokusai")[1]),
          axis.text = element_text(color = lisa_palette("KatsushikaHokusai")[1]),
          axis.ticks = element_line(color = lisa_palette("KatsushikaHokusai")[1]),
          legend.background = element_blank(),
          legend.box.background = element_blank(),
          legend.key = element_rect(fill = lisa_palette("KatsushikaHokusai")[5]),
          panel.background = element_rect(fill = lisa_palette("KatsushikaHokusai")[5],
                                          color = lisa_palette("KatsushikaHokusai")[1]),
          panel.grid = element_blank(),
          plot.background = element_rect(fill = lisa_palette("KatsushikaHokusai")[5],
                                          color = lisa_palette("KatsushikaHokusai")[5]),
          strip.background = element_rect(fill = lisa_palette("KatsushikaHokusai")[4]),
          strip.text = element_text(color = lisa_palette("KatsushikaHokusai")[1]))
)
```

You can undo this by executing `theme_set(theme_grey())`. Next we'll save the color names from a 9-color version of "KatsushikaHokusai" as a conveniently-named object, `kh`. We'll use `kh` to adjust the `fill` and `color` settings within our plots on the fly.

```{r}
kh <- lisa_palette("KatsushikaHokusai", 9, "continuous")
kh
```

Okay, it's time to get a sense of what we've done by making our version of Figure 10.2.

```{r, fig.width = 6, fig.height = 2}
length <- 101

d %>% 
  expand(nesting(factory, alpha, beta),
         theta = seq(from = 0, to = 1, length.out = length)) %>%
  mutate(label = str_c("factory ", factory)) %>% 
  
  ggplot(aes(x = theta, y = dbeta(x = theta, shape1 = alpha, shape2 = beta))) +
  geom_area(fill = kh[6]) +
  scale_y_continuous(NULL, breaks = NULL,
                     expand = expansion(mult = c(0, 0.05))) +
  xlab(expression(theta)) +
  facet_wrap(~ label)
```

We might recreate the top panel with `geom_col()`.

```{r, fig.width = 3, fig.height = 2}
tibble(Model = c("1", "2"), y = 1) %>% 
  ggplot(aes(x = Model, y = y)) +
  geom_col(width = .75, fill = kh[5]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)))
```

Consider the Bernoulli bar plots in the bottom panels of Figure 10.2. The heights of the bars are arbitrary and just intended to give a sense of the Bernoulli distribution. If we wanted the heights to correspond to the Beta distributions above them, we might do so like this.

```{r, fig.width = 5, fig.height = 2}
crossing(factory = str_c("factory ", 1:2),
         flip    = factor(c("tails", "heads"), levels = c("tails", "heads"))) %>% 
  mutate(prob = c(.75, .25, .25, .75)) %>% 
  
  ggplot(aes(x = flip, y = prob)) +
  geom_col(width = .75, fill = kh[4]) +
  scale_y_continuous(NULL, breaks = NULL,
                     expand = expansion(mult = c(0, 0.05))) +
  xlab(NULL) +
  theme(axis.ticks.x = element_blank(),
        panel.grid = element_blank()) +
  facet_wrap(~ factory)
```

But now

> suppose we flip the coin nine times and get six heads. Given those data, what are the posterior probabilities of the coin coming from the head-biased or tail-biased factories? We will pursue the answer three ways: via formal analysis, grid approximation, and MCMC. (p. 270)

Before we move on to a formal analysis, here's a more faithful version of Kruschke's Figure 10.2 based on the method from my blog post, [*Make model diagrams, Kruschke style*](https://solomonkurz.netlify.app/post/make-model-diagrams-kruschke-style/).

```{r, fig.width = 3.9, fig.height = 5, message = F}
library(patchwork)
library(ggforce)

p1 <-
  tibble(x = 1:2,
         d = c(.75, .75)) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = alpha(kh[5], .9), width = .45) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "categorical",
           size = 5, color = kh[1]) +
  annotate(geom = "text",
           x = 1.5, y = .85,
           label = "italic(P[m])",
           size = 5, color = kh[1], family = "Times", parse = TRUE) +
  coord_cartesian(xlim = c(-.5, 3.5),
                  ylim = 0:1) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = kh[1]))

## an annotated arrow
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")
p2 <-
  tibble(x    = .5,
         y    = 1,
         xend = .5,
         yend = 0) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = kh[1]) +
  annotate(geom = "text",
           x = .375, y = 1/3,
           label = "'~'",
           size = 10, color = kh[1], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

p3 <-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 5, 10) / max(dbeta(x, 5, 10)))) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = alpha(kh[4], .85)) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "beta",
           size = 5, color = kh[1]) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "list(italic(A)[1], italic(B)[1])",
           size = 5, color = kh[1], family = "Times", parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = kh[1]))

p4 <-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 10, 5) / max(dbeta(x, 10, 5)))) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = kh[6]) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "beta",
           size = 5, color = kh[1]) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "list(italic(A)[2], italic(B)[2])",
           size = 5, color = kh[1], family = "Times", parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = kh[1]))

# bar plot of Bernoulli data
p5 <-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = 1/3)) / max(dbinom(x, size = 1, prob = 1/3))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = alpha(kh[4], .85), width = .4) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "Bernoulli",
           size = 7, color = kh[1]) +
  annotate(geom = "text",
           x = .5, y = .94,
           label = "theta", 
           size = 7, color = kh[1], family = "Times", parse = T) +
  xlim(-.75, 1.75) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = kh[1]))

# another bar plot of Bernoulli data
p6 <-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = 2/3)) / max(dbinom(x, size = 1, prob = 2/3))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = kh[6], width = .4) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "Bernoulli",
           size = 7, color = kh[1]) +
  annotate(geom = "text",
           x = .5, y = .94,
           label = "theta", 
           size = 7, color = kh[1], family = "Times", parse = T) +
  xlim(-.75, 1.75) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = kh[1]))


# another annotated arrow
p7 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow, color = kh[1]) +
  xlim(0, 1) +
  theme_void()

# some text
p8 <-
  tibble(x     = 1,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = kh[1], parse = T, family = "Times") +
  xlim(0, 2) +
  theme_void()

# dashed borders
p9 <-
  tibble(x = c(0, 0.999, 0.999, 0, 1.001, 2, 2, 1.001),
         y = c(0, 0, 1, 1, 0, 0, 1, 1),
         z = rep(letters[1:2], each = 4)) %>% 
  
  ggplot(aes(x = x, y = y, group = z)) +
  geom_shape(fill = "transparent", color = kh[1], linetype = 2,
             radius = unit(1, 'cm')) +
  scale_x_continuous(NULL, breaks = NULL, expand=c(0,0)) +
  scale_y_continuous(NULL, breaks = NULL, expand=c(0,0)) +
  theme_void()

# define the layout
layout <- c(
  # cat
  area(t = 1, b = 5, l = 5, r = 9),
  area(t = 6, b = 8, l = 5, r = 9),
  
  # beta
  area(t = 9, b = 13, l = 2, r = 6),
  area(t = 9, b = 13, l = 8, r = 12),
  # arrow
  area(t = 14, b = 16, l = 2, r = 6),
  area(t = 14, b = 16, l = 8, r = 12),
  
  # bern
  area(t = 17, b = 21, l = 2, r = 6),
  area(t = 17, b = 21, l = 8, r = 12),
  
  area(t = 23, b = 25, l = 5, r = 9),
  area(t = 26, b = 27, l = 5, r = 9),
  
  area(t = 8, b = 23, l = 1, r = 13)
)

# combine and plot!
(p1 + p2 + p3 + p4 + p2 + p2 + p5 + p6 + p7 + p8 + p9) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Note how we used the `geom_shape()` function from the [**ggforce** package](https://CRAN.R-project.org/package=ggforce) [@R-ggforce] to make the two dashed borders with the rounded edges. You can learn more from Pedersen's [-@pedersenDrawPolygonsExpansion] vignette, [*Draw polygons with expansion/contraction and/or rounded corners — geom_shape*](https://ggforce.data-imaginist.com/reference/geom_shape.html).

### Solution by formal analysis.

Here we rehearse if we have $\operatorname{beta}(\theta, a, b)$ prior for $\theta$ of the Bernoulli likelihood function, then the analytic solution for the posterior is $\operatorname{beta}(\theta | z + a, N – z + b)$. Within this paradigm, if you would like to compute $p(D | m)$, don't use the following function. If suffers from [underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow) with large values.

```{r, eval = F}
p_d <- function(z, n, a, b) { 
  beta(z + a, n - z + b) / beta(a, b) 
}
```

This version is more robust.

```{r p_d}
p_d <- function(z, n, a, b) { 
  exp(lbeta(z + a, n - z + b) - lbeta(a, b)) 
}
```

You'd use it like this to compute $p(D|m_1)$.

```{r}
p_d(z = 6, n = 9, a = 3.5, b = 8.5)
```

So to compute our BF, $\frac{p(D|m_1)}{p(D|m_2)}$, you might use the `p_d()` function like this.

```{r}
p_d_1 <- p_d(z = 6, n = 9, a = 3.5, b = 8.5)
p_d_2 <- p_d(z = 6, n = 9, a = 8.5, b = 3.5)

p_d_1 / p_d_2
```

And if we computed the BF the other way, it'd look like this.

```{r}
p_d_2 / p_d_1
```

Since the BF itself is only $\text{BF} = \frac{p(D | m = 1)}{p(D | m = 2)}$, we'd need to bring in the priors for the models themselves to get the posterior probabilities, which follows the form

$$\frac{p(m = 1 | D)}{p(m = 2 | D)} = \left (\frac{p(D | m = 1)}{p(D | m = 2)} \right ) \left ( \frac{p(m = 1)}{p(m = 2)} \right).$$

If for both our models $p(m) = .5$, then the BF is the same it was, before.

```{r}
(p_d_1 * .5) / (p_d_2 * .5)
```

As Kruschke pointed out, because we're working in the probability metric, the sum of $p(m = 1 | D )$ and $p(m = 2 | D )$ must be 1. By simple algebra then,

$$p(m = 2 | D ) = 1 - p(m = 1 | D ).$$

Therefore, it's also the case that

$$\frac{p(m = 1 | D)}{1 - p(m = 1 | D)} = 0.2135266.$$

Thus, 0.2135266 is in an odds metric. If you want to convert odds to a probability, you follow the formula

$$\text{odds} = \frac{\text{probability}}{1 - \text{probability}}.$$

And with more algebraic manipulation, you can solve for the probability.

\begin{align*}
\text{odds} & =  \frac{\text{probability}}{1 - \text{probability}} \\
\text{odds} - \text{odds} \cdot \text{probability} & =  \text{probability} \\
\text{odds} & =  \text{probability} + \text{odds} \cdot \text{probability} \\
\text{odds} & =  \text{probability} (1 + \text{odds}) \\
\frac{\text{odds}}{1 + \text{odds}} & =  \text{probability}
\end{align*}

Thus, the posterior probability for $m = 1$ is

$$p(m = 1 | D) = \frac{0.2135266}{1 + 0.2135266}.$$

We can express that in code like so.

```{r}
odds <- (p_d_1 * .5) / (p_d_2 * .5)

odds / (1 + odds)
```

Relative to $m = 2$, our posterior probability for $m = 1$ is about .18. Therefore the posterior probability of $m = 2$ is 1 minus that.

```{r}
1 - (odds / (1 + odds))
```

Given the data, the two models and the prior assumption they were equally credible, we conclude $m = 2$ is .82 probable.

### Solution by grid approximation.

As in earlier chapters, we won't be able to make the wireframe plots on the left of Figure 10.3. But we can do some of the others. Here's the upper right panel.

```{r, fig.width = 3.5, fig.height = 3, warning = F, message = F}
p13 <-
  tibble(omega = seq(from = 0, to = 1, length.out = length)) %>% 
  mutate(m_p = ifelse(omega %in% c(.25, .75), 15, 0)) %>% 
  
  ggplot(aes(xmin = 0, xmax = m_p, y = omega)) +
  geom_ribbon(fill = kh[4], color = kh[4]) +
  scale_x_continuous(expand = expansion(mult = c(0.002, 0.05)), limits = c(0, 25)) +
  scale_y_continuous(expand = expansion(mult = c(0, 0))) +
  labs(x = expression(Marginal~p(omega)),
       y = expression(omega))

p13 + labs(subtitle = "Remember, the scale on the x is arbitrary.")
```

Building on that, here's the upper middle panel of the "two [prior] dorsal fins" (p. 271).

```{r, fig.height = 3, fig.width = 3.25}
d <-
  crossing(omega = seq(from = 0, to = 1, length.out = length),
           theta = seq(from = 0, to = 1, length.out = length)) %>% 
  mutate(prior = ifelse(omega == .25, dbeta(theta, 3.5, 8.5),
                          ifelse(omega == .75, dbeta(theta, 8.5, 3.5),
                                 0)))
p12 <-
  d %>% 
  ggplot(aes(x = theta, y = omega, fill = prior)) +
  geom_raster(interpolate = T) +
  scale_fill_gradient(low = kh[1], high = kh[9], breaks = NULL) +
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = c(0, 0)) +
  scale_y_continuous(expression(omega), breaks = 0:5 / 5, expand = c(0, 0))

p12
```

This time we'll separate $p_{m = 1}(\theta)$ and $p_{m = 2}(\theta)$ into the two short plots on the right of the next row down.

```{r, fig.width = 3.5, fig.height = 3, warning = F, message = F}
p23 <-
  d %>%
  filter(omega %in% c(.25, .75)) %>%
  mutate(omega = factor(str_c("omega == ", omega),
                        levels = str_c("omega == ", c(.75, .25)))) %>%
  
  ggplot(aes(x = theta, y = prior)) +
  geom_area(fill = kh[4]) +
  scale_x_continuous(expression(theta), breaks = 0:5 / 5,
                     expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expression(Marginal~p(theta*"|"*omega)),
                     expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +
  facet_wrap(~ omega, ncol = 1, scales = "free", labeller = label_parsed)

p23
```

We can continue to build on those sensibilities for the middle panel of the same row. Here we're literally adding $p_{m = 1}(\theta)$ to $p_{m = 2}(\theta)$ and taking their average.

```{r, fig.width = 3.5, fig.height = 3}
p22 <-
  tibble(theta = seq(from = 0, to = 1, length.out = length)) %>% 
  mutate(d_75 = dbeta(x = theta, shape1 = 8.5, shape2 = 3.5),
         d_25 = dbeta(x = theta, shape1 = 3.5, shape2 = 8.5)) %>% 
  mutate(mean_prior = (d_75 + d_25) / 2) %>% 

  ggplot(aes(x = theta, y = mean_prior)) +
  geom_area(fill = kh[4]) +
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, 
                     expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expression(Marginal~p(theta)), 
                     expand = expansion(mult = c(0, 0.05)), limits = c(0, 3))

p22
```

We need the Bernoulli likelihood function for the next step.

```{r}
bernoulli_likelihood <- function(theta, data) {
  
  n <- length(data)
  z <- sum(data)
  
  return(theta^z * (1 - theta)^(n - sum(data)))
}
```

Time to feed our data and the parameter space into `bernoulli_likelihood()`, which will allow us to make the 2-dimensional density plot at the heart of Figure 10.3.

```{r, fig.height = 3, fig.width = 3.25}
n <- 9
z <- 6

trial_data <- rep(0:1, times = c(n - z, z))

d <-
  d %>% 
  mutate(likelihood = bernoulli_likelihood(theta = theta, 
                                           data  = trial_data))

p32 <-
  d %>%
  ggplot(aes(x = theta, y = omega, fill = likelihood)) +
  geom_raster(interpolate = T) +
  scale_fill_gradient(low = kh[1], high = kh[9], breaks = NULL) +
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = c(0, 0)) +
  scale_y_continuous(expression(omega), breaks = 0:5 / 5, expand = c(0, 0))

p32
```

Now we just need the marginal likelihood, $p(D)$, to compute the posterior. Our first depiction will be the middle panel of the second row from the bottom--the panel with the uneven dolphin fins.

```{r, fig.height = 3, fig.width = 3.25}
d <-
  d %>% 
  mutate(marginal_likelihood = sum(prior * likelihood)) %>% 
  mutate(posterior = (prior * likelihood) / marginal_likelihood) 

p42 <-
  d %>% 
  ggplot(aes(x = theta, y = omega, fill = posterior)) +
  geom_raster(interpolate = T) +
  scale_fill_gradient(low = kh[1], high = kh[9], breaks = NULL) +
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, expand = c(0, 0)) +
  scale_y_continuous(expression(omega), breaks = 0:5 / 5, expand = c(0, 0))

p42
```

Here, then, is a way to get the panel in on the right of the second row from the bottom.
  
```{r, fig.width = 3.5, fig.height = 3}
p43 <-
  d %>% 
  mutate(marginal = (posterior / max(posterior)) * 25) %>% 

  ggplot(aes(xmin = 0, xmax = marginal, y = omega)) +
  geom_ribbon(fill = kh[6], color = kh[6]) +
  scale_x_continuous(expression(omega), 
                     expand = expansion(mult = c(0.002, 0.05)), limits = c(0, 25)) +
  scale_y_continuous(expression(Marginal~p(omega*"|"*D)),
                     expand = expansion(mult = c(0, 0)))

p43
```

To make the middle bottom panel of Figure 10.3, we have to average the posterior values of $\theta$ over the grid of $\omega$ values. That is, we have to marginalize.

```{r, fig.width = 3.5, fig.height = 3, message = F}
 p52 <-
  d %>%
  group_by(theta) %>% 
  summarise(marginal_theta = mean(posterior)) %>% 
  
  ggplot(aes(x = theta, y = marginal_theta)) +
  geom_area(fill = kh[6]) +
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, 
                     expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expression(Marginal~p(theta*"|"*D)), 
                     expand = expansion(mult = c(0, 0.05)), limits = c(0, NA))

p52
```

For the lower right panel of Figure 10.3, we'll `filter()` to our two focal values of $\omega$ and then facet by them.

```{r, fig.width = 3.5, fig.height = 3}
p53 <-
  d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  mutate(omega = factor(str_c("omega == ", omega),
                        levels = str_c("omega == ", c(.75, .25)))) %>%
  

  ggplot(aes(x = theta, y = posterior)) +
  geom_area(fill = kh[6]) +
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, 
                     expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expression(Marginal~p(theta*"|"*omega*", "*D)), 
                     expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + 
  facet_wrap(~ omega, ncol = 1, scales = "free", labeller = label_parsed)

p53
```

Do note the different scales on the $y$. Here's what they'd look like on the same scale.

```{r, fig.width = 3.5, fig.height = 3}
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  mutate(omega = str_c("omega == ", omega)) %>%

  ggplot(aes(x = theta, y = posterior)) +
  geom_area(fill = kh[6]) + 
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, 
                     expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expression(Marginal~p(theta*"|"*omega)), 
                     expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + 
  facet_wrap(~ omega, ncol = 1, labeller = label_parsed)
```

Hopefully that helps build the intuition of what Kruschke meant when he wrote "*visual inspection suggests that the ratio of the heights is about 5 to 1, which matches the Bayes factor of 4.68 that we computed exactly in the previous section*" (p. 273, *emphasis* in the original).

Before we move on to the BF, let's save a few more ggplots and combine them with the previous bunch to make the full version of Figure 10.3.

```{r, fig.width = 6.5, fig.height = 9, warning = F, message = F}
p21 <-
  tibble(x = 1,
         y = 8:7,
         label = c("Prior", "K==12"),
         size = c(2, 1)) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(aes(size = size), 
            color = kh[1], parse = T, hjust = 0, show.legend = F) +
  scale_size_continuous(range = c(3.5, 5.5)) +
  coord_cartesian(xlim = c(1, 2),
                  ylim = c(4, 11)) +
  theme(axis.text = element_text(color = kh[9]),
        axis.ticks = element_blank(),
        panel.background = element_rect(color = kh[9]),
        text = element_text(color = kh[9]))

p33 <-
  tibble(x = 1,
         y = 8:7,
         label = c("Likelihood", "D = 6 heads, 3 tails"),
         size = c(2, 1)) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(aes(size = size), 
            hjust = 0, show.legend = F, color = kh[1]) +
  scale_size_continuous(range = c(3.5, 5.5)) +
  coord_cartesian(xlim = c(1, 2),
                  ylim = c(4, 11)) +
  theme(axis.text = element_text(color = kh[9]),
        axis.ticks = element_blank(),
        panel.background = element_rect(color = kh[9]),
        text = element_text(color = kh[9]))

p51 <-
  ggplot() +
  annotate(geom = "text", x = 1, y = 8, 
           label = "Posterior", size = 6, hjust = 0, color = kh[1]) +
  coord_cartesian(xlim = c(1, 2),
                  ylim = c(3, 11)) +
  theme(axis.text = element_text(color = kh[9]),
        axis.ticks = element_blank(),
        panel.background = element_rect(color = kh[9]),
        text = element_text(color = kh[9]))

p11 <- plot_spacer()

# combine and plot!
(p11 / p21 / p11 / p11 / p51) | (p12 / p22 / p32 / p42 / p52) | (p13 / p23 / p33 / p43 / p53)
```

Oh mamma! Using the grid, you might compute that BF like this.

```{r, message = F}
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  group_by(omega) %>% 
  summarise(sum_posterior = sum(posterior)) %>% 
  mutate(model = c("model_1", "model_2")) %>% 
  pivot_wider(-omega, 
              names_from = model,
              values_from = sum_posterior) %>%
  mutate(BF = model_2 / model_1)
```

Please note[^3] how, in the previous section, Kruschke computed the BF as

$$\frac{p(m = 1 | D)}{p(m = 2 | D)} = .213,$$

which we achieved with this code:

```{r}
p_d_1 / p_d_2
```

Here, we're flipping the ratio to 

$$\frac{p(m = 2 | D)}{p(m = 1 | D)},$$

which is why we now have a BF near 5.

```{r}
p_d_2 / p_d_1
```

But anyway, both the posterior distributions in the figure and the BF indicate $\omega = .75$ is a better representation of the data than $\omega = .25$.

## Solution by MCMC

Kruschke started with: "For large, complex models, we cannot derive $p(D | m)$ analytically or with grid approximation, and therefore we will approximate the posterior probabilities using MCMC methods" (p. 274). He's not kidding. Welcome to modern Bayes.

### Nonhierarchical MCMC computation of each model's marginal likelihood.

Before you get excited, Kruschke warned: "For complex models, this method might not be tractable. [But] for the simple application here, however, the method works well, as demonstrated in the next section" (p. 277).

#### Implementation with ~~JAGS~~ brms.

Load **brms**.

```{r, warning = F, message = F}
library(brms)
```

Let's save the `trial_data` as a tibble.

```{r}
trial_data <- 
  tibble(y = trial_data)
```

Time to learn a new **brms** skill. When you want to enter variables into the parameters defining priors in `brms::brm()`, you need to specify them using the `stanvar()` function. Since we want to do this for two variables, we’ll use `stanvar()` twice and save the results as an object, conveniently named `stanvars`.

```{r}
omega <- .75
kappa <- 12

stanvars <-
  stanvar(     omega  * (kappa - 2) + 1, name = "my_alpha") +
  stanvar((1 - omega) * (kappa - 2) + 1, name = "my_beta")
```

Now we have our `stanvars` object, we are ready to fit the first model (i.e., the model for which $\omega = .75$).

```{r fit10.1}
fit10.1 <-
  brm(data = trial_data, 
      family = bernoulli(link = identity),
      y ~ 1,
      # stanvars lets us do this
      prior(beta(my_alpha, my_beta), class = Intercept, lb = 0, ub = 1),
      iter = 11000, warmup = 1000, chains = 4, cores = 4,
      seed = 10,
      stanvars = stanvars,
      file = "fits/fit10.01")
```

Note how we fed our `stanvars` object into the `stanvars` function.

Anyway, let's inspect the chains.

```{r, fig.width = 8, fig.height = 1.4}
plot(fit10.1, widths = c(2, 3))
```

They look great. Now we glance at the model summary.

```{r}
print(fit10.1)
```

Next we'll follow Kruschke and extract the posterior draws, saving them as `theta`.

```{r}
theta <- as_draws_df(fit10.1)

head(theta)
```

The `fixef()` function will return the posterior summaries for the model intercept (i.e., $\theta$). We can then index and save the desired summaries.

```{r}
fixef(fit10.1)

(mean_theta <- fixef(fit10.1)[1])
(sd_theta <- fixef(fit10.1)[2])
```

Now we'll convert them to the $\alpha$ and $\beta$ parameters, `a_post` and `b_post`, respectively.

```{r}
a_post <-      mean_theta  * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1)
b_post <- (1 - mean_theta) * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1)
```

Recall we've already defined several values.

```{r}
n     <- 9
z     <- 6
omega <- .75
kappa <- 12
```

The reason we're saving all these values is we're aiming to compute $p(D)$, the probability of the data (i.e., the marginal likelihood), given the model. But our intermediary step will be computing its reciprocal, $\frac{1}{p(D)}$. Here we'll express Kruschke's `oneOverPD` as a function, `one_over_pd()`.

```{r}
one_over_pd <- function(theta) {
  
  mean(dbeta(theta, a_post, b_post ) / 
         (theta^z * (1 - theta)^(n - z) * 
            dbeta(theta, omega * (kappa - 2) + 1, (1 - omega) * (kappa - 2) + 1 )))
  
}
```

We're ready to use `one_over_pd()` to help compute $p(D)$.

```{r}
theta %>% 
  summarise(pd = 1 / one_over_pd(theta = b_Intercept))
```

That matches up nicely with Kruschke's value at the top of page 278! Let's rinse, wash, and repeat for $\omega = .25$. First, we'll need to redefine `omega` and our `stanvars`.

```{r}
omega <- .25

stanvars <-
  stanvar(     omega  * (kappa - 2) + 1, name = "my_alpha") +
  stanvar((1 - omega) * (kappa - 2) + 1, name = "my_beta")
```

Fit the model.

```{r fit10.2}
fit10.2 <-
  brm(data = trial_data, 
      family = bernoulli(link = identity),
      y ~ 1,
      prior(beta(my_alpha, my_beta), class = Intercept, lb = 0, ub = 1),
      iter = 11000, warmup = 1000, chains = 4, cores = 4,
      seed = 10,
      stanvars = stanvars,
      file = "fits/fit10.02")
```

We'll do the rest in bulk.

```{r}
theta <- as_draws_df(fit10.2)

mean_theta <- fixef(fit10.2)[1]
sd_theta   <- fixef(fit10.2)[2]

a_post <-      mean_theta  * (mean_theta * (1 - mean_theta) / sd_theta^2 - 1)
b_post <- (1 - mean_theta) * (mean_theta * (1 - mean_theta) / sd_theta^2 - 1)

theta %>% 
  summarise(pd = 1 / one_over_pd(theta = b_Intercept))
```

Boom!

### Hierarchical MCMC computation ~~of relative model probability~~ is not available in brms: We'll cover information criteria instead.

I'm not aware of a way to specify a model "in which the top-level parameter is the index across models" in **brms** (p. 278). If you know of a way, [share your code](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues).

However, we do have options. We can compare and weight models using information criteria, about which you can learn more [here](https://youtu.be/t0pRuy1_190?t=978) or [here](https://www.youtube.com/watch?v=gjrsYDJbRh0&list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&index=8)
. In **brms**, the LOO and WAIC are two primary information criteria available. You can compute them for a given model with the `loo()` and `waic()` functions, respectively. Here's a quick example of how to use the `waic()` function.

```{r}
waic(fit10.1)
```

We'll explain that output in a bit. Before we do, you should know the current recommended workflow for information criteria with **brms** models is to use the `add_criterion()` function, which will allow us to compute information-criterion-related output and save it to our **brms** fit objects. Here's how to do that with both our fits.

```{r ic_fit10.1_fit10.2, message = F}
fit10.1 <- add_criterion(fit10.1, criterion = c("loo", "waic"))
fit10.2 <- add_criterion(fit10.2, criterion = c("loo", "waic"))
```

You can extract the same WAIC output for `fit10.1` we saw above by executing `fit10.1$criteria$waic`. Here we look at the LOO summary for `fit10.2`, instead.

```{r}
fit10.2$criteria$loo
```

You get a wealth of output, more of which can be seen by executing `str(fit10.1$criteria$loo)`. First, notice the message "All Pareto k estimates are good (k < 0.5)." Pareto $k$ values can be used for diagnostics [@vehtariUsingLooPackage2022, [*Plotting Pareto* $k$ *diagnostics*](https://cran.r-project.org/web/packages/loo/vignettes/loo2-example.html#plotting-pareto-k-diagnostics)]. Each case in the data gets its own $k$ value and we like it when those $k$s are low. The makers of the [**loo** package](https://github.com/stan-dev/loo) [@R-loo; @vehtariPracticalBayesianModel2017] get worried when $k$ values exceed 0.7 and, as a result, we will get warning messages when they do. Happily, we have no such warning messages in this example.

In the main section, we get estimates for the expected log predictive density (`elpd_loo`), the estimated effective number of parameters (`p_loo`), and the Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO; `looic`). Each estimate comes with a standard error (i.e., `SE`). Like other information criteria, the LOO values aren't of interest in and of themselves. However, the estimate of one model's LOO relative to that of another is of great interest. We generally prefer models with lower information criteria. With the `loo_compare()` function, we can compute a formal difference score between two models.

```{r}
loo_compare(fit10.1, fit10.2, criterion = "loo")
```

The `loo_compare()` output rank orders the models such that the best fitting model appears on top. All models receive a difference score relative to the best model. Here the best fitting model is `fit10.1` and since the LOO for `fit10.1` minus itself is zero, the values in the top row are all zero.

Each difference score also comes with a standard error. In this case, even though `fit10.1` has the lower estimates, the standard error is twice the magnitude of the difference score. So the LOO difference score puts the two models on similar footing. You can do a similar analysis with the WAIC estimates.

In addition to difference-score comparisons, you can also use the LOO or WAIC for AIC-type model weighting. In **brms**, you do this with the `model_weights()` function.

```{r mw_fit10.1_and_fit10.2}
(mw <- model_weights(fit10.1, fit10.2))
```

I don't know that I'd call these weights probabilities, but they do sum to one. In this case, the analysis suggests we put about five times more weight to `fit10.1` relative to `fit10.2`.

```{r}
mw[1] / mw[2]
```

With `brms::model_weights()`, we have a variety of weighting schemes available to us. Since we didn't specify any in the `weights` argument, we used the default `"stacking"`, which is--perhaps confusingly given the name--the stacking method according to the paper by @yaoUsingStackingAverage2018 Vehtari has [written about the paper](https://statmodeling.stat.columbia.edu/2017/04/11/stacking-pseudo-bma-and-aic-weights/) on Gelman's blog, too. But anyway, the point is that different weighting schemes might not produce the same results. For example, here's the result from weighting using the WAIC.

```{r}
model_weights(fit10.1, fit10.2, weights = "waic")
```

The results are similar, for sure. But they're not the same. The stacking method via the **brms** default `weights = "stacking"` is the current preferred method by the folks on the Stan team (e.g., the authors of the above linked paper).

For more on stacking and other weighting schemes, see Vehtari and Gabry's [-@vehtariBayesianStackingPseudoBMA] vignette, [*Bayesian Stacking and Pseudo-BMA weights using the loo package*](https://CRAN.R-project.org/package=loo/vignettes/loo2-weights.html), or Vehtari's [modelselection_tutorial GitHub repository](https://github.com/avehtari/modelselection_tutorial). But don't worry. We will have more opportunities to practice with information criteria, model weights, and such later in this ebook.

#### ~~Using~~ [No need to use] pseudo-priors to reduce autocorrelation.

Since we didn't use Kruschke's method from the last subsection, we don't have the same worry about autocorrelation. For example, here are the autocorrelation plots for `fit10.1`.

```{r, fig.width = 4, fig.height = 4, message = F, warning = F}
library(bayesplot)

color_scheme_set(scheme = c(lisa_palette("KatsushikaHokusai", n = 9, type = "continuous")[6:1]))


theta %>% 
  mutate(chain = .chain) %>% 
  mcmc_acf(pars = "b_Intercept",
           lags = 35)
```

Our autocorrelations were a little high for HMC, but nowhere near pathological. The results for `fit10.2` were similar. Before we move on, note our use of `bayesplot::color_scheme_set()`, which allowed us to customize the color scheme **bayesplot** used within the plot. Based on that code, here is our new color scheme for all plots made by **bayesplot**.

```{r, fig.height = 2.75}
color_scheme_view()
color_scheme_get()
```

In case you were curious, here is the default.

```{r, fig.height = 2.75}
color_scheme_view(scheme = "blue")
```

Anyway, as you might imagine from the moderate autocorrelations, the $N_{eff}/N$ ratio for `b_Intercept` wasn't great.

```{r, fig.width = 6, fig.height = 1.25}
neff_ratio(fit10.1)[1] %>% 
  mcmc_neff() +
  yaxis_text(hjust = 0)
```

But we specified a lot of post-warmup draws, so we're still in good shape. Plus, the $\hat R$ was fine.

```{r}
rhat(fit10.1)[1]
```

### Models with different "noise" distributions in ~~JAGS~~ brms.

> Probability distribution[s are] sometimes [called "noise"] distribution[s] because [they describe] the random variability of the data values around the underlying trend. In more general applications, different models can have different noise distributions. For example, one model might describe the data as log-normal distributed, while another model might describe the data as gamma distributed. (p. 288)

If there are more than one plausible noise distributions for our data, we might want to compare the models. Kruschke then gave us a general trick in the form of this JAGS code:

```{r, eval = F}
data {
  C <- 10000 # JAGS does not warn if too small!
  for (i in 1:N) {
    ones[i] <- 1 }
} model {
  for (i in 1:N) {
    spy1[i] <- pdf1(y[i], parameters1) / C # where pdf1 is a formula
    spy2[i] <- pdf2(y[i], parameters2) / C # where pdf2 is a formula
    spy[i]  <- equals(m,1) * spy1[i] + equals(m, 2) * spy2[i]
    ones[i] ~ dbern(spy[i])
  }
  parameters1 ~ dprior1...
  parameters2 ~ dprior2...
  m ~ dcat(mPriorProb[])
  mPriorProb[1] <- .5
  mPriorProb[2] <- .5
}
```

I'm not aware that we can do this within the Stan/**brms** framework. If I'm in error and you know how, [please share your code](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues). However, we do have options. In anticipation of [Chapter 16][Metric-Predicted Variable on One or Two Groups], let's consider Gaussian-like data with thick tails. We might generate some like this.

```{r}
# how many draws would you like?
n <- 1e3

set.seed(10)
(d <- tibble(y = rt(n, df = 7)))
```

The resulting data look like this.

```{r, fig.width = 3.5, fig.height = 2.5}
d %>% 
  ggplot(aes(x = y)) +
  geom_histogram(color = kh[9], fill = kh[3],
                 size = .2, bins = 30) +
  scale_y_continuous(NULL, breaks = NULL,
                     expand = expansion(mult = c(0, 0.05))) +
  theme(panel.grid = element_blank())
```

As you'd expect with a small-$\nu$ Student's $t$, some of our values are far from the central clump. If you don't recall, Student's $t$-distribution has three parameters: $\nu$, $\mu$, and $\sigma$. The Gaussian is a special case of Student's $t$ for which $\nu = \infty$. As $\nu$ gets small, the distribution allocates more mass in the tails. From a Gaussian perspective, the small-$\nu$ Student's $t$ expects more outliers--though it's a little odd calling them outliers from a small-$\nu$ Student's $t$ perspective.

Let's see how well the Gaussian versus the Student's $t$ likelihoods handle the data. Here we'll use fairly liberal priors.

```{r fit10.3_and_fit10.4}
fit10.3 <-
  brm(data = d,
      family = gaussian,
      y ~ 1,
      prior = c(prior(normal(0, 5), class = Intercept),
                prior(normal(0, 5), class = sigma)),  # by default, this has a lower bound of 0
      chains = 4, cores = 4,
      seed = 10,
      file = "fits/fit10.03")

fit10.4 <-
  brm(data = d,
      family = student,
      y ~ 1,
      prior = c(prior(normal(0, 5), class = Intercept),
                prior(normal(0, 5), class = sigma),
                prior(gamma(2, 0.1), class = nu)),  # this is the brms default prior for nu
      chains = 4, cores = 4,
      seed = 10,
      file = "fits/fit10.04")
```

In case you were curious, here's what that default `gamma(2, 0.1)` prior on `nu` looks like.

```{r, fig.width = 4, fig.height = 2}
tibble(x = seq(from = 1, to = 100, by = 1)) %>% 
  ggplot(aes(x = x, y = dgamma(x, 2, 0.1))) +
  geom_area(fill = kh[5]) +
  scale_y_continuous(NULL, breaks = NULL,
                     expand = expansion(mult = c(0, 0.05))) +
  scale_x_continuous(expression(italic(p)(nu)), expand = expansion(mult = c(0, 0.01)), limits = c(0, 100))
```

That prior puts most of the probability mass below 50, but the right tail gently fades off into the triple digits, allowing for the possibility of larger estimates.

We can use the `posterior_summary()` function to get a compact look at the model summaries.

```{r}
posterior_summary(fit10.3) %>% round(digits = 2)
posterior_summary(fit10.4) %>% round(digits = 2)
```

Now we can compare the two approaches using information criteria. For kicks, we'll use the WAIC.

```{r ic_fit10.3_and_fit10.4, warning = F, message = F}
fit10.3 <- add_criterion(fit10.3, criterion = c("loo", "waic"))
fit10.4 <- add_criterion(fit10.4, criterion = c("loo", "waic"))

loo_compare(fit10.3, fit10.4, criterion = "waic")
```

Based on the WAIC difference, we have some support for preferring the Student's $t$, but do notice how wide that `SE` was. We can also compare the models using model weights. Here we'll use the default weighting scheme.

```{r, warning = F, message = F}
model_weights(fit10.3, fit10.4)
```

Virtually all of the stacking weight was placed on the Student's-$t$ model, `fit10.4`.

Remember what that $p(\nu)$ looked like? Here's our posterior distribution for $\nu$.

```{r, fig.width = 3.5, fig.height = 2.5}
as_draws_df(fit10.4) %>% 
  ggplot(aes(x = nu)) +
  geom_histogram(color = kh[9], fill = kh[3],
                 size = .2, bins = 30) +
  scale_x_continuous(expression(italic(p)(nu*"|"*italic(D))), 
                     expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL,
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 21)) +
  labs(subtitle = expression("Recall that for the Gaussian, "*nu==infinity.))
```

Even though our prior for $\nu$ was relatively weak, the posterior ended up concentrated on values in the middle-single-digit range. Recall the data-generating value was 7.

We can also compare the models using posterior-predictive checks. There are a variety of ways we might do this, but the most convenient way is with `brms::pp_check()`, which is itself a wrapper for the family of `ppc` functions from the **bayesplot** package.

```{r, fig.width = 4, fig.height = 2, message = F}
pp_check(fit10.3, ndraws = 50) + coord_cartesian(xlim = c(-10, 10))
pp_check(fit10.4, ndraws = 50) + coord_cartesian(xlim = c(-10, 10))
```

The default `pp_check()` setting allows us to compare the density of the data $y$ (i.e., the dark blue) with 10 densities simulated from the posterior $y_\text{rep}$ (i.e., the light blue). By `ndraws = 50`, we adjusted that default to 50 simulated densities. We prefer models that produce $y_\text{rep}$ distributions resembling $y$. Though the results from both models were similar, the simulated distributions from `fit10.4` mimicked the original data a little more convincingly. To learn more about this approach to posterior predictive checks, check out Gabry's [-@gabryGraphicalPosteriorPredictive2022] vignette, [*Graphical posterior predictive checks using the bayesplot package*](https://CRAN.R-project.org/package=bayesplot/vignettes/graphical-ppcs.html).

## Prediction: Model averaging

> In many applications of model comparison, the analyst wants to identify the best model and then base predictions of future data on that single best model, denoted with index $b$. In this case, predictions of future $\hat y$ are based exclusively on the likelihood function $p_b(\hat y | \theta_b, m = b)$ and the posterior distribution $p_b(\theta_b | D, m = b)$ of the winning model:
>
> $$p_b(\hat y | D, m = b) = \int \text d \theta_b \; p_b (\hat y | \theta_b, m = b) p_b(\theta_b | D, m = b)$$
>
> But the full model of the data is actually the complete hierarchical structure that spans all the models being compared, as indicated in Figure 10.1 (p. 267). Therefore, if the hierarchical structure really expresses our prior beliefs, then the most complete prediction of future data takes into account all the models, weighted by their posterior credibilities. In other words, we take a weighted average across the models, with the weights being the posterior probabilities of the models. Instead of conditionalizing on the winning model, we have
>
> \begin{align*}
> p (\hat y | D) & = \sum_m p (\hat y | D, m) p (m | D) \\
> & = \sum_m \int \text d \theta_m \; p_m (\hat y | \theta_m, m) p_m(\theta_m | D, m) p (m | D)
> \end{align*}
>
> This is called model averaging. (p. 289)

Okay, while the concept of model averaging is of great interest, we aren't going to be able to follow this approach to it within the Stan/**brms** paradigm. This, recall, is because our paradigm doesn't allow for a hierarchical organization of models in the same way JAGS does. However, we can still play the model averaging game with extensions of our model weighting paradigm, above. Before we get into the details,

> recall that there were two models of mints that created the coin, with one mint being tail-biased with mode $\omega = 0.25$ and one mint being head-biased with mode $\omega = 0.75$ The two subpanels in the lower-right [of Figure 10.3] illustrate the posterior distributions on $\omega$ within each model, $p(\theta | D, \omega = 0.25)$ and $p(\theta | D, \omega = 0.75)$ The winning model was $\omega = 0.75$, and therefore the predicted value of future data, based on the winning model alone, would use $p(\theta | D, \omega = 0.75)$. (p. 289)

Here's the histogram for $p(\theta | D, \omega = 0.75)$, which we generate from our `fit10.1`.

```{r, fig.width = 3.5, fig.height = 3, warning = F, message = F}
library(tidybayes)

as_draws_df(fit10.1) %>% 
  ggplot(aes(x = b_Intercept, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = c(.95, .5),
                    fill = kh[6], slab_color = kh[5], color = kh[2],
                    breaks = 40, slab_size = .25, outline_bars = T) +
  scale_x_continuous(expression(italic(p)(theta*"|"*italic(D)*", "*omega==.75)), 
                     expand = expansion(mult = c(0, 0)), 
                     breaks = 0:5 / 5, limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL,
                     expand = expansion(mult = c(0.01, 0.05))) +
  labs(subtitle = "The posterior for the probability, given fit10.1") 
```

> But the overall model included $\omega = 0.75$, and if we use the overall model, then the predicted value of future data should be based on the complete posterior summed across values of $\omega$. The complete posterior distribution [is] $p(\theta | D)$ (p. 289).

The cool thing about the model weighting stuff we learned about earlier is that you can use those model weights to average across models. Again, we're not weighting the models by posterior probabilities the way Kruschke discussed in text. However, the spirit is similar. We can use the `brms::pp_average()` function to make posterior predictive prediction with mixtures of the models, weighted by our chosen weighting scheme. Here, we’ll go with the default stacking weights.

```{r pp_averaged_fit10.1_fit10.2, warning = F, message = F}
nd <- tibble(y = 1)

pp_a <-
  pp_average(fit10.1, fit10.2, 
             newdata = nd,
             # this line is not necessary, but you should see how to choose weighing methods
             weights = "stacking",
             method = "fitted",
             summary = F) %>% 
  as_tibble() %>% 
  set_names("theta")

# what does this produce?
head(pp_a) 
```

We can plot our model-averaged $\theta$ with a little help from good old `tidybayes::stat_histintervalh()`.

```{r, fig.width = 3.5, fig.height = 3}
pp_a %>% 
  ggplot(aes(x = theta, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = c(.95, .5),
                    fill = kh[6], slab_color = kh[5], color = kh[2],
                    breaks = 40, slab_size = .25, outline_bars = T) +
  scale_x_continuous(expression(italic(p)(theta*"|"*italic(D))), 
                     expand = expansion(mult = c(0, 0)), 
                     breaks = 0:5 / 5, limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL,
                     expand = expansion(mult = c(0.01, 0.05))) +
  labs(subtitle = "The posterior for the probability, given the\nweighted combination of fit10.1 and fit10.2")
```

As Kruschke concluded, "you can see the contribution of $p(\theta | D, \omega = 0.25)$ as the extended leftward tail" (p. 289). Interestingly enough, that looks a lot like the marginal density of $p (\theta | D)$ across values of $\omega$ we made with grid approximation in Figure 10.3, doesn't it?

## Model complexity naturally accounted for

> A complex model (usually) has an inherent advantage over a simpler model because the complex model can find some combination of its parameter values that match the data better than the simpler model. There are so many more parameter options in the complex model that one of those options is likely to fit the data better than any of the fewer options in the simpler model. The problem is that data are contaminated by random noise, and we do not want to always choose the more complex model merely because it can better fit noise. Without some way of accounting for model complexity, the presence of noise in data will tend to favor the complex model.
>
> Bayesian model comparison compensates for model complexity by the fact that each model must have a prior distribution over its parameters, and more complex models must dilute their prior distributions over larger parameter spaces than simpler models. Thus, even if a complex model has some particular combination of parameter values that fit the data well, the prior probability of that particular combination must be small because the prior is spread thinly over the broad parameter space. (pp. 289--290)

Now our two models are:

* $p(\theta | D, \kappa = 2000)$ (i.e., the "must-be-fair" model) and
* $p(\theta | D, \kappa = 2)$ (i.e., the "anything's-possible" model).

They look like this.

```{r, fig.width = 7, fig.height = 3}
# how granular to you want the theta sequence?
n <- 1e3

# simulate the data
tibble(omega = .5,
       kappa = c(1000, 2),
       model = c("The must-be-fair model", "The anything's-possible model")) %>% 
  expand(nesting(omega, kappa, model),
         theta = seq(from = 0, to = 1, length.out = n)) %>% 
  mutate(density = dbeta(theta, 
                         shape1 =      omega  * (kappa - 2) + 1, 
                         shape2 = (1 - omega) * (kappa - 2) + 1)) %>% 
  
  # plot
  ggplot(aes(x = theta, y = density)) +
  geom_area(fill = kh[5]) + 
  scale_x_continuous(expression(theta), expand = expansion(mult = c(0, 0)), 
                     breaks = 0:5 / 5, labels = c("0", ".2", ".4", ".6", ".8", "1"),
                     limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL,
                     expand = expansion(mult = c(0, 0.05))) +
  labs(subtitle = "Note that in this case, their y-axes are on the same scale.") +
  facet_wrap(~ model)
```

Here's how you might compute the $\alpha$ and $\beta$ values for the corresponding beta distributions.

```{r}
tibble(omega = .5,
       kappa = c(1000, 2),
       model = c("The must-be-fair model", "The anything's-possible model")) %>%
  mutate(alpha =      omega  * (kappa - 2) + 1,
         beta  = (1 - omega) * (kappa - 2) + 1)
```

With those in hand, we can use our `p_d()` function to compute the Bayes factor based on flipping a coin $N = 20$ times and observing $z = 15$ heads.

```{r}
# the data summaries
z <- 15
n <- 20

p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1)
```

Let's try again, this time supposing we observe $z = 15$ heads out of $N = 20$ coin flips.

```{r}
z <- 11

p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1)
```

> The anything's-possible model loses because it pays the price of having a small prior probability on the values of $\theta$ near the data proportion, while the must-be-fair model has large prior probability on $\theta$ values sufficiently near the data proportion to be credible. Thus, in Bayesian model comparison, a simpler model can win if the data are consistent with it, even if the complex model fits just as well. The complex model pays the price of having small prior probability on parameter values that describe simple data. (p. 291)

### Caveats regarding nested model comparison.

> A frequently encountered special case of comparing models of different complexity occurs when one model is "nested" within the other. Consider a model that implements all the meaningful parameters we can contemplate for the particular application. We call that the full model. We might consider various restrictions of those parameters, such as setting some of them to zero, or forcing some to be equal to each other. A model with such a restriction is said to be nested within the full model. (p. 291)

Kruschke didn't walk out the examples in this section. But for the sake of practice, let's work through the first one. "Recall the hierarchical model of baseball batting abilities" from [Chapter 9][Example: Baseball batting abilities by position.] (p. 291). Let's reload those data.

```{r, warning = F, message = F}
my_data <- read_csv("data.R/BattingAverage.csv")

glimpse(my_data)
```

"The full model has a distinct modal batting ability, $\omega_c$ , for each of the nine fielding positions. The full model also has distinct concentration parameters for each of the nine positions" (p. 291). Let's fit that model again.

```{r fit9.2}
fit9.2 <-
  brm(data = my_data,
      family = binomial(link = logit),
      Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 3500, warmup = 500, chains = 3, cores = 3,
      control = list(adapt_delta = .99),
      seed = 9,
      file = "fits/fit09.02")
```

Next we'll consider a restricted version of `fit9.2` "in which all infielders (first base, second base, etc.) are grouped together versus all outfielders (right field, center field, and left field). In this restricted model, we are forcing the modal batting abilities of all the outfielders to be the same, that is, $\omega_\text{left field} = \omega_\text{center field} = \omega_\text{right field}$" (p. 291). To fit that model, we'll need to make a new variable `PriPos_small` which is identical to its parent variable `PriPos` except that it collapses those three positions into our new category `Outfield`.

```{r}
 my_data <-
  my_data %>% 
  mutate(PriPos_small = if_else(PriPos %in% c("Center Field", "Left Field", "Right Field"),
                                "Outfield", PriPos))
```

Now use `update()` to fit the restricted model.

```{r fit10.5}
fit10.5 <-
  update(fit9.2,
         newdata = my_data,
         formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos_small) + (1 | PriPos_small:Player),
         iter = 3500, warmup = 500, chains = 3, cores = 3,
         control = list(adapt_delta = .99),
         seed = 10,
         file = "fits/fit10.05")
```

Unlike with what Kruschke alluded to in the prose, here we'll compare the two models with the WAIC.

```{r waic_fit9.2_fit10.5, warning = F, message = F}
fit9.2  <- add_criterion(fit9.2, criterion = "waic")
fit10.5 <- add_criterion(fit10.5, criterion = "waic")

loo_compare(fit9.2, fit10.5, criterion = "waic")
```

Based on the WAIC difference score, they're near equivalent. Now let's see how their WAIC weights shake out.

```{r}
model_weights(fit9.2, fit10.5, weights = "waic") %>% round(2)
```

In this case, just a little more of the weight went to the full model, `fit9.2`. The overall pattern between the WAIC difference and the WAIC weights was uncertainty. Make sure to use good substantive reasoning when comparing models.

## Extreme sensitivity to prior distribution

> In many realistic applications of Bayesian model comparison, the theoretical emphasis is on the difference between the models' likelihood functions. For example, one theory predicts planetary motions based on elliptical orbits around the sun, and another theory predicts planetary motions based on circular cycles and epicycles around the earth. The two models involve very different parameters. In these sorts of models, the form of the prior distribution on the parameters is not a focus, and is often an afterthought. But, when doing Bayesian model comparison, the form of the prior is crucial because the Bayes factor integrates the likelihood function weighted by the prior distribution. (p. 292)

However, "the sensitivity of Bayes factors to prior distributions is well known in the literature [e.g., @kassBayesFactors1995; @liuBayesFactorsPrior2008; @vanpaemelPriorSensitivityTheory2010]," and furthermore, when comparing Bayesian models using the methods Kruschke outlined in this chapter of the text, "different forms of vague priors can yield very different Bayes factors" (p. 293).

In the two BFs to follow, we compare the must-be-fair model and the anything's-possible models from 10.5 to new data: $z = 65, N = 100$.

```{r}
z <- 65
n <- 100 

p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1)
```

The resulting `r round(p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1), 2)` favored the anything's-possible model.

Another way to express the anything's-possible model is with the Haldane prior, which sets the two parameters within the beta distribution to be a) equivalent and b) quite small (i.e., 0.01 in this case).

```{r}
p_d(z, n, a = 500, b = 500) / p_d(z, n, a = .01, b = .01)
```

Now we flipped to favoring the must-be-fair model. You might be asking, *Wait, kind of distribution did that Haldane prior produce?* Here we compare it to the $\operatorname{beta}(1, 1)$.

```{r, fig.width = 7, fig.height = 3}
# how granular to you want the theta sequence?
length <- 1e3

# simulate the data
tibble(alpha = c(1, .01),
       beta  = c(1, .01),
       model = factor(c("Uninformative prior, beta(1, 1)", "Haldane prior, beta(0.01, 0.01)"),
                      levels = c("Uninformative prior, beta(1, 1)", "Haldane prior, beta(0.01, 0.01)"))) %>%
  expand(nesting(alpha, beta, model),
         theta = seq(from = 0, to = 1, length.out = length)) %>% 
  mutate(density = dbeta(theta, 
                         shape1 = alpha, 
                         shape2 = beta)) %>% 
  
  # plot
  ggplot(aes(x = theta, y = density)) +
  geom_area(fill = kh[3]) + 
  scale_x_continuous(expression(theta), expand = expansion(mult = c(0, 0)), 
                     breaks = 0:5 / 5, labels = c("0", ".2", ".4", ".6", ".8", "1"),
                     limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL,
                     expand = expansion(mult = c(0, 0.05))) +
  labs(title = "We have two anything’s-possible models.",
       subtitle = "These y-axes are on the same scale.") +
  facet_wrap(~ model)
```

Before we can complete the analyses of this subsection, we'll need to define our version of Kruschke's `HDIofICDF function()`, `hdi_of_icdf()`. Like we've done in previous chapters, here we mildly reformat the function.

```{r}
hdi_of_icdf <- function(name, width = .95, tol = 1e-8, ... ) {
  
  incredible_mass <- 1.0 - width
  interval_width <- function(low_tail_prob, name, width, ...) {
    name(width + low_tail_prob, ...) - name(low_tail_prob, ...)
  }
  opt_info <- optimize(interval_width, c(0, incredible_mass), 
                       name = name, width = width, 
                       tol = tol, ...)
  hdi_lower_tail_prob <- opt_info$minimum
  
  return(c(name(hdi_lower_tail_prob, ...),
           name(width + hdi_lower_tail_prob, ...)))
  
}
```

And here we'll make a custom variant to be more useful within the context of `map2()`.

```{r}
hdi_of_qbeta <- function(shape1, shape2) {
  
  hdi_of_icdf(name = qbeta,
              shape1 = shape1,
              shape2 = shape2) %>% 
    data.frame() %>% 
    mutate(level = c("ll", "ul")) %>% 
    spread(key = level, value = ".")
  
}
```

Recall that when we combine a $\operatorname{beta}(\theta | \alpha, \beta)$ prior with the results of a Bernoulli likelihood, we get a posterior defined by $\operatorname{beta}(\theta | z + \alpha, N - z + \beta)$.

```{r}
d <-
  tibble(model   = c("Uniform", "Haldane"),
         prior_a = c(1, .01),
         prior_b = c(1, .01)) %>% 
  mutate(posterior_a = z + prior_a,
         posterior_b = n - z + prior_b)

d
```

Now we'll use our custom `hdi_of_qbeta()` to compute the HDIs.

```{r}
(
  d <- 
  d %>% 
  mutate(levels = map2(posterior_a, posterior_b, hdi_of_qbeta)) %>% 
  unnest(levels)
 )
```

Let's compare those HDIs in a plot.

```{r, fig.width = 3.5, fig.height = 1.5}
d %>% 
  ggplot(aes(x = ll,    xend = ul,
             y = model, yend = model)) +
  geom_segment(size = 1, color = kh[2]) +
  scale_x_continuous(expression(theta), expand = expansion(mult = c(0, 0)), 
                     breaks = 0:5 / 5, labels = c("0", ".2", ".4", ".6", ".8", "1"),
                     limits = c(0, 1)) +
  labs(subtitle = "Those two sets of HDIs are quite similar.\nIt almost seems silly their respective BFs\nare so different.",
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())
```

"The HDIs are virtually identical. In particular, for either prior, the posterior distribution rules out $\theta = 0.5$, which is to say that the must-be-fair hypothesis is not among the credible values" (p. 294).

### Priors of different models should be equally informed.

"We have established that seemingly innocuous changes in the vagueness of a vague prior can dramatically change a model's marginal likelihood, and hence its Bayes factor in comparison with other models. What can be done to ameliorate the problem" (p. 294)? Kruschke posed one method might be taking a small representative portion of the data in hand and use them to make an empirically-based prior for the remaining set of data. From our previous example, "suppose that the 10% subset has 6 heads in 10 flips, so the remaining 90% of the data has $z = 65 − 6$ and $N  = 100 − 10$" (p. 294).

Here are the new Bayes factors based on that method.

```{r}
z <- 65 - 6
n <- 100 - 10

# Peaked vs Uniform
p_d(z, n, a = 500 + 6, b = 500 + 10 - 6) / p_d(z, n, a =   1 + 6, b =   1 + 10 - 6)

# Peaked vs Haldane
p_d(z, n, a = 500 + 6, b = 500 + 10 - 6) / p_d(z, n, a = .01 + 6, b = .01 + 10 - 6)
```

Now the two Bayes Factors are nearly the same.

It's not in the text, but let's compare these three models using **brms**, information criteria, model weights, model averaging, and posterior predictive checks. First, we'll save the $z$ and $N$ information as a tibble with a series of 0's and 1's.

```{r}
z <- 65
n <- 100

trial_data <- tibble(y = rep(0:1, times = c(n - z, z)))

glimpse(trial_data)
```

Next, fit the three models with `brms::brm()`.

```{r fit10.6}
fit10.6 <-
  brm(data = trial_data, 
      family = bernoulli(link = identity),
      y ~ 1,
      prior(beta(500, 500), class = Intercept, lb = 0, ub = 1),
      iter = 11000, warmup = 1000, chains = 4, cores = 4,
      seed = 10,
      file = "fits/fit10.06")

fit10.7 <-
  brm(data = trial_data, 
      family = bernoulli(link = identity),
      y ~ 1,
      # Uniform
      prior(beta(1, 1), class = Intercept, lb = 0, ub = 1),
      iter = 11000, warmup = 1000, chains = 4, cores = 4,
      seed = 10,
      file = "fits/fit10.07")

fit10.8 <-
  brm(data = trial_data, 
      family = bernoulli(link = identity),
      y ~ 1,
      # Haldane
      prior(beta(0.01, 0.01), class = Intercept, lb = 0, ub = 1),
      iter = 11000, warmup = 1000, chains = 4, cores = 4,
      seed = 10,
      file = "fits/fit10.08")
```

Compare the models by the LOO.

```{r loo_fit10.6_through_fit10.6, message = F}
fit10.6 <- add_criterion(fit10.6, criterion = "loo")
fit10.7 <- add_criterion(fit10.7, criterion = "loo")
fit10.8 <- add_criterion(fit10.8, criterion = "loo")

loo_compare(fit10.6, fit10.7, fit10.8)
```

Based on the LOO comparisons, none of the three models was a clear favorite. Although both versions of the anything's-possible model (i.e., `fit10.7` and `fit10.8`) had lower numeric estimates than the must-be-fair model (i.e., `fit10.6`), the standard errors on the difference scores were the same magnitude as the difference estimates themselves. As for comparing the two variants of the anything's-possible model directly, their LOO estimates were almost indistinguishable.

Now let's see what happens when we compute their model weights. Here we'll contrast the LOO weights with the stacking weights.

```{r mw_fit10.6_through_fit10.8, cache = T}
mw <-
  model_weights(fit10.6, fit10.7, fit10.8, weights = "stacking")

mw %>% 
  round(digits = 2)

model_weights(fit10.6, fit10.7, fit10.8, weights = "loo") %>% 
  round(digits = 2)
```

The evidence varied a bit by the specific weighting scheme. Across both, the model with the Haldane prior (`fit10.8`) did arguably the best, but the model with the uniform prior (`fit10.7`) was clearly in the running. Overall, the evidence for one versus another was weak.

Like we did earlier with `fit10.1` and `fit10.2`, we can use the `pp_average()` function to compute the stacking weighted posterior for $\theta$.

```{r, fig.width = 3.75, fig.height = 3}
pp_average(fit10.6, fit10.7, fit10.8, 
           newdata = nd,
           weights = mw,
           method = "fitted",
           summary = F) %>% 
  as_tibble() %>%
  
  ggplot(aes(x = V1, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = c(.95, .5),
                    fill = kh[6], slab_color = kh[5], color = kh[2],
                    breaks = 40, slab_size = .25, outline_bars = T) +
  scale_x_continuous(expression(italic(p)(theta*"|"*italic(D))), expand = c(0, 0), 
                     breaks = 0:5 / 5, labels = c("0", ".2", ".4", ".6", ".8", "1"),
                     limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL,
                     expand = expansion(mult = c(0.01, 0.05))) +
  labs(subtitle = "The posterior for the probability, given the weighted\ncombination of fit10.6, fit10.7, and fit10.8")
```

Did you notice the `weights = mw` argument, there? From the `pp_average.brmsfit` section of the [**brms** reference manual](https://CRAN.R-project.org/package=brms/brms.pdf) [@brms2022RM, pp. 177], we read "`weights` may also be be a numeric vector of pre-specified weights." Since we saved the results of `model_weights()` as an object `mw`, we were able to capitalize on that feature. If you leave out that argument, you'll have to wait a bit for **brms** to compute those weights again from scratch.

Just for the sake of practice, we can also compare the models with separate posterior predictive checks using `pp_check()`.

```{r, fig.width = 8, fig.height = 2.5, message = F}
p1 <-
  pp_check(fit10.6, type = "bars", ndraws = 1e3) +
  ggtitle("fit10.6",
          subtitle = expression("beta"*(500*", "*500)))

p2 <-
  pp_check(fit10.7, type = "bars", ndraws = 1e3) +
  ggtitle("fit10.7",
          subtitle = expression("beta"*(1*", "*1)))

p3 <-
  pp_check(fit10.8, type = "bars", ndraws = 1e3) +
  ggtitle("fit10.8",
          subtitle = expression("beta"*(0.01*", "*0.01)))

((p1 + p2 + p3) & 
    scale_x_continuous(breaks = 0:1) &
    scale_y_continuous(expand = expansion(mult = c(0, 0.05)),
                       limits = c(0, 80))) +
  plot_layout(guides = 'collect')
```

Instead of the default 10, this time we used 1,000 posterior simulations from each fit, which we summarized with dot and error bars. This method did a great job showing how little `fit10.6` learned from the data. Another nice thing about this method is it reveals how similar the results are between `fit10.7` and `fit10.8`, the two alternate versions of the anything's-possible model. Also, did you notice how we used `limits = c(0, 80)` when combining the plots with **patchwork**? Holding the scale of the $y$-axis constant makes it easier to compare results across plots.

## Bonus: There's danger ahead

If you're new to model comparison with Bayes factors, information criteria, model stacking and so on, you should know these methods are still subject to spirited debate amongst scholars. For a recent example, see Gronau and Wagenmakers' [-@gronauLimitationsBayesianLeaveoneout2019] [*Limitations of Bayesian leave-one-out cross-validation for model selection*](https://doi.org/10.1007/s42113-018-0011-7), which criticized the LOO. Their paper was commented on by
@navarroDevilDeepBlue2019, @chandramouliCommentaryGronauWagenmakers2019, and @vehtariLimitationsLimitationsBayesian2019. You can find Gronau and Wagenmakers' [-@gronauRejoinderMoreLimitations2019] rejoinder [here](https://doi.org/10.1007/s42113-018-0022-4).

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
# remove our objects
rm(d, kh, length, p1, p2, my_arrow, p3, p4, p5, p6, p7, p8, p9, layout, p_d, p_d_1, p_d_2, odds, bernoulli_likelihood, n, z, trial_data, p11, p21, p51, p12, p22, p32, p42, p52, p13, p23, p33, p43, p53, omega, kappa, stanvars, fit10.1, theta, mean_theta, sd_theta, a_post, b_post, one_over_pd, fit10.2, mw, nd, pp_a, fit10.3, fit10.4, my_data, fit9.2, fit10.5, fit10.6, hdi_of_icdf, hdi_of_qbeta, fit10.7, fit10.8)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

## Footnote {-}

[^3]: I point this out because in previous versions of this book, I accidentally flipped this order. Thankfully, Omid Ghasemi caught the mistake and kindly pointed it out in [GitHub issue #34](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/34).


<!--chapter:end:10.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

# Null Hypothesis Significance Testing

It's worth repeating a couple paragraphs from page 298 [@kruschkeDoingBayesianData2015, *emphasis* in the original]:

> The logic of conventional NHST goes like this. Suppose the coin is fair (i.e., $\theta$ = 0.50). Then, when we flip the coin, we expect that about half the flips should come up heads. If the actual number of heads is far greater or fewer than half the flips, then we should reject the hypothesis that the coin is fair. To make this reasoning precise, we need to figure out the exact probabilities of all possible outcomes, which in turn can be used to figure out the probability of getting an outcome as extreme as (or more extreme than) the actually observed outcome. This probability, of getting an outcome from the null hypothesis that is as extreme as (or more extreme than) the actual outcome, is called a "$p$ value." If the $p$ value is very small, say less than 5%, then we decide to reject the null hypothesis.
>
> Notice that this reasoning depends on defining a space of all possible outcomes from the null hypothesis, because we have to compute the probabilities of each outcome relative to the space of all possible outcomes. The space of all possible outcomes is based on how we intend to collect data. For example, was the intention to flip the coin exactly $N$ times? In that case, the space of possible outcomes contains all sequences of exactly $N$ flips. Was the intention to flip until the $z$th head appeared? In that case, the space of possible outcomes contains all sequences for which the $z$th head appears on the last flip. Was the intention to flip for a fixed duration? In that case, the space of possible outcomes contains all combinations of $N$ and $z$ that could be obtained in that fixed duration. Thus, a more explicit definition of a $p$ value is the probability of getting a sample outcome from the hypothesized population that is as extreme as or more extreme than the actual outcome *when using the intended sampling and testing procedures.*

## Paved with good intentions

Kruschke started off this section with a random sequence of 7 heads (H) and 17 tails (T). This is a little silly, but I wanted to challenge myself to randomly generate a series of 24 `H` and `T` characters for which there were 7 `H`s. The base **R** `sample()` function gets us part of the way there.

```{r}
sample(c("H", "T"), size = 24, replace = T)
```

I wanted the solution to be reproducible, which required I find the appropriate seed for `set.seed()`. To do that, I made a custom `h_counter()` function into which I could input an arbitrary seed value and retrieve the count of `H`. I then fed a sequence of integers into `h_counter()` and filtered the output to find which seed produces the desirable outcome.

```{r, warning = F, message = F}
library(tidyverse)

h_counter <- function(seed) {
  set.seed(seed)
  coins <- sample(c("H", "T"), size = 24, replace = T)
  length(which(coins == "H"))
}

coins <-
  tibble(seed = 1:200) %>% 
  mutate(n_heads = map_dbl(seed, h_counter))

coins %>% 
  filter(n_heads == 7)
```

Looks like `set.seed(115)` will work.

```{r}
set.seed(115)
sample(c("H", "T"), size = 24, replace = T)
```

The sequence isn't in the exact order as the data from page 300, but they do have the crucial ratio of heads to tails. We'll plot that in a moment.

Before we plot, let's talk theme and color scheme. In the last chapter, we experimented with making global alterations to the default **ggplot2** theme using the `theme_set()` function and based our new color settings on an iconic woodblock print of the sea. We'll extend that nautical theme in this chapter by basing our color scheme on [*RIFT SCULL*](https://www.jamesjean.com/riftscull) by contemporary artist, [James Jean](https://www.jamesjean.com/about-1) [-@jeanRiftScull2009]. We can get a prearranged color palette based on *RIFT SCULL* from the [**lisa** package](https://github.com/tyluRp/lisa).

```{r, warning = F, message = F, fig.width = 3, fig.height = 1}
library(lisa)

plot(lisa_palette("JamesJean"))
```

Use `lisa_palette("JamesJean")` to customize our global settings.

```{r}
theme_set(
  theme_grey() +
    theme(text = element_text(color = "lemonchiffon"),
          axis.text = element_text(color = "lemonchiffon"),
          axis.ticks = element_line(color = "lemonchiffon"),
          legend.background = element_blank(),
          legend.box.background = element_blank(),
          legend.key = element_rect(fill = lisa_palette("JamesJean")[4]),
          panel.background = element_rect(fill = lisa_palette("JamesJean")[4],
                                          color = "lemonchiffon"),
          panel.grid = element_blank(),
          plot.background = element_rect(fill = lisa_palette("JamesJean")[4],
                                          color = lisa_palette("JamesJean")[4]))
)

jj <- lisa_palette("JamesJean")

jj
```

You can undo the above with `ggplot2::theme_set(ggplot2::theme_grey())`. Here's the bar plot of our 24 flips.

```{r, fig.width = 3.5, fig.height = 3}
set.seed(115)
tibble(flips = sample(c("H", "T"), size = 24, replace = T)) %>% 
  
  ggplot(aes(x = flips)) +
  geom_hline(yintercept = c(7, 17), color = jj[2]) +
  geom_bar(fill = jj[1]) +
  scale_y_continuous(breaks = c(0, 7, 17), expand = expansion(mult = c(0, 0.05)))
```

> It seems that there were fewer heads than what we would expect from the hypothesis of fairness. We would like to derive the probability of getting a proportion of heads that is 7/24 or smaller if the null hypothesis is true. (p. 300)

### Definition of $p$ value.

> In summary, the likelihood function defines the probability for a single measurement, and the intended sampling process defines the cloud of possible sample outcomes. The null hypothesis is the likelihood function with its specific value for parameter $\theta$, and the cloud of possible samples is defined by the stopping and testing intentions, denoted $I$. Each imaginary sample generated from the null hypothesis is summarized by a descriptive statistic, denoted $D_{\theta, I}$. In the case of a sample of coin flips, the descriptive summary statistic is $z / N$ , the proportion of heads in the sample. Now, imagine generating infinitely many samples from the null hypothesis using stopping and testing intention $I$ ; this creates a cloud of possible summary values $D_{\theta, I}$, each of which has a particular probability. The probability distribution over the cloud of possibilities is the sampling distribution: $p (D_{\theta, I} | \theta, I )$.
>
> To compute the $p$ value, we want to know how much of that cloud is as extreme as, or more extreme than, the actually observed outcome. To define "extremeness" we must determine the typical value of $D_{\theta, I}$, which is usually defined as the expected value, $E [D_{\theta, I}]$ (recall Equations 4.5 and 4.6). This typical value is the center of the cloud of possibilities. An outcome is more "extreme" when it is farther away from the central tendency. The $p$ value of the actual outcome is the probability of getting a hypothetical outcome that is as or more extreme. Formally, we can express this as
>
> $$p \text{ value} = p (D_{\theta, I} \succcurlyeq D_\text{actual} | \theta, I)$$
>
> where "$\succcurlyeq$" in this context means "as extreme as or more extreme than, relative to the expected value from the hypothesis." Most introductory applied statistics textbooks suppress the sampling intention $I$ from the definition, but precedents for making the sampling intention explicit can be found in Wagenmakers [-@wagenmakers2007practical, Online Supplement A)] and additional references cited therein. (p. 301)

Here's how one might make a version of the grids of Figure 11.2.

```{r, fig.width = 8, fig.height = 2.75, warning = F, message = F}
library(patchwork)

level <- c(0:8, "...")

d <-
  crossing(N = factor(level, levels = level),
           z = factor(level, levels = level)) %>%
  mutate(
    label = case_when(
      N == "..." ~ "...",
      z == "..." ~ "...",
      as.double(z) > as.double(N) ~ "-",
      as.double(z) <= as.double(N) ~ ""
    ),
    fill = case_when(
      N == "5" ~ 1,
      N %in% c("4", "6") ~ 2,
      N %in% c("3", "7") ~ 3,
      N %in% c("1", "2", "8", "...") ~ 4
    )) %>% 
  mutate(z = fct_rev(z)) %>% 
  filter(N != "0") 

p1 <-
  d %>% 
  ggplot(aes(N, z, label = label, fill = N == "5")) +
  geom_tile(color = alpha(jj[2], .25)) +
  geom_text(color = jj[2]) + 
  scale_fill_manual(values = jj[c(4, 1)], breaks = NULL) +
  scale_x_discrete(position = "top", expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  theme(axis.ticks = element_blank())

p2 <-
  d %>% 
  ggplot(aes(N, z, label = label, fill = z == "4")) +
  geom_tile(color = alpha(jj[2], .25)) +
  geom_text(color = jj[2]) + 
  scale_fill_manual(values = jj[c(4, 1)], breaks = NULL) +
  scale_x_discrete(position = "top", expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  theme(axis.ticks = element_blank())

p3 <-
  d %>% 
  ggplot(aes(N, z, label = label, fill = fill)) +
  geom_tile(color = alpha(jj[2], .25)) +
  geom_text(color = jj[2]) + 
  scale_fill_gradient(low = jj[1], high = jj[4], breaks = NULL) +
  scale_x_discrete(position = "top", expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  theme(axis.ticks = element_blank())

# combine and plot
p1 | p2 | p3
```

### With intention to fix $N$.

In this section, "the space of possible outcomes is restricted to combinations of $z$ and $N$ for which $N$ is fixed at $N = 24$" (p. 302).

> What is the probability of getting a particular number of heads when $N$ is fixed? The answer is provided by the *binomial probability distribution*, which states that the probability of getting $z$ heads out of $N$ flips is
>
> $$ p(z | N, \theta) = \begin{pmatrix} N \\ z \end{pmatrix} \theta^z (1 - \theta)^{N - z}$$
> where the notation $\begin{pmatrix} N \\ z \end{pmatrix}$ [is a shorthand notation defined in more detail in the text. It has to do with factorials and, getting more to the point,] the number of ways of allocating $z$ heads among $N$ flips, without duplicate counting of equivalent allocations, is $N !/[(N − z)!z!]$. This factor is also called the number of ways of choosing $z$ items from $N$ possibilities, or "$N$ choose $z$" for short, and is denoted $\begin{pmatrix} N \\ z \end{pmatrix}$. Thus, the overall probability of getting $z$ heads in $N$ flips is the probability of any particular sequence of $z$ heads in $N$ flips times the number of ways of choosing $z$ slots from among the $N$ possible flips. (p. 303, *emphasis* in the original)

To do factorials in **R**, use the `factorial()` function. E.g., we can use the formula $N! / [(N − z)!z!]$ like so:

```{r}
n <- 24
z <- 7

factorial(n) / (factorial(n - z) * factorial(z))
```

That value, recall, is a count, "the number of ways of allocating $z$ heads among $N$ flips, without duplicate counting of equivalent allocations" (p. 303). That formula's a little cumbersome to work with. We can make our programming lives easier by wrapping it into a function.

```{r}
n_choose_z <- function(n, z) {
  factorial(n) / (factorial(n - z) * factorial(z))
}
```

Now we can employ our `n_choose_z()` function to help make the data we'll use for Figure 11.3.b. Here are the data.

```{r}
flips <-
  tibble(z = 0:24) %>% 
  mutate(n_choose_z = n_choose_z(n, z)) %>% 
  mutate(`p(z/N)`                = n_choose_z / sum(n_choose_z),  # this just rescales `n_choose_z`
         `Sample Proportion z/N` = z / n)

head(flips, n = 10)
```

Instead of our custom `n_choose_z()` function, we could have also used the base **R** choose function. Here's a quick comparison.

```{r}
tibble(z = 0:5) %>% 
  mutate(n_choose_z = n_choose_z(n, z),
         choose     = choose(n, z))
```

Now here's the histogram of that sampling distribution.

```{r, fig.width = 5, fig.height = 3.25}
flips %>% 
  ggplot(aes(x = `Sample Proportion z/N`, y = `p(z/N)`,
             fill = z <= 7, color = z <= 7)) +
  geom_col(width = .025) +
  scale_fill_manual(values = jj[1:2]) +
  scale_color_manual(values = jj[1:2]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Implied Sampling Distribution")
```

We can get the one-sided $p$-value with a quick `filter()` and `summarise()`.

```{r}
flips %>% 
  filter(z <= 7) %>% 
  summarise(p_value = sum(`p(z/N)`))
```

Here's Figure 11.3.a.

```{r, fig.width = 2.75, fig.height = 3.5}
tibble(y = factor(c("tail", "head"), levels = c("tail", "head")),
       `p(y)` = .5) %>% 
  
  ggplot(aes(x = y, y = `p(y)`)) +
  geom_col(fill = jj[1]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0)), limits = c(0, 1)) +
  labs(title = "Hypothetical Population",
       subtitle = expression(theta==".5")) +
  theme(axis.ticks.x = element_blank(),
        panel.grid = element_blank())
```

As Kruschke wrote on page 304, "It is important to understand that the sampling distribution is a probability distribution over samples of data, and is *not* a probability distribution over parameter values."

Here is the probability "of getting *exactly* $z = 7$ heads in $N = 24$ flips" (p. 304, *emphasis* in the original):

```{r}
flips %>% 
  filter(z == 7)
```

It was already sitting there in our `p(z/N)` column. Well okay, fine, you do have to multiply it by 100 to convert it to a percentage the way Kruschke presented it at the bottom of page 304.

```{r}
flips %>% 
  filter(z == 7) %>% 
  summarise(`probability in a percentage metric` = (100 * `p(z/N)`) %>% round(digits = 3))
```

Kruschke then pointed out that this is a one-tailed $p$-value. We often reject or fail to reject the null hypothesis based on two-sided $p$-values. In practice, we can convert a one-sided $p$-value to a two-sided $p$-value by multiplying it by 2. Here's what that looks like in this case.

```{r}
flips %>% 
  filter(z <= 7) %>% 
  summarise(`one-sided p-value` = sum(`p(z/N)`),
            `two-sided p-value` = 2 * sum(`p(z/N)`))
```

> Here's the conclusion for our particular case. The actual observation was $z/N = 7/24$. The one-tailed probability is $p = 0.032$, which was computed from Equation 11.4, and is shown in Figure 11.3. Because the $p$ value is not less than 2.5%, we do *not* reject the null hypothesis that $\theta = 0.5$. In NHST parlance, we would say that the result "has failed to reach significance." This does not mean we *accept* the null hypothesis; we merely suspend judgment regarding rejection of this particular hypothesis. Notice that we have not determined any degree of belief in the hypothesis that $\theta = 0.5$. The hypothesis might be true or might be false; we suspend judgment. (p. 305, *emphasis* in the original)

### With intention to fix $z$.

In this subsection, "$z$ is fixed in advance and $N$ is the random variable. We don't talk about the probability of getting $z$ heads out of $N$ flips, we instead talk about the probability of taking $N$ flips to get $z$ heads" (p. 306).

This time we're interested in

> *What is the probability of taking $N$ flips to get $z$ heads*? To answer this question, consider this: We know that the $N$th flip is the $z$th head, because that is what caused flipping to stop. Therefore the previous $N - 1$ flips had $z - 1$ heads in some random sequence. The probability of getting $z - 1$ heads in $N - 1$ flips is $\begin{pmatrix} N-1 \\ z-1 \end{pmatrix} \theta^{z-1} (1 - \theta)^{N - z}$. The probability that the last flip comes up heads is $\theta$. Therefore, the probability that it takes $N$ flips to get $z$ heads is
>
> \begin{align*}
> p(N | z, \theta) & = \begin{pmatrix} N-1 \\ z-1 \end{pmatrix} \theta^{z-1} (1 - \theta)^{N - z} \cdot \theta \\
>                  & = \begin{pmatrix} N-1 \\ z-1 \end{pmatrix} \theta^z (1 - \theta)^{N - z} \\
>                  & = \frac{z}{N} \begin{pmatrix} N \\ z \end{pmatrix} \theta^z (1 - \theta)^{N - z}
> \end{align*}
> 
> (This distribution is sometimes called the "negative binomial" [^4] but that term sometimes refers to other formulations and can be confusing, so I will not use it here.) This is a sampling distribution, like the binomial distribution, because it specifies the relative probabilities of all the possible data outcomes for the hypothesized fixed value of $\theta$ and the intended stopping rule. (p. 306, *emphasis* added)

With that formula in hand, here's how to generate the data for Figure 11.4.b.

```{r}
theta <- .5

# we have to stop somewhere. where should we stop?
highest_n <- 100

flips <-
  tibble(n = 7:highest_n) %>% 
  mutate(n_choose_z = n_choose_z(n, z)) %>% 
  mutate(`p(z/N)`                = n_choose_z / sum(n_choose_z),
         `Sample Proportion z/N` = z / n,
         `p(N|z,theta)`          = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z))
```

To keep things simple, we just went up to $N = 100$.

At the bottom of page 306, Kruschke described the probability "spikes" for various values of $N$ when $z/N = 7/7$, $z/N = 7/8$, and $z/N = 7/9$. We have those spike values in the `Sample Proportion z/N` column of our `flips` data. Here are those first three spikes.

```{r}
flips %>% 
  head(n = 3)
```

Those values correspond to the rightmost vertical lines in our Figure 11.4.b, below.

```{r, fig.width = 5, fig.height = 3.25, warning = F}
flips %>%
  ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`,
             fill = n >= 24, color = n >= 24)) +
  geom_col(width = .005) +
  scale_fill_manual(values = jj[1:2]) +
  scale_color_manual(values = jj[1:2]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Implied Sampling Distribution") +
  theme(legend.position = "none")
```

Our Figure 11.4.a is the same as Figure 11.3.a, above. I won't repeat it, here.

We got the formula for that last variable, `p(N|z,theta)`, from Formula 11.6 on page 306. You'll note how Kruschke continued to refer to it as $p(z|N)$ in his Figure 11.4. It's entirely opaque, to me, how $p(z|N) = p(N|z, \theta)$. I'm just going with it.

Here's the $p$-value, expressed two ways.

```{r}
flips %>% 
  filter(n >= 24) %>% 
  summarise(`one-sided p-value` = sum(`p(N|z,theta)`) %>% round(digits = 3),
            `two-sided p-value` = (2 * sum(`p(N|z,theta)`)) %>% round(digits = 3))
```

If you experiment a bit with the `highest_n` value from above, you'll see that the exact value for the $p$-value is dependent on what $N$ you go up to.

Even though our data ($z = 2, N = 24$) is the same as in the last section, we came ended up with different $p$-values and different conclusions about the null hypothesis. Though we failed to reject the null in the last section, we rejected it here. As Kruschke warned us in the beginning of the chapter, NHST depends on sampling intentions.

### With intention to fix duration.

In this subsection,

> neither $N$ nor $z$ is fixed...
>
> The key to analyzing this scenario is specifying how various combinations of $z$ and $N$ can arise when sampling for a fixed duration. There is no single, uniquely "correct" specification, because there are many different real-world constraints on sampling through time. But one approach is to think of the sample size $N$ as a random value. (p. 308).

Here's a glance at the Poisson distribution for which $\lambda = 24$. The mean is colored yellow.

```{r, fig.width = 5, fig.height = 3.25}
tibble(x = 1:50) %>%
  mutate(y = dpois(x = x, lambda = 24)) %>% 
  
  ggplot(aes(x = x, y = y,
             fill = x == 24, color = x == 24)) +
  geom_col(width = .5) +
  scale_fill_manual(values = jj[1:2]) +
  scale_color_manual(values = jj[1:2]) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  theme(legend.position = "none")
```

In the note for Figure 11.5, Kruschke explained the "sample sizes are drawn randomly from a Poisson distribution with mean $\lambda$". Earlier in the prose he explained "$\lambda$ was set to 24 merely to match $N$ and make the example most comparable to the preceding examples" (p. 309). To do such a simulation, one must choose how many draws to take from $\operatorname{Poisson}(24)$. Here's an example where we take just one.

```{r}
set.seed(11)

n_iter <- 1

flips <-
  tibble(iter = 1:n_iter,
         n    = rpois(n_iter, lambda = 24)) %>% 
  mutate(z = map(n, ~seq(from = 0, to = ., by = 1))) %>% 
  unnest(z) %>% 
  mutate(n_choose_z = n_choose_z(n, z)) %>% 
  mutate(`p(z/N)`                = n_choose_z / sum(n_choose_z),
         `Sample Proportion z/N` = z / n) 

flips
```

As indicated in our `n` column, by chance we drew a 21. We then computed the same values for all possible values of `z`, ranging from 0 to 21. But this doesn't make for a very interesting plot, nor does it make for the same kind of plot Kruschke made in Figure 11.5.b.

```{r, fig.width = 5, fig.height = 3.25, warning = F}
flips %>%
  ggplot(aes(x = `Sample Proportion z/N`, y = `p(z/N)`,
             fill = z <= 7, color = z <= 7)) +
  geom_col(width = .01) +
  scale_fill_manual(values = jj[1:2]) +
  scale_color_manual(values = jj[1:2]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme(legend.position = "none")
```

Instead we have to take many draws to take from $\operatorname{Poisson}(24)$. Here's what it looks like when we take 10,000.

```{r, fig.width = 5, fig.height = 3.25, warning = F}
n_iter <- 10000

set.seed(11)

flips <-
  tibble(iter = 1:n_iter,
         n    = rpois(n_iter, lambda = 24)) %>% 
  mutate(z = map(n, ~seq(from = 0, to = ., by = 1))) %>% 
  unnest(z) %>% 
  mutate(n_choose_z = n_choose_z(n, z)) %>% 
  mutate(`p(z/N)`                = n_choose_z / sum(n_choose_z),
         `Sample Proportion z/N` = z / n) 

flips %>%
  ggplot(aes(x = `Sample Proportion z/N`, y = `p(z/N)`,
             fill = z <= 7, color = z <= 7)) +
  geom_col(width = .003, size = 1/15) +
  scale_fill_manual(values = jj[1:2]) +
  scale_color_manual(values = jj[1:2]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(ylim = c(0, 0.03)) +
  theme(legend.position = "none")
```

I played around with the simulation a bit and this is about as good as I've gotten. If you have a solution that more faithfully reproduces what Kruschke did, please share your code in my [GitHub issue #16](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/16).

Here's my attempt at the $p$-value.

```{r}
flips %>% 
  filter(z <= 7) %>% 
  summarise(`one-sided p-value` = sum(`p(z/N)`))
```

It's unclear, to me, why it's so much lower than the one Kruschke reported in the text.

### With intention to make multiple tests.

> In the preceding sections we have seen that when a coin is flipped $N = 24$ times and comes up $z = 7$ heads, the $p$ value can be 0.032 or 0.017 or 0.024 or other values. The change in $p$ is caused by the dependence of the imaginary cloud of possibilities on the stopping intention. Typical NHST textbooks never mention the dependency of $p$ values on the stopping intention, but they often do discuss the dependency of $p$ values on the testing intention. In this section we will see how testing intentions affect the imaginary cloud of possibilities that determines the $p$ value. (p. 310)

Kruschke then went into an example of flipping two coins and how this would require we consider an *overall* false-positive rate based on proportions from *either* of the two coins. With respect to the probabilities of two independent events, I do recall that $p(A \text{ or } B) = p(A) + p(B)$. But sadly, I'm not sure how to incorporate this knowledge to reproduce the simulation for this section. If you know how to do the simulation properly, please share your code in my [GitHub issue #17](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/17).

```{r, echo = F, eval = F}
# This, for example, doesn't get the job done.
flips <-
  crossing(z    = 0:24,
           coin = letters[1:2]) %>% 
  group_by(coin) %>% 
  mutate(n_choose_z = n_choose_z(n, z)) %>% 
  mutate(`p(z/N)`                = n_choose_z / sum(n_choose_z),
         `Sample Proportion z/N` = z / n)

head(flips, n = 10)

# The result is a failed attempt at Figure 11.6:

flips %>% 
  ggplot(aes(x = `Sample Proportion z/N`, y = `p(z/N)`,
             fill = z <= 7, color = z <= 7)) +
  geom_col(width = .01) +
  scale_fill_manual(values = jj[1:2]) +
  scale_color_manual(values = jj[1:2]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Implied Sampling Distribution") +
  theme(legend.position = "none")
```

```{r, echo = F, eval = F}
# maybe this is closer
# I just couldn't figure out how to combine these properly to remake the figure
z1 <- 7
z2 <- 7
n1 <- 24
n2 <- 24


# flips <-
  tibble(z1 = 0:n1,
         z2 = 0:n2) %>% 
  mutate(n_choose_z1 = n_choose_z(n, z1),
         n_choose_z2 = n_choose_z(n, z2)) %>% 
  mutate(`p(z1/N1)`                = n_choose_z1 / sum(n_choose_z1),
         `p(z2/N2)`                = n_choose_z2 / sum(n_choose_z2),
         `Sample Proportion z1/N1` = z1 / n1,
         `Sample Proportion z2/N2` = z2 / n2)
```

In his footnote #4 on page 311, Kruschke reported there was a direct relation between the $p$-values in this section and in those from the simulation for Figure 11.3. They follow the equation

$$p_\text{figure 11.6} = 1 - (1 - p_\text{figure 11.3})^2.$$

Using code, we get this.

```{r}
1 - (1 - 0.03195733)^2
```

### Soul searching.

> Within the context of NHST, the solution is to establish the true intention of the researcher. This is the approach taken explicitly when applying corrections for multiple tests. The analyst determines what the truly intended tests are, and determines whether those testing intentions were honestly conceived *a priori* or *post hoc* (i.e., motivated only after seeing the data), and then computes the appropriate $p$ value. The same approach should be taken for stopping rules: The data analyst should determine what the truly intended stopping rule was, and then compute the appropriate $p$ value. Unfortunately, determining the true intentions can be difficult. Therefore, perhaps researchers who use $p$ values to make decisions should be required to publicly pre-register their intended stopping rule and tests, before collecting the data. (p. 314, *emphasis* in the original)

### Bayesian analysis.

Happily for us, "the Bayesian interpretation of data does not depend on the covert sampling and testing intentions of the data collector" (p. 314).

## Prior knowledge

The main thing to note in this section is Kruschke changed the motivating example to one of flipping a flat-headed nail. Now we're considering it "heads" when the nail lands on its head and "tails" when the nail lands such that its point is touching the ground. It's still the case that $N = 24, z = 7$.

### NHST analysis.

Nothing for us, here.

### Bayesian analysis.

If you recall from [Chapter 6][Inferring a Binomial Probability via Exact Mathematical Analysis], we need a function to compute the Bernoulli likelihood.

```{r}
bernoulli_likelihood <- function(theta, data) {
  
  n   <- length(data)
  z   <- sum(data)
  
  return(theta^z * (1 - theta)^(n - sum(data)))
  
}
```

We now consider the analysis based on two priors. The first, $\operatorname{beta}(2, 20)$, expresses the assumption the nail is biased towards "tails." The second, $\operatorname{beta}(11, 11)$, expresses the assumption the nail is fair.

There are a handful of steps before we can use our `bernoulli_likelihood()` function to make the plot data. All these are repeats from [Chapter 6][Inferring a Binomial Probability via Exact Mathematical Analysis].

```{r}
# we need these to compute the likelihood
n <- 24
z <- 7

trial_data <- c(rep(0, times = n - z), rep(1, times = z))            # (i.e., data)

d_nail <-
  tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %>%       # (i.e., theta)
  mutate(Prior = dbeta(x = theta, shape1 = 2, shape2 = 20)) %>% 
  mutate(Likelihood = bernoulli_likelihood(theta = theta,            # (i.e., p(D | theta))
                                           data = trial_data)) %>%
  mutate(evidence = sum(Likelihood * Prior)) %>%                     # (i.e., p(D))
  mutate(Posterior = Likelihood * Prior / evidence)                  # (i.e., p(theta | D))
  
glimpse(d_nail)
```

Here's the left column of Figure 11.7.

```{r, fig.width = 4, fig.height = 6, warning = F, message = F}
p1 <-
  d_nail %>% 
  ggplot(aes(x = theta, y = Prior)) +
  geom_area(fill = jj[1]) +
  scale_x_continuous(expand = expansion(mult = 0)) +
  scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Prior (beta)",
       x = expression(theta),
       y = expression(dbeta(theta*"|"*2*", "*20)))

p2 <-
  d_nail %>% 
  ggplot(aes(x = theta, y = Likelihood)) +
  geom_area(fill = jj[5]) +
  scale_x_continuous(expand = expansion(mult = 0)) +
  scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Likelihood (Bernoulli)",
       x = expression(theta),
       y = expression(p(D*"|"*theta)))

p3 <-
  d_nail %>% 
  ggplot(aes(x = theta, y = Posterior)) +
  geom_area(fill = jj[2]) +
  scale_x_continuous(expand = expansion(mult = 0)) +
  scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Posterior (beta)",
       x = expression(theta),
       y = expression(dbeta(theta*"|"*9*", "*37)))

(p1 / p2 / p3) & theme(panel.grid = element_blank())
```

If we'd like the 95% HDIs, we'll need to redefine the `hdi_of_icdf()` function.

```{r}
hdi_of_icdf <- function(name = qbeta, width = .95, tol = 1e-8, ... ) {

  incredible_mass <- 1.0 - width
  interval_width <- function(low_tail_prob, name, width, ...) {
    name(width + low_tail_prob, ...) - name(low_tail_prob, ...)
  }
  
  opt_info <- optimize(interval_width, c(0, incredible_mass), 
                       name = name, width = width, 
                       tol = tol, ...)
  
  hdi_lower_tail_prob <- opt_info$minimum
  
  return(c(name(hdi_lower_tail_prob, ...),
           name(width + hdi_lower_tail_prob, ...)))
  
}
```

Note that this time we made the `qbeta()` function the default setting for the `name` argument. Here are the HDIs for the prior and posterior, above.

```{r}
hdi_of_icdf(shape1 = 2,
            shape2 = 20)

hdi_of_icdf(shape1 = 2 + z,
            shape2 = 20 + (n - z))
```

To get the left column of Figure 11.7, we have to update the data with our new prior, $\operatorname{beta}(11, 11)$.

```{r, fig.width = 4, fig.height = 6, warning = F, message = F}
# here are the data based on our updated beta(11, 11) prior
d_coin <-
  tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %>%
  mutate(Prior = dbeta(x = theta, shape1 = 11, shape2 = 11)) %>% 
  mutate(Likelihood = bernoulli_likelihood(theta = theta,
                                           data = trial_data)) %>%
  mutate(Posterior = Likelihood * Prior / sum(Likelihood * Prior))

# The updated plots:
p1 <-
  d_coin %>% 
  ggplot(aes(x = theta, y = Prior)) +
  geom_area(fill = jj[1]) +
  labs(title = "Prior (beta)",
       x = expression(theta),
       y = expression(dbeta(theta*"|"*11*", "*11)))

p2 <-
  d_coin %>% 
  ggplot(aes(x = theta, y = Likelihood)) +
  geom_area(fill = jj[5]) +
  labs(title = "Likelihood (Bernoulli)",
       x = expression(theta),
       y = expression(p(D*"|"*theta)))

p3 <-
  d_coin %>% 
  ggplot(aes(x = theta, y = Posterior)) +
  geom_area(fill = jj[2]) +
  labs(title = "Posterior (beta)",
       x = expression(theta),
       y = expression(dbeta(theta*"|"*18*", "*28)))

(p1 / p2 / p3) & 
  scale_x_continuous(expand = expansion(mult = 0)) &
  scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) &
  theme(panel.grid = element_blank())
```

Here are the corresponding HDIs for $\operatorname{beta}(11, 11)$ and $\operatorname{beta}(11 + 7, 11 + 24 - 7)$.

```{r}
hdi_of_icdf(shape1 = 11,
            shape2 = 11)

hdi_of_icdf(shape1 = 11 + z,
            shape2 = 11 + (n - z))
```

#### Priors are overt and relevant.

In this subsection's opening paragraph, Kruschke opined:

> Prior beliefs are overt, explicitly debated, and founded on publicly accessible previous research. A Bayesian analyst might have personal priors that differ from what most people think, but if the analysis is supposed to convince an audience, then the analysis must use priors that the audience finds palatable. It is the job of the Bayesian analyst to make cogent arguments for the particular prior that is used. (p. 317)

## Confidence interval and highest density interval

> This section defines CIs and provides examples. It shows that, while CIs ameliorate some of the problems of $p$ values, ultimately CIs suffer the same problems as $p$ values because CIs are defined in terms of $p$ values. Bayesian posterior distributions, on the other hand, provide the needed information. (p. 318)

### CI depends on intention.

> The primary goal of NHST is determining whether a particular "null" value of a parameter can be rejected. One can also ask what *range* of parameter values would not be rejected. This range of nonrejectable parameter values is called the CI. (There are different ways of defining an NHST CI; this one is conceptually the most general and coherent with NHST precepts.) The 95% CI consists of all values of $\theta$ that would not be rejected by a (two-tailed) significance test that allows 5% false alarms. (p. 318, *emphasis* in the original)

Figure 11.8 depicts the sampling distributions for $\theta = .126$ (top row) and $\theta = .511$ (bottom row). Here's the upper- and lower-left panels of Figure 11.8.

```{r, fig.width = 2.5, fig.height = 6}
p1 <-
  tibble(y      = factor(c("tail", "head"), levels = c("tail", "head")),
         `p(y)` = c(1 - .126, .126)) %>% 
  
  ggplot(aes(x = y, y = `p(y)`)) +
  geom_col(fill = jj[1]) +
  labs(title = "Hypothetical Population",
       subtitle = expression(theta==".126"))

p2 <-
  tibble(y      = factor(c("tail", "head"), levels = c("tail", "head")),
         `p(y)` = c(1 - .511, .511)) %>% 
  
  ggplot(aes(x = y, y = `p(y)`)) +
  geom_col(fill = jj[1]) +
  labs(title = "Hypothetical Population",
       subtitle = expression(theta==".511"))

(p1 / p2) &
  scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) &
  theme(title = element_text(size = 10),
        axis.ticks.x = element_blank(),
        panel.grid = element_blank())
```

Here are the corresponding upper- and lower-right panels.

```{r, fig.width = 5, fig.height = 6}
p1 <-
  tibble(z = 0:24,
         y = dbinom(0:24, size = 24, prob = .126)) %>% 
  
  ggplot(aes(x = z/25, y = y,
             fill = z >= 7)) +
  geom_col(width = .025)

p2 <-
  tibble(z = 0:24,
       y = dbinom(0:24, size = 24, prob = .511)) %>% 
  
  ggplot(aes(x = z / 24, y = y,
             fill = z <= 7)) +
  geom_col(width = .025)
  
(p1 / p2) &
  labs(title = "Implied Sampling Distribution",
       x = "Sample Proportion z/N",
       y = "p(z/N)") &
  scale_fill_manual(values = jj[1:2]) &
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) &
  coord_cartesian(xlim = c(0, 1)) &
  theme(legend.position = "none")
```

Figure 11.9 considers the sampling distributions for two hypothetical populations, $\theta = .126$ (top row) and $\theta = .484$ (bottom row). This time the assumption is $z$ is fixed at 7. For this figure, we'll continue to develop our **patchwork** skills. Before the big reveal, we'll make the subplots in two phases. Here are the two on the left.

```{r, fig.width = 2.75, fig.height = 6}
p1 <-
  tibble(y      = factor(c("tail", "head"), levels = c("tail", "head")),
         `p(y)` = c(1 - .126, .126)) %>% 
  
  ggplot(aes(x = y, y = `p(y)`)) +
  geom_col(fill = jj[1]) +
  labs(title = "Hypothetical Population",
       subtitle = expression(theta==".126")) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, 1)) +
  theme(title = element_text(size = 10),
        axis.ticks.x = element_blank())


p2 <-
  tibble(y      = factor(c("tail", "head"), levels = c("tail", "head")),
         `p(y)` = c(1 - .484, .484)) %>% 
  
  ggplot(aes(x = y, y = `p(y)`)) +
  geom_col(fill = jj[1]) +
  labs(title = "Hypothetical Population",
       subtitle = expression(theta==".484")) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, 1)) +
  theme(title = element_text(size = 10),
        axis.ticks.x = element_blank())
```

Now make the two on the right.

```{r, fig.width = 5, fig.height = 6, warning = F, message = F}
theta <- .126

flips <-
  tibble(n = 7:100) %>% 
  mutate(n_choose_z = n_choose_z(n, z)) %>% 
  mutate(`p(z/N)`                = n_choose_z / sum(n_choose_z),
         `Sample Proportion z/N` = z / n,
         `p(N|z,theta)`          = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z))

p3 <-
  flips %>%
  ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`,
             fill = n <= 24, color = n <= 24)) +
  geom_col(width = .005) +
  scale_fill_manual(values = jj[1:2]) +
  scale_color_manual(values = jj[1:2]) +
  scale_x_continuous(breaks = 0:5 * 0.2, limits = c(0, 1)) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Implied Sampling Distribution") +
  theme(legend.position = "none")

theta <- .484

flips <-
  tibble(n = 7:100) %>% 
  mutate(n_choose_z = n_choose_z(n, z)) %>% 
  mutate(`p(z/N)`                = n_choose_z / sum(n_choose_z),
         `Sample Proportion z/N` = z / n,
         `p(N|z,theta)`          = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z))

p4 <-
  flips %>%
  ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`,
             fill = n >= 24, color = n >= 24)) +
  geom_col(width = .005) +
  scale_fill_manual(values = jj[1:2]) +
  scale_color_manual(values = jj[1:2]) +
  scale_x_continuous(breaks = 0:5 * 0.2, limits = c(0, 1)) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Implied Sampling Distribution") +
  theme(legend.position = "none")
```

Here we arrange all four of the Figure 11.9 subplots.

```{r, fig.width = 7, fig.height = 6, message = F, warning = F}
(p1 + p3 + p2 + p4) + plot_layout(widths = c(2, 4))
```

Figure 11.10 is a depiction of when an experimenter intended to stop when a fixed duration expired. This time the two rows are based on $\theta = .135$ (top row) and $\theta = .497$ (bottom row). We'll follow the same general procedure from the last figure. Here are the subplots on the left.

```{r, fig.width = 2.75, fig.height = 3.5}
p1 <-
  tibble(y      = factor(c("tail", "head"), levels = c("tail", "head")),
         `p(y)` = c(1 - .135, .135)) %>% 
  
  ggplot(aes(x = y, y = `p(y)`)) +
  geom_col(fill = jj[1]) +
  scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) +
  labs(title = "Hypothetical Population",
       subtitle = expression(theta==".135")) +
  coord_cartesian(ylim = c(0, 1)) +
  theme(axis.ticks.x = element_blank())

p2 <-
  tibble(y      = factor(c("tail", "head"), levels = c("tail", "head")),
         `p(y)` = c(1 - .497, .497)) %>% 
  
  ggplot(aes(x = y, y = `p(y)`)) +
  geom_col(fill = jj[1]) +
  scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) +
  labs(title = "Hypothetical Population",
       subtitle = expression(theta==".497")) +
  coord_cartesian(ylim = c(0, 1)) +
  theme(axis.ticks.x = element_blank())
```

Like with Figure 11.5.b, my attempts for the right panels of Figure 11.10 just aren't quite right. If you understand where I'm going wrong with the simulation, please share your code in my [GitHub issue #18](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/18).

```{r, fig.width = 7, fig.height = 6, message = F, warning = F}
z_maker <- function(i) {
  set.seed(i)
  n <- rpois(n = 1, lambda = 24)
  seq(from = 0, to = n, by = 1)
}

theta <- .135

p3 <-
  tibble(seed = 1:100) %>% 
  mutate(z = map(seed, z_maker)) %>% 
  unnest(z) %>% 
  group_by(seed) %>% 
  mutate(n = n()) %>% 
  mutate(n_choose_z = n_choose_z(n, z)) %>% 
  mutate(`p(z/N)`                = n_choose_z / sum(n_choose_z),
         `Sample Proportion z/N` = z / n,
         `p(N|z,theta)`          = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) %>% 
  
  ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`,
             fill = `Sample Proportion z/N` >= 7 / 24)) +
  geom_col(width = .004) +
  scale_fill_manual(values = jj[1:2]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Implied Sampling Distribution") +
  theme(legend.position = "none")

theta <- .497

p4 <-
  tibble(seed = 1:100) %>% 
  mutate(z = map(seed, z_maker)) %>% 
  unnest(z) %>% 
  group_by(seed) %>% 
  mutate(n = n()) %>% 
  mutate(n_choose_z = n_choose_z(n, z)) %>% 
  mutate(`p(z/N)`                = n_choose_z / sum(n_choose_z),
         `Sample Proportion z/N` = z / n,
         `p(N|z,theta)`          = (z / n) * n_choose_z * (theta^z) * (1 - theta)^(n - z)) %>% 
  
  ggplot(aes(x = `Sample Proportion z/N`, y = `p(N|z,theta)`,
             fill = `Sample Proportion z/N` <= 7 / 24)) +
  geom_col(width = .004) +
  scale_fill_manual(values = jj[1:2]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Implied Sampling Distribution") +
  theme(legend.position = "none")

(p1 + p3 + p2 + p4) + plot_layout(widths = c(2, 4))
```

```{r , fig.width = 1.33, fig.height = 3, eval = F, echo = F}
# If and when you figure out the simulations for the right panels,
# here is how to make the middle plots with the text

p5 <- tibble(x = 1.5,
         y = c(.925, .8, .7, .4),
         label = c("Fixed", "N[1]==24", "N[2]==24", "4%=>%4"),
         size = c(5, 5, 5, 15)) %>% 
  
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(aes(size = size), 
            parse = T, hjust = .5, show.legend = F) +
  scale_size_continuous(range = c(5, 15)) +
  scale_y_continuous(breaks = 0:5 / 5) +
  coord_cartesian(xlim = c(1, 2),
                  ylim = c(0, 1)) +
  theme_void()
  # theme(text = element_text(color = "white"),
  #       panel.grid = element_blank())
```

Let's leave failure behind. Here's the two left panels for Figure 11.11.

```{r, fig.width = 2.5, fig.height = 6, message = F}
p1 <-
  tibble(y      = factor(c("tail", "head"), levels = c("tail", "head")),
         `p(y)` = c(1 - .11, .11)) %>% 
  
  ggplot(aes(x = y, y = `p(y)`)) +
  geom_col(fill = jj[1]) +
  scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) +
  coord_cartesian(ylim = c(0, 1)) +
  labs(title = "Hypothetical Population",
       subtitle = expression(theta[1]==".11;"*~theta[2]==".11"))

p2 <-
  tibble(y      = factor(c("tail", "head"), levels = c("tail", "head")),
         `p(y)` = c(1 - .539, .539)) %>% 
  
  ggplot(aes(x = y, y = `p(y)`)) +
  geom_col(fill = jj[1]) +
  scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) +
  coord_cartesian(ylim = c(0, 1)) +
  labs(title = "Hypothetical Population",
       subtitle = expression(theta[1]==".539;"*~theta[2]==".539"))

(p1 / p2) &
  coord_cartesian(ylim = c(0, 1)) &
  theme(title = element_text(size = 10),
        axis.ticks.x = element_blank())
```

Much like with Figure 11.6, I don't understand how to do the simulation properly for the right panels of Figure 11.11. If you've got it, please share your code in my [GitHub issue #19](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/19).

#### CI is not a distribution.

> A CI is merely two end points. A common misconception of a confidence interval is that it indicates some sort of probability distribution over values of $\theta$. It is very tempting to think that values of $\theta$ in the middle of a CI should be more believable than values of $\theta$ at or beyond the limits of the CI.
>
... Methods for imposing a distribution upon a CI seem to be motivated by a natural Bayesian intuition: Parameter values that are consistent with the data should be more credible than parameter values that are not consistent with the data (subject to prior credibility). If we were confined to frequentist methods, then the various proposals outlined above would be expressions of that intuition. But we are not confined to frequentist methods. Instead, we can express our natural Bayesian intuitions in fully Bayesian formalisms. (pp. 323--324)

### Bayesian HDI.

"The 95% HDI consists of those values of $\theta$ that have at least some minimal level of posterior credibility, such that the total probability of all such $\theta$ values is 95%" (p. 324).

Once again, here's how to analytically compute the 95% HDIs for our example of $z = 7, N = 24$ and the prior of $\operatorname{beta}(11, 11)$.

```{r}
hdi_of_icdf(shape1 = 11 + z,
            shape2 = 11 + (n - z))
```

## Multiple comparisons

It's worth quoting Kruschke at length:

> When comparing multiple conditions, a key goal in NHST is to keep the overall false alarm rate down to a desired maximum such as 5%. Abiding by this constraint depends on the number of comparisons that are to be made, which in turn depends on the intentions of the experimenter. In a Bayesian analysis, however, there is just one posterior distribu- tion over the parameters that describe the conditions. That posterior distribution is unaffected by the intentions of the experimenter, and the posterior distribution can be examined from multiple perspectives however is suggested by insight and curiosity. (p. 325)

### NHST correction for experiment wise error.

> In NHST, we have to take into account all comparisons we intend for the whole experiment. Suppose we set a criterion for rejecting the null such that each decision has a "per-comparison" (PC) false alarm rate of $\alpha_\text{PC}$, e.g., 5%. Our goal is to determine the overall false alarm rate when we conduct several comparisons. To get there, we do a little algebra. First, suppose the null hypothesis is true, which means that the groups are identical, and we get apparent differences in the samples by chance alone. This means that we get a false alarm on a proportion $\alpha_\text{PC}$ of replications of a comparison test. Therefore, we do *not* get a false alarm on the complementary proportion $1 - \alpha_\text{PC}$ of replications. If we run *c* independent comparison tests, then the probability of not getting a false alarm on *any* of the tests is $(1 - \alpha_\text{PC})^c$. Consequently, the probability of getting at least one false alarm is $1 - (1 - \alpha_\text{PC})^c$. We call that probability of getting at least one false alarm, across all the comparisons in the experiment, the "experimentwise" false alarm rate, denoted $\alpha_\text{EW}$. (pp. 325--326, *emphasis* in the original)

Here's what this looks like in when $\alpha_\text{PC} = .05$ and $c = 36$.

```{r}
alpha_pc <- .05
c        <- 36

# the probability of not getting a false alarm on any of the tests
(1 - alpha_pc)^c

# the probability of getting at least one false alarm is
1 - (1 - alpha_pc)^c
```

For kicks and giggles, it might be interesting to plot this.

```{r, fig.width = 5, fig.height = 3.5}
tibble(c = 1:100) %>% 
  mutate(p1 = (1 - alpha_pc)^c,
         p2 = 1 - (1 - alpha_pc)^c) %>% 
  pivot_longer(-c, values_to = "probability") %>% 

  ggplot(aes(x = c, y = probability, color = name)) +
  geom_line(size = 1.2) +
  geom_text(data = tibble(
    c           = c(85, 75, 70),
    probability = c(.08, .9, .82),
    label       = c("no false alarms",
                    "at least one false alarm",
                    "(i.e., experimentwise false alarm rate)"),
    name        = c("p1", "p2", "p2")
  ),
  aes(label = label)) +
  scale_color_manual(values = jj[1:2]) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +
  scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) +
  xlab("the number of independent tests, c") +
  theme(legend.position = "none")
```

One way to keep the experimentwise false alarm rate down to 5% is by reducing the permitted false alarm rate for the individual comparisons, i.e., setting a more stringent criterion for rejecting the null hypothesis in individual comparisons. One often-used re-setting is the Bonferroni correction, which sets $\alpha_\text{PC} = \alpha_\text{EW}^\text{desired} / c$.

Here's how to apply the Bonferroni correction to our example if the desired false-alarm rate is .05.

```{r}
alpha_pc <- .05
c        <- 36

# the Bonferroni correction
alpha_pc / c
```

Again, it might be useful to plot the consequence of Bonferroni's correction on $\alpha$ for different levels of $c$.

```{r, fig.width = 5, fig.height = 3.5}
tibble(c = 1:100) %>% 
  mutate(a_ew = alpha_pc^c) %>% 
  
  ggplot(aes(x = c, y = a_ew)) +
  geom_line(color = jj[2], size = 1.2) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +
  scale_y_continuous(expand = expansion(mult = c(0.0015, 0)), limits = c(0, .05)) +
  xlab("the number of independent tests, c") +
  theme(panel.grid = element_blank())
```

A little shocking, isn't it? If you put it on a log scale, you'll see the relationship is linear.

```{r, fig.width = 5, fig.height = 3.5}
tibble(c = 1:100) %>% 
  mutate(a_ew = alpha_pc^c) %>% 
  
  ggplot(aes(x = c, y = a_ew)) +
  geom_line(color = jj[1], size = 1.2) +
  scale_y_log10() +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +
  xlab("the number of independent tests, c") +
  theme(panel.grid = element_blank())
```

But just look at how low the values on the $y$-axis get. Frequentists have other correction procedures available to them. The Bayesian approach is different.

### Just one Bayesian posterior no matter how you look at it.

> In a Bayesian analysis, the interpretation of the data is not influenced by the experimenter's stopping and testing intentions (assuming that those intentions do not affect the data). A Bayesian analysis yields a posterior distribution over the parameters of the model. The posterior distribution is the complete implication of the data. The posterior distribution can be examined in as many different ways as the analyst deems interesting; various comparisons of groups are merely different perspectives on the posterior distribution. (p. 328)

### How Bayesian analysis mitigates false alarms.

From page 329: "How, then, does a Bayesian analysis address the problem of false alarms? By incorporating prior knowledge into the structure of the model." One of the more powerful ways to do so is by using hierarchical models whenever possible [e.g., @gelman2012we].

## What a sampling distribution is good for

"Sampling distributions tell us the probability of imaginary outcomes given a parameter value and an intention, $p(D_{\theta, I}|\theta, I)$, instead of the probability of parameter values given the actual data, $(\theta|D_\text{actual})$."

### Planning an experiment.

Gelman touched on these sensibilities in a [recent blog post](http://andrewgelman.com/2018/07/19/idea-replication-central-not-just-scientific-practice-also-formal-statistics-frequentist-statistics-relies-reference-set-repeated-experiments-bayesian-statist/).

### Exploring model predictions (posterior predictive check).

There's no shortage of PPC talk on Gelman's blog (e.g., [here](http://andrewgelman.com/2017/09/07/touch-want-feel-data/) or [here](http://andrewgelman.com/2014/08/11/discussion-sander-greenland-posterior-predictive-checks/) or [here](http://andrewgelman.com/2009/02/07/confusions_abou/)).

## Session info {-}

```{r}
sessionInfo()
```

```{r, message = F, warning = F, echo = F}
# Here we'll remove our objects
rm(h_counter, coins, jj, level, d, n, z, n_choose_z, flips, highest_n, bernoulli_likelihood, trial_data, d_nail, d_coin, p1, p2, p3, p4, hdi_of_icdf, theta, alpha_pc, c, z_maker, n_iter)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

## Footnote {-}

[^4]: When your criterion variable is a count, you typically model it with the Poisson likelihood. We'll get some practice with the Poisson likelihood in [Chapter 24][Count Predicted Variable]. Though we won't cover this in this text, it turns out the Poisson makes some strong assumptions. The negative binomial likelihood is sometimes used as a robust alternative to the Poisson when those assumptions are violated. Sadly, we won't cover negative-binomial regression at all in this ebook. If you'd like to learn more, check out either edition of McElreath's *Statistical Rethinking* [-@mcelreathStatisticalRethinkingBayesian2015; -@mcelreathStatisticalRethinkingBayesian2020], my **brms** translations of those texts [@kurzStatisticalRethinkingBrms2020; @kurzStatisticalRethinkingSecondEd2021], the authoritative text by Agresti [-@agrestiFoundationsLinearGeneralized2015], or the great tutorial paper by @atkinsTutorialOnCount2013.


<!--chapter:end:11.Rmd-->


```{r, echo = FALSE, cachse = FALSE}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

# Bayesian Approaches to Testing a Point ("Null") Hypothesis

> Suppose that you have collected some data, and now you want to answer the question, Is there a non-zero effect or not? Is the coin fair or not? Is there better-than-chance accuracy or not? Is there a difference between groups or not? In the previous chapter, [Kruschke] argued that answering this type of question via null hypothesis significance testing (NHST) has deep problems. This chapter describes Bayesian approaches to the question. [@kruschkeDoingBayesianData2015, p 335]

## The estimation approach

> Throughout this book, we have used Bayesian inference to derive a posterior distribution over a parameter of interest, such as the bias $\theta$ of a coin. We can then use the posterior distribution to discern the credible values of the parameter. If the null value is far from the credible values, then we reject the null value as not credible. But if all the credible values are virtually equivalent to the null value, then we can accept the null value. (p. 336)

### Region of practical equivalence.

Kruschke began: "A *region of practical equivalence* (ROPE) indicates a small range of parameter values that are considered to be practically equivalent to the null value for purposes of the particular application" (p. 336, *emphasis* in the original)

Before we get to plotting, let's talk about themes and color. For the plots in this chapter, we'll take our color palette from the [**fishualize** package](https://CRAN.R-project.org/package=fishualize) [@R-fishualize], which provides a range of color palettes based on fish species. Our palette will be `"Ostorhinchus_angustatus"`, which is based on [Ostorhinchus angustatus](https://www.fishbase.se/summary/Ostorhinchus-angustatus.html).

```{r, warning = F, message = F, fig.height = 3.5}
library(fishualize)

scales::show_col(fish(n = 5, option = "Ostorhinchus_angustatus"))
```

We'll base our overall global plot theme on `cowplot::theme_cowplot()`, and use `theme()` to make a few color changes based on `"Ostorhinchus_angustatus"`.

```{r, warning = F, message = F}
library(tidyverse)
library(cowplot)

oa <- fish(n = 5, option = "Ostorhinchus_angustatus")

theme_set(
  theme_cowplot() +
  theme(panel.background = element_rect(fill = oa[1], color = oa[1]),
        strip.background = element_rect(fill = oa[3]),
        strip.text = element_text(color = oa[5]))
)

oa
```

You can undo the above with `ggplot2::theme_set(ggplot2::theme_grey())`. Here's a plot of Kruschke's initial coin flip ROPE.

```{r, fig.width = 6, fig.height = 1.5, warning = F, message = F}
tibble(xmin = .45,
       xmax = .55) %>% 
ggplot() +
  geom_rect(aes(xmin = xmin, xmax = xmax,
                ymin = -Inf, ymax = Inf),
            color = "transparent", fill = oa[2]) +
  annotate(geom = "text", x = .5, y = .5, 
           label = "ROPE", color = oa[5]) +
  scale_x_continuous(breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Kruschke's coin flip ROPE",
       x = expression(theta))
```

In the first example (p. 336), we have $z = 325$ heads out of $N = 500$ coin flips. To visualize the analysis, we'll need the Bernoulli likelihood.

```{r}
bernoulli_likelihood <- function(theta, data) {
  
  n <- length(data)
  z <- sum(data)
  
  return(theta^z * (1 - theta)^(n - sum(data)))
  
}
```

Now we'll follow the typical steps to combine the prior, which is flat in this case, and the likelihood to get the posterior.

```{r}
# the data summaries
n <- 500
z <- 325

trial_data <- c(rep(0, times = n - z), rep(1, times = z))            # (i.e., data)

d <-
  tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %>%        # (i.e., theta)
  # recall Beta(1, 1) is flat
  mutate(prior      = dbeta(theta, shape1 = 1, shape2 = 1),          # (i.e., p(theta))
         likelihood = bernoulli_likelihood(theta = theta,            # (i.e., p(D | theta))
                                           data = trial_data)) %>%
  mutate(posterior = likelihood * prior / sum(prior * likelihood))   # (i.e., p(theta | D))
  
glimpse(d)
```

Now we can plot the results.

```{r, fig.width = 6, fig.height = 2, warning = F, message = F}
ggplot(data = d) +
  geom_rect(xmin = .45,  xmax = .55,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = oa[2]) +
  geom_area(aes(x = theta, y = posterior),
            fill = oa[4]) +
  annotate(geom = "text", x = .5, y = .01, 
           label = "ROPE", color = oa[5]) +
  scale_x_continuous(breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Nope, that density ain't in that ROPE.",
       x = expression(theta)) 
```

With the formula by $\operatorname{Beta}(\theta | z + \alpha, N - z + \beta)$, we can analytically compute the Beta parameters for the posterior.

```{r}
(alpha <- z + 1)
(beta <- n - z + 1)
```

With the `hdi_of_icdf()` function, we'll compute the HDIs.

```{r}
hdi_of_icdf <- function(name, width = .95, tol = 1e-8, ... ) {
 
  incredible_mass <- 1.0 - width
  interval_width <- function(low_tail_prob, name, width, ...) {
    name(width + low_tail_prob, ...) - name(low_tail_prob, ...)
  }
  opt_info <- optimize(interval_width, c(0, incredible_mass), 
                       name = name, width = width, 
                       tol = tol, ...)
  hdi_lower_tail_prob <- opt_info$minimum
  
  return(c(name(hdi_lower_tail_prob, ...),
           name(width + hdi_lower_tail_prob, ...)))
  
}
```

Compute those HDIs and save them as `h`.

```{r}
(
  h <-
    hdi_of_icdf(name = qbeta,
                shape1 = alpha,
                shape2 = beta)
)
```

Now let's remake the plot from above, this time with the analytically-derived HDI values.

```{r, fig.width = 6, fig.height = 2, warning = F, message = F}
tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %>% 
  
  ggplot() +
  geom_rect(xmin = .45,  xmax = .55,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = oa[2]) +
  geom_area(aes(x = theta, y = dbeta(theta, shape1 = alpha, shape2 = beta)),
            fill = oa[4]) +
  geom_segment(x = h[1], xend = h[2],
               y = 0, yend = 0,
               size = 1, color = oa[3]) +
  annotate(geom = "text", x = .5, y = 17.5, 
           label = "ROPE", color = oa[5]) +
  annotate(geom = "text", x = .65, y = 4, label = "95%\nHDI", color = oa[1]) +
  scale_x_continuous(breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "That `hdi_of_icdf()` function really came through, for us.",
       x = expression(theta))
```

In his second example (p. 337), Kruschke considered $z = 490$ heads out of $N = 1000$ flips.

```{r, fig.width = 6, fig.height = 2, warning = F, message = F}
# we need these to compute the likelihood
n <- 1000
z <- 490

trial_data <- c(rep(0, times = n - z), rep(1, times = z))

tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %>%               # (i.e., theta)
  mutate(prior      = dbeta(theta, shape1 = 1, shape2 = 1),               # (i.e., p(theta))
         likelihood = bernoulli_likelihood(theta = theta,                 # (i.e., p(D | theta))
                                           data  = trial_data)) %>%
  mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) %>%  # (i.e., p(theta | D))
  
  ggplot() +
  geom_rect(xmin = .45,  xmax = .55,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = oa[2]) +
  geom_area(aes(x = theta, y = posterior),
            fill = oa[4]) +
  scale_x_continuous(breaks = 0:5 / 5, expand = expansion(mult = 0), limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "This posterior sits right within the ROPE.",
       x = expression(theta))
```

Here are the new HDIs.

```{r}
hdi_of_icdf(name = qbeta,
            shape1 =     z + 1,
            shape2 = n - z + 1)
```

Further down the section, Kruschke offered some perspective on the ROPE approach.

> The ROPE limits, by definition, cannot be uniquely "correct," but instead are established by practical aims, bearing in mind that wider ROPEs yield more decisions to accept the ROPEd value and fewer decision to reject the ROPEd value. In many situations, the exact limit of the ROPE can be left indeterminate or tacit, so that the audience of the analysis can use whatever ROPE is appropriate at the time, as competing theories and measuring devices evolve. When the HDI is far from the ROPEd value, the exact ROPE is inconsequential because the ROPEd value would be rejected for any reasonable ROPE. When the HDI is very narrow and overlaps the target value, the HDI might again fall within any reasonable ROPE, again rendering the exact ROPE inconsequential. When, however, the HDI is only moderately narrow and near the target value, the analysis can report how much of the posterior falls within a ROPE as a function of different ROPE widths...
>
> It is important to be clear that any discrete decision about rejecting or accepting a null value does *not* exhaustively capture our knowledge about the parameter value. Our knowledge about the parameter value is described by the full posterior distribution. When making a binary decision, we have merely compressed all that rich detail into a single bit of information. The broader goal of Bayesian analysis is conveying an informative summary of the posterior, and where the value of interest falls within that posterior. Reporting the limits of an HDI region is more informative than reporting the declaration of a reject/accept decision. By reporting the HDI and other summary information about the posterior, different readers can apply different ROPEs to decide for themselves whether a parameter is practically equivalent to a null value. The decision procedure is separate from the Bayesian inference. The Bayesian part of the analysis is deriving the posterior distribution. The decision procedure uses the posterior distribution, but does not itself use Bayes’ rule. (pp. 338--339, *emphasis* in the original)

Full disclosure: I'm not a fan of the ROPE method. Though we're following along with the text and covering it, here, I will deemphasize it in later sections.

Kruschke then went on to compare the ROPE with frequentist equivalence tests. This is a part of the literature I have not waded into, yet. It appears psychologist Daniël Lakens and colleagues gave written a bit in the topic, recently. Interested readers might start with @lakensImprovingInferencesNull2020, @lakensEquivalenceTestingPsychological2018, or @lakensEquivalenceTestingSecond2018.

### Some examples.

Kruschke referenced an analysis from way back in [Chapter 9][Hierarchical Models]. We'll need to re-fit the model. First we import data.

```{r, warning = F, message = F}
my_data <- read_csv("data.R/BattingAverage.csv")

glimpse(my_data)
```

Let's load **brms** and, while we're at it, **tidybayes**.

```{r, warning = F, message = F}
library(brms)
library(tidybayes)
```

Fit the model and retain its original name, `fit9.2`.

```{r fit9.2}
fit9.2 <-
  brm(data = my_data,
      family = binomial(link = logit),
      Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 3500, warmup = 500, chains = 3, cores = 3,
      control = list(adapt_delta = .99),
      seed = 9,
      file = "fits/fit09.02")
```

Let's use `coef()` to pull the relevant posterior draws.

```{r, warning = F}
c <-
  coef(fit9.2, summary = F)$PriPos %>% 
  as_tibble()
  
str(c)
```

As we pointed out in Chapter 9, keep in mind that `coef()` returns the values in the logit scale when used for logistic regression models. So we'll have to use `brms::inv_logit_scaled()` to convert the estimates to the probability metric. We can make the difference distributions after we've converted the estimates.

```{r}
c_small <-
  c %>%
  mutate_all(inv_logit_scaled) %>% 
  transmute(`Pitcher - Catcher`  = Pitcher.Intercept - Catcher.Intercept,
            `Catcher - 1st Base` = Catcher.Intercept - `1st Base.Intercept`)

head(c_small)
```

After a little wrangling, we'll be ready to re-plot the relevant parts of Figure 9.14.

```{r, fig.width = 8, fig.height = 2.75}
c_small %>% 
  pivot_longer(everything()) %>%  
  mutate(name = factor(name, levels = c("Pitcher - Catcher", "Catcher - 1st Base"))) %>% 
  
  ggplot(aes(x = value)) +
  geom_rect(xmin = -0.05, xmax = 0.05,
            ymin = -Inf,  ymax = Inf,
            color = "transparent", fill = oa[2]) +
  stat_histinterval(aes(y = 0),
                    point_interval = mode_hdi, .width = .95,
                    fill = oa[4], colour = oa[3], 
                    breaks = 20, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "The ROPE ranges from −0.05 to +0.05",
       x = expression(theta)) +
  coord_cartesian(xlim = c(-.125, .125)) +
  theme(legend.position = "none") +
  facet_wrap(~ name, scales = "free")
```

In order to re-plot part of Figure 9.15, we'll need to employ `fitted()` to snatch the player-specific posteriors.

```{r, warning = F}
# this will make life easier. just go with it
name_list <- c("ShinSoo Choo", "Ichiro Suzuki")

# we'll define the data we'd like to feed into `fitted()`, here
nd <-
  my_data %>% 
  filter(Player %in% c(name_list)) %>% 
  # these last two lines aren't typically necessary, but they allow us to 
  # arrange the rows in the same order we find the names in Figures 9.15 and 9/16
  mutate(Player = factor(Player, levels = c(name_list))) %>% 
  arrange(Player)

f <-
  fitted(fit9.2, 
         newdata = nd,
         scale = "linear",
         summary = F) %>% 
  as_tibble() %>% 
  mutate_all(inv_logit_scaled) %>% 
  set_names(name_list) %>% 
  # in this last section, we make our difference distributions 
  mutate(`ShinSoo Choo - Ichiro Suzuki` = `ShinSoo Choo` - `Ichiro Suzuki`)
    
glimpse(f)
```

Now we're ready to go.

```{r, fig.width = 4, fig.height = 2.5}
f %>% 
  ggplot() +
  geom_rect(xmin = -0.05, xmax = 0.05,
            ymin = -Inf,  ymax = Inf,
            color = "transparent", fill = oa[2]) +
  stat_histinterval(aes(x = `ShinSoo Choo - Ichiro Suzuki`, y = 0),
                    point_interval = mode_hdi, .width = .95,
                    fill = oa[4], color = oa[3], breaks = 40) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "ShinSoo Choo - Ichiro Suzuki",
       x = expression(theta)) +
  coord_cartesian(xlim = c(-.125, .125))
```

### Differences of correlated parameters.

Kruschke didn't explicate where he got the data for Figure 12.1. If we're willing to presume a multivariate normal distribution, we can get close using the `MASS::mvrnorm()` function. You can get the basic steps from [Sven Hohenstein's answer to this stats.stacheschange question](https://stats.stackexchange.com/questions/164471/generating-a-simulated-dataset-from-a-correlation-matrix-with-means-and-standard).

```{r}
# first we'll make a correlation matrix
# a correlation of .9 seems about right
correlation_matrix <- 
  matrix(c(1, .9, 
           .9, 1), 
         nrow = 2, ncol = 2)

# next we'll specify the means and standard deviations
mu <- c(.58, .42)
sd <- c(.1, .1)

# now we'll use the correlation matrix and standard deviations to make a covariance matrix
covariance_matrix <- 
  sd %*% t(sd) * correlation_matrix

# after setting our seed, we're ready to simulate
set.seed(12)
d <- 
  MASS::mvrnorm(n = 1000, 
                mu = mu, 
                Sigma = covariance_matrix) %>%
  as_tibble() %>%
  set_names(str_c("theta[", 1:2, "]"))
```

Now it only takes some light wrangling to prepare the data to make the three histograms in the left panel of Figure 12.1.

```{r, fig.width = 8, fig.height = 2.75}
d %>% 
  mutate(`theta[1]-theta[2]` = `theta[1]` - `theta[2]`) %>% 
  pivot_longer(everything()) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = oa[4], color = oa[3],
                    breaks = 30, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(theta)) +
  facet_wrap(~ name, scales = "free_y", labeller = label_parsed)
```

Here's the scatter plot, showing the correlation. I think we got pretty close!

```{r, fig.height = 2.75}
d %>%
  ggplot(aes(x = `theta[1]`, y = `theta[2]`)) +
  geom_abline(color = oa[2]) +
  geom_point(size = 1/2, color = oa[3], alpha = 1/4) +
  scale_x_continuous(expression(theta[1]), breaks = 0:5 / 5, 
                     expand = expansion(mult = 0), limits = c(0, 1)) +
  scale_y_continuous(expression(theta[2]), breaks = 0:5 / 5, 
                     expand = expansion(mult = 0), limits = c(0, 1)) +
  coord_equal()
```

To make the plots in the right panel of Figure 12.1, we just need to convert the correlation from .9 to -.9.

```{r}
# this time we'll make the correlations -.9
correlation_matrix <- 
  matrix(c(1, -.9, 
           -.9, 1), 
         nrow = 2, ncol = 2)

# we'll have to redo the covariance matrix
covariance_matrix <- 
  sd %*% t(sd) * correlation_matrix

# here's the updated data
set.seed(1)
d <- MASS::mvrnorm(n = 1000, mu = mu, Sigma = covariance_matrix) %>%
  as_tibble() %>%
  set_names(str_c("theta[", 1:2, "]"))
```

Here are our right-panel Figure 12.1 histograms.

```{r, fig.width = 8, fig.height = 2.75}
d %>% 
  mutate(`theta[1]-theta[2]` = `theta[1]` - `theta[2]`) %>% 
  pivot_longer(everything()) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = oa[4], color = oa[3],
                    breaks = 20, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(theta)) +
  facet_wrap(~ name, scales = "free_y",  labeller = label_parsed)
```

Behold the second scatter plot.

```{r, fig.height = 2.75}
d %>%
  ggplot(aes(x = `theta[1]`, y = `theta[2]`)) +
  geom_abline(color = oa[2]) +
  geom_point(size = 1/2, color = oa[3], alpha = 1/4) +
  scale_x_continuous(expression(theta[1]), breaks = 0:5 / 5, 
                     expand = expansion(mult = 0), limits = c(0, 1)) +
  scale_y_continuous(expression(theta[2]), breaks = 0:5 / 5, 
                     expand = expansion(mult = 0), limits = c(0, 1)) +
  coord_equal()
```

> In summary, the marginal distributions of two parameters do not indicate the relationship between the parameter values. The joint distribution of the two parameters might have positive or negative correlation (or even a non-linear dependency), and therefore the difference of the parameter values should be explicitly examined. (pp. 341--342)

### Why HDI and not equal-tailed interval?

Though Kruschke told us Figure 12.2 was of a gamma distribution, he didn't tell us the parameters for that particular gamma. After playing around for a bit, it appeared `dgamma(x, 2, .2)` worked pretty well.

```{r, fig.width = 4, fig.height = 2}
tibble(x = seq(from = 0, to = 40, by = .1)) %>% 
  
  ggplot(aes(x = x, y = dgamma(x, 2, .2))) +
  geom_area(fill = oa[4]) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 35))
```

If you want to get the quantile-based intervals (i.e., the ETIs), you can plug in the desired quantiles into the `qgamma()` function.

```{r}
(ex <- qgamma(c(.025, .975), shape = 2, rate = .2))
```

To analytically derive the gamma HDIs, we just use the good old `hdi_of_icdf()` function.

```{r}
(
  hx <-
    hdi_of_icdf(name = qgamma,
                shape = 2,
                rate = .2)
)
```

Next you need to determine how high up to go on the y-axis. For the quantile-based intervals, the ETIs, you can use `dgamma()`. The trick is pump the output of `qgamma()` into `dgamma()`.

```{r}
(
  ey <-
    qgamma(c(.025, .975), shape = 2, rate = .2) %>% 
    dgamma(shape = 2, rate = .2)
)
```

We follow the same basic principle to get the $y$-axis values for the HDIs.

```{r}
(
  hy <-
    hdi_of_icdf(name = qgamma, shape = 2, rate = .2) %>% 
    dgamma(shape = 2, rate = .2)
)
```

Now we've computed all those values, we can collect them into a tibble with the necessary coordinates to make the ETI and HDI lines in our plot.

```{r}
(
  lines <-
    tibble(interval = rep(c("eti", "hdi"), each = 4),
           x        = c(ex, hx) %>% rep(., each = 2),
           y        = c(ey[1], 0.0003, 0.0003, ey[2], 0, hy, 0))
)
```

Technically, those second and third `y`-values should be zero. I've set them a touch higher so they don't get obscured by the $x$-axis in the plot. Anyway, we're finally ready to plot a more complete version of Figure 12.2.
                     
```{r, fig.width = 6, fig.height = 3}
# for the annotation
text <-
  tibble(x        = c(15, 12),
         y        = c(.004, .012),
         label    = c("95% ETI", "95% HDI"),
         interval = c("eti", "hdi"))

# plot!
tibble(x = seq(from = 0, to = 40, by = .1)) %>% 
  ggplot(aes(x = x)) +
  geom_area(aes(y = dgamma(x, 2, .2)),
            fill = oa[4]) +
  geom_path(data = lines,
            aes(y = y, color = interval),
            size = 1) +
  geom_text(data = text,
            aes(y = y, color = interval, label = label)) +
  scale_color_manual(values = oa[c(5, 1)]) +
  scale_x_continuous("Density Value", expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 35)) +
  theme(legend.position = "none")
```

To repeat, ETIs are the only types of intervals available directly by the **brms** package. When using the default `print()` or `summary()` output for a `brm()` model, the 95% ETIs are displayed in the 'l-95% CI' and 'u-95% CI' columns.

```{r}
print(fit9.2)
```

In the output of most other **brms** functions, the 95% ETIs appear in the `Q2.5` and `Q97.5` columns. Take `fitted()`, for example.

```{r}
fitted(fit9.2, 
       newdata = nd,
       scale = "linear")
```

But as we just did, above, you can always use the convenience functions from the **tidybayes** package (e.g., `mean_hdi()`) to get HDIs from a **brms** fit.

```{r}
fitted(fit9.2, 
       newdata = nd,
       scale = "linear",
       summary = F) %>% 
  as_tibble() %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  mean_hdi(value)
```

As you may have gathered, Kruschke clearly prefers using HDIs over ETIs. His preference isn't without controversy. If you'd like to explore the topic further in the form of saucy twitter banter, the inimitable [Dan Simpson](https://twitter.com/dan_p_simpson) has just [the thread](https://twitter.com/dan_p_simpson/status/1134095728190676992) for you. Here's [the link](https://discourse.mc-stan.org/t/quantiles-vs-hpdi-from-rstan-summary/9025) to the corresponding thread in the Stan discourse forum.

## The model-comparison approach

> Recall that the motivating issue for this chapter is the question, Is the null value of a parameter credible? The previous section answered the question in terms of parameter estimation. In that approach, we started with a possibly informed prior distribution and examined the posterior distribution.
>
> In this section we take a different approach. Some researchers prefer instead to pose the question in terms of model comparison. In this framing of the question, the focus is not on estimating the magnitude of the parameter. Instead, the focus is on deciding which of two hypothetical prior distributions is least incredible. One prior expresses the hypothesis that the parameter value is exactly the null value. The alternative prior expresses the hypothesis that the parameter could be any value, according to some form of broad distribution. (p. 344)

### Is a coin fair or not?

Some [e.g., @leeModelingIndividualDifferences2005; @zhuCounterintuitiveNoninformativePrior2004] have argued the Haldane prior is superior to the uniform $\operatorname{Beta}(1, 1)$ when choosing an uninformative prior for $\theta$. The Haldane, recall, is $\operatorname{Beta}(\epsilon, \epsilon)$, where $\epsilon$ is some small value approaching zero (e.g., 0.01). We'll use our typical steps with the grid approximation to compute the data for the left column of Figure 12.3 (i.e., the column based on the Haldane prior).

```{r}
# we need these to compute the likelihood
n <- 24
z <- 7
epsilon <- .01

trial_data <- c(rep(0, times = n - z), rep(1, times = z))

d <-
  tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %>%
  mutate(prior      = dbeta(x = theta, shape1 = epsilon, shape2 = epsilon),
         likelihood = bernoulli_likelihood(theta = theta,
                                           data = trial_data)) %>%
  # we have to slice off the first and last values because they go to infinity on the prior, 
  # which creates problems when computing the denominator `sum(likelihood * prior)` (i.e., p(D))
  slice(2:999) %>% 
  mutate(posterior = likelihood * prior / sum(likelihood * prior))

head(d)
```

Here's the left column of Figure 12.3.

```{r, fig.width = 4.5, fig.height = 7, warning = F, message = F}
p1 <-
  d %>% 
  ggplot(aes(x = theta, y = prior)) +
  geom_area(fill = oa[4]) +
  annotate(geom = "text", x = .1, y = 4, 
           label = expression(epsilon == 0.01), 
           size = 3.5, color = oa[5]) +
  labs(title = "Prior (beta)",
       y = expression("beta"*(theta*"|"*epsilon*", "*epsilon)))

p2 <-
  d %>% 
  ggplot(aes(x = theta, y = likelihood)) +
  geom_area(fill = oa[4]) +
  labs(title = "Likelihood (Bernoulli)",
       y = expression(p(D*"|"*theta)))

p3 <-
  d %>% 
  ggplot(aes(x = theta, y = posterior)) +
  geom_area(fill = oa[4]) +
  labs(title = "Posterior (beta)",
       y = expression("beta"*(theta*"|"*7.01*", "*17.01)))

library(patchwork)

(p1 / p2 / p3) & 
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, 
                     expand = expansion(mult = 0), limits = c(0, 1)) &
  scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05))) &
  theme(panel.grid = element_blank())
```

We can calculate the beta parameters for the posterior using the formula $\operatorname{Beta}(\theta | z + \alpha, N - z + \beta)$.

```{r}
# alpha
z + epsilon

# beta
n - z + epsilon
```

We need updated data for the right column, based on the $\operatorname{Beta}(2, 4)$ prior.

```{r}
alpha <- 2
beta  <- 4

d <-
  tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %>%
  mutate(prior      = dbeta(x = theta, shape1 = alpha, shape2 = beta),
         likelihood = bernoulli_likelihood(theta = theta,
                                           data = trial_data)) %>%
  # no need to `slice(2:999)` this time
  mutate(posterior = likelihood * prior / sum(likelihood * prior))

head(d)
```

Now here's the right column of Figure 12.3.

```{r, fig.width = 4.5, fig.height = 7, warning = F, message = F}
p1 <-
  d %>% 
  ggplot(aes(x = theta, y = prior)) +
  geom_area(fill = oa[4]) +
  labs(title = "Prior (beta)",
       y = expression("beta"*(theta*"|"*2*", "*4)))

p2 <-
  d %>% 
  ggplot(aes(x = theta, y = likelihood)) +
  geom_area(fill = oa[4]) +
  labs(title = "Likelihood (Bernoulli)",
       y = expression(p(D*"|"*theta)))

p3 <-
  d %>% 
  ggplot(aes(x = theta, y = posterior)) +
  geom_area(fill = oa[4]) +
  labs(title = "Posterior (beta)",
       y = expression("beta"*(theta*"|"*9*", "*21)))

(p1 / p2 / p3) & 
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, 
                     expand = expansion(mult = 0), limits = c(0, 1)) &
  scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05)))
```

Here are those beta parameters for that posterior.

```{r}
# alpha
z + alpha

# beta
n - z + beta
```

Following the formula for the null hypothesis,

$$p(z, N|M_\text{null}) = \theta_\text{null}^z(1 - \theta_\text{null})^{(N - z)},$$

we can compute the probability of the data given the null hypothesis.

```{r}
theta <- .5

(p_d_null <- theta ^ z * (1 - theta) ^ (n - z))
```

The formula for the marginal likelihood for the alternative hypothesis $M_\text{alt}$ is

$$p(z, N| M_\text{alt}) = \frac{\operatorname{Beta}(z + \alpha_\text{alt}, N - z + \beta_\text{alt})}{\operatorname{Beta}(\alpha_\text{alt}, \beta_\text{alt})}.$$

We can make our own `p_d()` function to compute the probability of the data given alternative hypotheses. Here we'll simplify the function a bit to extract `z` and `n` out of the environment.

```{r}
p_d <- function(a, b) { 
  beta(z + a, n - z + b) / beta(a, b) 
}
```

With `p_d_null` and our `p_d()` function in hand, we can reproduce and extend the results in Kruschke's Equation 12.4.

```{r}
options(scipen = 999)

tibble(shape1 = c(2, 1, .1, .01, .001, .0001, .00001),
       shape2 = c(4, 1, .1, .01, .001, .0001, .00001)) %>% 
  mutate(p_d      = p_d(a = shape1, b = shape2),
         p_d_null = p_d_null) %>% 
  mutate(bf = p_d / p_d_null) %>% 
  # this just reduces the amount of significant digits in the output
  mutate_all(round, digits = 6)

options(scipen = 0)
```

Did you notice our use of `options(scipen)`? With the first line, we turned off scientific notation in the print output. We turned scientific notation back on with the second line. But back to the text,

> for now, notice that when the alternative prior is uniform, with $a_\text{alt} = b_\text{alt} = 1.000$, the Bayes' factor shows a (small) preference for the alternative hypothesis, but when the alternative prior approximates the Haldane, the Bayes' factor shows a strong preference for the null hypothesis. As the alternative prior gets closer to the Haldane limit, the Bayes' factor changes by orders of magnitude. Thus, as we have seen before (e.g. [Section 10.6][Extreme sensitivity to prior distribution], p. 292), the Bayes' factor is *very sensitive to the choice of prior distribution*. (p. 345, *emphasis* added)

On page 346, Kruschke showed some of the 95% HDIs for the marginal distributions of the various $M_\text{alt}$s. We could compute those one at a time with `hdi_of_icdf()`. But why not work in bulk? Like we did in Chapter 10, let's make a custom variant `hdi_of_qbeta()`, which will be more useful within the context of `map2()`.

```{r}
hdi_of_qbeta <- function(shape1, shape2) {
  
  hdi_of_icdf(name = qbeta,
              shape1 = shape1,
              shape2 = shape2) %>% 
    data.frame() %>% 
    mutate(level = c("ll", "ul")) %>% 
    spread(key = level, value = ".")
  
}
```

Compute the HDIs.

```{r}
tibble(shape1 =     z + c(2, 1, .1, .01, .001, .0001, .00001),
       shape2 = n - z + c(4, 1, .1, .01, .001, .0001, .00001)) %>% 
  mutate(h = map2(shape1, shape2, hdi_of_qbeta)) %>% 
  unnest(h) %>% 
  mutate_at(vars(ends_with("l")), .funs = ~round(., digits = 4))
```

As Kruschke mused,

> if we consider the posterior distribution instead of the Bayes' factor, we see that the posterior distribution on $\theta$ within the alternative model is only slightly affected by the prior... In all cases, the 95% HDI excludes the null value, although a wide ROPE might overlap the HDI. Thus, the explicit estimation of the bias parameter robustly indicates that the null value should be rejected, but perhaps only marginally. This contrasts with the Bayes' factor, model-comparison approach, which rejected the null or accepted the null depending on the alternative prior. 

Further,

> of the Bayes' factors in Equation 12.4, which is most appropriate? If your analysis is driven by the urge for a default, uninformed alternative prior, then the prior that best approximates the Haldane is most appropriate. Following from that, we should strongly prefer the null hypothesis to the Haldane alternative. While this is mathematically correct, it is meaningless for an applied setting because the Haldane alternative represents nothing remotely resembling a credible alternative hypothesis. The Haldane prior sets prior probabilities of virtually zero at all values of $\theta$ except $\theta = 0$ and $\theta = 1$. There are very few applied settings where such a U-shaped prior represents a genuinely meaningful theory. (p. 346).

### Bayes' factor can accept null with poor precision.

Here are the steps to make the left column of Figure 12.4 (i.e., the column based on very weak data and the Haldane prior).

```{r, fig.width = 4.5, fig.height = 7, warning = F, message = F}
# we need these to compute the likelihood
n <- 2
z <- 1

trial_data <- c(rep(0, times = n - z), rep(1, times = z))

d <-
  tibble(theta = seq(from = 0, to = 1, length.out = 1000)) %>%
  mutate(prior      = dbeta(x = theta, shape1 = epsilon, shape2 = epsilon),
         likelihood = bernoulli_likelihood(theta = theta,
                                           data = trial_data)) %>%
  # like before, we have to slice off the first and last values because they go to infinity on the
  # prior, which creats problems when computing the denominator `sum(likelihood * prior)` (i.e., p(D))
  slice(2:999) %>% 
  mutate(posterior = likelihood * prior / sum(likelihood * prior))

p1 <-
  d %>% 
  ggplot(aes(x = theta, y = prior)) +
  geom_area(fill = oa[4]) +
  annotate(geom = "text", x = .1, y = 4, 
           label = expression(epsilon == 0.01), 
           size = 3.5, color = oa[5]) +
  labs(title = "Prior (beta)",
       y = expression("Beta"*(theta*"|"*epsilon*", "*epsilon)))

p2 <-
  d %>% 
  ggplot(aes(x = theta, y = likelihood)) +
  geom_area(fill = oa[4]) +
  labs(title = "Likelihood (Bernoulli)",
       y = expression(p(D*"|"*theta)))

p3 <-
  d %>% 
  ggplot(aes(x = theta, y = posterior)) +
  geom_area(fill = oa[4]) +
  labs(title = "Posterior (beta)",
       y = expression("Beta"*(theta*"|"*7.01*", "*17.01)))

(p1 / p2 / p3) & 
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, 
                     expand = expansion(mult = 0), limits = c(0, 1)) &
  scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05)))
```

That is one flat posterior! Here are the shape parameters and the HDIs.

```{r}
(alpha <- z + epsilon)
(beta <- n - z + epsilon)

hdi_of_icdf(name = qbeta,
            shape1 = alpha,
            shape2 = beta) %>% 
  round(digits = 3)
```

How do we compute the BF?

```{r}
theta <- .5
a <- epsilon
b <- epsilon

# pD_{null}                            pD_{alternative}
(theta ^ z * (1 - theta) ^ (n - z)) / (beta(z + a, n - z + b) / beta(a, b))
```

Just like in the text, "the Bayes' factor is 51.0 in favor of the null hypothesis" (p. 347)!

Here are the steps to make the right column of Figure 12.4, which is based on stronger data and a flat $\operatorname{Beta}(1, 1)$ prior.

```{r, fig.width = 4.5, fig.height = 7, warning = F, message = F}
# we need these to compute the likelihood
n <- 14
z <- 7

trial_data <- c(rep(0, times = n - z), rep(1, times = z))

d <-
  tibble(theta = seq(from = 0, to = 1, length.out = 1e3)) %>%
  mutate(prior      = dbeta(x = theta, shape1 = alpha, shape2 = beta),
         likelihood = bernoulli_likelihood(theta = theta,
                                           data = trial_data)) %>%
  # no need to `slice(2:999)` this time
  mutate(posterior = likelihood * prior / sum(likelihood * prior))

p1 <-
  d %>% 
  ggplot(aes(x = theta, y = prior)) +
  geom_area(fill = oa[4]) +
  labs(title = "Prior (beta)",
       y = expression("Beta"*(theta*"|"*1*", "*1)))

p2 <-
  d %>% 
  ggplot(aes(x = theta, y = likelihood)) +
  geom_area(fill = oa[4]) +
  labs(title = "Likelihood (Bernoulli)",
       y = expression(p(D*"|"*theta)))

p3 <-
  d %>% 
  ggplot(aes(x = theta, y = posterior)) +
  geom_area(fill = oa[4]) +
  labs(title = "Posterior (beta)",
       y = expression("Beta"*(theta*"|"*8*", "*8)))

(p1 / p2 / p3) & 
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, 
                     expand = expansion(mult = 0), limits = c(0, 1)) &
  scale_y_continuous(breaks = NULL, expand = expansion(mult = c(0, 0.05)))
```

Here are the updated shape parameters and the HDIs.

```{r}
(alpha <- z + epsilon)
(beta <- n - z + epsilon)

hdi_of_icdf(name = qbeta,
            shape1 = alpha,
            shape2 = beta) %>% 
  round(digits = 3)
```

Those HDIs are still pretty wide, but much less so than before. Let's compute the BF.

```{r}
a <- 1
b <- 1

# pD_{null}                            pD_{alternative}
(theta ^ z * (1 - theta) ^ (n - z)) / (beta(z + a, n - z + b) / beta(a, b))
```

A BF of 3.14 in favor of the null is lackluster evidence. And happily so given the breadth of the HDIs.

Kruschke discussed how we'd need $z = 1200$ and $N = 2400$ before the posterior HDIs would fit within a narrow ROPE like .48 and .52. Here's what that would look like based on the priors from Figure 12.4.

```{r, fig.width = 6, fig.height = 4, warning = F, message = F}
z <- 1200
n <- 2400
alpha <-     z + epsilon
beta  <- n - z + epsilon

d <- tibble(theta = seq(from = 0, to = 1, length.out = 1e3))
  
# the Haldane-based plot
p1 <-
  d %>% 
  ggplot(aes(x = theta, y = dbeta(theta, shape1 = alpha, shape2 = beta))) +
  geom_rect(xmin = .48,  xmax = .52,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = oa[2]) +
  geom_area(fill = oa[4]) +
  annotate(geom = "text", x = .05, y = 35, 
           label = expression(epsilon == 0.01), size = 3.5) +
  ggtitle("This posterior used the Haldane prior.")

# redefine the Beta parameters
alpha <-     z + 1
beta  <- n - z + 1

# the Beta(1, 1)-based plot
p2 <-
  d %>% 
  ggplot(aes(x = theta, y = dbeta(theta, shape1 = alpha, shape2 = beta))) +
  geom_rect(xmin = .48,  xmax = .52,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = oa[2]) +
  geom_area(fill = oa[4]) +
  ggtitle("This time we used the flat beta (1, 1).")

(p1 / p2) & 
  scale_x_continuous(expression(theta), breaks = 0:5 / 5, 
                     expand = expansion(mult = 0), limits = c(0, 1)) &
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05)))
```

> There is no way around this inconvenient statistical reality: high precision demands a large sample size (and a measurement device with minimal possible noise). But when we are trying to accept a specific value of $\theta$, is seems logically appropriate that we should have a reasonably precise estimate indicating that specific value. (p. 348)

### Are different groups equal or not?

> Researchers often want to ask the question, Are the groups different or not?
>
> As a concrete example, suppose we conduct an experiment about the effect of background music on the ability to remember. As a simple test of memory, each person tries to memorize the same list of 20 words (such as "chair," "shark," "radio," etc.). They see each word for a specific time, and then, after a brief retention interval, recall as many words as they can. (p. 348)

If you look in Kruschke's `OneOddGroupModelComp2E.R` file, you can get his simulation code. Here we've dramatically simplified it. This attempt does not exactly reproduce what his script did, but it gets it in spirit.

```{r}
# For each subject, specify the condition s/he was in,
# the number of trials s/he experienced, and the number correct.
n_g <- 20  # number of subjects per group
n_t <- 20  # number of trials per subject

set.seed(12)
my_data <-
  tibble(condition   = factor(c("Das Kruschke", "Mozart", "Bach", "Beethoven"),
                              levels = c("Das Kruschke", "Mozart", "Bach", "Beethoven")),
         group_means = c(.40, .50, .51, .52)) %>% 
  expand(nesting(condition, group_means),
         row = 1:20) %>% 
  mutate(id  = 1:80,
         n_g = n_g,
         n_t = n_t) %>% 
  mutate(n_recalled = rbinom(n_g, n_t, group_means))

head(my_data)
```

Here are the means for `n_recalled`, by `condition`.

```{r}
my_data %>% 
  group_by(condition) %>% 
  summarise(mean_n_recalled = mean(n_recalled))
```

#### Model specification in ~~JAGS~~ brms.

Recall that although **brms** does accommodate models based on the Bernoulli likelihood, it doesn't do so when the data are aggregated. With our aggregate Bernoulli data, we'll have to use the conventional binomial likelihood, instead. We'll compute two models. Our full model will be

\begin{align*}
\text{n_recalled}_{ij}            & \sim \operatorname{Binomial}(n = 20, \theta_{j}), \text{where} \\
\operatorname{logit}(\theta_j) & = \beta_{0_j}.
\end{align*}

In our equation, $\beta_{0_j}$ is the group-specific intercept within the logistic regression model. We'll use the $N(0, 1.5)$ prior for the intercept. Though it appears strongly regularizing in the log-odds space, it's quite flat on the $\theta$ space. If we wanted to be more conservative in the $\theta$ space, we might use something more like $N(0, 1)$.

```{r fit12.1}
fit12.1 <-
  brm(data = my_data, 
      family = binomial,
      n_recalled | trials(20) ~ 0 + condition,
      prior(normal(0, 1.5), class = b),
      iter = 3000, warmup = 1000, cores = 4, chains = 4,
      seed = 12,
      file = "fits/fit12.01") 
```

Here's the summary for the full model.

```{r}
print(fit12.1)
```

Do keep in mind that our results will differ from Kruschke's because of two factors. First, we simulated slightly different data. In the limit, I suspect our data simulation approaches would converge. But we're far from the limit. Second, we used a different likelihood to model the data, which resulted in slightly different priors. Yet even with those substantial limitations, our results are pretty close.

To make the top portion of Figure 12.5, we'll need to extract the `condition`-specific parameters. For that, we'll employ `fixef()` and then wrangle a bit.

```{r}
post <-
  fixef(fit12.1, summary = F) %>% 
  as_tibble() %>% 
  transmute(theta_1 = conditionDasKruschke, 
            theta_2 = conditionMozart, 
            theta_3 = conditionBach, 
            theta_4 = conditionBeethoven) %>%
  mutate_all(inv_logit_scaled) %>% 
  transmute(`theta[1]-theta[2]` = theta_1 - theta_2,
            `theta[1]-theta[3]` = theta_1 - theta_3,
            `theta[1]-theta[4]` = theta_1 - theta_4,
            `theta[2]-theta[3]` = theta_2 - theta_3,
            `theta[2]-theta[4]` = theta_2 - theta_4,
            `theta[3]-theta[4]` = theta_3 - theta_4)

glimpse(post)
```

Now we have the wrangled data, we're ready to convert them to the long format and plot the top of Figure 12.5.

```{r, fig.width = 8, fig.height = 4.25}
post %>% 
  pivot_longer(everything()) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  geom_vline(xintercept = 0, color = oa[2]) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = oa[4], color = oa[3],
                    breaks = 30, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-.25, .25)) +
  facet_wrap(~ name, labeller = label_parsed)
```

Also, do note we're working with the $\theta$ parameters in our aggregated binomial models, rather than $\omega$s.

Here's how you'd get the posterior mean and HDI summaries.

```{r}
post %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  mode_hdi(value) %>% 
  mutate_if(is.double, round, digits = 3)
```

If we wanted to know what proportion of the difference distributions were greater than zero, we could do something like this.

```{r, message = F}
post %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  summarise(p = mean(value > 0)) %>% 
  mutate_if(is.double, round, digits = 3)
```

I got this idea from the great [Tristan Mahr](https://twitter.com/tjmahr), who [pointed out](https://twitter.com/tjmahr/status/1097980989387755521) that conditional tests like `value > 0` compute a vector of `TRUE` and `FALSE` values. By nesting that within `mean()`, you end up with the proportion of those values that are `TRUE`.

With our Stan/**brms** method, we don't have an analogue to the lower portion of Figure 12.5 because we are not fitting the full and restricted models within a single run. Thus, there's no plot to show the chains traversing from $M_\text{full}$ to $M_\text{restricted}$. Rather, our `fit12.1` was just of $M_\text{full}$. Now we'll fit $M_\text{restricted}$, which we'll save as `fit12.2`.

```{r fit12.2}
fit12.2 <-
  brm(data = my_data, 
      family = binomial,
      n_recalled | trials(20) ~ 1,
      prior(normal(0, 1.5), class = Intercept),
      iter = 3000, warmup = 1000, cores = 4, chains = 4,
      seed = 12,
      file = "fits/fit12.02") 
```

Here we'll compare the two models with the LOO.

```{r loo_fit12.1_fit12.2, message = F}
fit12.1 <- add_criterion(fit12.1, criterion = "loo")
fit12.2 <- add_criterion(fit12.2, criterion = "loo")

loo_compare(fit12.1, fit12.2) %>% 
  print(simplify = F)
```

The LOO comparison suggests `fit12.1`, the full model with the `condition`-specific intercepts, is an improvement over the restricted one-intercept-only model.  We can also compare the models with their weights via the `model_weights()` function. Here we'll use `weights = "loo"` criterion.

```{r 12_loo_weights, cache = T}
model_weights(fit12.1, fit12.2, weights = "loo") %>% 
  round(digits = 3)
```

Recall that within a given comparison, the weights sum to 1, with better fitting models tending closer to 1 than the other(s). In this case, almost all the weight went to the $M_\text{full}$, `fit12.1`.

#### Bonus: Hypothesis testing in brms.

Disclaimer: I am not a fan of hypothesis testing within the Bayesian framework. Outside of pedagogical material like this, I do not use these methods. However, it'd seem negligent not to at least mention the convenience function designed for that purpose in **brms**: the `hypothesis()` function. From the `hypothesis.brmsfit` section in the [**brms** reference manual](https://CRAN.R-project.org/package=brms/brms.pdf) [@brms2022RM, p. 109] we read:

> Among others, `hypothesis` computes an evidence ratio (`Evid.Ratio`) for each hypothesis. For a one-sided hypothesis, this is just the posterior probability (`Post.Prob`) under the hypothesis against its alternative. That is, when the hypothesis is of the form `a > b`, the evidence ratio is the ratio of the posterior probability of `a > b` and the posterior probability of `a < b`. In this example, values greater than one indicate that the evidence in favor of `a > b` is larger than evidence in favor of `a < b`. For an two-sided (point) hypothesis, the evidence ratio is a Bayes factor between the hypothesis and its alternative computed via the Savage-Dickey density ratio method. That is the posterior density at the point of interest divided by the prior density at that point. Values greater than one indicate that evidence in favor of the point hypothesis has increased after seeing the data. In order to calculate this Bayes factor, all parameters related to the hypothesis must have proper priors and argument `sample_prior` of function `brm` must be set to `"yes"`. Otherwise `Evid.Ratio` (and `Post.Prob`) will be `NA`. Please note that, for technical reasons, we cannot sample from priors of certain parameters classes. Most notably, these include overall intercept parameters (prior class `"Intercept"`) as well as group-level coefficients. When interpreting Bayes factors, make sure that your priors are reasonable and carefully chosen, as the result will depend heavily on the priors. In particular, avoid using default priors.

Following the `a < b` format, let's say we wanted to test the hypothesis $\theta_\text{Das Kruschke} < \theta_\text{Bach}$, based on `fit12.1`. If we convert the relevant parameters from the log-odds metric to the probability scale with `inv_logit_scaled()`, we can specify that hypothesis as a string and place it into the `hypothesis()` function.

```{r}
hypothesis(fit12.1, 
           "inv_logit_scaled(conditionDasKruschke) < inv_logit_scaled(conditionBach)")
```

In the `Estimate` through `CI.Upper` columns, we got the typical **brms** summary statistics for model parameters. Those CIs, recall, are ETIs rather than HDIs. To interpret the rest, we read further from the **brms** reference manual that

> The `Evid.Ratio` may sometimes be `0` or `Inf` implying very small or large evidence, respectively, in favor of the tested hypothesis. For one-sided hypotheses pairs, this basically means that all posterior samples are on the same side of the value dividing the two hypotheses. In that sense, instead of `0` or `Inf`, you may rather read it as `Evid.Ratio` smaller `1 / S` or greater `S`, respectively, where `S` denotes the number of posterior samples used in the computations.
>
> The argument alpha specifies the size of the credible interval (i.e., Bayesian confidence interval). For instance, if we tested a two-sided hypothesis and set `alpha = 0.05` (5%) an, the credible interval will contain `1 -alpha = 0.95` (95%) of the posterior values. Hence, `alpha * 100`% of the posterior values will lie foutside of the credible interval. Although this allows testing of hypotheses in a similar manner as in the frequentist null-hypothesis testing framework, we strongly argue against using arbitrary cutoffs (e.g., `p < .05`) to determine the 'existence' of an effect.

In this case, the entire posterior distribution (i.e., all the iterations of the chains) was below zero and we ended up with an `Evid.Ratio = Inf`. Our Bayes factor blew up. If we'd like to test a point null hypothesis, we might reformat the equation to $\theta_\text{Das Kruschke} = \theta_\text{Bach}$.

```{r}
hypothesis(fit12.1, 
           "inv_logit_scaled(conditionDasKruschke) = inv_logit_scaled(conditionBach)")
```

Here we no longer get summary information in the `Evid.Ratio` and `Post.Prob` columns. But we do get that posterior summary information and we also get that little `*` symbol in the `Star` column, which was based on the brms default `alpha = 0.05`.

Let's see what happens when we test a different kind of directional hypothesis, $\theta_\text{Motzart} - \theta_\text{Bach} > 0$.

```{r}
hypothesis(fit12.1, 
           "inv_logit_scaled(conditionMozart) - inv_logit_scaled(conditionBach) > 0")
```

Here we get an underwhelming BF of 1.49. The posterior probability that hypothesis versus its logical alternative is .59. Notice we no longer have a `*` in the `Star` column.

One last thing about the `hypothesis()` function: you can feed it into the `plot()` function to get a quick plot of the results. Here's what that looks like from our last example.

```{r, fig.width = 5, fig.height = 2.5}
hypothesis(fit12.1, 
           "inv_logit_scaled(conditionMozart) - inv_logit_scaled(conditionBach) > 0") %>% 
  plot()
```

We won't use the `hypothesis()` much in this ebook. But if you're interested, there are other trick ways to make good use of it. To learn more, check out Vourre's handy blog post, [*How to calculate contrasts from a fitted brms model*](https://vuorre.netlify.com/post/2020/02/06/how-to-calculate-contrasts-from-a-fitted-brms-model/).

## Relations of parameter estimation and model comparison

Back to the text, Kruschke wrapped up this section by explaining

> the model comparison focuses on the null value and whether its local probability increases from prior to posterior. The parameter estimation considers the entire posterior distribution, including the uncertainty (i.e., HDI) of the parameter estimate relative to the ROPE.
>
> The derivation of the Bayes' factor by considering the null value in parameter estimation is known as the Savage-Dickey method. A lucid explanation is provided by Wagenmakers, Lodewyckx, Kuriyal, and Grasman [-@wagenmakersBayesianHypothesisTesting2010], who also provide some historical references and applications to MCMC analysis of hierarchical models. (pp. 353--354)

Hey, we just read about that Savage-Dickey method when learning about the `brms::hypothesis()` function!

## Estimation and model comparison?

I'll leave this for you to decide. Here's Kruschke: "As mentioned above, neither method for null value assessment (parameter estimation or model comparison) is uniquely 'correct.' The two approaches merely pose the question of the null value in different ways" (p. 354). If you'd like to read more on comparisons between the HDI, ROPE, and Bayes factor methods, check out the [-@linde2021DecisionsAboutequivalence] simulation study by Linde and colleagues or the follow-up [-@campbell2021re] preprint by Campbell and Gustafson.

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
# here we'll remove our objects
rm(oa, bernoulli_likelihood, n, z, trial_data, d, alpha, beta, hdi_of_icdf, h, my_data, fit9.2, c, c_small, name_list, nd, correlation_matrix, mu, sd, covariance_matrix, ex, hx, ey, hy, lines, text, epsilon, p1, p2, p3, p_d_null, p_d, hdi_of_qbeta, a, b, n_g, n_t, fit12.1, post, fit12.2, f, theta)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:12.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

# Goals, Power, and Sample Size

> Researchers collect data in order to achieve a goal. Sometimes the goal is to show that a suspected underlying state of the world is credible; other times the goal is to achieve a minimal degree of precision on whatever trends are observed. Whatever the goal, it can only be probabilistically achieved, as opposed to definitely achieved, because data are replete with random noise that can obscure the underlying state of the world. Statistical *power* is the probability of achieving the goal of a planned empirical study, if a suspected underlying state of the world is true. [@kruschkeDoingBayesianData2015, p. 359, *emphasis* in the original]

## The will to power

"In this section, [Kruschke laid out a] framework for research and data analysis [that might lead] to a more precise definition of power and how to compute it" (p. 360).

### Goals and obstacles.

The three research goals Kruschke dealt with in this chapter were:

* to reject a null value for a parameter,
* to confirm the legitimacy of a particular parameter value, and
* to estimate a parameter with reasonable precision.

All these could, of course, be extended to contexts involving multiple parameters and all of these were dealt with using 95% HDIs.

### Power.

> Because of random noise, the goal of a study can be achieved only probabilistically. The probability of achieving the goal, given the hypothetical state of the world and the sampling plan, is called the *power* of the planned research. In traditional null hypothesis significance testing (NHST), power has only one goal (rejecting the null hypothesis), and there is one conventional sampling plan (stop at predetermined sample size) and the hypothesis is only a single specific value of the parameter. In traditional statistics, that is *the* definition of power. That definition is generalized in this book to include other goals, other sampling plans, and hypotheses that involve an entire distribution on parameters. (p. 361, *emphasis* in the original)

Three primary methods to increase power are:

* reducing measurement error,
* increasing the effect size, and
* increasing the sample size.

Kruschke then laid out a five-step procedure to compute power within a Bayesian workflow.

1. Use theory/prior information to specify hypothetical distributions for all parameter values in the model.
2. Use those distributions to generate synthetic data according to the planned sampling method.
3. Fit the proposed model--including the relevant priors--with the synthetic data.
4. Use the posterior to determine whether we attained the research goal.
5. Repeat the procedure many times (i.e., using different `set.seed()` values) to get a distribution of results.

### Sample size.

> *The best that a large sample can do is exactly reflect the data-generating distribution.* If the data-generating distribution has considerable mass straddling the null value, then the best we can do is get estimates that include and straddle the null value. As a simple example, suppose that we think that a coin may be biased, and the data-generating hypothesis entertains four possible values of $\theta$ , with $p (\theta = 0.5) = 25 \%$, $p (\theta = 0.6) = 25 \%$, $p (\theta = 0.7) = 25 \%$, and $p (\theta = 0.8) = 25 \%$. Because $25 \%$ of the simulated data come from a fair coin, the maximum probability of excluding $\theta = 0.5$, even with a huge sample, is $75 \%$. 
>
> Therefore, when planning the sample size for an experiment, it is crucial to decide what a realistic goal is. If there are good reasons to posit a highly certain data-generating hypothesis, perhaps because of extensive previous results, then a viable goal may be to exclude a null value. On the other hand, if the data-generating hypothesis is somewhat vague, then a more reasonable goal is to attain a desired degree of precision in the posterior. (p. 364, *emphasis* in the original)

### Other expressions of goals.

I'm going to skip over these.

> In the remainder of the chapter, it will be assumed that the goal of the research is estimation of the parameter values, starting with a viable prior. The resulting posterior distribution is then used to assess whether the goal was achieved. (p. 366)

## Computing power and sample size

> As our first worked-out example, consider the simplest case: Data from a single coin. Perhaps we are polling a population and we want to precisely estimate the preferences for candidates A or B. Perhaps we want to know if a drug has more than a 50% cure rate. (p. 366)

### When the goal is to exclude a null value.

> Usually it is more intuitively accessible to get prior data, or to think of idealized prior data, than to directly specify a distribution over parameter values. For example, based on knowledge about the application domain, we might have 2000 actual or idealized flips of the coin for which the result showed 65% heads. Therefore we'll describe the data-generating hypothesis as a beta distribution with a mode of 0.65 and concentration based on 2000 flips after a uniform "proto-prior": $\operatorname{beta}(\theta | 0.65 \cdot (2000 - 2) + 1, (1 - 0.65) \cdot (2000 - 2) + 1)$. (p. 366)

We'll look at that in a plot in just a moment. In the last chapter, we settled on a color palette and augmented our global plotting theme with help from the [**fishualize** package](https://CRAN.R-project.org/package=fishualize). In this chapter we'll keep with our fish-centric palette approach, this time based on [Chaetodon ephippium](https://www.fishbase.se/summary/Chaetodon-ephippium).

```{r, warning = F, message = F, fig.height = 3.5}
library(tidyverse)
library(fishualize)

scales::show_col(fish(n = 9, option = "Chaetodon_ephippium"))

ce <- fish(n = 9, option = "Chaetodon_ephippium")

theme_set(
  theme_grey() +
    theme(text = element_text(color = ce[1]),
          axis.text = element_text(color = ce[1]),
          axis.ticks = element_line(color = ce[1]),
          legend.background = element_blank(),
          legend.box.background = element_blank(),
          legend.key = element_rect(fill = ce[5]),
          panel.background = element_rect(fill = ce[5], color = ce[4]),
          panel.grid = element_blank(),
          strip.background = element_rect(fill = ce[1], color = ce[1]),
          strip.text = element_text(color = "white"))
)
```

Here's what $\operatorname{Beta}(\theta | 0.65 \cdot (2{,}000 - 2) + 1, (1 - 0.65) \cdot (2{,}000 - 2) + 1)$ looks like.

```{r, fig.width = 4, fig.height = 2, message = F, warning = F}
kappa <- 2000
omega <- .65

tibble(theta = seq(from = 0, to = 1, by = .001)) %>% 
  mutate(prior = dbeta(theta,
                       shape1 =      omega  * (kappa - 2) + 1,
                       shape2 = (1 - omega) * (kappa - 2) + 1)) %>% 
  
  ggplot(aes(x = theta, y = prior)) +
  geom_area(fill = ce[3]) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(subtitle = expression("Behold our beta"*(1299.7*', '*700.3)*" prior. It's rather peaked"),
       x = expression(theta))
```

If we wanted to take some random draws from that prior, say 5, we'd do something like this.

```{r}
n <- 5

set.seed(13)
rbeta(n,
      shape1 =      omega  * (kappa - 2) + 1,
      shape2 = (1 - omega) * (kappa - 2) + 1)
```

Now let's just take one draw and call it `bias`.

```{r}
n <- 1

set.seed(13)
bias <-
  rbeta(n,
        shape1 =      omega  * (kappa - 2) + 1,
        shape2 = (1 - omega) * (kappa - 2) + 1)

print(bias)
```

Do note that whereas Kruschke  based his discussion on a bias of 0.638, we're moving forward with our randomly-drawn `r round(bias, 3)`. Anyways, now we

> simulate flipping a coin with that bias $N$ times. The simulated data have $z$ heads and $N − z$ tails. The proportion of heads, $z/N$, will tend to be around [`r round(bias, 3)`], but will be higher or lower because of randomness in the flips. (p. 367)

```{r}
# pick some large number
n <- 1e3

set.seed(13)
tibble(flips = rbernoulli(n = n, p = bias)) %>% 
  summarise(n = n(),
            z = sum(flips)) %>% 
  mutate(`proportion of heads` = z / n)
```

And indeed our samples did tend around $\theta =.643$. Had we increased our number of draws by an order of magnitude or two, our proportion of heads would have been even closer to the true data-generating value.

Though he presented Table 13.1 in this section, Kruschke walked out how he came to those values in the following sections. We'll get to them in just a bit.

### Formal solution and implementation in R.

I've been playing around with this a bit. If you look closely at the code block on page 369, you'll see that Kruschke's `minNforHDIpower()` function requires the `HDIofICDF()` function from his `DBDA2E-utilities.R` file, which we usually recast as `hdi_of_icdf()`.

```{r}
hdi_of_icdf <- function(name, width = .95, tol = 1e-8, ... ) {
  
  incredible_mass <- 1.0 - width
  interval_width <- function(low_tail_prob, name, width, ...) {
    name(width + low_tail_prob, ...) - name(low_tail_prob, ...)
  }
  opt_info <- optimize(interval_width, c(0, incredible_mass), 
                       name = name, width = width, 
                       tol = tol, ...)
  hdi_lower_tail_prob <- opt_info$minimum
  
  return(c(name(hdi_lower_tail_prob, ...),
           name(width + hdi_lower_tail_prob, ...)))
  
}
```

Just to warm up, consider a beta distribution for which $\omega = .5$ and $\kappa = 2{,}000$. Here are the 95% HDIs.

```{r}
omega <- .5

hdi_of_icdf(name = qbeta,
            shape1 =      omega  * (kappa - 2) + 1,
            shape2 = (1 - omega) * (kappa - 2) + 1)
```

Those look a whole lot like the ROPE values Kruschke specified in his example at the bottom of page 370. But we're getting ahead of ourselves. Now that we have our `hdi_of_icdf()` function, we're ready to define our version of `minNforHDIpower()`, which I'm calling `min_n_for_hdi_power()`.

```{r}
min_n_for_hdi_power <- 
  function(gen_prior_mode, gen_prior_n,
           hdi_max_width = NULL, null_value = NULL,
           rope = c(max(0, null_value - 0.02), min(1, null_value + 0.02)),
           desired_power = 0.8, aud_prior_mode = 0.5, aud_prior_n = 2,
           hdi_mass = 0.95, init_samp_size = 20, verbose = TRUE) {
    # Check for argument consistency:
    if (!xor(is.null(hdi_max_width), is.null(null_value))) {
      stop("One and only one of `hdi_max_width` and `null_value` must be specified.")
    }
    # Convert prior mode and N to a, b parameters of beta distribution:
    gen_prior_a <-        gen_prior_mode  * (gen_prior_n - 2) + 1
    gen_prior_b <- (1.0 - gen_prior_mode) * (gen_prior_n - 2) + 1
    aud_prior_a <-        aud_prior_mode  * (aud_prior_n - 2) + 1
    aud_prior_b <- (1.0 - aud_prior_mode) * (aud_prior_n - 2) + 1
    # Initialize loop for incrementing `sample_size`:
    sample_size <- init_samp_size
    not_powerful_enough = TRUE
    # Increment `sample_size` until desired power is achieved:
    while(not_powerful_enough) {
      z_vec <- 0:sample_size # vector of all possible z values for N flips.
      # Compute probability of each z value for data-generating prior:
      p_z_vec <- exp(lchoose(sample_size, z_vec)
                     + lbeta(z_vec + gen_prior_a, sample_size - z_vec + gen_prior_b)
                     - lbeta(gen_prior_a, gen_prior_b))
      # For each z value, compute posterior HDI:
      # `hdi_matrix` will hold HDI limits for each z:
      hdi_matrix <- matrix(0, nrow = length(z_vec), ncol = 2)
      for (z_id_x in 1:length(z_vec)) {
        z <- z_vec[z_id_x]
        hdi_matrix[z_id_x, ] <- hdi_of_icdf(qbeta,
                                            shape1 = z + aud_prior_a,
                                            shape2 = sample_size - z + aud_prior_b,
                                            width  = hdi_mass)
      }
      # Compute HDI widths:
      hdi_width <- hdi_matrix[, 2] - hdi_matrix[, 1]
      # Sum the probabilities of outcomes with satisfactory HDI widths:
      if (!is.null(hdi_max_width)) {
        power_hdi <- sum(p_z_vec[hdi_width < hdi_max_width])
      }
      # Sum the probabilities of outcomes with HDI excluding `rope`:
      if (!is.null(null_value)) {
        power_hdi <- sum(p_z_vec[hdi_matrix[, 1] > rope[2] | hdi_matrix[, 2] < rope[1]])
      }
      if (verbose) {
        cat(" For sample size = ", sample_size, ", power = ", power_hdi,
            "\n", sep = ""); flush.console() 
      }
      if (power_hdi > desired_power) {  # If desired power is attained,
        not_powerful_enough = FALSE
      } else {
        sample_size <- sample_size + 1
        # set flag to stop,
        # otherwise
        # increment the sample size.
      }
    } # End while( not_powerful_enough ).
    # Return the sample size that achieved the desired power:
    return(sample_size)
}
```

Other than altering Kruschke's formatting a little bit, the only meaningful change I made to the code was removing the line that checked for the `HDIofICD()` function and then `source()`ed it, if necessary. Following along with Kruschke on page 370, here's an example for which $\omega_\text{data generating} = .75$, $\kappa = 2{,}000$, the ROPE is $[.48, .52]$, and the desired power is the conventional .8.

```{r}
min_n_for_hdi_power(gen_prior_mode = .75, 
                    gen_prior_n    = 2000,
                    hdi_max_width  = NULL, 
                    null_value     = .5, 
                    rope           = c(.48, .52),
                    desired_power  = .8,
                    aud_prior_mode = .5, 
                    aud_prior_n    = 2,
                    hdi_mass       = .95, 
                    init_samp_size = 20, 
                    verbose        = TRUE)
```

Just like in the text, the necessary $N = 30$.

Unlike in the text, I increased the value of `init_samp_size` from 5 to 20 to keep the output a reasonable length. To clarify what we just did,

> in that function call, the data-generating distribution has a mode of 0.75 and concentration of 2000, which means that the hypothesized world is pretty certain that coins have a bias of 0.75. The goal is to exclude a null value of 0.5 with a ROPE from 0.48 to 0.52. The desired power [is] 80%. The audience prior is uniform. When the function is executed, it displays the power for increasing values of sample size, until stopping at $N = 30$. (p. 370)

If it's unclear why the "audience prior is uniform", consider this.

```{r, fig.width = 4, fig.height = 2}
kappa <- 2
omega <- .5

tibble(theta = seq(from = 0, to = 1, by = .01)) %>% 
  mutate(prior = dbeta(theta,
                       shape1 =      omega  * (kappa - 2) + 1,
                       shape2 = (1 - omega) * (kappa - 2) + 1)) %>% 
  
  ggplot(aes(x = theta, y = prior)) +
  geom_area(fill = ce[3]) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(ylim = c(0, 1.25)) +
  labs(title = "Behold the uniform audience prior.",
       x = expression(theta))
```

If you work out the algebra with `omega` and `kappa`, you'll see this is a $\operatorname{Beta}(1, 1)$. Thus, `aud_prior_n` is $\kappa$ and `aud_prior_mode` is $\omega$.

Here we'll wrap our `min_n_for_hdi_power()` function into a simple `sim_power()` function for use with `purrr::map2()`.

```{r}
sim_power <- function(mode, power) {
  
  min_n_for_hdi_power(gen_prior_mode = mode, 
                      gen_prior_n    = 2000,
                      hdi_max_width  = NULL, 
                      null_value     = .5, 
                      rope           = c(.48, .52),
                      desired_power  = power,
                      aud_prior_mode = .5, 
                      aud_prior_n    = 2,
                      hdi_mass       = .95, 
                      init_samp_size = 1, 
                      verbose        = TRUE)
  
}
```

Here we use the two functions to compute the values in Table 13.1 on page 367. 

```{r sim, results = "hide"}
sim <-
  crossing(mode  = seq(from = .6, to = .85, by = .05),
           power = c(.7, .8, .9)) %>% 
  mutate(results = map2_dbl(mode, power, sim_power))
```

The results look like this.

```{r}
print(sim)
```

It takes just a tiny bit of wrangling to reproduce Table 13.1.

```{r}
sim %>% 
  pivot_wider(names_from = mode,
              values_from = results) %>%
  knitr::kable()
```

### When the goal is precision.

Recall that if we have $\operatorname{Beta}(a, b)$ prior for $\theta$ of the Bernoulli likelihood function, then the analytic solution for the posterior is $\operatorname{Beta}(\theta | z + a, N – z + b)$. In our first example, $z = 6$ out of $N = 10$ randomly selected voters preferred candidate A and we started with a flat $\operatorname{Beta}(\theta | 1, 1)$ prior. We can check that our posterior is indeed $\operatorname{Beta}(7, 5)$ by working through the algebra.

```{r}
z <- 6
n <- 10

# posterior alpha
z + 1

# posterior beta
n - z + 1
```

Here's how we compute the 95% HDIs.

```{r}
(
  h <-
    hdi_of_icdf(name = qbeta,
                shape1 = 7,
                shape2 = 5)
)
```

The $\operatorname{Beta}(7, 5)$ distribution looks like this.

```{r, fig.width = 4, fig.height = 2}
tibble(theta = seq(from = 0, to = 1, by = .01)) %>% 
  mutate(density = dbeta(theta,
                         shape1 = 7,
                         shape2 = 5)) %>% 
  
  ggplot(aes(x = theta, y = density)) +
  geom_area(fill = ce[3]) +
  geom_segment(x = h[1], xend = h[2],
               y = 0.01,    yend = 0.01,
               size = 1.2, color = ce[9]) +
  annotate(geom = "text", x = .6, y = 1/3, label = "95% HDI", color = "white") +
  scale_x_continuous(NULL, breaks = c(0, h[1], z / n, h[2], 1) %>% round(2)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  ggtitle(expression("Beta"*(7*", "*5)))
```

"It turns out, in this case, that we can never have a sample size large enough to achieve the goal of 80% of the HDIs falling above $\theta = 0.5$. To see why," keep reading in the text (p. 371). Happily,

> there is a more useful goal, however. Instead of trying to reject a particular value of $\theta$, we set as our goal a desired degree of precision in the posterior estimate. For example, our goal might be that the 95% HDI has width less than 0.2, at least 80% of the time. (p. 371)

If you look back up at our `min_n_for_hdi_power()` defining code, above, you'll see that "One and only one of `hdi_max_width` and `null_value` must be specified." So if we want to determine the necessary $N$ for an 95% HDI width of less than .2, we need to set `hdi_max_width = .2` and `null_value = NULL`.

```{r}
min_n_for_hdi_power(gen_prior_mode = .75, 
                    gen_prior_n    = 10,
                    hdi_max_width  = .2,  # look here
                    null_value     = NULL, 
                    rope           = NULL,
                    desired_power  = .8,
                    aud_prior_mode = .5, 
                    aud_prior_n    = 2,
                    hdi_mass       = .95, 
                    init_samp_size = 75, 
                    verbose        = TRUE)
```

Just like in the last section, here I set `init_samp_size` to a higher value than in the text in order to keep the output reasonably short. To reproduce the results in Table 13.2, we’ll need to adjust the `min_n_for_hdi_power()` parameters within our `sim_power()` function.

```{r, results = "hide"}
sim_power <- function(mode, power) {
  
  min_n_for_hdi_power(gen_prior_mode = mode, 
                      gen_prior_n    = 10,
                      hdi_max_width  = .2, 
                      null_value     = NULL, 
                      rope           = NULL,
                      desired_power  = power,
                      aud_prior_mode = .5, 
                      aud_prior_n    = 2,
                      hdi_mass       = .95, 
                      init_samp_size = 50, 
                      verbose        = TRUE)
  
}

sim <-
  crossing(mode  = seq(from = .6, to = .85, by = .05),
           power = c(.7, .8, .9)) %>% 
  mutate(results = map2_dbl(mode, power, sim_power))
```

Let's make that table.

```{r}
sim %>%
  pivot_wider(names_from = mode,
              values_from = results) %>%
  knitr::kable()
```

What did that audience prior look like?

```{r, fig.width = 4, fig.height = 2}
kappa <- 2
omega <- .5

tibble(theta = seq(from = 0, to = 1, by = .1)) %>% 
  mutate(density = dbeta(theta,
                         shape1 =      omega  * (kappa - 2) + 1,
                         shape2 = (1 - omega) * (kappa - 2) + 1)) %>% 
  
  ggplot(aes(x = theta, y = density)) +
  geom_area(fill = ce[3]) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Behold the uniform audience prior.",
       x = expression(theta))
```

Here are what the beta distributions based on the `sim` look like.

```{r, fig.width = 8, fig.height = 2.5}
sim %>% 
  rename(n = results) %>% 
  expand(nesting(mode, power, n), 
         theta = seq(from = 0, to = 1, by = .01)) %>% 
  mutate(density = dbeta(theta,
                         shape1 =      mode  * (n - 2) + 1,
                         shape2 = (1 - mode) * (n - 2) + 1),
         mode    = str_c("omega == ", mode)) %>% 
  
  ggplot(aes(x = theta, y = density)) +
  geom_vline(xintercept = .5, color = ce[8]) +
  geom_area(fill = ce[3]) +
  scale_x_continuous(expression(theta), labels = c("0", "", ".5", "", "1")) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  ggtitle("The power and mode values are in the rows and columns, respectively.") +
  facet_grid(power ~ mode, labeller = label_parsed)
```

Toward the end of the section, Kruschke mentioned the required sample size shoots up if our desired HDI width is 0.1. Here's the simulation.

```{r sim_3, results = "hide"}
sim_power <- function(mode, power) {
  
    min_n_for_hdi_power(gen_prior_mode = mode, 
                        gen_prior_n    = 10,
                        hdi_max_width  = .1, 
                        null_value     = NULL, 
                        rope           = NULL,
                        desired_power  = power,
                        aud_prior_mode = .5, 
                        aud_prior_n    = 2,
                        hdi_mass       = .95, 
                        init_samp_size = 300,  # save some time and up this parameter 
                        verbose        = TRUE)
  
}

sim <-
  crossing(mode  = seq(from = .6, to = .85, by = .05), 
           power = c(.7, .8, .9)) %>% 
  mutate(results = map2_dbl(mode, power, sim_power))
```

Display the results in a table like before.

```{r}
sim %>%
  pivot_wider(names_from = mode,
              values_from = results) %>%
  knitr::kable()
```

### Monte Carlo approximation of power.

> The previous sections illustrated the ideas of power and sample size for a simple case in which the power could be computed by mathematical derivation. [If your field is like mine, this will not be the norm for your research projects.] In this section, we approximate the power by Monte Carlo simulation. The R script for this simple case serves as a template for more realistic applications. The R script is named `Jags-Ydich-Xnom1subj-MbernBeta-Power.R`, which is the name for the JAGS program for dichotomous data from a single "subject" suffixed with the word "Power." As you read through the script, presented below, remember that you can find information about any general R command by using the help function in R, as explained in Section 3.3.1 (p. 39). (p. 372)

The code in Kruschke's `Jags-Ydich-Xnom1subj-MbernBeta-Power.R` file also makes use of the content in his `Jags-Ydich-Xnom1subj-MbernBeta.R` file. As is often the case, the code in both is JAGS and base-**R** centric. We'll be taking a different approach. I'll walk you through. First, let's fire up **brms**.

```{r, warning = F, message = F}
library(brms)
```

This won't be of much concern for some of the complex models we'll be fitting in later chapters. But for simple models like this, a lot of the time you spend waiting for `brms::brm()` to return your posterior and its summary has to do with compilation time. The issue of compilation goes into technical details I just don't have the will to go through right now. But if we can avoid or minimize compilation time, it'll be a boon for our power simulations. As it turns out, we can. The first time we fit our desired model, we have to compile. But once we have that initial fit object in hand, we can reuse it with the `update()` function, which will allow us to avoid further compilation. So that's what we’re going to do, here. We're going to fit the model once and save it.

```{r fit13.1}
# how many rows of 0's and 1's should we have in the data?
n <- 74

# should the values in the data be of single draws (i.e., 1), or summaries?
size <- 1

# what is the population mode for theta we'd like to base this all on?
omega <- .7

# fit that joint
fit13.1 <-
  brm(data = tibble(y = rbinom(n, size, omega)),
      family = bernoulli(link = identity),
      y ~ 1,
      prior(beta(1, 1), class = Intercept, lb = 0, ub = 1),
      warmup = 1000, iter = 3000, chains = 4, cores = 1,
      seed = 13,
      file = "fits/fit13.01")
```

You may (or not) recall that we covered how to time an operation in **R** back in [Section 3.7.5][Measuring processing time.]. When you're setting up a Monte Carlo power study, it can be important to use those time-tracking skills to get a sense of how long it takes to fit your models. While I was setting this model up, I experimented with keeping the default `cores = 1` or setting my typical `cores = 4`. As it turns out, with a very simple model like this, `cores = 1` was a little faster. If you're fitting one model, that's no big deal. But in a situation where you're fitting 100 or 1,000, you'll want to make sure you're fitting them as efficiently as possible.

But anyway, our practice will be to keep all the specifications in `fit` constant across the simulations. So choose them wisely. If you look deep into the bowels of the `Jags-Ydich-Xnom1subj-MbernBeta.R` file, you'll see Kruschke used the flat $\operatorname{Beta}(1, 1)$ prior, which is where our `prior(beta(1, 1), class = Intercept)` code came from. This is the audience prior. We aren't particularly concerned about the data we simulated with the `data = tibble(y = rbinom(n, size, omega))` line. The main thing is that they follow the same basic structure our subsequent data will.

To make sure we're not setting ourselves up to fail, we might make sure the chains look okay.

```{r, fig.width = 8, fig.height = 1.5}
plot(fit13.1, widths = c(2, 3))
```

Looks like a dream. Let's move forward and run the simulation proper. In his script file, Kruschke suggested we simulate with large $N$s like 1,000 or so. Since this is just an example, I'm gonna cut that to 100.

```{r, echo = F, eval = F}
# 1.179328 mins
```

```{r, eval = F}
# how many simulations would you like?
n_sim <- 100

# specify omega and kappa of the hypothetical parameter distribution
omega <- .7
kappa <- 2000

# make it reproducible
set.seed(13)

sim1 <-
  # define some of the parameters
  tibble(n     = n,
         size  = size,
         theta = rbeta(n_sim, 
                       shape1 =      omega  * (kappa - 2) + 1,
                       shape2 = (1 - omega) * (kappa - 2) + 1)) %>% 
  # simulate the data
  mutate(data = pmap(list(n, size, theta), rbinom)) %>% 
  # fit the models on the simulated data
  mutate(fit = map(data, ~update(fit13.1, newdata = list(y = .))))
```

```{r load_sims, echo = F}
# Even after reducing `sim1` through `sim5` by an order of magnitude, some of them take a long time to
# complete. So instead of completing them on the fly for each new version of this project, I've
# saved the sims in external files. 

# save(sim1, file = "sims/sim13.01.rds")
# save(sim2, file = "sims/sim13.02.rds")
# save(sim3, file = "sims/sim13.03.rds")
# save(sim4, file = "sims/sim13.04.rds")
# save(sim5, file = "sims/sim13.05.rds")

load(file = "sims/sim13.01.rds")
load(file = "sims/sim13.02.rds")
load(file = "sims/sim13.03.rds")
load(file = "sims/sim13.04.rds")
load(file = "sims/sim13.05.rds")

# for later on, we also need
n_sim <- 100
```

*What have we done?* you might ask.

```{r}
head(sim1)
```

The `theta` column contains the draws from the hypothesized parameter distribution, which we've indicated is hovering tightly around .7. The `data` column is nested in that within each row, we've saved an entire $N = 74$ row tibble. Most importantly, the `fit` column contains the `brms::brm()` objects for each of our 100 simulations. See that last `mutate()` line, above? That's where those came from. Within the `purrr::map()` function, we fed our simulated data sets, one row at a time, into the `update()` function via the `newdata` argument. Because we used `update()` based on our initial `fit`, we avoided subsequent compilation times and just sampled like a boss.

Before we move on, I should give some credit. The foundations of this workflow come from Wickham's talk, [*Managing many models with R*](https://www.youtube.com/watch?time_continue=426&v=rz3_FDVt9eg). I got some additional [help on twitter](https://twitter.com/PStrafo/status/1107447953709383681) from [Phil Straforelli](https://twitter.com/PStrafo).

We still have some work to do. Next, we'll want to make a custom function that will make it easy to compute the intercept HDIs for each of our fits.

```{r, message = F, warning = F}
library(tidybayes)

get_hdi <- function(fit) {
  fit %>%
    as_draws_df() %>%
    # yields the highest-density *continuous* interval
    mode_hdci(b_Intercept) %>%
    select(.lower:.upper)
}

# how does it work?
get_hdi(fit13.1)
```

Now we'll apply that function to our `fits` tibble to pull those simulated HDIs. Then we'll program in the markers for the ROPE and HDI width criteria, perform logical tests to see whether they were passed within each of the 100 simulations, and summarize the tests.

```{r, warning = F, message = F}
sim1 %>% 
  # get those HDIs and `unnest()`
  mutate(hdi = map(fit, get_hdi)) %>% 
  unnest(hdi) %>% 
  # define our test criteria
  mutate(rope_ll   = .48,
         rope_ul   = .52,
         hdi_width = .2) %>% 
  mutate(pass_rope  = .lower > rope_ul | .upper < rope_ll,
         pass_width = (.upper - .lower) < hdi_width) %>%
  
  # summarize those joints
  summarise(power_rope  = mean(pass_rope),
            power_width = mean(pass_width))
```

Those are our power estimates. To compute their HDIs, just increase them by a factor of 100 and plug them into the formulas within the shape arguments in `hdi_of_icdf()`.

```{r}
# HDIs for the ROPE power estimate
hdi_of_icdf(name = qbeta,
            shape1 = 1 + 91,
            shape2 = 1 + n_sim - 91) %>% 
  round(digits = 2)

# HDIs for the width power estimate
hdi_of_icdf(name = qbeta,
            shape1 = 1 + 39,
            shape2 = 1 + n_sim - 39) %>% 
  round(digits = 2)
```

Following the middle of page 375, we'll want to do the whole thing again with $\kappa = 10$ and $N = 91$.

Before we run the next simulation, notice how our first approach had us saving the model fits within our `sim1` object. When the models are simple and based on small data and when you’re only simulating 100 times, this isn't a huge deal. But saving 1,000+ `brms::brm()` fit objects of hierarchical models will bog you down. So for our next simulation, we'll only save the HDIs from our `get_hdi()` function.

```{r, echo = F, eval = F}
# 1.30995 mins
```

```{r, eval = F}
# how many rows of 0s and 1s should we have in the data?
n <- 91

# how many simulations would you like?
n_sim <- 100

# specify omega and kappa of the hypothetical parameter distribution
omega <- .7
kappa <- 10

# make it reproducible
set.seed(13)

# simulate
sim2 <-
  tibble(n     = n,
         size  = size,
         theta = rbeta(n_sim, 
                       shape1 =      omega  * (kappa - 2) + 1,
                       shape2 = (1 - omega) * (kappa - 2) + 1)) %>% 
  mutate(data = pmap(list(n, size, theta), rbinom)) %>% 
  mutate(hdi = map(data, ~update(fit13.1, newdata = list(y = .)) %>% get_hdi()))
```

Since we saved the HDI estimates in the `hdi` column, we can just `unnest()` then and summarize our power results.

```{r}
sim2 %>% 
  unnest(hdi) %>% 
  mutate(rope_ll   = .48,
         rope_ul   = .52,
         hdi_width = .2) %>% 
  mutate(pass_rope  = .lower > rope_ul | .upper < rope_ll,
         pass_width = (.upper - .lower) < hdi_width) %>%
  summarise(power_rope  = mean(pass_rope),
            power_width = mean(pass_width))
```

Compute the corresponding HDIs.

```{r}
# HDIs for the ROPE power estimate
hdi_of_icdf(name = qbeta,
            shape1 = 1 + 71,
            shape2 = 1 + n_sim - 71) %>% 
  round(digits = 2)

# HDIs for the width power estimate
hdi_of_icdf(name = qbeta,
            shape1 = 1 + 92,
            shape2 = 1 + n_sim - 92) %>% 
  round(digits = 2)
```

### Power from idealized or actual data.

> In practice, it is often more intuitive to specify actual or idealized *data* that express the hypothesis, than it is to specify top-level parameter properties. The idea is that we start with the actual or idealized data and then use Bayes' rule to generate the corresponding distribution on parameter values. (p. 376, *emphasis* in the original)

Here are the idealized parameters Kruschke outlied on pages 376--377.

```{r}
# specify idealized hypothesis:
ideal_group_mean <- 0.65
ideal_group_sd   <- 0.07

ideal_n_subj         <- 100  # more subjects => higher confidence in hypothesis
ideal_n_trl_per_subj <- 100  # more trials => higher confidence in hypothesis
```

These parameters are for binomial data. To parameterize $\theta$ in terms of a mean and standard deviation, we need to define the `beta_ab_from_mean_sd()` function.

```{r}
beta_ab_from_mean_sd <- function(mean, sd) {
  
  if (mean <= 0 | mean >= 1) stop("must have 0 < mean < 1")
  if (sd <= 0) stop("sd must be > 0")
  kappa <- mean * (1 - mean) / sd^2 - 1
  if (kappa <= 0) stop("invalid combination of mean and sd")
  a <- mean * kappa
  b <- (1.0 - mean) * kappa
  return(list(a = a, b = b))
  
}
```

Now generate data consistent with these values using a **tidyverse**-style workflow.

```{r}
b <- beta_ab_from_mean_sd(ideal_group_mean, ideal_group_sd)

# make the results reproducible
set.seed(13)

d <-
  # make a subject index and generate random theta values for idealized subjects
  tibble(s     = 1:ideal_n_subj,
         theta = rbeta(ideal_n_subj, b$a, b$b)) %>% 
  # transform the theta values to exactly match idealized mean and SD
  mutate(theta_transformed = ((theta - mean(theta)) / sd(theta)) * ideal_group_sd + ideal_group_mean) %>% 
  # `theta_transformed` must be between 0 and 1
  mutate(theta_transformed = ifelse(theta_transformed >= 0.999, 0.999,
                                    ifelse(theta_transformed <= 0.001, 0.001,
                                           theta_transformed))) %>% 
  # generate idealized data very close to thetas
  mutate(z = round(theta_transformed * ideal_n_trl_per_subj)) %>% 
  # create vector of 0's and 1's matching the z values generated above
  mutate(y = map(z, ~c(rep(1, .), rep(0, ideal_n_trl_per_subj - .)))) %>% 
  unnest(y)
```

Our main variables are `s` and `y`. You can think of the rest as showing our work. Here's a peek.

```{r}
str(d)
```

We are going to follow the same procedure we did when we originally fit the model to the therapeutic touch data in Chapter 9. Instead of reproducing the model Kruschke presented in his scripts, we are going to fit a hierarchical logistic regression model.

```{r fit13.2}
fit13.2 <-
  brm(data = d,
      family = bernoulli(link = logit),
      y ~ 1 + (1 | s),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 13,
      file = "fits/fit13.02")
```

Unlike in the text, we had no need for thinning our chains. Our effective sample size estimates were fine.

```{r}
print(fit13.2)
```

Here's a look at our two main parameters, our version of the top panels of Figure 13.3.

```{r, fig.width = 6, fig.height = 2.5, warning = F}
as_draws_df(fit13.2) %>% 
  pivot_longer(b_Intercept:sd_s__Intercept) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = ce[3], color = ce[9],
                    breaks = 40, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Remember, these are in the log-odds metric.",
       x = NULL) +
  facet_wrap(~ name, scales = "free")
```

> Now we have a distribution of parameter values consistent with our idealized hypothesis, but we did not have to figure out the top-level constants in the model. We merely specified the idealized tendencies in the data and expressed our confidence by its amount... So we now have a large set of representative parameter values for conducting a power analysis. (pp. 378--379)

With **brms**, you can sample from those model-implied parameter values with the `fitted()` function. By default, it will return values in the probability metric for our logistic model. Here we'll specify a group-level (i.e., `s`) value that was not in the data. We'll feed that new value into the `newdata` argument and set `allow_new_levels = T`. We'll also set `summary = F`, which will return actual probability values rather than a summary.

```{r}
set.seed(13)

f <-
  fitted(fit13.2,
         newdata = tibble(s = 0),
         allow_new_levels = T,
         summary = F) %>% 
  data.frame() %>% 
  set_names("theta")

str(f)
```

Here's what that looks like.

```{r, fig.width = 4, fig.height = 3}
f %>% 
  ggplot(aes(x = theta, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = ce[3], color = ce[9], breaks = 20) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Behold our \"distribution of parameter values consistent\nwith our idealized hypothesis.\"",
       x = expression(theta)) +
  xlim(0, 1)
```

We can make a custom function to sample from $\theta$. We might call it `sample_theta()`.

```{r}
sample_theta <- function(seed, n_subj) {
  
  set.seed(seed)
  
  bind_cols(s = 1:n_subj,
            sample_n(f, size = n_subj, replace = T))
  
}

# take it for a spin
sample_theta(seed = 13, n_subj = 5)
```

Now let's say I wanted to use our little `sample_theta()` function to sample $\theta$ values for three people `s` and then use those $\theta$ values to sample three draws from the corresponding Bernoulli distribution. We might do that like this.

```{r}
sample_theta(seed = 13, n_subj = 3) %>% 
  mutate(y = map(theta, rbinom, n = 3, size = 1)) %>% 
  unnest(y)
```

Notice how after we sampled from $\theta$, we still needed to take two more steps to simulate the desired data. So perhaps a better approach would be to wrap all those steps into one function and call it something like `sample_data()`.

```{r}
sample_data <- function(seed, n_subj, n_trial) {
  
  set.seed(seed)
  
  bind_cols(s = 1:n_subj,
            sample_n(f, size = n_subj, replace = T)) %>% 
    mutate(y = map(theta, rbinom, n = n_trial, size = 1)) %>% 
    unnest(y)
  
}

# test it out
sample_data(seed = 13, n_subj = 3, n_trial = 3) 
```

Here's how we'd use our `sample_data()` function to make several data sets within the context of a nested tibble.

```{r}
tibble(seed = 1:4) %>% 
  mutate(data = map(seed, sample_data, n_subj = 14, n_trial = 47))
```

With this data type, Kruschke indicated he ran

> the power analysis twice, using different selections of subjects and trials. In both cases there [was] a total of 658 trials, but in the first case there [were] 14 subjects with 47 trials per subject, and in the second case there [were] seven subjects with 94 trials per subject. (p. 381)

Before running the simulations in full, we fit the model once and save that fit to iteratively reuse with `update()`.

```{r fit13.3}
# how many subjects should we have?
n_subj <- 14

# how many trials should we have?
n <- 47

# fit that joint
fit13.3 <-
  brm(data = sample_theta(seed = 13, n_subj = 14) %>% 
        mutate(y = map(theta, rbinom, n = n, size = 1)) %>% 
        unnest(y),
      family = bernoulli(link = logit),
      y ~ 1 + (1 | s),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 13,
      file = "fits/fit13.03")
```

Check real quick to make sure the fit turned out okay.

```{r}
print(fit13.3)
```

Looks fine. Our new model simulation carries with it some new goals.

> In this example, [Kruschke] considered goals for achieving precision and exceeding a ROPE around the null value, at both the group level and individual level. For the group level, the goals are for the 95% HDI on the group mode, $\omega$, to fall above the ROPE around the null value, and for the width of the HDI to be less than 0.2. For the individual level, the goals are for at least one of the $\theta_s$s 95% HDIs to exceed the ROPE with none that fall below the ROPE, and for all the $\theta_s$s 95% HDIs to have widths less than 0.2. (pp. 379--380)

Now since we used an aggregated binomial model, we don't have a population-level $\omega$ parameter. Rather, we have a population $\theta$. So like before, our first goal is for the population $\theta$ to fall above the range $[.48, .52]$. The second corresponding width goal is also like before; we want $\theta$ to have a width of less than 0.2. But since our aggregated binomial model parameterized $\theta$ in the log-odds metric, we'll have to update our `get_hdi()` function, which we'll strategically rename `get_theta_hdi()`.

```{r, warning = F, message = F}
get_theta_hdi <- function(fit) {
  
  fit %>% 
    as_draws_df() %>% 
    transmute(theta = inv_logit_scaled(b_Intercept)) %>%
    # yields the highest-density *continuous* interval
    mode_hdci() %>% 
    select(.lower:.upper)
  
}

# how does it work?
get_theta_hdi(fit13.3)
```

As for the individual-level goals, the two Kruschke outlined in the text apply to our model in a straightforward way. But we will need one more custom function designed to pull the $\theta_s$s for the $\theta_s$s. Let's call this one `get_theta_s_hdi()`.

```{r get_theta_s_hdi, warning = F, message = F}
get_theta_s_hdi <- function(fit) {
  
  n_col <-
    coef(fit, summary = F)$s[, , "Intercept"] %>% 
    ncol()
    
  coef(fit, summary = F)$s[, , "Intercept"] %>% 
    data.frame() %>% 
    set_names(1:n_col) %>% 
    mutate_all(inv_logit_scaled) %>% 
    pivot_longer(everything(),
                 names_to = "s") %>% 
    mutate(s = as.numeric(s)) %>% 
    group_by(s) %>% 
    # yields the highest-density *continuous* interval
    mode_hdci(value) %>% 
    select(s, .lower:.upper) %>% 
    rename(.lower_s = .lower,
           .upper_s = .upper)
  
}

# how does it work?
get_theta_s_hdi(fit13.3)
```

With `sim2`, we avoided saving our model `brms::brm()` fit objects by using `map(data, ~update(fit1, newdata = list(y = .)) %>% get_hdi())`. That is, within the `purrr::map()` function, we first used `update()` to update the fit to the new data and then pumped that directly into `get_hdi()`, which simply returned our intervals. Though slick, this approach won't work here because we want to pump our updated model fit into two functions, both `get_theta_hdi()` and `get_theta_s_hdi()`. Our work-around will be to make a custom function that updates the fit, saves it as an object, inserts that fit object into both `get_theta_hdi()` and `get_theta_s_hdi()`, binds their results together, and the only returns the intervals. We'll call this function `fit_then_hdis()`.

```{r}
fit_then_hdis <- function(data, seed) {
  
  fit <- update(fit13.3, 
                newdata = data, 
                seed = seed)
  
  cbind(get_theta_hdi(fit),
        get_theta_s_hdi(fit))
}
```

Now we're ready to simulate.

```{r, echo = F, eval = F}
# 11.82841 mins
```

```{r, eval = F}
# how many subjects should we have?
n_subj <- 14

# how many trials should we have?
n_trial <- 47

# how many simulations would you like?
n_sim <- 100

sim3 <-
  tibble(seed = 1:n_sim) %>% 
  mutate(data = map(seed, sample_data, n_subj = n_subj, n_trial = n_trial)) %>% 
  mutate(hdi = map2(data, seed, fit_then_hdis))
```

If we hold these by the criteria of each $\text{HDI}_{\theta_s} > \text{ROPE}$ and all to have widths less than 0.2, It looks like our initial data-generating `fit13.3` is in the ballpark. Here are the results for the full power analysis, `sim3`.

```{r, eval = F}
sim3 <-
  sim3 %>% 
  unnest(hdi) %>%
  # here we determine whether we passed at the group level
  mutate(pass_rope_theta  = .lower > .52 | .upper < .48,
         pass_width_theta = (.upper - .lower) < .2) %>% 
  # the s-level thetas require two steps.
  # first, we'll outline the three criteria
  mutate(exceed_rope_theta_s  = .lower_s > .52,
         below_rope_theta_s   = .upper_s < .48,
         narrow_width_theta_s = (.upper_s - .lower_s) < .2) %>% 
  # second, we'll evaluate those criteria by group
  group_by(seed) %>% 
  mutate(pass_rope_theta_s  = sum(exceed_rope_theta_s) > 0 & sum(below_rope_theta_s) == 0,
         pass_width_theta_s = mean(narrow_width_theta_s) == 1) %>% 
  ungroup()

head(sim3)
```

Summarize the results.

```{r}
sim3 %>% 
  summarise(power_rope_theta  = mean(pass_rope_theta),
            power_width_theta = mean(pass_width_theta))

sim3 %>% 
  summarise(power_rope_theta_s  = mean(pass_rope_theta_s),
            power_width_theta_s = mean(pass_width_theta_s))
```

The power estimates for `power_rope_theta`, `power_width_theta`, and `power_rope_theta_s` were all the same, 1. Only the estimate for `power_width_theta_s` was unique. Here are the two sets of HDIs for the power estimate values.

```{r}
hdi_of_icdf(name = qbeta,
            shape1 = 1 + 100,
            shape2 = 1 + n_sim - 100) %>% 
  round(digits = 2)

hdi_of_icdf(name = qbeta,
            shape1 = 1 + 35,
            shape2 = 1 + n_sim - 35) %>% 
  round(digits = 2)
```

Hopefully it isn't a surprise our values differ from those in the text. We (a) used a different model and (b) used fewer simulation iterations. But I trust you get the overall idea. Like in the text, let's do the simulation again.

```{r, echo = F, eval = F}
# 10.18966 mins
```

```{r, eval = F}
# how many subjects should we have?
n_subj <- 7

# how many trials should we have?
n_trial <- 94

# how many simulations would you like?
n_sim <- 100

sim4 <-
  tibble(seed = 1:n_sim) %>% 
  mutate(data = map(seed, sample_data, n_subj = n_subj, n_trial = n_trial)) %>%
  mutate(hdi = map2(data, seed, fit_then_hdis))
```

Wrangle before summarizing.

```{r, eval = F}
sim4 <-
  sim4 %>% 
  unnest(hdi) %>%
  mutate(pass_rope_theta  = .lower > .52 | .upper < .48,
         pass_width_theta = (.upper - .lower) < .2) %>% 
  mutate(exceed_rope_theta_s  = .lower_s > .52,
         below_rope_theta_s   = .upper_s < .48,
         narrow_width_theta_s = (.upper_s - .lower_s) < .2) %>% 
  group_by(seed) %>% 
  mutate(pass_rope_theta_s  = sum(exceed_rope_theta_s) > 0 & sum(below_rope_theta_s) == 0,
         pass_width_theta_s = mean(narrow_width_theta_s) == 1) %>% 
  ungroup()
```

Summarize the results.

```{r}
sim4 %>% 
  summarise(power_rope_theta  = mean(pass_rope_theta),
            power_width_theta = mean(pass_width_theta))

sim4 %>% 
  summarise(power_rope_theta_s  = mean(pass_rope_theta_s),
            power_width_theta_s = mean(pass_width_theta_s))
```

Now compute the HDIs for `power_rope_theta` and `power_width_theta`.

```{r}
hdi_of_icdf(name = qbeta,
            shape1 = 1 + 97,
            shape2 = 1 + n_sim - 97) %>% 
  round(digits = 2)

hdi_of_icdf(name = qbeta,
            shape1 = 1 + 95,
            shape2 = 1 + n_sim - 95) %>% 
  round(digits = 2)
```

Second, we now compute the HDIs for `power_rope_theta_s` and `power_width_theta_s`.

```{r}
hdi_of_icdf(name = qbeta,
            shape1 = 1 + 100,
            shape2 = 1 + n_sim - 100) %>% 
  round(digits = 2)

hdi_of_icdf(name = qbeta,
            shape1 = 1 + 88,
            shape2 = 1 + n_sim - 88) %>% 
  round(digits = 2)
```

The results from our simulations contrast with those in the text. Though the results are similar with respect to $\theta_s$, they are markedly different with regards to our $\theta$ versus the text's $\omega$. But Kruschke's point is still sound:

> This example illustrates a general trend in hierarchical estimates. If you want high precision at the individual level, you need lots of data within individuals. If you want high precision at the group level, you need lots of individuals (without necessarily lots of data per individual, but more is better). (p. 382)

```{r}
ideal_group_mean <- 0.65
ideal_group_sd   <- 0.07

ideal_n_subj         <- 10  # instead of 100
ideal_n_trl_per_subj <- 10  # instead of 100

b <- beta_ab_from_mean_sd(ideal_group_mean, ideal_group_sd)

set.seed(13)

d <-
  tibble(s     = 1:ideal_n_subj,
         theta = rbeta(ideal_n_subj, b$a, b$b)) %>% 
  mutate(theta_transformed = ((theta - mean(theta)) / sd(theta)) * ideal_group_sd + ideal_group_mean) %>% 
  mutate(theta_transformed = ifelse(theta_transformed >= 0.999, 0.999,
                                    ifelse(theta_transformed <= 0.001, 0.001,
                                           theta_transformed))) %>% 
  mutate(z = round(theta_transformed * ideal_n_trl_per_subj)) %>% 
  mutate(y = map(z, ~c(rep(1, .), rep(0, ideal_n_trl_per_subj - .)))) %>% 
  unnest(y)

head(d)
```

Fit the $\theta$-generating model.

```{r fit13.4}
fit13.4 <-
  update(fit13.2,
         newdata = d,
         cores = 4,
         seed = 13,
         file = "fits/fit13.04")
```

Check to make sure things look alright.

```{r}
print(fit13.4)
```

Here's a look at our two main parameters, our version of the bottom panels of Figure 13.3.

```{r, fig.width = 6, fig.height = 2.5, warning = F}
as_draws_df(fit13.4) %>% 
  pivot_longer(b_Intercept:sd_s__Intercept) %>% 

  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = ce[3], color = ce[9],
                    breaks = 40, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Remember, these are in the log-odds metric.",
       x = NULL) +
  facet_wrap(~ name, scales = "free")
```

Now redefine our `fitted()` object, `f`, which gets pumped into the `sample_data()` function.

```{r}
set.seed(13)

f <-
  fitted(fit13.4,
         newdata = tibble(s = 0),
         allow_new_levels = T,
         summary = F) %>% 
  data.frame() %>% 
  set_names("theta")
```

Here's what our updated distribution of $\theta$ values looks like.

```{r, fig.width = 4, fig.height = 3}
f %>% 
  ggplot(aes(x = theta, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = c(.95, .5),
                    fill = ce[3], color = ce[9], breaks = 40) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Behold our \"distribution of parameter values consistent\nwith our idealized hypothesis.\"",
       x = expression(theta)) +
  coord_cartesian(xlim = c(0, 1))
```

Note the distribution is wider than the previous one. Anyway, now we're good to go. Here's our version of the first power analysis.

```{r, echo = F, eval = F}
# 12.25984 mins
```

```{r, eval = F}
# how many subjects should we have?
n_subj <- 14

# how many trials should we have?
n_trial <- 47

# how many simulations would you like?
n_sim <- 100

sim5 <-
  tibble(seed = 1:n_sim) %>% 
  mutate(data = map(seed, sample_data, n_subj = n_subj, n_trial = n_trial)) %>%
  mutate(hdi = map2(data, seed, fit_then_hdis))
```

Wrangle before summarizing.

```{r, eval = F}
sim5 <-
  sim5 %>% 
  unnest(hdi) %>%
  mutate(pass_rope_theta  = .lower > .52 | .upper < .48,
         pass_width_theta = (.upper - .lower) < .2) %>% 
  mutate(exceed_rope_theta_s  = .lower_s > .52,
         below_rope_theta_s   = .upper_s < .48,
         narrow_width_theta_s = (.upper_s - .lower_s) < .2) %>% 
  group_by(seed) %>% 
  mutate(pass_rope_theta_s  = sum(exceed_rope_theta_s) > 0 & sum(below_rope_theta_s) == 0,
         pass_width_theta_s = mean(narrow_width_theta_s) == 1) %>% 
  ungroup()
```

Summarize the results.

```{r}
sim5 %>% 
  summarise(power_rope_theta  = mean(pass_rope_theta),
            power_width_theta = mean(pass_width_theta))

sim5 %>% 
  summarise(power_rope_theta_s  = mean(pass_rope_theta_s),
            power_width_theta_s = mean(pass_width_theta_s))
```

First compute the HDIs for `power_rope_theta` and `power_width_theta`.

```{r}
hdi_of_icdf(name = qbeta,
            shape1 = 1 + 100,
            shape2 = 1 + n_sim - 100) %>% 
  round(digits = 2)

hdi_of_icdf(name = qbeta,
            shape1 = 1 + 100,
            shape2 = 1 + n_sim - 100) %>% 
  round(digits = 2)
```

Second, we compute the HDIs for `power_rope_theta_s` and `power_width_theta_s`.

```{r}
hdi_of_icdf(name = qbeta,
            shape1 = 1 + 99,
            shape2 = 1 + n_sim - 99) %>% 
  round(digits = 2)

hdi_of_icdf(name = qbeta,
            shape1 = 1 + 21,
            shape2 = 1 + n_sim - 21) %>% 
  round(digits = 2)
```

> The classical definition of power in NHST assumes a specific value for the parameters without any uncertainty. The classical approach can compute power for different specific parameter values, but the approach does not weigh the different values by their credibility. One consequence is that for the classical approach, retrospective power is extremely uncertain, rendering it virtually useless, because the estimated powers at the two ends of the confidence interval are close to the baseline false alarm rate and 100% [Gerard, Smith, & Weerakkody, -@millerWhatProbabilityReplicating2009; @nakagawaCaseRetrospectiveStatistical2004; @okeefeBriefReportPost2007; Steidl, Hayes, & Schauber, -@steidlStatisticalPowerAnalysis1997; Sun, Pan, & Wang, -@sunRethinkingObservedPower2011; L. Thomas, -@thomasRetrospectivePowerAnalysis1997]. (p. 383)

## Sequential testing and the goal of precision

> In classical power analysis, it is assumed that the goal is to reject the null hypothesis. For many researchers, the *sine qua non* of research is to reject the null hypothesis. The practice of NHST is so deeply institutionalized in scientific journals that it is difficult to get research findings published without showing "significant" results, in the sense of $p < 0.05$. As a consequence, many researchers will monitor data as they are being collected and stop collecting data only when  $p < 0.05$ (conditionalizing on the current sample size) or when their patience runs out. This practice seems intuitively not to be problematic because the data collected after testing previous data are not affected by the previously collected data. For example, if I flip a coin repeatedly, the probability of heads on the next flip is not affected by whether or not I happened to check whether $p < 0.05$ on the previous flip. 
>
> Unfortunately, that intuition about independence across flips only tells part of story. What's missing is the realization that the stopping procedure biases which data are sampled, because the procedure stops only when extreme values happen to be randomly sampled...
>
> The remainder of this section shows examples of sequential testing with different decision criteria. We consider decisions by $p$ values, BFs, HDIs with ROPEs, and precision. We will see that decisions by $p$ values not only lead to 100% false alarms (with infinite patience), but also lead to biased estimates that are more extreme than the true value. The two Bayesian methods both can decide to accept the null hypothesis, and therefore do not lead to 100% false alarms, but both do produce biased estimates because they stop when extreme values are sampled. Stopping when precision is achieved produces accurate estimates. (pp. 383--385, *emphasis* in the original)

### Examples of sequential tests.

To start our sequence of simulated coin flips, we'll set the total number of trials we'd like, `n_trial`, specify our `bias`, and set out seed. Like Kruschke did in the text we'll do `bias <- .5`, first.

```{r}
n_trial <- 700
bias    <- .5

set.seed(13)

coin.5 <-
  tibble(n    = 1:n_trial,
         flip = rbinom(n = n_trial, size = 1, prob = bias)) %>% 
  mutate(z = cumsum(flip))

head(coin.5)
```

Here's a little custom function that will fit frequentist logistic regression models for each combination of `n` and `z` and then extract the associated $p$-value.

```{r}
fit_glm <- function(n, z) {
  
  d <- tibble(y = rep(1:0, times = c(z, n - z)))
  
  glm(data = d, y ~ 1, family = binomial(link = "logit")) %>% 
    broom::tidy() %>% 
    select(p.value) %>% 
    pull()
  
}

# here's how it works
fit_glm(n = 5, z = 2)
```

Use `fit_glm()` to compute the $p$-values.

```{r}
coin.5 <-
  coin.5 %>% 
  mutate(p = map2_dbl(n, z, fit_glm))

head(coin.5)
```

For the Bayes factors, Kruschke indicated these were computed based on Equation 12.3 from page 344. That equation followed the form

$$
\frac{p(z, N | M_\text{alt})}{p(z, N | M_\text{null})} = \frac{B (z + a_\text{alt}, N - z + b_\text{alt}) / B (a_\text{alt}, b_\text{alt})}{\theta_\text{null}^z (1 - \theta_\text{null})^{(N - z)}}.
$$

To ground ourselves a bit, here's some of the content from the page that followed the equation:

> For a default alternative prior, the beta distribution is supposed to be uninformed, according to particular mathematical criteria. Intuition might suggest that a uniform distribution suits this requirement, that is, $\operatorname{beta} (\theta | 1, 1)$. Instead, some argue that the most appropriate uninformed beta distribution is $\operatorname{beta}(\theta | \epsilon, \epsilon)$, where $\epsilon$ is a small number approaching zero (p. 344)

The $\operatorname{Beta}(\epsilon, \epsilon)$, recall, is the Haldane prior. Often times, $\epsilon = 0.01$. That will be our approach here, too. Let's make another custom function.

```{r}
log_bf <- function(n, z, theta) { 
  
  # define epsilon for the Haldane prior
  e <- 0.01
  
  # compute p(d | H_0)
  p_d_null <- theta ^ z * (1 - theta) ^ (n - z)
  
  # compute p(d | H_1)
  p_d_alt <- beta(z + e, n - z + e) / beta(e, e) 
  
  # compute BF
  bf <- p_d_alt / p_d_null
  
  # take the log
  log(bf)
  
}
```

Here's how it works.

```{r}
log_bf(n = 6, z = 2, theta = bias)
```

Now we'll use it in bulk.

```{r}
coin.5 <-
  coin.5 %>% 
  mutate(log_bf = map2_dbl(n, z, log_bf, theta = .5))

head(coin.5)
```

To compute the HDIs for each iteration, we'll want to use the `hdi_of_qbeta()` function from Chapters 10 and 12.

```{r}
hdi_of_qbeta <- function(shape1, shape2) {
  
  hdi_of_icdf(name = qbeta,
              shape1 = shape1,
              shape2 = shape2) %>% 
    data.frame() %>% 
    mutate(level = c("ll", "ul")) %>% 
    spread(key = level, value = ".")
  
}
```

Here's how it works.

```{r}
hdi_of_qbeta(3, 3)
```

Put it to use.

```{r}
coin.5 <-
  coin.5 %>% 
  mutate(hdi = map2(n, z, ~hdi_of_qbeta(.y + 1, .x - .y + 1))) %>% 
  unnest(hdi) %>% 
  mutate(width = ul - ll)

head(coin.5)
```

We're finally ready to define the five subplots for our version of Figure 13.4.

```{r, fig.width = 8, fig.height = 1.75}
p1 <-
  coin.5 %>% 
  ggplot(aes(x = n, y = z / n)) +
  geom_hline(yintercept = .5, color = ce[9]) +
  geom_line(color = ce[1]) +
  geom_point(size = 2/3, color = ce[1]) +
  scale_x_continuous(NULL, breaks = 0:7 * 100) +
  scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1))

p2 <-
  coin.5 %>% 
  ggplot(aes(x = n, y = p)) +
  geom_hline(yintercept = .05, color = ce[9]) +
  geom_line(aes(color = p < .05)) +
  geom_point(aes(color = p < .05), 
             size = 2/3) +
  scale_color_manual(values = ce[c(2, 7)], breaks = NULL) +
  scale_x_continuous(NULL, breaks = 0:7 * 100) +
  scale_y_continuous(expression(italic(p)*"-value"), expand = expansion(mult = 0), limits = c(0, 1))
  
p3 <-
  coin.5 %>% 
  ggplot(aes(x = n, y = log_bf)) +
  geom_hline(yintercept = -1.1, color = ce[9]) +
  geom_line(color = ce[7]) +
  geom_point(aes(color = log_bf < -1.1 | log_bf > 1.1),
             alpha = 1/2, size = 2/3) +
  annotate(geom = "text", x = 60, y = -1.5, 
           label = "accept the null", color = ce[1]) +
  scale_color_manual(values = ce[c(2, 7)], breaks = NULL) +
  scale_x_continuous(NULL, breaks = 0:7 * 100) +
  ylab(expression(log(BF)))

p4 <-
  coin.5 %>% 
  ggplot(aes(x = n)) +
  geom_hline(yintercept = c(.45, .55), color = ce[9]) +
  geom_linerange(aes(ymin = ll, ymax = ul,
                     color = ll > .45 & ul < .55),
                 alpha = 1/2) +
  scale_color_manual(values = ce[c(2, 7)], breaks = NULL) +
  scale_x_continuous(NULL, breaks = 0:7 * 100) +
  scale_y_continuous("95% HDI", expand = expansion(mult = 0), limits = c(0, 1))

p5 <-
  coin.5 %>% 
  ggplot(aes(x = n, y = width)) +
  geom_hline(yintercept = .08, color = ce[9]) +
  geom_line(aes(color = width < .08)) +
  geom_point(aes(color = width < .08),
             alpha = 1/2, size = 2/3) +
  scale_color_manual(values = ce[c(2, 7)], breaks = NULL) +
  scale_x_continuous(NULL, breaks = 0:7 * 100) +
  scale_y_continuous("HDI width", expand = expansion(mult = 0), limits = c(0, 1))
```

With syntax from the **patchwork** package, we'll arrange them one atop another.

```{r, fig.width = 8, fig.height = 8, warning = F, message = F}
library(patchwork)

(p1 / p2 / p3 / p4 / p5)
```

Now let's compute data of the same form, but based on $\theta = .65$. This time we'll do all the data wrangling steps in one code block.

```{r}
n_trial <- 700
bias    <- .65

set.seed(13)

coin.65 <-
  # n, flip, and z
  tibble(n    = 1:n_trial,
         flip = rbinom(n = n_trial, size = 1, prob = bias)) %>% 
  mutate(z = cumsum(flip)) %>%
  # p-values
  mutate(p = map2_dbl(n, z, fit_glm)) %>% 
  # log(BF)
  mutate(log_bf = map2_dbl(n, z, log_bf, theta = .5)) %>%
  # HDIs
  mutate(hdi = map2(n, z, ~hdi_of_qbeta(.y + 1, .x - .y + 1))) %>% 
  unnest(hdi) %>% 
  # HDI width
  mutate(width = ul - ll)

head(coin.65)
```

Here is the code for our version of Figure 13.5.

```{r, fig.width = 8, fig.height = 8}
p1 <-
  coin.65 %>% 
  ggplot(aes(x = n, y = z / n)) +
  geom_hline(yintercept = .65, color = ce[9]) +
  geom_line(color = ce[1]) +
  geom_point(size = 2/3) +
  scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1))

p2 <-
  coin.65 %>% 
  ggplot(aes(x = n, y = p)) +
  geom_hline(yintercept = .05, color = ce[9]) +
  geom_line(aes(color = p < .05)) +
  geom_point(aes(color = p < .05),
             alpha = 1/2, size = 2/3) +
  scale_color_manual(values = ce[c(2, 7)], breaks = NULL) +
  scale_y_continuous(expression(italic(p)*"-value"), expand = expansion(mult = 0), limits = c(0, 1))
  

p3 <-
  coin.65 %>% 
  ggplot(aes(x = n, y = log_bf)) +
  geom_hline(yintercept = c(-1.1, 1.1), color = ce[9]) +
  geom_line(color = ce[1]) +
  geom_point(aes(color = log_bf < -1.1 | log_bf > 1.1),
             alpha = 1/2, size = 2/3) +
  annotate(geom = "text", x = 60, y = c(-8, 28), 
           label = c("accept the null", "reject the null"), color = ce[1]) +
  scale_color_manual(values = ce[c(2, 7)], breaks = NULL) +
  scale_y_continuous(expression(log(BF)), limits = c(-10, 30))

p4 <-
  coin.65 %>% 
  ggplot(aes(x = n)) +
  geom_hline(yintercept = c(.45, .55), color = ce[9]) +
  geom_linerange(aes(ymin = ll, ymax = ul,
                     color = ll > .55 | ul < .45),
                 alpha = 1/2) +
  scale_color_manual(values = ce[c(2, 7)], breaks = NULL) +
  scale_y_continuous("95% HDI", expand = expansion(mult = 0), limits = c(0, 1))

p5 <-
  coin.65 %>% 
  ggplot(aes(x = n, y = width)) +
  geom_hline(yintercept = .08, color = ce[9]) +
  geom_line(aes(color = width < .08)) +
  geom_point(aes(color = width < .08),
             alpha = 1/2, size = 2/3) +
  scale_color_manual(values = ce[c(2, 7)], breaks = NULL) +
  scale_y_continuous("HDI width", expand = expansion(mult = 0), limits = c(0, 1))

(p1 / p2 / p3 / p4 / p5) & 
  scale_x_continuous(NULL, breaks = 0:7 * 100)
```

### Average behavior of sequential tests.

**This section is still in the works**. In short, I'm not quite sure how to pull off the simulations. If you've got the chops, please share your code in my [GitHub issue #20](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/20).

## Discussion

### Power and multiple comparisons.

> In NHST, the overall p value for any particular test is increased when the test is considered in the space of all other intended tests...
>
> Bayesian power analysis is not affected by intending multiple tests. In Bayesian analysis, the decision is based on the posterior distribution, which is determined by the data in hand, whether actual or simulated, and not by what other tests are intended. In Bayesian analysis, the probability of achieving a goal, that is the power, is determined only by the data-generating process (which includes the stopping rule) and not by the cloud of counterfactual samples (which includes other tests). (p. 393)

For more thoughts on the multiple comparisons issue, check out [this post](https://statmodeling.stat.columbia.edu/2017/06/25/analyze-comparisons-thats-better-looking-max-difference-trying-multiple-comparisons-correction/) from Gelman's blog.

### Power: prospective, retrospective, and replication.

> There are different types of power analysis, depending on the source of the hypothetical distribution over parameter values and the prior used for analyzing the simulated data. The most typical and useful type is *prospective* power analysis. In prospective power analysis, research is being planned for which there has not yet been any data collected. The hypothetical distribution over parameter values comes from either theory or idealized data or actual data from related research. (p. 393, *emphasis* in the original)

Prospective is probably the type that comes to mind with you think of "power analyses."

> On the other hand, *retrospective* power analysis refers to a situation in which we have already collected data from a research project, and we want to determine the power of the research we conducted. In this case, we can use the posterior distribution, derived from the actual data, as the representative parameter values for generating new simulated data. (This is tantamount to a posterior predictive check.) In other words, at a step in the posterior MCMC chain, the parameter values are used to generate simulated data. The simulated data are then analyzed with the same Bayesian model as the actual data, and the posterior from the simulated data is examined for whether or not the goals are achieved. (p. 393, *emphasis* in the original)

Though researchers sometimes use retrospective power analyses and are sometimes asked to perform them during the peer-review process, they are generally looked down upon within methodological circles. A recent controversy arose on the issue within the surgical literature. To dip your toes into the topic, check out this post by [Reaction Watch](https://retractionwatch.com/2019/06/19/statisticians-clamor-for-retraction-of-paper-by-harvard-researchers-they-say-uses-a-nonsense-statistic/) or [this post](https://lesslikely.com/statistics/observed-power-magic/) by [Zad Chow](https://twitter.com/dailyzad) or these two ([here](https://statmodeling.stat.columbia.edu/2018/09/24/dont-calculate-post-hoc-power-using-observed-estimate-effect-size/), [here](https://statmodeling.stat.columbia.edu/2019/01/13/post-hoc-power-calculation-like-shit-sandwich/)) posts from Gelman's blog.

> Finally, suppose that we have already collected some data, and we want to know the probability that we would achieve our goal if we exactly replicated the experiment. In other words, if we were simply to collect a new batch of data, what is the probability that we would achieve our goal in the replicated study, also taking into account the results of the first set of data? This is the *replication* power. As with retrospective power analysis, we use the actual posterior derived from the first sample of data as the data generator. But for analysis of the simulated data, we again use the actual posterior from first sample of data, because that is the best-informed prior for the follow-up experiment. An easy way to execute this analysis by MCMC is as follows: Use the actual set of data with a skeptical- audience prior to generate representative parameter values and representative simulated data. Then, *concatenate the original data with the novel simulated data* and update the original skeptical-audience prior with the enlarged data set. This technique is tantamount to using the posterior of the original data set as the prior for the novel simulated data. (p. 394, *emphasis* in the original)

### Power analysis requires verisimilitude of simulated data.

> Power analysis is only useful when the simulated data imitate actual data. We generate simulated data from a descriptive model that has uncertainty in its parameter values, but we assume that the model is a reasonably good description of the actual data. If the model is instead a poor description of the actual data, then the simulated data do not imitate actual data, and inferences from the simulated data are not very meaningful. It is advisable, therefore, to check that the simulated data accurately reflect the actual data. (p. 394)

### The importance of planning.

> Conducting a power analysis in advance of collecting data is very important and valuable. Often in real research, a fascinating theory and clever experimental manipulation imply a subtle effect. It can come as a shock to the researcher when power analysis reveals that detecting the subtle effect would take many hundreds of subjects! But the shock of power analysis is far less than the pain of actually running dozens of subjects and finding highly uncertain estimates of the sought-after effect. (p. 395)

For more handy uses of power analyses, keep reading in the text. For more practice with simulation approaches to Bayesian power analyses with **brms**, check out my blog series on the topic. You might start with the first post, [*Bayesian power analysis: Part I. Prepare to reject* $H_0$ *with simulation*](https://solomonkurz.netlify.com/post/bayesian-power-analysis-part-i/). For a critique of the precision approach to Bayesian power analysis, check out the blog post by [Richard Morey](https://twitter.com/richarddmorey), [*Power and precision*](https://medium.com/@richarddmorey/power-and-precision-47f644ddea5e).

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:13.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

# Stan

> Stan is the name of a software package that creates representative samples of parameter values from a posterior distribution for complex hierarchical models, analogous to JAGS...
> 
> According to the Stan reference manual, Stan is named after [Stanislaw Ulam](https://en.wikipedia.org/wiki/Stanislaw_Ulam) (1909–1984), who was a pioneer of Monte Carlo methods. (Stan is not named after the slang term referring to an overenthusiastic or psychotic fanatic, formed by a combination of the words "stalker" and "fan.") The name of the software package has also been unpacked as the acronym, Sampling Through Adaptive Neighborhoods [@gelman2013bayesian, p. 307], but it is usually written as Stan not STAN.
>
> Stan uses a different method than JAGS for generating Monte Carlo steps. The method is called *Hamiltonian Monte Carlo* (HMC). HMC can be more effective than the various samplers in JAGS and BUGS, especially for large complex models. Moreover, Stan operates with compiled C++ and allows greater programming flexibility, which again is especially useful for unusual or complex models. For large data sets or complex models, Stan can provide solutions when JAGS (or BUGS) takes too long or fails. (pp. 399--400, *emphasis* in the original)

To learn more about Stan from the Stan team themselves, check out the main website [https://mc-stan.org/](https://mc-stan.org/). If you like to dive deep, bookmark the [*Stan user's guide*](https://mc-stan.org/docs/2_29/stan-users-guide/index.html) [@standevelopmentteamStanUserGuide2022] and the [*Stan reference manual*](https://mc-stan.org/docs/2_29/reference-manual/) [@standevelopmentteamStanReferenceManual2022].

We won't be using Stan directly in this ebook. I prefer working with it indirectly through the interface of Bürkner's **brms** package instead. If you haven't already, bookmark the **brms** [GitHub repository](https://github.com/paul-buerkner/brms), [CRAN page](https://CRAN.R-project.org/package=brms), and [reference manual](https://CRAN.R-project.org/package=brms/brms.pdf) [@brms2022RM]. You can also view Bürkner's talk from the useR! International R User 2017 Conference, [*brms: Bayesian multilevel models using Stan*](https://www.youtube.com/watch?v=40o0_0XTB6E). Here's how Bürkner described **brms** in its GitHub repo:

> The **brms** package provides an interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan, which is a C++ package for performing full Bayesian inference (see [http://mc-stan.org/](http://mc-stan.org/)). The formula syntax is very similar to that of the package lme4 to provide a familiar and simple interface for performing regression analyses. A wide range of response distributions are supported, allowing users to fit – among others – linear, robust linear, count data, survival, response times, ordinal, zero-inflated, and even self-defined mixture models all in a multilevel context. Further modeling options include non-linear and smooth terms, auto-correlation structures, censored data, missing value imputation, and quite a few more. In addition, all parameters of the response distribution can be predicted in order to perform distributional regression. Multivariate models (i.e., models with multiple response variables) can be fit, as well. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. Model fit can easily be assessed and compared with posterior predictive checks, cross-validation, and Bayes factors. (**emphasis** in the original)

## HMC sampling

"Stan generates random representative samples from a posterior distribution by using a variation of the Metropolis algorithm called HMC" (p. 400).

I'm not going to walk through the the details of HMC sampling, at this time. In addition to Kruschke's explanation, you might check out McElreath's [lecture on HMC from January, 2019](https://www.youtube.com/watch?v=v-j0UmWf3Us) or one of these lectures ([here](https://www.youtube.com/watch?v=jUSZboSq1zg), [here](https://www.youtube.com/watch?v=_fnDz2Bz3h8), or [here](https://www.youtube.com/watch?v=pHsuIaPbNbY)) by Michael Betancourt. I'm also not sufficiently up on the math required to properly make the figures in this section. But we can at least get the ball rolling.

```{r, warning = F, message = F}
library(tidyverse)
library(patchwork)
```

Here's the primary data for the two upper left panels for Figure 14.1.

```{r}
d <- 
  tibble(theta = seq(from = -4, to = 4, by = 0.1)) %>% 
  mutate(density = dnorm(theta, mean = 0, sd = 1)) %>% 
  mutate(`-log(density)` = -log(density))
  
head(d)
```

We need a couple more tibbles for the annotation.

```{r}
position <-
  tibble(theta           = -0.5,
         density         = 0,
         `-log(density)` = 1.5)

text <-
  tibble(theta           = -0.5,
         density         = 0.2,
         `-log(density)` = 2.75,
         label1          = "current position",
         label2          = "random\ninitial momentum")
```

Plot.

```{r, fig.width = 4.5, fig.height = 4.5}
theme_set(
  theme_grey() +
    theme(panel.grid = element_blank())
)

p1 <-
  d %>% 
  ggplot(aes(x = theta, y = density)) +
  geom_line(size = 2, color = "grey67") +
  geom_point(data = position,
             size = 4) +
  geom_text(data = text,
            aes(label = label1)) +
  geom_segment(x = -0.5, xend = -0.5,
               y = 0.16, yend = 0.04,
               arrow = arrow(length = unit(0.2, "cm")), 
               size = 1/4, color = "grey50") +
  ggtitle("Posterior Distrib.") +
  coord_cartesian(xlim = c(-3, 3))

p2 <-
  d %>% 
  ggplot(aes(x = theta, y = `-log(density)`)) +
  geom_line(size = 2, color = "grey67") +
  geom_point(data = position,
             size = 4) +
  geom_text(data = text,
            aes(label = label2)) +
  geom_segment(x = -1.1, xend = 0.1,
               y = 1.5, yend = 1.5,
               arrow = arrow(length = unit(0.275, "cm"), ends = "both")) +
  ggtitle("Negative Log Posterior ('Potential')") +
  coord_cartesian(xlim = c(-3, 3),
                  ylim = c(0, 5))

(p1 / p2) & 
  scale_x_continuous(breaks = -3:3)
```

For the plots in this chapter, we keep things simple and rely on the **ggplot2** defaults with one exception: we omitted those unnecessary white gridlines with the `theme_set()` argument at the top of that block. You can undo that with `theme_set(ggplot2::theme_grey())`.

Because I'm not sure how to make the dots and trajectories depicted in the third row, I also won't be able to make proper histograms for the bottom rows. This will go for Figures 14.2 and 14.3, too. If you know how to reproduce them properly, please share your code in my [GitHub issue #21](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/21). Let's let Kruschke close this section out:

> Mathematical theories that accurately describe the dynamics of mechanical systems have been worked out by physicists. The formulation here, in terms of kinetic and potential energy, is named after [William Rowan Hamilton](https://en.wikipedia.org/wiki/William_Rowan_Hamilton) (1805–1865). HMC was described in the physics literature by @duaneHybridMonteCarlo1987 (who called it "hybrid" Monte Carlo), and HMC was applied to statistical problems by @nealImprovedAcceptanceProcedure1994. A brief mathematical overview of HMC is presented by [@mackay2003information, chap. 30]. A more thorough mathematical review of HMC is provided by [@neal2011mcmc]. Details of how HMC is implemented in Stan can be found in the Stan reference manual and in the book by @gelman2013bayesian. (pp. 405--406)

## Installing Stan

You can learn about installing Stan at [https://mc-stan.org/users/interfaces/](https://mc-stan.org/users/interfaces/). We, of course, have already been working with Stan via **brms**. Bürkner has some nice information on how to install **brms** in the FAQ section of the [**brms** GitHub repository](https://github.com/paul-buerkner/brms).

To install the latest official release from CRAN, execute `install.packages("brms")`. If you'd like to install the current developmental version, you can execute the following.

```{r, eval = F}
if (!requireNamespace("remotes")) {
  install.packages("remotes")
}
remotes::install_github("paul-buerkner/brms")
```

As Kruschke advised, it's a good idea to "be sure that your versions of R and RStudio are up to date" (p. 407) when installing **brms** and/or Stan.

## A Complete example

If you'd like to learn how to fit models in Stan itself, you might consult the updated versions of the *Stan User's Guide* and *Stan Reference Manual*, which you can find at [https://mc-stan.org/users/documentation/](https://mc-stan.org/users/documentation/). You might also check out the Stan [Case Studies](https://mc-stan.org/users/documentation/case-studies.html) and [other tutorials](https://mc-stan.org/users/documentation/tutorials.html) listed by the Stan team.

We will continue using Stan via **brms**.

The model Kruschke walked through in this section followed the form

\begin{align*}
y_i & \sim \operatorname{Bernoulli} (\theta) \\
\theta & \sim \operatorname{Beta} (1, 1),
\end{align*}

where $\theta$ is the probability $y = 1$. Kruschke showed how to simulate the data at the top of page 409. Here's our **tidyverse** version.

```{r}
n <- 50
z <- 10

my_data <- tibble(y = rep(1:0, times = c(z, n - z)))

glimpse(my_data)
```

Time to fire up **brms**.

```{r, warning = F, message = F}
library(brms)
```

In the absence of predictors, you might think of this as an intercept-only model. You can fit the simple intercept-only Bernoulli model with `brms::brm()` like this.

```{r fit14.1}
fit14.1 <-
  brm(data = my_data, 
      family = bernoulli(link = identity),
      y ~ 1,
      prior(beta(1, 1), class = Intercept, lb = 0, ub = 1),
      iter = 1000, warmup = 200, chains = 3, cores = 3,
      seed = 14,
      file = "fits/fit14.01")
```

As Kruschke wrote,

> `iter` is the total number of steps per chain, including `warmup` steps in each chain. Thinning merely marks some steps as not to be used; thinning does not increase the number of steps taken. Thus, the total number of steps that Stan takes is `chains`·`iter`. Of those steps, the ones actually used as representative have a total count of `chains`·(`iter`−`warmup`)/`thin`. Therefore, if you know the desired total steps you want to keep, and you know the warm-up, chains, and thinning, then you can compute that the necessary `iter` equals the desired total multiplied by `thin`/`chains`+`warmup`.
>
> We did not specify the initial values of the chains in the example above, instead letting Stan randomly initialize the chains by default. The chains can be initialized by the user with the argument `init`, analogous to JAGS. (p. 409)

Unlike what Kruschke showed on page 409, we did not use the `thin` argument, above, and will generally avoid thinning in this ebook. You just don't tend to need to thin your chains when using Stan. I do, however, tend to use the `seed` argument. Because computers use [pseudorandom number generators](https://en.wikibooks.org/wiki/R_Programming/Random_Number_Generation) to take random draws, I prefer to make my random draws reproducible by setting my seed. Others have argued against this. You do you.

Kruschke mentioned trace plots and model summaries. Here's our trace plot, which comes with a marginal density plot by **brms** default.

```{r, fig.width = 8, fig.height = 1.5}
plot(fit14.1, widths = c(2, 3))
```

Here's the summary.

```{r}
print(fit14.1)
```

### Reusing the compiled model.

"Because model compilation can take a while in Stan, it is convenient to store the DSO of a successfully compiled model and use it repeatedly for different data sets" (p. 410). This true for our **brms** paradigm, too. To reuse a compiled `brm()` model, we typically use the `update()` function. To demonstrate, we'll first want some new data. Here we'll increase our `z` value to 20.

```{r}
z <- 20

my_data <- tibble(y = rep(1:0, times = c(z, n - z)))

glimpse(my_data)
```

For the first and most important argument, you need to tell `update()` what fit you're reusing. We'll use `fit14.1`. You also need to tell `update()` about your new data with the `newdata` argument. Because the model `formula` and `prior`s are the same as before, we don't need to use those arguments, here.

```{r fit14.2}
fit14.2 <-
  update(fit14.1,
         newdata = my_data,
         iter = 1000, warmup = 200, chains = 3, cores = 3,
         seed = 14,
         file = "fits/fit14.02")
```

Here's the summary using the `fixef()` function.

```{r}
fixef(fit14.2)
```

### General structure of Stan model specification.

"The general structure of model specifications in Stan consist of six blocks" (p. 410). We don't need to worry about this when using **brms**. Just use `brm()` or `update()`. But if you're curious about what the underlying Stan code is for your **brms** models, index the model fit with `$model`.

```{r}
fit14.2$model
```

### Think log probability to think like Stan.

The material in this subsection is outside of the scope of this ebook.

### Sampling the prior in Stan.

"There are several reasons why we might want to examine a sample from the prior distribution of a model" (p. 413). Happily, we can do this with **brms** with the `sample_prior` argument. By default, it is set to `"no"` and does not take prior samples. If you instead set `sample_prior = "yes"` or `sample_prior = TRUE`, samples are drawn solely from the prior.

Here's how to do that with an updated version of the model from `fit14.2`.

```{r fit14.3}
fit14.3 <-
  brm(data = my_data, 
      family = bernoulli(link = identity),
      y ~ 1,
      prior(beta(1, 1), class = Intercept, lb = 0, ub = 1),
      iter = 1000, warmup = 200, chains = 3, cores = 3,
      sample_prior = "yes",
      seed = 14,
      file = "fits/fit14.03")
```

Now we can gather the prior draws with the `prior_draws()` function.

```{r}
prior_draws(fit14.3) %>% 
  head()
```

Here's a look at the prior distribution.

```{r, fig.width = 3.5, fig.height = 2.5}
prior_draws(fit14.3) %>% 
  ggplot(aes(x = Intercept)) +
  geom_histogram(binwidth = 0.1, boundary = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = expression("Beta"*(1*", "*1)),
       x = expression(italic(p)(theta)))
```

### Simplified scripts for frequently used analyses.

This is not our approach when using **brms**. Throughout the chapters of this ebook, we will learn to make skillful use of the `brms::brm()` function to fit all our models. Once in a while we'll take a shortcut and reuse a precompiled fit with `update()`.

## Specify models top-down in Stan

> For humans, descriptive models begin, conceptually, with the data that are to be described. We first know the measurement scale of the data and their structure. Then we conceive of a likelihood function for the data. The likelihood function has meaningful parameters, which we might want to re-express in terms of other data (called covariates, predictors, or regressors). Then we build a meaningful hierarchical prior on the parameters. Finally, at the top level, we specify constants that express our prior knowledge, which might be vague or noncommittal. (p. 414)

If you look at how I typically organize the arguments within `brms::brm()`, you'll see this is generally the case there, too. Take another look at the code for `fit14.1`:

```{r, eval = F}
fit14.1 <-
  brm(data = my_data, 
      family = bernoulli(link = identity),
      y ~ 1,
      prior(beta(1, 1), class = Intercept, lb = 0, ub = 1),
      iter = 1000, warmup = 200, chains = 3, cores = 3,
      seed = 14,
      file = "fits/fit14.01")
```

The first line within `brm()` defined the data. The second line defined the likelihood function and its link function. We haven't talked much about link functions, yet, but that will start in [Chapter 15][Overview of the Generalized Linear Model]. Likelihoods contain parameters and our third line within `brm()` defined the equation we wanted to use to predict/describe our parameter of interest, $\theta$. We defined our sole prior in the fourth line. The remaining arguments contain the unsexy technical specifications, such as how many MCMC chains we'd like to use and into what folder we'd like to save our fit as an external file.

You do not need to arrange `brm()` arguments this way. For other arrangements, take a look at the examples in the [**brms** reference manual](https://CRAN.R-project.org/package=brms/brms.pdf) or in some of Bürkner's vignettes, such as his [*Estimating multivariate models with brms*](https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html) [-@Bürkner2022Multivariate]. However you go about fitting your models with `brm()`, I mainly recommend you find a general style and stick with it. Standardizing your approach will make your code more readable for others and yourself.

## Limitations and extras

> At the time of this writing, one of the main limitations of Stan is that it does not allow discrete (i.e., categorical) parameters. The reason for this limitation is that Stan has HMC as its foundational sampling method, and HMC requires computing the gradient (i.e., derivative) of the posterior distribution with respect to the parameters. Of course, gradients are undefined for discrete parameters. (p. 415)

To my knowledge this is still the case, which means **brms** has this limitation, too. As wildly powerful as it is, **brms** it not as flexible as working directly with Stan. However, [Bürkner and others](https://github.com/paul-buerkner/brms/graphs/contributors) are constantly expanding its capabilities. Probably the best places keep track of the new and evolving features of **brms** are the [issues](https://github.com/paul-buerkner/brms/issues) and [news](https://github.com/paul-buerkner/brms/blob/master/NEWS.md) sections in its GitHub repo, [https://github.com/paul-buerkner/brms](https://github.com/paul-buerkner/brms).

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
# remove our objects
rm(d, position, text, p1, p2, n, z, my_data, fit14.1, fit14.2, fit14.3, fit14.4)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

## Footnote {-}


<!--chapter:end:14.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

```{r, echo = F, eval = F}
# (PART) THE GENERALIZED LINEAR MODEL {-}

# We're finally ready for the workhorse of applied statistics, the generalized linear model (GLM)! "The GLM encompasses multiple linear regression, logistic regression, and Bayesian analogues to classical analysis of variance (ANOVA) and frequency-table analysis, among other cases" [@kruschkeDoingBayesianData2015, p. 417].

```

# Overview of the Generalized Linear Model

Along with Kruschke's text, in this part if the project we're moving away from simple Bernoulli coin flipping examples to more complicated analyses of the type we'd actually see in applied data analysis. As Kruschke explained, we'll be using a

> versatile family of models known as the generalized linear model [GLM; @nelder1972generalized; @nelder1972generalized]. This family of models comprises the traditional "off the shelf" analyses such as $t$ tests, analysis of variance (ANOVA), multiple linear regression, logistic regression, log-linear models, etc. [@kruschkeDoingBayesianData2015, p. 420]

## Types of variables

"To understand the GLM and its many specific cases, we must build up a variety of component concepts regarding relationships between variables and how variables are measured in the first place (p. 420)."

### Predictor and predicted variables.

It's worth repeating the second paragraph of this subsection in its entirety.

> The key mathematical difference between predictor and predicted variables is that the likelihood function expresses the probability of values of the predicted variable as a function of values of the predictor variable. The likelihood function does not describe the probabilities of values of the predictor variable. The value of the predictor variable comes from outside the system being modeled, whereas the value of the predicted variable depends on the value of the predictor variable. (p. 420)

This is one of those fine points that can be easy to miss when you're struggling through the examples in this book or chest-deep in the murky waters of your own real-world data problem. But write this down on a sticky note and put it in your sock drawer or something. There are good reasons to fret about the distributional properties of your predictor variables--rules of thumb about the likelihood aren't among them.

### Scale types: metric, ordinal, nominal, and count.

I don't know that I'm interested in detailing the content of this section. But it's worth while considering what Kruschke wrote in its close.

> **Why we care:** We care about the scale type because the likelihood function must specify a probability distribution on the appropriate scale. If the scale has two nominal values, then a Bernoulli likelihood function may be appropriate. If the scale is metric, then a normal distribution may be appropriate as a probability distribution to describe the data. Whenever we are choosing a model for data, we must answer the question, What kind of scale are we dealing with? (p. 423, **emphasis** in the original)

## Linear combination of predictors

"The core of the GLM is expressing the combined influence of predictors as their weighted sum. The following sections build this idea by scaffolding from the simplest intuitive cases" (p. 423).

### Linear function of a single metric predictor.

"A linear function is the generic, 'vanilla,' off-the-shelf dependency that is used in statistical models" (p. 424). Its basic form is

$$y = \beta_0 + \beta_1 x,$$

where $y$ is the variable being predicted and $x$ is the predictor. $\beta_0$ is the intercept (i.e., the expected value when $x$ is zero) and $\beta_1$ is the expected increase in $y$ after a one-unit increase in $x$.

Before we make our version of Figure 15.1, let's talk about our theme and color palette. In this chapter, we'll base our overall on `ggplot2::theme_linedraw()`. We'll take selections from `option = "E"` from the [**viridis** package](https://github.com/sjmgarnier/viridis) to make our color palette. Based on our **ggplot2** skills from the last few chapters, these moves are pretty mundane. We can go further.

In this chapter, we'll extend our skillset by practicing how to alter the default settings of our **ggplot2** geoms. If you browse through the content of the text, you'll see we'll a lot of the upcomming plots will require lines of various sorts. We'll make the bulk of those lines with `geom_line()`, `geom_hline()`, `geom_vline()`, and `geom_segment()`. as a first step, it'd be handy to see how their default aesthetic settings are defined. Here's how to do that for `geom_line()`.

```{r}
ggplot2:::check_subclass("line", "Geom")$default_aes
```

We can use the `ggplot2::update_geom_defaults()` function to change those default settings. Here's how we might go about increasing the default `size` and `color` parameters.

```{r, warning = F, message = F}
library(tidyverse)
library(viridis)

default_line <- ggplot2:::check_subclass("line", "Geom")$default_aes

update_geom_defaults(
  geom = "line", 
  new = list(
    color = viridis_pal(option = "E")(9)[2],
    size = 1)
)
```

Now confirm our new defaults.

```{r}
ggplot2:::check_subclass("line", "Geom")$default_aes
```

Critically, notice how we saved the original default settings for `geom_line()` as `default_line` **before** we changed them. That way, all we have to do is execute this to change things back to normal.

```{r}
update_geom_defaults(
  geom = "line", 
  new = default_line
)

ggplot2:::check_subclass("line", "Geom")$default_aes
```

Now you see how this works, let's save the default settings for the other three geoms.

```{r}
default_hline <- ggplot2:::check_subclass("vline", "Geom")$default_aes
default_vline <- ggplot2:::check_subclass("hline", "Geom")$default_aes
default_segment <- ggplot2:::check_subclass("segment", "Geom")$default_aes
```

Here we'll update the defaults of all four of our line-oriented geoms and change the settings for our global plot theme.

```{r}
update_geom_defaults(
  geom = "line", 
  new = list(
    color = viridis_pal(option = "E")(9)[2],
    size = 1)
)

update_geom_defaults(
  geom = "hline", 
  new = list(
    color = viridis_pal(option = "E")(9)[8],
    size = 1/4,
    linetype = 2)
)

update_geom_defaults(
  geom = "vline", 
  new = list(
    color = viridis_pal(option = "E")(9)[8],
    size = 1/4,
    linetype = 2)
)

update_geom_defaults(
  geom = "segment", 
  new = list(
    color = viridis_pal(option = "E")(9)[5],
    size = 1/4)
)

theme_set(
  theme_linedraw() +
    theme(panel.grid = element_blank())
)
```

You can learn more about these methods [here](https://ggplot2.tidyverse.org/reference/update_defaults.html) and [here](https://stackoverflow.com/questions/40307499/resetting-update-geom-defaults-in-ggplot2). Let's see what we've done by making the left panel of Figure 15.1.

```{r, fig.width = 3, fig.height = 4, message = F, warning = F}
tibble(x = -3:7) %>%
  mutate(y_1 = -5 + 2 * x,
         y_2 = 10 + 2 * x) %>% 
  
  ggplot(aes(x = x)) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_line(aes(y = y_1)) +
  geom_line(aes(y = y_2)) +
  geom_text(data = tibble(
    x     = 2.25,
    y     = c(-5, 10),
    label = c("y = -5 + 2x", "y = 10 + 2x")
    ),
    aes(y = y, label = label),
    size = 4.5) +
  labs(title = "Different Intercepts",
       y = "y") +
  coord_cartesian(xlim = c(-2, 6),
                  ylim = c(-10, 25))
```

Did you notice how we followed the form of the basic linear function when we defined the values for `y_1` and `y_2`? It's that simple! Here's the right panel.

```{r, fig.width = 3, fig.height = 4, message = F, warning = F}
tibble(x = -3:7) %>%
  mutate(y_1 = 10 + -0.5 * x,
         y_2 = 10 + 2 * x) %>% 
  
  ggplot(aes(x = x)) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_line(aes(y = y_1)) +
  geom_line(aes(y = y_2)) +
  geom_text(data = tibble(
    x     = 4,
    y     = c(11, 13.75),
    label = c("y = 10 + -0.5x", "y = 10 + 2x")
    ),
    aes(y = y, label = label),
    size = 4.5) +
  labs(title = "Different Slopes",
       y = "y") +
  coord_cartesian(xlim = c(-2, 6),
                  ylim = c(-10, 25))
```

> **Summary of why we care.** The likelihood function includes the form of the dependency of $y$ on $x$. When $y$ and $x$ are metric variables, the simplest form of dependency, both mathematically and intuitively, is one that preserves proportionality. The mathematical expression of this relation is a so-called linear function. The usual mathematical expression of a line is the $y$-intercept form, but sometimes a more intuitive expression is the $x$ threshold form. Linear functions form the core of the GLM. (pp. 424--425, **emphasis** in the original)

### Additive combination of metric predictors.

The linear combination of $K$ predictors has the general form

\begin{align*}
y & = \beta_0 + \beta_1 x_1 + \dots + \beta_K x_K \\
  & = \beta_0 + \sum_{k = 1}^K \beta_k x_k.
\end{align*}

In the special case where $K = 0$, you have an intercept-only model,

$$y = \beta_0,$$

in which $\beta_0$ simply models the mean of $y$.

We won't be able to reproduce the wireframe plots of Figure 15.2, exactly. But we can use some `geom_raster()` tricks from back in [Chapter 10][Model Comparison and Hierarchical Modeling] to express the third $y$ dimension as fill gradients.

```{r, fig.height = 5, fig.align = "left"}
crossing(x1 = seq(from = 0, to = 10, by = .5),
         x2 = seq(from = 0, to = 10, by = .5)) %>% 
  mutate(`y[1] ==  0 + 1 * x[1] + 0 * x[2]` =  0 + 1 * x1 + 0 * x2,
         `y[2] ==  0 + 0 * x[1] + 2 * x[2]` =  0 + 0 * x1 + 2 * x2,
         `y[3] ==  0 + 1 * x[1] + 2 * x[2]` =  0 + 1 * x1 + 2 * x2,
         `y[4] == 10 + 1 * x[1] + 2 * x[2]` = 10 + 1 * x1 + 2 * x2) %>%
  pivot_longer(cols = -c(x1, x2),
               names_to = "key",
               values_to = "y") %>% 
  
  ggplot(aes(x = x1, y = x2, fill = y)) +
  geom_raster() +
  scale_fill_viridis_c(option = "E") +
  scale_x_continuous(expression(x[1]), expand = c(0, 0),
                     breaks = seq(from = 0, to = 10, by = 2)) +
  scale_y_continuous(expression(x[2]), expand = c(0, 0),
                     breaks = seq(from = 0, to = 10, by = 2)) +
  coord_equal() +
  facet_wrap(~ key, labeller = label_parsed)
```

Here we've captured some of Kruschke's grid aesthetic by keeping the `by` argument within `seq()` somewhat coarse and omitting the `interpolate = T` argument from `geom_raster()`. If you'd prefer smoother fill transitions for the y-values, set `by = .1` and `interpolate = T`.

### Nonadditive interaction of metric predictors.

As it turns out, "the combined influence of two predictors does not have to be additive" (p. 427). Let's explore what that can look like with our version of Figure 15.3.

```{r, fig.height = 5, fig.align = "left"}
crossing(x1 = seq(from = 0, to = 10, by = .5),
         x2 = seq(from = 0, to = 10, by = .5)) %>% 
  mutate(`y[1] == 0 +  0 * x[1] + 0 * x[2] +  0.2 * x[1] * x[2]` = 0 +  0 * x1 + 0 * x2 +  0.2 * x1 * x2,
         `y[2] == 0 +  1 * x[1] + 1 * x[2] +  0   * x[1] * x[2]` = 0 +  1 * x1 + 1 * x2 +  0   * x1 * x2,
         `y[3] == 0 +  1 * x[1] + 1 * x[2] + -0.3 * x[1] * x[2]` = 0 +  1 * x1 + 1 * x2 + -0.3 * x1 * x2,
         `y[4] == 0 + -1 * x[1] + 1 * x[2] +  0.2 * x[1] * x[2]` = 0 + -1 * x1 + 1 * x2 +  0.2 * x1 * x2) %>%
  pivot_longer(cols = -c(x1, x2),
               names_to = "key",
               values_to = "y") %>% 
  
  ggplot(aes(x = x1, y = x2, fill = y)) +
  geom_raster() +
  scale_fill_viridis_c(option = "E") +
  scale_x_continuous(expression(x[1]), expand = c(0, 0),
                     breaks = seq(from = 0, to = 10, by = 2)) +
  scale_y_continuous(expression(x[2]), expand = c(0, 0),
                     breaks = seq(from = 0, to = 10, by = 2)) +
  coord_equal() +
  facet_wrap(~ key, labeller = label_parsed)
```

Did you notice all those `*` operators in the `mutate()` code? We're no longer just additive. We're square in multiplicative town!

> There is a subtlety in the use of the term "linear" that can sometimes cause confusion in this context. The interactions shown in Figure 15.3 are *not* linear on the *two* predictors $x_1$ and $x_2$. But if the product of the two predictors, $x_1 x_2$, is thought of as a third predictor, then the model *is* linear on the *three* predictors, because the predicted value of $y$ is a weighted additive combination of the three predictors. This reconceptualization can be useful for implementing nonlinear interactions in software for linear models, but we will not be making that semantic leap to a third predictor, and instead we will think of a nonadditive combination of two predictors.
>
> A nonadditive interaction of predictors does not have to be multiplicative. Other types of interaction are possible. (p. 428, *emphasis* in the original)

### Nominal predictors.

#### Linear model for a single nominal predictor.

> Instead of representing the value of the nominal predictor by a single scalar value $x$, we will represent the nominal predictor by a vector $\vec{x} = \langle x_{[1]},...,x_{[J]} \rangle$ where $J$ is the number of categories that the predictor has... 
>
> We will denote the baseline value of the prediction as $\beta_0$. The deflection for the $j$th level of the predictor is denoted $\beta_{[j]}$. Then the predicted value is
>
> \begin{align*}
> y & = \beta_0 + \beta_{[1]} x_{[1]} + \dots + \beta_{[J]} x_{[J]} \\
>   & = \beta_0 + \vec{\beta} \cdot \vec{x}
> \end{align*}
>
> where the notation $\vec{\beta} \cdot \vec{x}$ is sometimes called the 'dot product' of the vectors. (p. 429)

#### Additive combination of nominal predictors.

When you have two additive nominal predictors, the model follows the form

\begin{align*}
y & = \beta_0 + \vec \beta_1 \vec x_1 + \vec \beta_2 \vec x_2 \\
  & = \beta_0 + \sum_{j} \beta_{1[j]} x_{1[j]} + \sum_{k} \beta_{2[k]} x_{2[k]},
\end{align*}

given the constraints

$$\sum_{j} \beta_{1[j]} = 0 \;\;\; \text{and} \;\;\; \sum_{j} \beta_{2[k]} = 0.$$

Both panels in Figure 15.4 are going to require three separate data objects. Here's the code for the top panel.

```{r, fig.width = 4, fig.height = 3}
arrows <-
  tibble(x    = c(0.1, 1.1, 2.1),
         y    = c(1, 1.69, 1.69),
         yend = c(1.69, 1.69 + .07, 1.69 - .07))

text <-
  tibble(x     = c(0.44, 1.46, 2.51),
         y     = c(1.68, 1.753, 1.625),
         label = c("beta[0] == 1.69", "beta['[1]'] == 0.07", "beta['[2]'] == -0.07"))

tibble(x = 1:2,
       y = c(1.69 + .07, 1.69 - .07)) %>% 
  
  # plot!
  ggplot(aes(x = x, y = y)) +
  geom_hline(yintercept = 1.69) +
  geom_col(width = .075, fill = viridis_pal(option = "E")(9)[2]) +
  geom_segment(data = arrows,
               aes(xend = x, yend = yend),
               arrow = arrow(length = unit(0.2, "cm"))) +
  geom_text(data = text,
            aes(label = label), 
            size = 3.5, parse = T) +
  scale_x_continuous(breaks = 1:2, labels = c("<1,0>", "<0,1>")) +
  coord_cartesian(xlim = c(0, 3),
                  ylim = c(1.5, 1.75)) +
  theme(axis.ticks.x = element_blank())
```

Here's the code for the bottom panel.

```{r, fig.width = 7, fig.height = 3}
arrows <-
  tibble(x    = c(0.1, 1.1, 2.1, 3.1, 4.1, 5.1),
         y    = rep(c(50, 101), times = c(1, 5)),
         yend = c(101, 101 + 4, 101 - 3, 101 - 2, 101 + 6, 101 - 5))

text <-
  tibble(x     = c(0.41, 1.36, 2.41, 3.4, 4.35, 5.4),
         y     = c(100.5, 104.5, 98.5, 99.5, 106.5, 96.5),
         label = c("beta[0] == 101", "beta['[1]'] == 4", "beta['[2]'] == -3", "beta['[3]'] == -2", "beta['[4]'] == 6", "beta['[5]'] == -5"))

tibble(x = 1:5,
       y = c(101 + 4, 101 - 3, 101 - 2, 101 + 6, 101 - 5)) %>% 

  # the plot
  ggplot(aes(x = x, y = y)) +
  geom_hline(yintercept = 101) +
  geom_col(width = .075, fill = viridis_pal(option = "E")(9)[2]) +
  geom_segment(data = arrows,
               aes(xend = x, yend = yend),
               arrow = arrow(length = unit(0.2, "cm"))) +
  geom_text(data = text,
            aes(label = label), 
            size = 3.5, parse = T) +
  scale_x_continuous(breaks = 1:5,
                     labels = c("<1,0,0,0,0>", "<0,1,0,0,0>", "<0,0,1,0,0>", "<0,0,0,1,0>", "<0,0,0,0,1>")) +
  coord_cartesian(xlim = c(0, 5.5),
                  ylim = c(90, 106.5)) +
  theme(axis.ticks.x = element_blank())
```

Before we make our versions of the two panels in Figure 15.5, we should note that the values in the prose are at odds with the values implied in the figure. For simplicity, our versions of Figure 15.5 will match up with the values in the original figure, *not* the prose. For a look at what the figure might have looked like had it been based on the values in the prose, check out Kruschke's [Corrigenda](https://sites.google.com/site/doingbayesiandataanalysis/corrigenda).

Here's the code for the left panel of Figure 15.5.

```{r, fig.width = 4, fig.height = 3}
d <-
  tibble(x_1  = rep(c(" <1,0>", "<0,1> "), times = 4),
         x_2  = c(rep(0:2, each = 2), -0.25, 0.25),
         y    = c(8, 10, 3, 5, 4, 6, 8.5, 10.5),
         type = rep(c("number", "text"), times = c(6, 2))) %>% 
  mutate(x_1 = factor(x_1, levels = c(" <1,0>", "<0,1> ")))

d %>% 
  filter(type == "number") %>% 
  
  ggplot(aes(x = x_2, y = y, fill = x_1)) +
  geom_col(position = "dodge") +
  geom_text(data = d %>% filter(type == "text"),
            aes(label = x_1, color = x_1)) +
  scale_fill_viridis_d(NULL, option = "E", begin = .25, end = .75) +
  scale_color_viridis_d(NULL, option = "E", begin = .25, end = .75) +
  scale_x_continuous(breaks = 0:2, labels = c("<1,0,0>", "<0,1,0>", "<0,0,1>")) +
  scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2), expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Additive (no interaction)") +
  theme(axis.ticks.x = element_blank(),
        legend.position = "none")
```

Did you notice our `filter()` trick in the code?

#### Nonadditive interaction of nominal predictors.

> We need new notation to formalize the nonadditive influence of a combination of nominal values. Just as $\vec x_1$ refers to the value of predictor 1, and $\vec x_2$ refers to the value of predictor 2, the notation $\vec x_{1 \times 2}$ will refer to a particular *combination* of values of predictors 1 and 2. If there are $J$ levels of predictor 1 and $K$ levels of predictor 2, then there are $J \times K$ combinations of the two predictors. To indicate a particular combination of levels from predictors 1 and 2, the corresponding component of $\vec x_{1 \times 2}$ is set to 1 while all other components are set to 0. A nonadditive interaction of predictors is formally represented by including a term for the influence of combinations of predictors, beyond the additive influences, as follows: $y = \beta_0 + \vec \beta_1 \cdot \vec x_1 + \vec \beta_2 \cdot \vec x_2 + \vec \beta_{1 \times 2} \cdot \vec x_{1 \times 2}$. (pp. 432--433, *emphasis* in the original)

Now we're in nonadditive interaction land, here's the code for the right panel of Figure 15.5.

```{r, fig.width = 4, fig.height = 3}
d <-
  tibble(x_1  = rep(c(" <1,0>", "<0,1> "), times = 4),
         x_2  = c(rep(0:2, each = 2), -0.25, 0.25),
         y    = c(8, 10, 5, 3, 3, 7, 8.5, 10.5),
         type = rep(c("number", "text"), times = c(6, 2))) %>% 
  mutate(x_1 = factor(x_1, levels = c(" <1,0>", "<0,1> ")))

d %>% 
  filter(type == "number") %>% 
  
  ggplot(aes(x = x_2, y = y, fill = x_1)) +
  geom_col(position = "dodge") +
  geom_text(data = d %>% filter(type == "text"),
            aes(label = x_1, color = x_1)) +
  scale_fill_viridis_d(NULL, option = "E", begin = .25, end = .75) +
  scale_color_viridis_d(NULL, option = "E", begin = .25, end = .75) +
  scale_x_continuous(breaks = 0:2, labels = c("<1,0,0>", "<0,1,0>", "<0,0,1>")) +
  scale_y_continuous(breaks = 0:5 * 2, expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Non-Additive Interaction") +
  theme(axis.ticks.x = element_blank(),
        legend.position = "none")
```

"The main point to understand now is that *the term 'interaction' refers to a nonadditive influence of the predictors on the predicted, regardless of whether the predictors are measured on a nominal scale or a metric scale*" (p. 434, *emphasis* in the original).

## Linking from combined predictors to noisy predicted data

### From predictors to predicted central tendency.

> After the predictor variables are combined, they need to be mapped to the predicted variable. This mathematical mapping is called the *(inverse) link* function, and is denoted by $f()$ in the following equation:
>
> $$y = f(\operatorname{lin}(x))$$
> 
> Until now, we have been assuming that the link function is merely the identity function, $f(\operatorname{lin}(x)) = \operatorname{lin}(x)$. (p. 436, *emphasis* in the original)

Yet as we'll see, many models use links other than the identity function.

#### The logistic function.

The logistic link function follows the form

$$y = \operatorname{logistic}(x) = \frac{1}{ \big (1 + \exp (-x) \big )}.$$

We can write the logistic function for a univariable metric predictor as

$$y = \operatorname{logistic}(x; \beta_0, \beta_1) = \frac{1}{\big (1 + \exp (-[\beta_0 + \beta_1]) \big )}.$$

And if we prefer to parameterize it in terms of gain $\gamma$ and threshold $\theta$, it'd be

$$y = \operatorname{logistic}(x; \gamma, \theta) = \frac{1}{ \big (1 + \exp (-\gamma [x - \theta]) \big )}.$$

We can make the sexy logistic curves of Figure 15.6 with `stat_function()`, into which we'll plug our very own custom `make_logistic()` function. Here's the left panel.

```{r, fig.width = 3.25, fig.height = 3.5}
make_logistic <- function(x, gamma, theta) {
  1 / (1 + exp(-gamma * (x - theta)))
}

# annotation
text <-
  tibble(x     = c(-1, 3),
         y     = c(.9, .1),
         label = str_c("list(gamma == 0.5, theta == ", c(-1, 3), ")"))

# plot
tibble(x = c(-11, 11)) %>% 
  ggplot(aes(x = x)) +
  geom_vline(xintercept = c(-1, 3)) +
  geom_hline(yintercept = .5) +
  stat_function(fun  = make_logistic, 
                args = list(gamma = .5, theta = -1), 
                size = 1, color = viridis_pal(option = "E")(9)[2]) +
  stat_function(fun  = make_logistic, 
                args = list(gamma = .5, theta = 3), 
                size = 1, color = viridis_pal(option = "E")(9)[2]) +
  geom_text(data = text,
            aes(y = y, label = label),
            size = 3.5, parse = T) +
  scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) +
  coord_cartesian(xlim = c(-10, 10)) +
  ggtitle("Different Thresholds") 
```

For kicks, we'll take a different approach for the right panel. Instead of pumping values through `stat_function()` within our plot code, we'll use our `make_logistic()` function within `mutate()` before beginning the plot code.

```{r, fig.width = 3.25, fig.height = 3.5}
# define the annotation values
text <-
  tibble(x     = c(2, -2),
         y     = c(.92, .4),
         label = str_c("list(gamma == ", c(2, 0.2), ", theta == 4)"))

# make the data
crossing(gamma = c(2, .2),
         x     = seq(from = -11, to = 11, by = .2)) %>% 
  mutate(y = make_logistic(x, gamma, theta = 4)) %>% 
  
  # plot!
  ggplot(aes(x = x, y = y)) +
  geom_vline(xintercept = 4) +
  geom_hline(yintercept = .5) +
  geom_line(aes(group = gamma),
            size = 1, color = viridis_pal(option = "E")(9)[2]) +
  geom_text(data = text,
            aes(label = label),
            size = 3.5, parse = T) +
  scale_y_continuous(expand = expansion(mult = 0), limits = c(0, 1)) +
  coord_cartesian(xlim = c(-10, 10)) +
  ggtitle("Different Gains")
```

I don't know that one plotting approach is better than the other. It's good to have options.

To make our two-dimensional version of the wireframe plots of Figure 15.7, we'll first want to define the `logistic()` function.

```{r}
logistic <- function(x) {
  1 / (1 + exp(-x))
}
```

Now we'll just extend the same method we used for Figures 15.2 and 15.3.

```{r, fig.height = 5, fig.align = "left"}
crossing(x1 = seq(from = -6, to = 6, by = .5),
         x2 = seq(from = -6, to = 6, by = .5)) %>% 
  mutate(`y[1] == logistic(1 * ( 0    * x[1] + 1    * x[2] -  0))` = logistic(1 * ( 0    * x1 + 1    * x2 -  0)),
         `y[2] == logistic(1 * ( 0.71 * x[1] + 0.71 * x[2] -  0))` = logistic(1 * ( 0.71 * x1 + 0.71 * x2 -  0)),
         `y[3] == logistic(2 * ( 0    * x[1] + 1    * x[2] - -3))` = logistic(2 * ( 0    * x1 + 1    * x2 - -3)),
         `y[4] == logistic(2 * (-0.71 * x[1] + 0.71 * x[2] -  3))` = logistic(2 * (-0.71 * x1 + 0.71 * x2 -  3))) %>%
  pivot_longer(cols = c(-x1, -x2), values_to = "y") %>% 
  
  ggplot(aes(x = x1, y = x2, fill = y)) +
  geom_raster() +
  scale_fill_viridis_c(option = "E", limits = c(0, 1)) +
  scale_x_continuous(expression(x[1]), expand = c(0, 0),
                     breaks = seq(from = -6, to = 6, by = 2)) +
  scale_y_continuous(expression(x[2]), expand = c(0, 0),
                     breaks = seq(from = -6, to = 6, by = 2),
                     position = "right") +
  coord_equal() +
  theme(legend.position = "left",
        strip.text = element_text(size = 8)) +
  facet_wrap(~ name, labeller = label_parsed)
```

"The threshold determines the *position* of the logistical cliff. In other words, the threshold determines the x values for which $y = 0.5$... The gain determines the *steepness* of the logistical cliff" (pp. 437--439, *emphasis* in the original).

#### The cumulative normal function.

> The cumulative normal is denoted $\Phi (x; \mu, \sigma)$, where x is a real number and where $\mu$ and $\sigma$ are parameter values, called the mean and standard deviation of the normal distribution. The parameter $\mu$ governs the point at which the cumulative normal, $\Phi(x)$, equals 0.5. In other words, $\mu$ plays the same role as the threshold in the logistic. The parameter $\sigma$ governs the steepness of the cumulative normal function at $x = \mu$, but inversely, such that a *smaller* value of $\sigma$ corresponds to a steeper cumulative normal. (p. 440, *emphasis* in the original)

Here we plot the standard normal density in the top panel of Figure 15.8.

```{r, fig.width = 3.25, fig.height = 3}
tibble(x = seq(from = -4, to = 4, by = .05)) %>% 
  mutate(d = dnorm(x, mean = 0, sd = 1)) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(aes(fill = x <= 1), show.legend = F) +
  geom_line(size = 1, color = viridis_pal(option = "E")(9)[2]) +
  scale_fill_manual(values = c("white", alpha(viridis_pal(option = "E")(9)[2], 2/3))) +
  scale_y_continuous(expression(p(x)), expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(-3, 3)) +
  ggtitle("Normal Density") 
```

Here's the analogous cumulative normal function depicted in the bottom panel of Figure 15.8.

```{r, fig.width = 3.25, fig.height = 3}
tibble(x = seq(from = -4, to = 4, by = .05)) %>%
  
  ggplot(aes(x = x)) +
  stat_function(fun = pnorm, 
                args = list(mean = 0, sd = 1), 
                size = 1, color = viridis_pal(option = "E")(9)[2]) +
  geom_segment(aes(x = 1, xend = 1, 
                   y = 0, yend = pnorm(1, 0, 1)),
               arrow = arrow(length = unit(0.2, "cm"))) +
  scale_y_continuous(expression(Phi(x)), expand = expansion(mult = 0), limits = c(0, 1)) +
  coord_cartesian(xlim = c(-3, 3)) +
  ggtitle("Cumulative Normal")
```

> Terminology: The inverse of the cumulative normal is called the *probit* function. ["Probit" stands for "probability unit"; @bliss1934method]. The probit function maps a value $p$, for $0.0 \leq p \leq 1.0$, onto the infinite real line, and a graph of the probit function looks very much like the logit function. (p. 439, *emphasis* in the original)

### From predicted central tendency to noisy data.

> In the real world, there is always variation in $y$ that we cannot predict from $x$. This unpredictable "noise" in $y$ might be deterministically caused by sundry factors we have neither measured nor controlled, or the noise might be caused by inherent non-determinism in $y$. It does not matter either way because in practice the best we can do is predict the *probability* that $y$ will have any particular value, dependent upon $x$...
>
> To make this notion of probabilistic tendency precise, we need to specify a probability distribution for $y$ that depends on $f (\operatorname{lin} (x))$. To keep the notation tractable, first define $\mu = f (\operatorname{lin} (x))$. The value $\mu$ represents the central tendency of the predicted $y$ values, which might or might not be the mean. With this notation, we then denote the probability distribution of $y$ as some to-be-specified probability density function, abbreviated as "pdf":
>
> $y \sim \operatorname{pdf} \big ( \mu, [\text{scale, shape, etc.}] \big )$
>
> As indicated by the bracketed terms after $\mu$, the pdf might have various additional parameters that control the distribution’s scale (i.e., standard deviation), shape, etc. 
>
> The form of the pdf depends on the measurement scale of the predicted variable. (pp. 440--441, *emphasis* in the original)

The top panel of Figure 15.9 is tricky. One way to make those multiple densities tipped on their sides is with `ggridges::geom_ridgeline()` followed by `coord_flip()`. However, explore a different approach that'll come in handy in many of the plots we'll be making in later chapters. The method requires we make a data set with the necessary coordinates for the side-tipped densities. Let's walk through that step slowly.

```{r}
curves <-
  # define the 3 x-values we want the Gaussians to originate from
  tibble(x = seq(from = -7.5, to = 7.5, length.out = 4)) %>%
  
  # use the formula 10 + 2x to compute the expected y-value for x
  mutate(y_mean = 10 + (2 * x)) %>%
  
  # based on a Gaussian with `mean = y_mean` and `sd = 2`, compute the 99.5% intervals
  mutate(ll = qnorm(.0025, mean = y_mean, sd = 2),
         ul = qnorm(.9975, mean = y_mean, sd = 2)) %>%
  
  # now use those interval bounds to make a sequence of y-values
  mutate(y = map2(ll, ul, seq, length.out = 100)) %>%
  
  # since that operation returned a nested column, we need to `unnest()`
  unnest(y) %>%
  
  # compute the density values
  mutate(density = map2_dbl(y, y_mean, dnorm, sd = 2)) %>%
  
  # now rescale the density values to be wider.
  # since we want these to be our x-values, we'll 
  # just redefine the x column with these results
  mutate(x = x - density * 2 / max(density))

str(curves)
```

In case it's not clear, we'll be plotting with the `x` and `y` columns. Think of the other columns as showing our work. But now we've got those `curves` data, we're ready to simulate the points and plot.

```{r, fig.width = 4, fig.height = 3.75, message = F, warning = F}
# how many points would you like?
n_samples <- 750

# generate the points
tibble(x = runif(n = n_samples, -10, 10)) %>% 
  mutate(y = rnorm(n = n_samples, mean = 10 + 2 * x, sd = 2)) %>% 
  
  # plot!
  ggplot(aes(x = x, y = y)) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_point(size = 1/5) +
  geom_abline(intercept = 10, slope = 2,
              size = 1, color = viridis_pal(option = "E")(9)[2]) +
  geom_path(data = curves,
            aes(group = y_mean),
            color = viridis_pal(option = "E")(9)[2], size = 3/4) +
  labs(title = "Normal PDF around Linear Function",
       y = "y") +
  coord_cartesian(ylim = c(-10, 30))
```

We'll revisit this method in Chapter 17. The wireframe plots at the bottom of Figure 15.9 and in Figure 15.10 are outside of our **ggplot2** purview.

## Formal expression of the GLM

We can write the GLM as

\begin{align*}
y   & \sim \operatorname{pdf} \big (\mu, [\text{parameters}] \big ), \text{where} \\
\mu & = f \big (\operatorname{lin}(x), [\text{parameters}] \big ).
\end{align*}

> As has been previously explained, the predictors $x$ are combined in the linear function $\operatorname{lin}(x)$, and the function $f$ in [the first equation] is called the inverse link function. The data, $y$, are distributed around the central tendency $\mu$ according to the probability density function labeled "pdf." (p. 444)

### Cases of the GLM.

> When a client brings an application to a [statistical] consultant, one of the first things the consultant does is find out from the client which data are supposed to be predictors and which data are supposed to be predicted, and the measurement scales of the data... When you are considering how to analyze data, your first task is to be your own consultant and find out which data are predictors, which are predicted, and what measurement scales they are. (pp. 445--446)

Soak this last bit in. It's gold. I've found it to be true in my exchanges with other researchers and with my own data problems, too.

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
update_geom_defaults(
  geom = "line", 
  new = default_line
)

update_geom_defaults(
  geom = "hline", 
  new = default_hline
)

update_geom_defaults(
  geom = "vline", 
  new = default_vline
)

update_geom_defaults(
  geom = "segment", 
  new = default_segment
)
```

```{r, echo = F}
# remove the objects
rm(default_line, default_hline, default_vline, default_segment, arrows, text, d, make_logistic, logistic, curves, n_samples)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:15.Rmd-->


```{r, echo = FALSE, cachse = FALSE}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

# Metric-Predicted Variable on One or Two Groups

> In the context of the generalized linear model (GLM) introduced in the previous chapter, this chapter's situation involves the most trivial cases of the linear core of the GLM, as indicated in the left cells of Table 15.1 (p. 434), with a link function that is the identity along with a normal distribution for describing noise in the data, as indicated in the first row of Table 15.2 (p. 443). We will explore options for the prior distribution on parameters of the normal distribution, and methods for Bayesian estimation of the parameters. We will also consider alternative noise distributions for describing data that have outliers. [@kruschkeDoingBayesianData2015, pp. 449--450]

Although I agree this chapter covers the "most trivial cases of the linear core of the GLM", Kruschke's underselling himself a bit, here. In addition to "trivial" Gaussian models, Kruschke went well beyond and introduced robust Student's $t$ modeling. It's a testament to Kruschke's rigorous approach that he did so so early in the text. IMO, we could use more robust Student's $t$ models in the social sciences. So heed well, friends.

## Estimating the mean and standard deviation of a normal distribution

The Gaussian probability density function follows the form

$$p(y | \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left (-\frac{1}{2} \frac{(y - \mu)^2}{\sigma^2} \right ),$$

where the two parameters to estimate are $\mu$ (i.e., the mean) and $\sigma$ (i.e., the standard deviation). If you prefer to think in terms of $\sigma^2$, that's the variance. In case is wasn't clear, $\pi$ is the actual number $\pi$, not a parameter to be estimated.

We'll divide Figure 16.1 into data and plot steps. I came up with the primary data like so:

```{r, warning = F, message = F}
library(tidyverse)

sequence_length <- 100

d <-
  crossing(y     = seq(from = 50, to = 150, length.out = sequence_length),
           mu    = c(87.8, 100, 112),
           sigma = c(7.35, 12.2, 18.4)) %>% 
  mutate(density = dnorm(y, mean = mu, sd = sigma),
         mu      = factor(mu, labels = str_c("mu==", c(87.8, 100, 112))),
         sigma   = factor(sigma, labels = str_c("sigma==", c(7.35, 12.2, 18.4))))

head(d)
```

Instead of putting the coordinates for the three data points in our `d` tibble, I just threw them into their own tibble in the `geom_point()` function.

Okay, let's talk color and theme. For this chapter, we'll take our color palette from the [**beyonce** package](https://github.com/dill/beyonce) [@R-beyonce]. As one might guess, the **beyonce** package provides an array of palettes based on pictures of [Beyoncé](https://www.beyonce.com/). The origins of the palettes come from [https://beyoncepalettes.tumblr.com/](https://beyoncepalettes.tumblr.com/). Our palette will be #126.

```{r, warning = F, message = F, fig.width = 3, fig.height = 1}
library(beyonce)

beyonce_palette(126)

bp <- beyonce_palette(126)[]

bp
```

Our overall theme will be based on the default `ggplot2::theme_grey()`.

```{r}
theme_set(
  theme_grey() +
    theme(text = element_text(color = "white"),
          axis.text = element_text(color = beyonce_palette(126)[5]),
          axis.ticks = element_line(color = beyonce_palette(126)[5]),
          legend.background = element_blank(),
          legend.box.background = element_rect(fill = beyonce_palette(126)[5],
                                               color = "transparent"),
          legend.key = element_rect(fill = beyonce_palette(126)[5],
                                    color = "transparent"),
          legend.text = element_text(color = beyonce_palette(126)[1]),
          legend.title = element_text(color = beyonce_palette(126)[1]),
          panel.background = element_rect(fill = beyonce_palette(126)[5],
                                          color = beyonce_palette(126)[5]),
          panel.grid = element_blank(),
          plot.background = element_rect(fill = beyonce_palette(126)[1],
                                          color = beyonce_palette(126)[1]),
          strip.background = element_rect(fill = beyonce_palette(126)[4]),
          strip.text = element_text(color = beyonce_palette(126)[1]))
)
```

Here's Figure 16.1.

```{r}
d %>% 
  ggplot(aes(x = y)) +
  geom_area(aes(y = density),
            fill = bp[2]) +
  geom_vline(xintercept = c(85, 100, 115), 
             linetype = 3, color = bp[5]) +
  geom_point(data = tibble(y = c(85, 100, 115)),
             aes(y = 0.002),
             size = 2, color = bp[6]) +
  scale_y_continuous(expression(italic(p)(italic(y)*"|"*mu*", "*sigma)), 
                     expand = expansion(mult = c(0, 0.05)), breaks = NULL) +
  ggtitle("Competing Gaussian likelihoods given the same data") +
  coord_cartesian(xlim = c(60, 140)) +
  facet_grid(sigma ~ mu, labeller = label_parsed)
```

### ~~Solution by mathematical analysis~~ Heads up on precision.

Not much for us, here. But we might reiterate that sometimes we talk about the *precision* (see page 453), which is the reciprocal of the variance (i.e., $\frac{1}{\sigma^2}$). As we'll see, the **brms** package doesn't use priors parameterized in terms of precision. But JAGS does, which means we'll need to be able to translate Kruschke's precision-laden JAGS code into $\sigma$-oriented **brms** code in many of the remaining chapters. Proceed with caution.

### Approximation by ~~MCMC in JAGS~~ HMC in brms.

Let's load and `glimpse()` at the data.

```{r, message = F}
my_data <- read_csv("data.R/TwoGroupIQ.csv")

glimpse(my_data)
```

The data file included values from two groups.

```{r}
my_data %>% 
  distinct(Group)
```

Kruschke clarified that for the following example, "the data are IQ (intelligence quotient) scores from a group of people who have consumed a 'smart drug'" (p. 456). That means we'll want to subset the data.

```{r}
my_data <-
  my_data %>% 
  filter(Group == "Smart Drug")
```

It's a good idea to take a look at the data before modeling.

```{r, fig.width = 4, fig.height = 3}
my_data %>% 
  ggplot(aes(x = Score)) +
  geom_histogram(fill = bp[2], binwidth = 5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  ggtitle("The ticks show individual data points.")
```

Here are the mean and $SD$ of the `Score` data.

```{r}
(mean_y <- mean(my_data$Score))
(sd_y <- sd(my_data$Score))
```

Those values will come in handy in just a bit. But first, let's load **brms**.

```{r, message = F, warning = F}
library(brms)
```

If we want to pass user-defined values into our `brm()` prior code, we'll need to define them first in using `brms::stanvar()`.

```{r}
stanvars <- 
  stanvar(mean_y, name = "mean_y") + 
  stanvar(sd_y,   name = "sd_y")
```

It's been a while since we used the `stanvar()` function, so we should review. Though we've saved that as `stanvars`, you could name it whatever you want. The main trick is to them tell `brms::brm()` about your values in a `stanvars` statement.

Kruschke mentioned that the "the conventional noncommittal gamma prior [for the precision] has shape and rate constants that are close to zero, such as Sh = 0.01 and R = 0.01" (p. 456). Here's what that looks like.

```{r, fig.width = 6, fig.height = 3}
tibble(x = seq(from = 0, to = 12, by = .025)) %>% 
  mutate(d = dgamma(x, shape = 0.01, rate = 0.01)) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = bp[3]) +
  geom_vline(xintercept = 1 / sd_y, linetype = 2, color = bp[5]) +
  labs(subtitle = "The grey density in the background is the conventional gamma prior for precision.\nThe dashed vertical line is our precision value.") +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)), breaks = NULL) +
  coord_cartesian(xlim = c(0, 8),
                  ylim = c(0, 0.35))
```

The thing is, with **brms** we typically estimate $\sigma$ rather than precision. Though gamma is also a feasible prior distribution for $\sigma$, we won't use it here. But we won't be using Kruschke's uniform prior, either. The Stan team [discourages uniform priors for variance parameters](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations), such as our $\sigma$. I'm not going to get into the details of why, but you've got that hyperlink above and the good old [Stan user's guide](https://mc-stan.org/docs/2_29/stan-users-guide/index.html) [@standevelopmentteamStanUserGuide2022] if you'd like to dive deeper.

Here we'll use the half normal. By "half normal," we mean that the mean is zero and it's bounded from zero to positive infinity--no negative $\sigma$ values for us! By the "half normal," we also mean to suggest that smaller values are more credible than those approaching infinity. When working with unstandardized data, an easy default for a weakly-regularizing half normal is to set the $\sigma$ hyperparameter (i.e., *S*) to the standard deviation of the criterion variable (i.e., $s_Y$). Here's that that looks like for this example.

```{r, fig.width = 6, fig.height = 3}
tibble(x   = seq(from = 0, to = 110, by = .1)) %>% 
  mutate(d = dnorm(x, mean = 0, sd = sd_y)) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = bp[3]) +
  geom_vline(xintercept = sd_y, linetype = 2, color = bp[5]) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)), breaks = NULL) +
  labs(subtitle = "The gray density in the background is the half-normal prior for sigma.\nThe dashed vertical line is our 'sd_y' value.") +
  coord_cartesian(xlim = c(0, 100))
```

This prior isn't quite as non-committal as the conventional gamma prior for precision. It discourages the HMC algorithm from exploring $\sigma$ values much larger than two or three times the standard deviation in the data themselves. In practice, I've found it to have a minimal influence on the posterior. If you'd like to make it even less committal, try setting that $\sigma$ hyperparameter to some multiple of $s_Y$ like $2 \times s_Y$ or $10 \times s_Y$. Compare this to Kruschke's recommendations for setting a noncommittal uniform prior for $\sigma$. When using the uniform distribution, $\operatorname{Uniform}(L, H)$,

> we will set the high value $H$ of the uniform prior on $\sigma$ to a huge multiple of the standard deviation in the data, and set the low value $L$ to a tiny fraction of the standard deviation in the data. Again, this means that the prior is vague no matter what the scale of the data happens to be. (p. 455)

On page 456, Kruschke gave an example of such a uniform prior with the code snip `dunif( sdY/1000 , sdY*1000 )`. Here's what that would look like with our data.

```{r, fig.width = 6, fig.height = 3}
tibble(x = 0:(sd_y * 1000)) %>% 
  mutate(d = dunif(x, min = sd_y / 1000, max = sd_y * 1000)) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = bp[3]) +
  geom_vline(xintercept = sd_y, linetype = 2, color = bp[5]) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)), breaks = NULL) +
  labs(subtitle = "The gray density in the background is Kruschke's uniform prior for sigma.\nThe dashed vertical line is our 'sd_y' value.") +
  coord_cartesian()
```

That's really noncommittal. I'll stick with my half normal. You do you. Kruschke has this to say about the prior for the mean:

> In this application we seek broad priors relative to typical data, so that the priors have minimal influence on the posterior. One way to discover the constants is by asking an expert in the domain being studied. But in lieu of that, we will use the data themselves to tell us what the typical scale of the data is. We will set $M$ to the mean of the data, and set $S$ to a huge multiple (e.g., 100) of the standard deviation of the data. This way, no matter what the scale of the data is, the prior will be vague. (p. 455)

In case you're not following along closely in the text, we often use the normal distribution for the intercept and slope parameters in a simple regression model. By $M$ and $S$, Kruschke was referring to the $\mu$ and $\sigma$ parameters of the normal prior for our intercept. Here's what that prior looks like in this data example.

```{r, fig.width = 6, fig.height = 3}
tibble(x = seq(from = -10000, to = 10000, by = 1)) %>% 
  mutate(d = dnorm(x, mean = mean_y, sd = sd_y * 100)) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = bp[3]) +
  geom_vline(xintercept = mean_y, linetype = 2, color = bp[5]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)), breaks = NULL) +
  labs(subtitle = "The grey density in the background is the normal prior for mu.\nThe dashed vertical line is our 'mean_y' value.")
```

Yep, Kruschke's right. That is one noncommittal prior given our data. We could tighten that up by an order of magnitude and still have little influence on the posterior. Now we've decided on our parameterization ($\sigma$, not $\tau$) and our priors (half-normal, not uniform or gamma), we are ready to make our version of the model diagram in Figure 16.2.

```{r, fig.width = 3.9, fig.height = 3.5, message = F}
library(patchwork)

# normal density
p1 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bp[5]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)", "italic(S)[mu]"), 
           size = 7, color = bp[5], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[4]))

# half-normal density
p2 <-
  tibble(x = seq(from = 0, to = 3, by = .01),
         d = (dnorm(x)) / max(dnorm(x))) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = bp[5]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma]", 
           size = 7, color = bp[5], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[4]))

## two annotated arrows
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")

p3 <-
  tibble(x    = c(.43, 1.5),
         y    = c(1, 1),
         xend = c(.43, .8),
         yend = c(.2, .2)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bp[4]) +
  annotate(geom = "text",
           x = c(.3, 1), y = .6,
           label = "'~'",
           size = 10, color = bp[5], family = "Times", parse = T) +
  xlim(0, 2) +
  theme_void()

# a second normal density
p4 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bp[5]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("mu", "sigma"), 
           size = 7, color = bp[5], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[4]))

# the final annotated arrow
p5 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = bp[5], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               color = bp[4], arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p6 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = bp[5], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 1, r = 2),
  area(t = 1, b = 2, l = 3, r = 4),
  area(t = 4, b = 5, l = 1, r = 2),
  area(t = 3, b = 4, l = 1, r = 4),
  area(t = 6, b = 6, l = 1, r = 2),
  area(t = 7, b = 7, l = 1, r = 2)
)

# combine and plot!
(p1 + p2 + p4 + p3 + p5 + p6) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5)) 
```

Two things about the notation in our diagram: Because we have two $\sigma$ hyperparameters, we've denoted the one for the prior on $\mu$ as $S_\mu$ and the one for the prior on $\sigma$ as $S_\sigma$. Also, note that we fixed the $\mu$ hyperparameter for half-normal prior to zero. This won't always be the case, but it's so common within the **brms** ecosystem that I'm going to express it this way throughout most of this ebook. This is our default.

Here's how to put these priors to use with **brms**.

```{r fit16.1}
fit16.1 <-
  brm(data = my_data,
      family = gaussian,
      Score ~ 1,
      prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept),
                prior(normal(0, sd_y), class = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      seed = 16,
      file = "fits/fit16.01")
```

To be more explicit, the `stanvars = stanvars` argument at the bottom of our code is what allowed us to define our intercept prior as `normal(mean_y, sd_y * 100)` instead of requiring us to type in the parameters as `normal(107.8413, 25.4452 * 100)`. The same basic point goes for our $\sigma$ prior.

Also, notice our prior code for $\sigma$, `prior(normal(0, sd_y), class = sigma)`. Nowhere in there did we actually say we wanted a half normal as opposed to a typical normal. That's because the **brms** default is to set the lower bound for priors of `class = sigma` to zero. There's no need for us to fiddle with it.

Let's examine the chains.

```{r, fig.width = 8, fig.height = 2.5}
plot(fit16.1, widths = c(2, 3))
```

They look good! The model summary looks sensible, too.

```{r}
print(fit16.1)
```

Compare those values with `mean_y` and `sd_y`.

```{r}
mean_y
sd_y
```

Good times. Let's extract the posterior draws and save them in a data frame `draws`.

```{r}
draws <- as_draws_df(fit16.1)
```

Here's the leg work required to make our version of the three histograms in Figure 16.3.

```{r, fig.width = 8, fig.height = 2, message = F, warning = F}
# we'll need this for `stat_pointinterval()`
library(tidybayes)

# we'll use this to mark off the ROPEs as white strips in the background
rope <-
  tibble(name = c("Mean", "Standard Deviation", "Effect Size"), 
         xmin = c(99, 14, -0.1),
         xmax = c(101, 16, 0.1))

# annotate the ROPE
text <-
  tibble(x     = 0,
         y     = 0.98,
         label = "ROPE",
         name  = "Effect Size")

# here are the primary data
draws %>% 
  transmute(Mean                 = b_Intercept, 
            `Standard Deviation` = sigma) %>% 
  mutate(`Effect Size` = (Mean - 100) / `Standard Deviation`) %>% 
  pivot_longer(everything()) %>% 
  
  # the plot
  ggplot() +
  geom_rect(data = rope,
            aes(xmin = xmin, xmax = xmax,
                ymin = -Inf, ymax = Inf),
            color = "transparent", fill = "white") +
  stat_histinterval(aes(x = value, y = 0), 
                    point_interval = mode_hdi, .width = .95,
                    fill = bp[2], color = bp[6],
                    normalize = "panels") +
  geom_text(data = text,
            aes(x = x, y = y, label = label),
            size = 2.5, color = bp[4]) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

If we wanted those exact 95% HDIs, we'd execute this.

```{r, warning = F}
draws %>% 
  transmute(Mean                 = b_Intercept, 
            `Standard Deviation` = sigma) %>% 
  mutate(`Effect Size` = (Mean - 100) / `Standard Deviation`) %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  mode_hdi(value)
```

For the next part, we should look at the structure of the posterior draws, `draws`.

```{r}
head(draws)
```

By default, `head()` returned six rows, each one corresponding to the credible parameter values from a given posterior draw. Following our model equation $\text{Score}_i \sim N(\mu, \sigma)$, we might reformat the first two columns as:

1. `Score` ~ $N$(`r round(data.frame(draws)[1, 1], 3)`, `r round(data.frame(draws)[1, 2], 3)`)
2. `Score` ~ $N$(`r round(data.frame(draws)[2, 1], 3)`, `r round(data.frame(draws)[2, 2], 3)`)
3. `Score` ~ $N$(`r round(data.frame(draws)[3, 1], 3)`, `r round(data.frame(draws)[3, 2], 3)`)
4. `Score` ~ $N$(`r round(data.frame(draws)[4, 1], 3)`, `r round(data.frame(draws)[4, 2], 3)`)
5. `Score` ~ $N$(`r round(data.frame(draws)[5, 1], 3)`, `r round(data.frame(draws)[5, 2], 3)`)
6. `Score` ~ $N$(`r round(data.frame(draws)[6, 1], 3)`, `r round(data.frame(draws)[6, 2], 3)`)

Each row of `draws` yields a full model equation with which we might credibly describe the data--or at least as credibly as we can within the limits of the model. We can give voice to a subset of these credible distributions with our version of the upper right panel of Figure 16.3.

Before I show that plotting code, it might make sense to slow down on the preparatory data wrangling steps. There are several ways to overlay multiple posterior predictive density lines like those in our upcoming plots. We'll practice a few over the next few chapters. For the method we'll use in this chapter, it's handy to first determine how many you'd like. Here we'll follow Kruschke and choose 63, which we'll save as `n_lines`.

```{r}
# how many credible density lines would you like?
n_lines <- 63
```

Now we've got our `n_lines` value, we'll use it to subset the rows in `draws` with the `slice()` function. We'll then use `expand()` to include a sequence of `Score` values to correspond to the formula implied in each of the remaining rows of `post`. Notice how we also kept the `.draw` index in the game. That will help us with the plot in a bit. But the main event is how we used `Score`, `b_Intercept`, and `sigma` as the input for the arguments in the `dnorm()`. The output is a column of the corresponding density values.

```{r}
draws <-
  draws %>% 
  slice(1:n_lines) %>% 
  expand(nesting(.draw, b_Intercept, sigma),
         Score = seq(from = 40, to = 250, by = 1)) %>% 
  mutate(density = dnorm(x = Score, mean = b_Intercept, sd = sigma))

str(draws)
```

Note that after using `expand()`, we have a rather long data frame. Anyway, we're ready to plot.

```{r, fig.width = 3, fig.height = 2.25, warning = F, message = F}
draws %>% 
  ggplot(aes(x = Score)) + 
  geom_histogram(data = my_data, 
                 aes(y = stat(density)),
                 fill = bp[2],
                 size = .2, binwidth = 5, boundary = 0) +
  geom_line(aes(y = density, group = .draw),
            size = 1/4, alpha = 1/3, color = bp[6]) +
  scale_x_continuous("y", limits = c(50, 210)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Data with Post. Pred.")
```

Note the `stat(density)` argument in the `geom_histogram()` function. That's what rescaled the histogram to the density metric. If you leave that part out, all the density lines will drop to the bottom of the plot. Also, did you see how we used `.draw` to group the density lines within the `geom_line()` function? That's why we kept that information. Without that `group = .draw` argument, the resulting lines are a mess.

Kruschke pointed out this last plot

> constitutes a form of posterior-predictive check, by which we check whether the model appears to be a reasonable description of the data. With such a small amount of data, it is difficult to visually assess whether normality is badly violated, but there appears to be a hint that the normal model is straining to accommodate some outliers: The peak of the data protrudes prominently above the normal curves, and there are gaps under the shoulders of the normal curves. (p. 458)

We can perform a similar posterior-predictive check with the `brms::pp_check()` function. By default, it will return 10 simulated density lines. Like we did above, we'll increase that by setting the `nsamples` argument to our `n_lines` value.

```{r, fig.width = 4, fig.height = 2.25, warning = F, message = F}
library(bayesplot)

color_scheme_set(scheme = bp[c(3, 1, 2, 5, 4, 6)])

pp_check(fit16.1, nsamples = n_lines)
```

In principle, we didn't need to load the **bayesplot** package to use the `brms::pp_check()` function. But doing so gave us access to the `bayesplot::color_scheme_set()`, which allowed us to apply the colors from our color palette to the plot.

Before we move on, we should talk a little about effect sizes, which we all but glossed over in our code.

> *Effect size* is simply the amount of change induced by the treatment relative to the standard deviation: $(\mu - 100) / \sigma$. In other words, the effect size is the "standardized" change... A conventionally "small" effect size in psychological research is 0.2 [@cohenStatisticalPowerAnalysis1988], and the ROPE limits are set at half that size for purposed of illustration. (p. 457, *emphasis* in the original).

Another way to describe this kind of effect size is as a *standardized mean difference*. In addition to the seminal work by Cohen, you might brush up on effect sizes with Kelley and Preacher's [-@kelley2012effect] [*On effect size*](https://www3.nd.edu/~kkelley/publications/articles/Kelley_and_Preacher_Psychological_Methods_2012.pdf).

## Outliers and robust estimation: The $t$ distribution

Here's the code for our version of Figure 16.4.

```{r, fig.width = 5.5, fig.height = 3.5}
# wrangle
crossing(nu = c(Inf, 4, 2, 1),
         y  = seq(from = -8, to = 8, length.out = 500)) %>% 
  mutate(density = dt(x = y, df = nu)) %>% 
  # this line is unnecessary, but will help with the plot legend
  mutate(nu = factor(nu, levels = c("Inf", "4", "2", "1"))) %>% 
  
  # plot
  ggplot(aes(x = y, y = density, group = nu, color = nu)) +
  geom_line() +
  scale_color_manual(expression(paste(italic(t)[nu])), values = bp[c(6, 3:1)]) +
  scale_y_continuous(expression(p(y)), expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(-6, 6)) +
  theme(legend.position = c(.92, .75))
```

> Although the $t$ distribution is usually conceived as a sampling distribution for the NHST $t$ test, we will use it instead as a convenient descriptive model of data with outliers... Outliers are simply data values that fall unusually far from a model's expected value. Real data often contain outliers relative to a normal distribution. Sometimes the anomalous values can be attributed to extraneous influences that can be explicitly identified, in which case the affected data values can be corrected or removed. But usually we have no way of knowing whether a suspected outlying value was caused by an extraneous influence, or is a genuine representation of the target being measured. Instead of deleting suspected outliers from the data according to some arbitrary criterion, we retain all the data but use a noise distribution that is less affected by outliers than is the normal distribution. (p. 459)

Here's Figure 16.5.a.

```{r, fig.width = 6, fig.height = 3}
tibble(y = seq(from = -10, to = 20, length.out = 1e3)) %>% 
  ggplot(aes(x = y)) +
  geom_area(aes(y = dnorm(y, mean = 2.5, sd = 5.73)),
            fill = bp[2], alpha = 1/2) +
  geom_area(aes(y = metRology::dt.scaled(y, df = 1.14, mean = .12, sd = 1.47)),
            fill = bp[2], alpha = 1/2) +
  geom_vline(xintercept = c(.12, 2.5), color = bp[5], linetype = 3) +
  annotate(geom = "point",  
           x = c(-2:2, 15), y = 0.002,
           size = 2, color = bp[6]) +
  scale_y_continuous(expression(p(y)), expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Maximum Likelihood Estimates") +
  coord_cartesian(xlim = c(-5, 15))
```

I'm not aware that we have the data for the bottom panel of Figure 16.5. However, we can simulate similar data with the `rt.scaled()` function from the [**metRology** package](https://sourceforge.net/projects/metrology/) [@R-metRology].

```{r, fig.width = 6, fig.height = 3}
set.seed(145)

# simulate the data
d <- tibble(y = metRology::rt.scaled(n = 177, df = 2.63, mean = 1.11, sd = 0.15))

# plot
tibble(y = seq(from = -3, to = 12, length.out = 1e3)) %>% 
  ggplot(aes(y)) +
  geom_histogram(data = d,
                 aes(y = stat(density)),
                 fill = bp[3],
                 size = .2, binwidth = .1) +
  geom_line(aes(y = dnorm(y, mean = 1.16, sd = 0.63)),
            color = bp[2]) +
  geom_line(aes(y = metRology::dt.scaled(y, df = 2.63, mean = 1.11, sd = 0.15)),
            color = bp[2]) +
  scale_x_continuous(breaks = seq(from = -2, to = 10, by = 2)) +
  scale_y_continuous(expression(p(y)), expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Maximum Likelihood Estimates") +
  coord_cartesian(xlim = c(-1.5, 10.25))
```

In case you were curious, this is how I selected the seed for the plot. Run the code yourself to get a sense of how it works.

```{r, fig.width = 2, fig.height = 8, eval = F}
# in the R Notebook code block settings, I used: fig.width = 2, fig.height = 8

t_maker <- function(seed) {
  set.seed(seed)
  tibble(y = metRology::rt.scaled(n = 177, df = 2.63, mean = 1.11, sd = 0.15)) %>% 
    summarise(min = min(y),
              max = max(y)) %>% 
    mutate(spread = max - min)
}

tibble(seed = 1:200) %>% 
  mutate(t = map(seed, t_maker)) %>% 
  unnest(t) %>%
  
  ggplot(aes(x = reorder(seed, spread), ymin = min, ymax = max)) +
  geom_hline(yintercept = 0, color = "white") +
  geom_linerange() +
  coord_flip()
```

> It is important to understand that the scale parameter $\sigma$ in the $t$ distribution is not the standard deviation of the distribution. (Recall that the standard deviation is the square root of the variance, which is the expected value of the squared deviation from the mean, as defined back in Equation 4.8, p. 86.) The standard deviation is actually larger than $\sigma$ because of the heavy tails... While this value of the scale parameter is not the standard deviation of the distribution, it does have an intuitive relation to the spread of the data. Just as the range $\pm \sigma$ covers the middle 68% of a *normal* distribution, the range $\pm \sigma$ covers the middle 58% of a $t$ distribution when $\nu = 2$, and the middle 50% when $\nu = 1$. These areas are illustrated in the left column of Figure 16.6. The right column of Figure 16.6 shows the width under the middle of a $t$ distribution that is needed to span 68.27% of the distribution, which is the area under a normal distribution for $\sigma = \pm 1$. (pp. 459--461, *emphasis* in the original)

Speaking of which, here's the code for the left column for Figure 16.6.

```{r}
# the primary data
d <-
  crossing(y  = seq(from = -8, to = 8, length.out = 1e3),
           nu = c(Inf, 5, 2, 1)) %>%
  mutate(label = str_c("nu == ", nu) %>% 
           factor(., levels = c("nu == Inf", "nu == 5", "nu == 2", "nu == 1")))

# the subplot
p1 <-
  d %>% 
  ggplot(aes(x = y)) +
  geom_area(aes(y = dt(y, df = nu)),
            fill = bp[2]) +
  geom_area(data = . %>% filter(y >= -1 & y <= 1),
            aes(y = dt(y, df = nu)),
            fill = bp[1]) +
  # note how this function has its own data
  geom_text(data = tibble(
    y     = 0,
    text  = c("68%", "64%", "58%", "50%"),
    label = factor(c("nu == Inf", "nu == 5", "nu == 2", "nu == 1"))),
    aes(y = .175, label = text),
    color = "white") +
  scale_y_continuous(expression(p(y)), expand = expansion(mult = c(0, 0.05)), breaks = c(0, .2, .4)) +
  labs(subtitle = "Shaded from y = - 1 to y = 1") +
  coord_cartesian(xlim = c(-6, 6)) +
  facet_wrap(~ label, ncol = 1, labeller = label_parsed)
```

Here's the code for the right column.

```{r}
# the primary data
d <-
  tibble(nu   = c(Inf, 5, 2, 1),
         ymin = c(-1, -1.11, -1.32, -1.84)) %>%
  mutate(ymax = -ymin) %>% 
  expand(nesting(nu, ymin, ymax),
         y = seq(from = -8, to = 8, length.out = 1e3)) %>%
  mutate(label = factor(str_c("nu==", nu), 
                        levels = str_c("nu==", c(Inf, 5, 2, 1))))

# the subplot
p2 <-
  d %>% 
  ggplot(aes(x = y)) +
  geom_area(aes(y = dt(y, df = nu)),
            fill = bp[2]) +
  geom_area(data = . %>% 
              # notice our `filter()` argument has changed
              filter(y >= ymin & y <= ymax),
            aes(y = dt(y, df = nu)),
            fill = bp[1]) +
  annotate(geom = "text", 
           x = 0, y = .175, 
           label = "68%", color = "white") +
  scale_y_continuous(expression(p(y)), expand = expansion(mult = c(0, 0.05)), breaks = c(0, .2, .4)) +
  labs(subtitle = "Shaded for the middle 68.27%") +
  coord_cartesian(xlim = c(-6, 6)) +
  facet_wrap(~ label, ncol = 1, labeller = label_parsed)
```

You may have noticed that we just pulled the values in the `ymin` column directly from Kruschke's version of the figure on page 461. If you'd like a better understanding of where those values came from, you can reproduce them with the `qt()` function.

```{r}
qt(p = (1 - .6827) / 2, 
   df = c(Inf, 5, 2, 1)) %>% 
  round(digits = 2)
```

Anyway, let's bind the two ggplots together with the **patchwork** package to make the full version of Figure 16.6.

```{r, fig.width = 7, fig.height = 6, warning = F, message = F}
p1 + p2
```

> The use of a heavy-tailed distribution is often called *robust estimation* because the estimated value of the central tendency is stable, that is, "robust," against outliers. The $t$ distribution is useful as a likelihood function for modeling outliers at the level of observed data. But the $t$ distribution is also useful for modeling outliers at higher levels in a hierarchical prior. We will encounter several applications. (p. 462, *emphasis* in the original)

### Using the $t$ distribution in ~~JAGS~~ brms.

It's easy to use Student's $t$ in **brms**. Make sure to specify `family = student`. By default, **brms** already sets the lower bound for $\nu$ to 1. But we do still need to use 1/29. To get a sense, let's simulate exponential data using the `rexp()` function. Like Kruschke covered in the text (p. 462), the `rexp()` function has one parameter, `rate`, which is the reciprocal of the mean. Here we'll set the mean to 29.

```{r, message = F}
n_draws <- 1e7
mu      <- 29

set.seed(16)

tibble(y = rexp(n = n_draws, rate = 1 / mu)) %>% 
  mutate(y_at_least_1 = ifelse(y < 1, NA, y)) %>%
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  summarise(mean = mean(value, na.rm = T))
```

The simulation showed that when we define the exponential rate as 1/29 and use the typical boundary at 0, the mean of our samples converges to 29. But when we only consider the samples of 1 or greater, the mean converges to 30. Thus, our exponential(1/29) prior with a boundary at 1 is how we get a shifted exponential distribution when we use it as our prior for $\nu$ in **brms**. Just make sure to remember that if you want the mean to be 30, you'll need to specify the rate of 1/29.

Also, Stan will bark if you simply try to set that exponential prior with the code `prior(exponential(1/29), class = nu)`:

> DIAGNOSTIC(S) FROM PARSER:
Info: integer division implicitly rounds to integer. Found int division: 1 / 29
 Positive values rounded down, negative values rounded up or down in platform-dependent way.

To avoid this, just do the division beforehand and save the results with `stanvar()`.

```{r}
stanvars <- 
  stanvar(mean_y, name = "mean_y") + 
  stanvar(sd_y,   name = "sd_y") + 
  stanvar(1/29,   name = "one_over_twentynine")
```

Here's the `brm()` code. Note that we set the prior for our new $\nu$ parameter by specifying `class = nu` within the last `prior()` line.

```{r fit16.2}
fit16.2 <-
  brm(data = my_data,
      family = student,
      Score ~ 1,
      prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept),
                prior(normal(0, sd_y), class = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvars,
      seed = 16,
      file = "fits/fit16.02")
```

We can make the shifted exponential distribution (i.e., Figure 16.7) with simple addition.

```{r, fig.width = 5, fig.height = 4, message = F, warning = F}
# how many draws would you like?
n_draws <- 1e6

# here are the data
d <-
  tibble(exp = rexp(n_draws, rate = 1/29)) %>% 
  transmute(exp_plus_1        = exp + 1,
            log_10_exp_plus_1 = log10(exp + 1))
  
# this is the plot in the top panel
p1 <-
  d %>% 
  ggplot(aes(x = exp_plus_1)) +
  geom_histogram(fill = bp[2],
                 size = .2, binwidth = 5, boundary = 1) +
  stat_pointinterval(aes(y = 0), 
                     point_interval = mode_hdi, .width = .5, color = bp[6]) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = expression(exponential(lambda==29)~shifted~+1),
       x = expression(nu)) +
  coord_cartesian(xlim = c(0, 150))

# the bottom panel plot
p2 <-
  d %>% 
  ggplot(aes(x = log_10_exp_plus_1)) +
  geom_histogram(fill = bp[2],
                 size = .2, binwidth = .1, boundary = 0) +
  stat_pointinterval(aes(y = 0), 
                     point_interval = mode_hdi, .width = .5, color = bp[6]) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(log10(nu))) +
  coord_cartesian(xlim = c(0, 2.5))

# bind them together
(p1 / p2) & scale_x_continuous(expand = expansion(mult = c(0, 0.05)))
```

Here are the scatter plots of Figure 16.8.

```{r, fig.width = 4.5, fig.height = 4}
pairs(fit16.2,
      off_diag_args = list(size = 1/3, alpha = 1/3))
```

I'm not aware of an easy way to use `log10(nu)` instead of `nu` with `brms::pairs()`. However, you can get those plots with the `as_draws_df()` function and a little wrangling.

```{r, fig.width = 2.5, fig.height = 4, warning = F}
draws <- as_draws_df(fit16.2)

draws %>% 
  mutate(`log10(nu)` = log10(nu)) %>% 
  select(b_Intercept, sigma, `log10(nu)`) %>% 
  pivot_longer(-`log10(nu)`) %>% 

  ggplot(aes(x = `log10(nu)`, y = value)) +
  geom_point(color = bp[6], size = 1/3, alpha = 1/3) +
  labs(x = expression(log10(nu)),
       y = NULL) +
  facet_grid(name ~ ., scales = "free", switch = "y")
```

If you want the Pearson's correlation coefficients, you can use base **R** `cor()`.

```{r, warning = F}
draws %>% 
  mutate(`log10(nu)` = log10(nu)) %>% 
  select(b_Intercept, sigma, `log10(nu)`) %>% 
  cor() %>% 
  round(digits = 3)
```

The correlations among our parameters are a similar magnitude as those Kruschke presented in the text. Here are four of the panels for Figure 16.9.

```{r, fig.width = 6, fig.height = 4, warning = F}
# we'll use this to mark off the ROPEs as white strips in the background
rope <-
  tibble(name = c("Mean", "Scale", "Effect Size"), 
         xmin = c(99, 14, -.1),
         xmax = c(101, 16, .1))

# here are the primary data
draws %>% 
  transmute(Mean      = b_Intercept, 
            Scale     = sigma,
            Normality = log10(nu)) %>% 
  mutate(`Effect Size` = (Mean - 100) / Scale) %>% 
  pivot_longer(everything()) %>% 
  
  # the plot
  ggplot() +
  geom_rect(data = rope,
            aes(xmin = xmin, xmax = xmax,
                ymin = -Inf, ymax = Inf),
            color = "transparent", fill = "white") +
  stat_histinterval(aes(x = value, y = 0), 
                    point_interval = mode_hdi, .width = .95,
                    fill = bp[2], color = bp[6],
                    normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

For the final panel of Figure 16.9, we'll make our $t$ lines in much the same way we did, earlier. But last time, we just took the first $\mu$ and $\sigma$ values from the first 63 rows of the `post` tibble. This time we'll use `dplyr::slice_sample()` to take *random draws* from the `post` rows instead. We tell `slice_sample()` how many draws we'd like with the `n` argument.

In addition to the change in our row selection strategy, this time we'll slightly amend the code within the last `mutate()` line. Since we'd like to work with the $t$ distribution, we specified `metRology::dt.scaled()` function instead of `dnorm()`.

```{r, fig.width = 3, fig.height = 2.25}
# how many credible density lines would you like?
n_lines <- 63

# setting the seed makes the results from `slice_sample()` reproducible
set.seed(16)

# wrangle
draws %>% 
  tibble() %>% 
  slice_sample(n = n_lines) %>% 
  expand(nesting(.draw, b_Intercept, sigma, nu),
         Score = seq(from = 40, to = 250, by = 1)) %>% 
  mutate(density = metRology::dt.scaled(x = Score, df = nu, mean = b_Intercept, sd = sigma)) %>% 
  
  # plot
  ggplot(aes(x = Score)) + 
  geom_histogram(data = my_data, 
                 aes(y = stat(density)),
                 fill = bp[2],
                 size = .2, binwidth = 5, boundary = 0) +
  geom_line(aes(y = density, group = .draw),
            size  = 1/3, alpha = 1/3, color = bp[6]) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Data with Post. Pred.",
       x = "y") +
  coord_cartesian(xlim = c(50, 210))
```

Much like Kruschke mused in the text, this plot

> shows that the posterior predictive $t$ distributions appear to describe the data better than the normal distribution in Figure 16.3, insofar as the data histogram does not poke out at the mode and the gaps under the shoulders are smaller. (p. 464)

In case you were wondering, here's the model `summary()`.

```{r}
summary(fit16.2)
```

It's easy to miss how

> $\sigma$ in the robust estimate is much smaller than in the normal estimate. What we had interpreted as increased standard deviation induced by the smart drug might be better described as increased outliers. Both of these differences, that is, $\mu$ more tightly estimated and $\sigma$ smaller in magnitude, are a result of there being outliers in the data. The only way a normal distribution can accommodate the outliers is to use a large value for $\sigma$. In turn, that leads to "slop" in the estimate of $\mu$ because there is a wider range of $\mu$ values that reasonably fit the data when the standard deviation is large. (p. 464)

We can use the `brms::VarCorr()` function to pull the summary statistics for $\sigma$ from both models.

```{r}
VarCorr(fit16.1)$residual__$sd
VarCorr(fit16.2)$residual__$sd
```

It is indeed the case that estimate for $\sigma$ is smaller in the $t$ model. That smaller $\sigma$ resulted in a more precise estimate for $\mu$, as can be seen in the 'Est.Error' columns from the `fixef()` output.

```{r}
fixef(fit16.1)
fixef(fit16.2)
```

Here that is in a coefficient plot using `tidybayes::stat_interval()`.

```{r, fig.width = 6, fig.height = 1.25, warning = F}
bind_rows(as_draws_df(fit16.1) %>% select(b_Intercept),
          as_draws_df(fit16.2) %>% select(b_Intercept)) %>% 
  mutate(fit = rep(c("fit16.1", "fit16.2"), each = n() / 2)) %>% 
  
  ggplot(aes(x = b_Intercept, y = fit)) +
  stat_interval(point_interval = mode_hdi, .width = c(.5, .8, .95)) +
  scale_color_manual("HDI", values = c(bp[c(4, 3, 1)]),
                     labels = c("95%", "80%", "50%")) +
  ylab(NULL) +
  theme(legend.key.size = unit(0.45, "cm"))
```

### Using the $t$ distribution in Stan.

Kruschke expressed concern about high autocorrelations in the chains of his JAGS model. Here are the results of our Stan/**brms** attempt.

```{r, fig.width = 6, fig.height = 4, message = F, warning = F}
# rearrange the bayesplot color scheme
color_scheme_set(scheme = bp[c(6, 2, 2, 2, 1, 2)])

draws %>%
  mutate(chain = .chain) %>% 
  mcmc_acf(pars = vars(b_Intercept:nu), lags = 35)
```

For all three parameters, the autocorrelations were near zero by lag 3 or 4. Not bad. The $N_{eff}/N$ ratios are okay.

```{r, fig.width = 6, fig.height = 1.25}
# rearrange the bayesplot color scheme again
color_scheme_set(scheme = bp[c(2:1, 4:3, 5:6)])

neff_ratio(fit16.2) %>% 
  mcmc_neff(size = 2.5) +
  yaxis_text(hjust = 0)
```

The trace plots look fine.

```{r, fig.width = 8, fig.height = 4}
# rearrange the bayesplot color scheme one more time
color_scheme_set(scheme = c("white", bp[c(2, 2, 2, 6, 3)]))

plot(fit16.2, widths = c(2, 3))
```

The values for `nu` are pretty skewed, but hopefully it makes sense to you why that might be the case. Here are the overlaid density plots.

```{r, fig.width = 8, fig.height = 2}
draws %>%
  mutate(chain = .chain) %>%
  mcmc_dens_overlay(pars = vars(b_Intercept:nu))
```

The $\widehat R$ values are right where we like them.

```{r}
rhat(fit16.2)[1:3]
```

If you peer into the contents of a `brm()` fit object (e.g., `fit16.2 %>% str()`), you'll discover it contains the Stan code. Here it is for our `fit16.2`.

```{r}
fit16.2$fit@stanmodel
```

Note the last line in the parameters block, "real\<lower=1\> nu;  // degrees of freedom or shape." By default, **brms** set the lower bound for $\nu$ to 1.

Just for kicks and giggles, the `pp_check()` offers us a handy way to compare the performance of our Gaussian `fit16.2` and our Student's $t$ `fit16.2`. If we set `type = "ecdf_overlay"` within `pp_check()`, we'll get the criterion `Score` displayed as a cumulative distribution function (CDF) rather than a typical density. Then, `pp_check()` presents CDF's based on draws from the posterior for comparison. Just like with the default `pp_check()` plots, we like it when those simulated distributions mimic the one from the original data.

```{r, fig.width = 7, fig.height = 3}
color_scheme_set(scheme = bp[c(1, 1, 1, 1, 1, 6)])

# fit16.1 with Gaus
set.seed(16)
p1 <-
  pp_check(fit16.1, ndraws = n_lines, type = "ecdf_overlay") + 
  labs(subtitle = "fit16.1 with `family = gaussian`") +
  coord_cartesian(xlim = range(my_data$Score)) +
  theme(legend.position = "none")

# fit16.2 with Student's t
p2 <-
  pp_check(fit16.2, ndraws = n_lines, type = "ecdf_overlay") + 
  labs(subtitle = "fit16.2 with `family = student`") +
  coord_cartesian(xlim = range(my_data$Score))

# combine the subplots
p1 + p2
```

It's subtle, but you might notice that the simulated CDFs from `fit16.1` have steeper slopes in the middle when compared to the original data in the dark blue. However, the `fit16.2`-based simulated CDFs match up more closely with the original data. This suggests an edge for `fit16.2`. Revisiting our skills from [Chapter 10][Model Comparison and Hierarchical Modeling], we might also compare their model weights.

```{r, warning = F, message = F}
model_weights(fit16.1, fit16.2) %>% 
  round(digits = 6)
```

Almost all the stacking weight [see @yaoUsingStackingAverage2018] went to `fit16.2`, our robust Student's $t$ model.

## Two groups

> When there are two groups, we estimate the mean and scale for each group. When using $t$ distributions for robust estimation, we could also estimate the normality of each group separately. But because there usually are relatively few outliers, we will use a single normality parameter to describe both groups, so that the estimate of the normality is more stably estimated. (p. 468)

To get a sense of what this looks like, here's our version of the model diagram in Figure 16.11.

```{r, fig.width = 5.8, fig.height = 3.5, message = F}
# exponential density
p1 <-
  tibble(x = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "exp",
           size = 7, color = bp[5]) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(K)",
           size = 7, color = bp[5], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[4]))

# normal density
p2 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bp[5]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)", "italic(S)[mu]"), 
           size = 7, color = bp[5], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[4]))

# half-normal density
p3 <-
  tibble(x = seq(from = 0, to = 3, by = .01),
         d = (dnorm(x)) / max(dnorm(x))) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = bp[5]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma]", 
           size = 7, color = bp[5], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[4]))

# four annotated arrows
p4 <-
  tibble(x    = c(.43, .43, 1.5, 2.5),
         y    = c(1, .55, 1, 1),
         xend = c(.43, 1.15, 1.5, 1.8),
         yend = c(.8, .15, .2, .2)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bp[4]) +
  annotate(geom = "text",
           x = c(.3, .65, 1.38, 1.62, 2, 2.3), y = c(.92, .25, .6, .6, .6, .6),
           label = c("'~'", "'='", "'~'", "italic(j)", "'~'", "italic(j)"),
           size = c(10, 10, 10, 7, 10, 7), 
           color = bp[5], family = "Times", parse = T) +
  annotate(geom = "text",
           x = .43, y = .7,
           label = "nu*minute+1",
           size = 7, color = bp[5], family = "Times", parse = T) +
  xlim(0, 3) +
  theme_void()

# student-t density
p5 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "student t",
           size = 7, color = bp[5], family = "Times") +
  annotate(geom = "text",
           x = 0, y = .6,
           label = "nu~~~mu[italic(j)]~~~sigma[italic(j)]",
           size = 7, color = bp[5], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[4]))

# the final annotated arrow
p6 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)*'|'*italic(j)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = bp[5], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               color = bp[4], arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p7 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y)[italic(i)*'|'*italic(j)]") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = bp[5], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 1, r = 2),
  area(t = 1, b = 2, l = 3, r = 4),
  area(t = 1, b = 2, l = 5, r = 6),
  area(t = 4, b = 5, l = 3, r = 4),
  area(t = 3, b = 4, l = 1, r = 6),
  area(t = 6, b = 6, l = 3, r = 4),
  area(t = 7, b = 7, l = 3, r = 4)
)

# combine and plot!
(p1 + p2 + p3 + p5 + p4 + p6 + p7) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Since we subset the data, earlier, we'll just reload it to get the full data set.

```{r, message = F}
my_data <- read_csv("data.R/TwoGroupIQ.csv")
```

This time, we'll compute `mean_y` and `sd_y` from the full data.

```{r}
(mean_y <- mean(my_data$Score))
(sd_y   <- sd(my_data$Score))

stanvars <- 
  stanvar(mean_y, name = "mean_y") + 
  stanvar(sd_y,   name = "sd_y") + 
  stanvar(1/29,   name = "one_over_twentynine")
```

Within the **brms** framework, Bürkner calls it distributional modeling when you model more than the mean. Since we're now modeling both $\mu$ and $\sigma$, we're fitting a distributional model. When doing so with **brms**, you typically wrap your `formula` syntax into the `bf()` function. It's also important to know that when modeling $\sigma$, **brms** defaults to modeling its log. So we'll use `log(sd_y)` in its prior. For more on all this, see Bürkner's [-@Bürkner2022Distributional] vignette, [*Estimating distributional models with brms*](https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html).
f
```{r fit16.3}
fit16.3 <-
  brm(data = my_data,
      family = student,
      bf(Score ~ 0 + Group, 
         sigma ~ 0 + Group),
      prior = c(prior(normal(mean_y, sd_y * 100), class = b),
                prior(normal(0, log(sd_y)), class = b, dpar = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvars,
      seed = 16,
      file = "fits/fit16.03")
```

Did you catch our `~ 0 + Group` syntax? That suppressed the usual intercept for our estimates of both $\mu$ and $\log (\sigma)$. Since `Group` is a categorical variable, that results in `brm()` fitting separate intercepts for each category. This is our **brms** analogue to the `x[i]` syntax Kruschke mentioned on page 468. It's what allowed us to estimate $\mu_j$ and $\log (\sigma_j)$.

Let's look at the model summary.

```{r}
print(fit16.3)
```

Remember that the $\sigma$s are now in the log scale. If you want a quick conversion, you might just exponentiate the point estimates from `fixef()`.

```{r}
fixef(fit16.3)[3:4, 1] %>% exp()
```

But please don't stop there. Get your hands dirty with the full posterior. Speaking of which, if we want to make the histograms in Figure 16.12, we'll need to first extract the posterior draws.

```{r}
draws <- as_draws_df(fit16.3)

glimpse(draws)
```

Along with transforming the metrics of a few of the parameters, we may as well rename them to match those in the text.

```{r, warning = F}
draws <-
  draws %>% 
  mutate(`Placebo Mean`     = b_GroupPlacebo,
         `Smart Drug Mean`  = b_GroupSmartDrug,
         # we need to transform the next three parameters
         `Placebo Scale`    = b_sigma_GroupPlacebo   %>% exp(),
         `Smart Drug Scale` = b_sigma_GroupSmartDrug %>% exp(),
         Normality          = nu                     %>% log10()) %>% 
  mutate(`Difference of Means`  = `Smart Drug Mean` - `Placebo Mean`,
         `Difference of Scales` = `Smart Drug Scale` - `Placebo Scale`,
         `Effect Size` = (`Smart Drug Mean` - `Placebo Mean`) / sqrt((`Smart Drug Scale`^2 + `Placebo Scale`^2) / 2)) %>% 
  select(.draw, `Placebo Mean`:`Effect Size`)

glimpse(draws)
```

Now we're ready for the bulk of Figure 16.12.

```{r, fig.width = 6, fig.height = 8}
# save the factor levels
levels <- c("Placebo Mean", "Smart Drug Mean",  "Placebo Scale", "Difference of Means", 
            "Smart Drug Scale", "Difference of Scales", "Normality", "Effect Size")

# we'll use this to mark off the ROPEs as white strips in the background
rope <-
  tibble(name = factor(c("Difference of Means", "Difference of Scales", "Effect Size"),
                       levels = levels), 
         xmin = c(-1, -1, -.1),
         xmax = c(1, 1, .1))

# here are the primary data
draws %>% 
  pivot_longer(-.draw) %>% 
  # this isn't necessary, but it arranged our subplots like those in the text
  mutate(name = factor(name, levels = levels)) %>% 
  
  # the plot
  ggplot() +
  geom_rect(data = rope,
            aes(xmin = xmin, xmax = xmax,
                ymin = -Inf, ymax = Inf),
            color = "transparent", fill = "white") +
  stat_histinterval(aes(x = value, y = 0), 
                    point_interval = mode_hdi, .width = .95,
                    fill = bp[2], color = bp[6],
                    normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

Now let's make the upper two panels in the right column of Figure 16.12.

You might note a few things in our wrangling code. First, in the `mutate()` function, we defined the density values for the two `Group` conditions one at a time. If you look carefully within those definitions, you’ll see we used `10^Normality` for the `df` argument, rather than just `Normality`. *Why?* Remember how a few code blocks up we transformed the original `nu` column to `Normality` by placing it within `log10()`? Well, if you want to undo that, you have to take 10 to the power of `Normality`. Next, notice that we subset the `post` tibble with `select()`. This wasn't technically necessary, but it made the next line easier. Finally, with the `gather()` function, we transformed the `Placebo` and `Smart Drug` columns into an index column strategically named `Group`, which matched up with the original `my_data` data, and a `density` column, which contained the actual density values for the lines.

I know. That's a lot to take in. If you're confused, run the lines in the code one at a time to see how they work. But anyway, here's the final result!

```{r, fig.width = 6, fig.height = 2.5}
# how many credible density lines would you like?
n_lines <- 63

# setting the seed makes the results from `sample_n()` reproducible
set.seed(16)

# wrangle
draws %>% 
  tibble() %>% 
  slice_sample(n = n_lines) %>% 
  expand(nesting(.draw, `Placebo Mean`, `Smart Drug Mean`, `Placebo Scale`, `Smart Drug Scale`, Normality),
         Score = seq(from = 40, to = 250, by = 1)) %>% 
  
  mutate(Placebo      = metRology::dt.scaled(x = Score, df = 10^Normality, mean = `Placebo Mean`,    sd = `Placebo Scale`),
         `Smart Drug` = metRology::dt.scaled(x = Score, df = 10^Normality, mean = `Smart Drug Mean`, sd = `Smart Drug Scale`)) %>% 
  select(.draw, Score:`Smart Drug`) %>% 
  pivot_longer(cols = c(-.draw, -Score),
               names_to = "Group", values_to = "density") %>% 
  
  # plot
  ggplot(aes(x = Score)) + 
  
  geom_histogram(data = my_data, 
                 aes(y = stat(density)),
                 fill = bp[2],
                 size = .2, binwidth = 5, boundary = 0) +
  geom_line(aes(y = density, group = .draw),
            size  = 1/3, alpha = 1/3, color = bp[6]) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Data with Post. Pred.",
       x = "y") +
  coord_cartesian(xlim = c(50, 210)) +
  facet_wrap(~ Group)
```

#### Bonus: Pooled standard deviation.

It was easy to miss it, but you might have noticed we changed how we computed our effect size in our version of Figure 16.12. Earlier in the chapter, we simply computed our standardized mean difference by dividing by $\sigma$, which is a fine thing to do when you have a well-defined population standard deviation. However, when you want to compute a standardized mean difference between two groups, you might not have a well-defined population standard deviation. In such a case, a common approach is to standardize the effect using a *pooled standard deviation*. In the case where you have two groups with equal sample sizes, the pooled standard deviation, $\sigma_p$, is defined as

$$\sigma_p = \sqrt{\frac{\sigma_1^2 + \sigma_2^2}{2}},$$

which is the equation we used in Figure 16.12. When you don't have equal sample sizes in groups, a popular alternative formula is

$$\sigma_p^* = \sqrt{\frac{(n_1 - 1)\sigma_1^2 + (n_2 - 1)\sigma_2^2}{n_1 + n_2 - 2}},$$

which uses the group-based sample sizes to weight their influence on $\sigma_p^*$. Thus, the complete formulas for computing standardized mean differences for two-group experimental data are 

$$d = \frac{\mu_2 - \mu_1}{\sqrt{(\sigma_1^2 + \sigma_2^2) / 2}},$$

when $n_1 = n_2$, or

$$d = \frac{\mu_2 - \mu_1}{\sqrt{ \left[(n_1 - 1)\sigma_1^2 + (n_2 - 1)\sigma_2^2 \right] / (n_1 + n_2 - 2)}},$$

when $n_1 \neq n_2$. Another key insight with this notation is that there are many formulas one can use to compute a standardized mean difference, often called a Cohen's $d$. Thus, when you are computing one for a scientific presentation, it's imperative that you tell your audience exactly how you computed your effect size. *Show them the formula*. Otherwise, your ambiguity can muddle how people interpret your work.

### Analysis by NHST.

Here's how we might perform a $t$-test.

```{r}
t.test(data = my_data,
       Score ~ Group)
```

To help with the comparison, notice how the 95% CIs from the $t$-test range from -15.7 to 0.1. That indicates that the $t$-test subtracted the `Smart Drug` from `Placebo`. I point this out because the difference scores we computed along with Kruschke for Figure 16.12 did the subtraction in the other direction. But don't fret. The magnitude of the difference stays the same either way. Only the signs changed. For example:

```{r}
fixef(fit16.3)["GroupPlacebo", 1] - fixef(fit16.3)["GroupSmartDrug", 1]
fixef(fit16.3)["GroupSmartDrug", 1] - fixef(fit16.3)["GroupPlacebo", 1]
```

But anyway, in addition to the difference distributions from Figure 16.12, we might also use the `brms::hypothesis()` function to do something of a Bayesian $t$-test.

```{r}
hypothesis(fit16.3, "GroupPlacebo - GroupSmartDrug = 0")
```

See how our 95% ETIs are both negative and how we have that little `*` in the `Star` column? If you like the visual approach, you can even feed the `hypothesis()` code into `plot()`.

```{r, fig.width = 5, fig.height = 2.5}
hypothesis(fit16.3, "GroupPlacebo - GroupSmartDrug = 0") %>% 
  plot()
```

Here's how one might customize that plot.

```{r, fig.width = 4.5, fig.height = 2.5}
h <- hypothesis(fit16.3, "GroupPlacebo - GroupSmartDrug = 0")

h$samples %>% 
  
  ggplot(aes(x = H1, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    breaks = 40, fill = bp[2], color = bp[6]) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("GroupPlacebo - GroupSmartDrug = 0")
```

Execute `str(h)` for more insights. Back to the text:

> The reason that the $t$ test is less sensitive than the Bayesian estimation in this example is that the $t$ test assumes normality and therefore its estimate of the within-group variances is too large when there are outliers.
>
> The $t$ test has other problems. Unlike the Bayesian analysis, the $t$ test provides only a test of the equality of means, without a test of the equality of variances. To test equality of variances, we need to run an additional test, namely an $F$ test of the ratio of variances, which would inflate the $p$ values of both tests. Moreover, both tests compute $p$ values based on hypothetical normally distributed data, and the $F$ test is particularly sensitive to violations of this assumption. Therefore it would be better to use resampling methods to compute the $p$ values (and correcting them for multiple tests). (pp. 471--472)

Don't do all that. Use robust Bayes, instead. Just for kicks, we can compare the $\sigma$s with `hypothesis()`, too.

```{r}
hypothesis(fit16.3, "exp(sigma_GroupPlacebo) - exp(sigma_GroupSmartDrug) = 0")
```

See? We don't need an $F$ test.

## Other noise distributions and transforming data

It's worth repeating a portion of this section.

> If the initially assumed noise distribution does not match the data distribution, there are two ways to pursue a better description. The preferred way is to use a better noise distribution. The other way is to transform the data to a new scale so that they tolerably match the shape of the assumed noise distribution. In other words, we can either change the shoe to fit the foot, or we can squeeze the foot to fit in the shoe. Changing the shoe is preferable to squeezing the foot. In traditional statistical software, users were stuck with the pre-packaged noise distribution, and had no way to change it, so they transformed their data and squeezed them into the software. This practice can lead to confusion in interpreting the parameters because they are describing the transformed data, not the data on the original scale. In software such as [**brms**, we can spend less time squeezing our feet into ill-fitting shoes]. (p. 472)

We'll have more practice with the robust Student's $t$ in some of the following chapters. But if you'd like even more, you might check out my blog post on the topic, [*Robust Linear Regression with Student's $t$-Distribution*](https://solomonkurz.netlify.com/post/robust-linear-regression-with-the-robust-student-s-t-distribution/), and the companion post, [*Bayesian robust correlations with brms (and why you should love Student's $t$)*](https://solomonkurz.netlify.com/post/bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/).

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
# here we'll remove our objects
rm(sequence_length, d, beyonce_palettes, bp, my_data, mean_y, sd_y, stanvars, p1, p2, my_arrow, p3, p4, p5, p6, layout, fit16.1, draws, rope, text, n_lines, n_draws, mu, fit16.2, p7, fit16.3, h, levels)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:16.Rmd-->


```{r, echo = FALSE, cachse = FALSE}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

# Metric Predicted Variable with One Metric Predictor

> We will initially describe the relationship between the predicted variable, $y$ and predictor, $x$, with a simple linear model and normally distributed residual randomness in $y$. This model is often referred to as 'simple linear regression.' We will generalize the model in three ways. First, we will give it a noise distribution that accommodates outliers, which is to say that we will replace the normal distribution with a $t$ distribution as we did in the previous chapter. The model will be implemented in [**brms**]. Next, we will consider differently shaped relations between the predictor and the predicted, such as quadratic trend. Finally, we will consider hierarchical models of situations in which every individual has data that can be described by an individual trend, and we also want to estimate group-level typical trends across individuals. [@kruschkeDoingBayesianData2015, p. 478]

## Simple linear regression

It wasn't entirely clear how Kruschke simulated the bimodal data on the right panel of Figure 17.1. I figured an even split of two Gaussians would suffice and just sighted their $\mu$s and $\sigma$s.

```{r, message = F, warning = F}
library(tidyverse)

# how many draws per panel would you like?
n_draw <- 1000

set.seed(17)
d <-
  tibble(panel = rep(letters[1:2], each = n_draw),
         x = c(runif(n = n_draw, min = -10, max = 10),
               rnorm(n = n_draw / 2, mean = -7, sd = 2),
               rnorm(n = n_draw / 2, mean =  3, sd = 2))) %>% 
  mutate(y = 10 + 2 * x + rnorm(n = n(), mean = 0, sd = 2))

head(d)
```

In case you missed it, Kruschke defied the formula for these data in Figure 17.1. It is

\begin{align*}
y_i & \sim \operatorname{Normal}(\mu, \sigma = 2), \text{where} \\
\mu & = 10 + 2 x_i.
\end{align*}

"Note that the model only specifies the dependency of $y$ on $x$. The model does not say anything about what generates $x$, and there is no probability distribution assumed for describing $x$" (p. 479). Let this sink into your soul. It took a long time, for me. E.g., a lot of people fret over the distributions of their $x$ variables. Now one might should examine them to make sure nothing looks off, such as for data coding mistakes. But if they're not perfectly or even approximately Gaussian, that isn't necessarily an issue. The typical linear model makes no presumption about the distribution of the predictors. Often times, the largest issue is whether the $x$ variables are categorical or continuous.

Before we make our Figure 17.1, we'll want to make a separate tibble of the values necessary to plot those sideways Gaussians. Here are the steps.

```{r}
curves <-
  # define the 3 x-values we want the Gaussians to originate from
  tibble(x = seq(from = -7.5, to = 7.5, length.out = 4)) %>%
  
  # use the formula 10 + 2x to compute the expected y-value for x
  mutate(y_mean = 10 + (2 * x)) %>%
  
  # based on a Gaussian with `mean = y_mean` and `sd = 2`, compute the 99% intervals
  mutate(ll = qnorm(.005, mean = y_mean, sd = 2),
         ul = qnorm(.995, mean = y_mean, sd = 2)) %>%
  
  # now use those interval bounds to make a sequence of y-values
  mutate(y = map2(ll, ul, seq, length.out = 100)) %>%
  
  # since that operation returned a nested column, we need to `unnest()`
  unnest(y) %>%
  
  # compute the density values
  mutate(density = map2_dbl(y, y_mean, dnorm, sd = 2)) %>%
  
  # now rescale the density values to be wider.
  # since we want these to be our x-values, we'll 
  # just redefine the x column with these results
  mutate(x = x - density * 2 / max(density))

str(curves)
```

Before we make Figure 17.1, let's talk color. Like last chapter, we'll take our color palette from the **beyonce** package. Our palette will be a nine-point version of #41.

```{r, warning = F, message = F, fig.width = 3, fig.height = 1}
library(beyonce)

bp <- beyonce_palette(41, n = 9, type = "continuous")

bp
```

The global theme will be `ggplot2::theme_linedraw()` with the grid lines removed. Make Figure 17.1.

```{r, fig.width = 7, fig.height = 4, message = F}
theme_set(
  theme_linedraw() +
    theme(panel.grid = element_blank())
)

d %>% 
  ggplot(aes(x = x, y = y)) +
  geom_vline(xintercept = 0, size = 1/3, linetype = 2, color = bp[9]) +
  geom_hline(yintercept = 0, size = 1/3, linetype = 2, color = bp[9]) +
  geom_point(size = 1/3, alpha = 1/3, color = bp[5]) +
  stat_smooth(method = "lm", se = F, fullrange = T, color = bp[1]) +
  geom_path(data = curves,
            aes(group = y_mean),
            color = bp[2], size = 1) +
  labs(title = "Normal PDF around Linear Function",
       subtitle = "We simulated x from a uniform distribution in the left panel and simulated it from a mixture of\n two Gaussians on the right.") +
  coord_cartesian(xlim = c(-10, 10),
                  ylim = c(-10, 30)) +
  theme(strip.background = element_blank(),
        strip.text = element_blank()) +
  facet_wrap(~ panel)
```

Concerning causality,

> *the simple linear model makes no claims about causal connections between* $x$ *and* $y$. *The simple linear model merely describes a tendency for* $y$ *values to be linearly related to* $x$ *values*, hence "predictable" from the $x$ values. When describing data with this model, we are starting with a scatter plot of points generated by an unknown process in the real world, and estimating parameter values that would produce a smattering of points that might mimic the real data. Even if the descriptive model mimics the data well (and it might not), the mathematical "process" in the model may have little if anything to do with the real-world process that created the data. Nevertheless, the parameters in the descriptive model are meaningful because they describe tendencies in the data. (p. 479, *emphasis* added)

I emphasized these points because I've heard and seen a lot of academics conflate linear regression models with causal models. For sure, it might well be preferable if your regression model was also a causal model. But good old prediction is fine, too.

## Robust linear regression

> There is no requirement to use a normal distribution for the noise distribution. The normal distribution is traditional because of its relative simplicity in mathematical derivations. But real data may have outliers, and the use of (optionally) heavy-tailed noise distributions is straight forward in contemporary Bayesian software[, like **brms**]. (pp. 479--480)

Let's make our version of the model diagram in Figure 17.2 to get a sense of where we're going.

```{r, fig.width = 6.75, fig.height = 5, message = F}
library(patchwork)

# normal density
p1 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# a second normal density
p2 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)[1]", "italic(S)[1]"), 
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

## two annotated arrows
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")
p3 <-
  tibble(x    = c(.33, 1.67),
         y    = c(1, 1),
         xend = c(.75, 1.1),
         yend = c(0, 0)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bp[1]) +
  annotate(geom = "text",
           x = c(.4, 1.25), y = .5,
           label = "'~'",
           size = 10, color = bp[1], family = "Times", parse = T) +
  xlim(0, 2) +
  theme_void()

# exponential density
p4 <-
  tibble(x = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "exp",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(K)",
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# likelihood formula
p5 <-
  tibble(x = .5,
         y = .25,
         label = "beta[0]+beta[1]*italic(x)[italic(i)]") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = bp[1], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# half-normal density
p6 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma]", 
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# four annotated arrows
p7 <-
  tibble(x    = c(.43, .43, 1.5, 2.5),
         y    = c(1, .55, 1, 1),
         xend = c(.43, 1.225, 1.5, 1.75),
         yend = c(.8, .15, .2, .2)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bp[1]) +
  annotate(geom = "text",
           x = c(.3, .7, 1.38, 2), y = c(.92, .22, .65, .6),
           label = c("'~'", "'='", "'='", "'~'"),
           size = 10, 
           color = bp[1], family = "Times", parse = T) +
  annotate(geom = "text",
           x = .43, y = .7,
           label = "nu*minute+1",
           size = 7, color = bp[1], family = "Times", parse = T) +
  xlim(0, 3) +
  theme_void()

# student-t density
p8 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "student t",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = 0, y = .6,
           label = "nu~~~mu[italic(i)]~~~sigma",
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# the final annotated arrow
p9 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = bp[1], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               color = bp[1], arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p10 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = bp[1], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 3, r = 5),
  area(t = 1, b = 2, l = 7, r = 9),
  area(t = 4, b = 5, l = 1, r = 3),
  area(t = 4, b = 5, l = 5, r = 7),
  area(t = 4, b = 5, l = 9, r = 11),
  area(t = 3, b = 4, l = 3, r = 9),
  area(t = 7, b = 8, l = 5, r = 7),
  area(t = 6, b = 7, l = 1, r = 11),
  area(t = 9, b = 9, l = 5, r = 7),
  area(t = 10, b = 10, l = 5, r = 7)
)

# combine and plot!
(p1 + p2 + p4 + p5 + p6 + p3 + p8 + p7 + p9 + p10) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Here's Kruschke's `HtWtDataGenerator()` code.

```{r}
HtWtDataGenerator <- function(nSubj, rndsd = NULL, maleProb = 0.50) {
  # Random height, weight generator for males and females. Uses parameters from
  # Brainard, J. & Burmaster, D. E. (1992). Bivariate distributions for height and
  # weight of men and women in the United States. Risk Analysis, 12(2), 267-275.
  # Kruschke, J. K. (2011). Doing Bayesian data analysis:
  # A Tutorial with R and BUGS. Academic Press / Elsevier.
  # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition:
  # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier.
  
  # require(MASS)
  
  # Specify parameters of multivariate normal (MVN) distributions.
  # Men:
  HtMmu   <- 69.18
  HtMsd   <- 2.87
  lnWtMmu <- 5.14
  lnWtMsd <- 0.17
  Mrho    <- 0.42
  Mmean   <- c(HtMmu, lnWtMmu)
  Msigma  <- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd,
                      Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2)
  # Women cluster 1:
  HtFmu1   <- 63.11
  HtFsd1   <- 2.76
  lnWtFmu1 <- 5.06
  lnWtFsd1 <- 0.24
  Frho1    <- 0.41
  prop1    <- 0.46
  Fmean1   <- c(HtFmu1, lnWtFmu1)
  Fsigma1  <- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1,
                       Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2)
  # Women cluster 2:
  HtFmu2   <- 64.36
  HtFsd2   <- 2.49
  lnWtFmu2 <- 4.86
  lnWtFsd2 <- 0.14
  Frho2    <- 0.44
  prop2    <- 1 - prop1
  Fmean2   <- c(HtFmu2, lnWtFmu2)
  Fsigma2  <- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2,
                       Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2)
  
  # Randomly generate data values from those MVN distributions.
  if (!is.null(rndsd)) {set.seed(rndsd)}
  datamatrix <- matrix(0, nrow = nSubj, ncol = 3)
  colnames(datamatrix) <- c("male", "height", "weight")
  maleval <- 1; femaleval <- 0 # arbitrary coding values
  for (i in 1:nSubj)  {
    # Flip coin to decide sex
    sex <- sample(c(maleval, femaleval), size = 1, replace = TRUE,
                  prob = c(maleProb, 1 - maleProb))
    if (sex == maleval) {datum = MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)}
    if (sex == femaleval) {
      Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2))
      if (Fclust == 1) {datum = MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)}
      if (Fclust == 2) {datum = MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)}
    }
    datamatrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1))
  }
  
  return(datamatrix)
}
```

Let's take this baby for a spin to simulate our data.

```{r}
d <- 
  HtWtDataGenerator(nSubj = 300, rndsd = 17, maleProb = .50) %>% 
  as_tibble() %>% 
  # this will allow us to subset 30 of the values into their own group
  mutate(subset = rep(0:1, times = c(9, 1)) %>% rep(., 30))

head(d)
```

> Fortunately, we do not have to worry much about analytical derivations because we can let JAGS or Stan generate a high resolution picture of the posterior distribution. Our job, therefore, is to specify sensible priors and to make sure that the MCMC process generates a trustworthy posterior sample that is converged and well mixed. (p. 483)

### Robust linear regression in ~~JAGS~~ brms.

Presuming a data set with a sole standardized predictor `x_z` for a sole standardized criterion `y_z`, the basic **brms** code corresponding to the JAGS code Kruschke showed on page 483 looks like this.

```{r eval = F}
fit <-
  brm(data = my_data,
      family = student,
      y_z ~ 1 + x_z,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(normal(0, 1), class = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      stanvars = stanvar(1/29, name = "one_over_twentynine"))
```

Like we discussed in Chapter 16, we won't be using the uniform prior for $\sigma$. Since we're presuming standardized data, a half-unit normal is a fine choice. But do note this is much tighter than Kruschke's $\operatorname{Uniform} (0.001, 1000)$ and it will have down-the-road consequences for our results versus those in the text.

Also, look at how we just pumped the definition of our sole `stanvar(1/29, name = "one_over_twentynine")` operation right into the `stanvar` argument. If we were defining multiple values this way, I'd prefer to save this as an object first and then just pump that object into `stanvars`. But in this case, it was simple enough to just throw directly into the `brm()` function.

#### Standardizing the data for MCMC sampling.

Kruschke mentioned how standardizing your data before feeding it into JAGS often helps the algorithm operate smoothly. The same basic principle holds for **brms** and Stan. Stan can often handle unstandardized data just fine. But if you ever run into estimation difficulties, consider standardizing your data and trying again.

We'll make a simple function to standardize the `height` and `weight` values.

```{r}
standardize <- function(x) {
  (x - mean(x)) / sd(x)
}

d <-
  d %>% 
  mutate(height_z = standardize(height),
         weight_z = standardize(weight))
```

Somewhat analogous to how Kruschke standardized his data within the JAGS code, you could standardize the data within the `brm()` function. That would look something like this.

```{r eval = F}
fit <-
  brm(data = d %>%  # the standardizing occurs in the next two lines
        mutate(height_z = standardize(height),
               weight_z = standardize(weight)),
      family = student,
      weight_z ~ 1 + height_z)
```

But anyway, let's open **brms**.

```{r, message = F, warning = F}
library(brms)
```

We'll fit the two models at once. `fit1` will be of the total data sample. `fit2` is of the $n = 30$ subset.

```{r fit17.1}
fit17.1 <-
  brm(data = d,
      family = student,
      weight_z ~ 1 + height_z,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(normal(0, 1), class = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvar(1/29, name = "one_over_twentynine"),
      seed = 17,
      file = "fits/fit17.01")

fit17.2 <-
  update(fit17.1,
         newdata = d %>% 
           filter(subset == 1),
         chains = 4, cores = 4,
         seed = 17,
         file = "fits/fit17.02")
```

Here are the results.

```{r}
print(fit17.1)
print(fit17.2)
```

Based on Kruschke's Equation 17.2, we can convert the standardized coefficients back to their original metric using the formulas

\begin{align*}
\beta_0 & = \zeta_0 \operatorname{SD}_y + M_y - \frac{\zeta_1 M_x \operatorname{SD}_y}{\operatorname{SD}_x} \;\; \text{and}  \\
\beta_1 & = \frac{\zeta_1 \operatorname{SD}_y}{\operatorname{SD}_x},
\end{align*}

where $\zeta_0$ and $\zeta_1$ denote the intercept and slope for the model of the standardized data, respectively, and that model follows the familiar form

$$z_{\hat y} = \zeta_0 + \zeta_1 z_x.$$

To implement those equations, we'll first extract the posterior draws. We begin with `fit17.1`, the model for which $N = 300$.

```{r}
draws <- as_draws_df(fit17.1)

head(draws)
```

Let's wrap the consequences of Equation 17.2 into two functions.

```{r}
make_beta_0 <- function(zeta_0, zeta_1, sd_x, sd_y, m_x, m_y) {
  zeta_0 * sd_y + m_y - zeta_1 * m_x * sd_y / sd_x
}

make_beta_1 <- function(zeta_1, sd_x, sd_y) {
  zeta_1 * sd_y / sd_x
}
```

After saving a few values, we're ready to use our custom functions to convert our posteriors for `b_Intercept` and `b_height_z` to their natural metric.

```{r}
sd_x <- sd(d$height)
sd_y <- sd(d$weight)
m_x  <- mean(d$height)
m_y  <- mean(d$weight)

draws <-
  draws %>% 
  mutate(b_0 = make_beta_0(zeta_0 = b_Intercept,
                           zeta_1 = b_height_z,
                           sd_x   = sd_x,
                           sd_y   = sd_y,
                           m_x    = m_x,
                           m_y    = m_y),
         b_1 = make_beta_1(zeta_1 = b_height_z,
                           sd_x   = sd_x,
                           sd_y   = sd_y))

glimpse(draws)
```

Now we're finally ready to make the top panel of Figure 17.4.

```{r, fig.width = 4.5, fig.height = 4}
# how many posterior lines would you like?
n_lines <- 100

d %>% 
  ggplot(aes(x = height, y = weight)) +
  geom_abline(data = draws %>% slice(1:n_lines),
              aes(intercept = b_0, slope = b_1, group = .draw),
              color = bp[2], size = 1/4, alpha = 1/3) + 
  geom_point(alpha = 1/2, color = bp[5]) +
  labs(subtitle = eval(substitute(paste("Data with", n_lines, "credible regression lines"))),
       x = "height",
       y = "weight") +
  coord_cartesian(xlim = c(50, 80),
                  ylim = c(-50, 470))
```

We'll want to open the **tidybayes** package to help make the histograms.

```{r, fig.width = 6, fig.height = 4, warning = F, message = F}
library(tidybayes)

# we'll use this to mark off the ROPEs as white strips in the background
rope <-
  tibble(name = "Slope", 
         xmin = -.5,
         xmax = .5)

# annotate the ROPE
text <-
  tibble(x     = 0,
         y     = 0.98,
         label = "ROPE",
         name  = "Slope")

# here are the primary data
draws %>% 
  transmute(Intercept = b_0,
            Slope     = b_1,
            Scale     = sigma * sd_y,
            Normality = nu %>% log10()) %>% 
  pivot_longer(everything()) %>% 
  
  # the plot
  ggplot() +
  geom_rect(data = rope,
            aes(xmin = xmin, xmax = xmax,
                ymin = -Inf, ymax = Inf),
            color = "transparent", fill = bp[9]) +
  stat_histinterval(aes(x = value, y = 0), 
                    point_interval = mode_hdi, .width = .95,
                    fill = bp[6], color = bp[1], slab_color = bp[5],
                    breaks = 40, normalize = "panels") +
  geom_text(data = text,
            aes(x = x, y = y, label = label),
            size = 2.75, color = "white") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

Here's the scatter plot for the slope and intercept.

```{r, fig.width = 3.25, fig.height = 3}
draws %>% 
  ggplot(aes(x = b_1, y = b_0)) +
  geom_point(color = bp[3], size = 1/3, alpha = 1/3) +
  labs(x = expression(beta[1]),
       y = expression(beta[0]))
```

That is one strong correlation! Finally, here's the scatter plot for $\operatorname{log10}(\nu)$ and $\sigma_{\text{transformed back to its raw metric}}$.

```{r, fig.width = 3.25, fig.height = 3, warning = F}
draws %>% 
  transmute(Scale     = sigma * sd_y,
            Normality = nu %>% log10()) %>% 
  ggplot(aes(x = Normality, y = Scale)) +
  geom_point(color = bp[3], size = 1/3, alpha = 1/3) +
  labs(x = expression(log10(nu)),
       y = expression(sigma))
```

Let's back track and make the plots for Figure 17.3 with `fit17.2`. We'll need to extract the posterior draws and wrangle, as before.

```{r}
draws <- as_draws_df(fit17.2)

draws <-
  draws %>% 
  mutate(b_0 = make_beta_0(zeta_0 = b_Intercept,
                           zeta_1 = b_height_z,
                           sd_x   = sd_x,
                           sd_y   = sd_y,
                           m_x    = m_x,
                           m_y    = m_y),
         b_1 = make_beta_1(zeta_1 = b_height_z,
                           sd_x   = sd_x,
                           sd_y   = sd_y))

glimpse(draws)
```

Here's the top panel of Figure 17.3.

```{r, fig.width = 4.5, fig.height = 4}
# how many posterior lines would you like?
n_lines <- 100

ggplot(data = d %>% 
         filter(subset == 1), 
       aes(x = height, y = weight)) +
  geom_vline(xintercept = 0, color = bp[9]) +
  geom_abline(data = draws %>% slice(1:n_lines),
              aes(intercept = b_0, slope = b_1, group = .draw),
              color = bp[6], size = 1/4, alpha = 1/3) +
  geom_point(alpha = 1/2, color = bp[3]) +
  scale_y_continuous(breaks = seq(from = -300, to = 200, by = 100)) +
  labs(subtitle = eval(substitute(paste("Data with", n_lines, "credible regression lines"))),
       x = "height",
       y = "weight") +
  coord_cartesian(xlim = c(0, 80),
                  ylim = c(-350, 250))
```

Next we'll make the histograms.

```{r, fig.width = 6, fig.height = 4, warning = F}
# here are the primary data
draws %>% 
  transmute(Intercept = b_0,
            Slope     = b_1,
            Scale     = sigma * sd_y,
            Normality = nu %>% log10()) %>% 
  pivot_longer(everything()) %>% 
  
  # the plot
  ggplot() +
  geom_rect(data = rope,
            aes(xmin = xmin, xmax = xmax,
                ymin = -Inf, ymax = Inf),
            color = "transparent", fill = bp[9]) +
  stat_histinterval(aes(x = value, y = 0), 
                    point_interval = mode_hdi, .width = .95,
                    fill = bp[6], color = bp[1], slab_color = bp[5],
                    breaks = 40, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

And we'll finish up with the scatter plots.

```{r, fig.width = 3.25, fig.height = 3, warning = F}
draws %>% 
  ggplot(aes(x = b_1, y = b_0)) +
  geom_point(color = bp[4], size = 1/3, alpha = 1/3) +
  labs(x = expression(beta[1]),
       y = expression(beta[0]))

draws %>% 
  transmute(Scale = sigma * sd_y,
            Normality = nu %>% log10()) %>% 
  ggplot(aes(x = Normality, y = Scale)) +
  geom_point(color = bp[4], size = 1/3, alpha = 1/3) +
  labs(x = expression(log10(nu)),
       y = expression(sigma))
```

### Robust linear regression in Stan.

> Recall from [Section 14.1][HMC sampling] (p. 400) that Stan uses Hamiltonian dynamics to find proposed positions in parameter space. The trajectories use the gradient of the posterior distribution to move large distances even in narrow distributions. Thus, HMC by itself, without data standardization, should be able to efficiently generate a representative sample from the posterior distribution. (p. 487)

To be clear, we're going to fit the models with Stan/**brms** twice. Above, we used the standardized data like Kruschke did with his JAGS code. Now we're getting ready to follow along with the text and use Stan/**brms** to fit the models with the unstandardized data.

#### Constants for vague priors.

The issues is we want a system where we can readily specify vague priors on our regression models when the data are not standardized. As it turns out,

> a regression slope can take on a maximum value of $\operatorname{SD}_y / \operatorname{SD}_x$ for data that are perfectly correlated. Therefore, the prior on the slope will be given a standard deviation that is large compared to that maximum. The biggest that an intercept could be, for data that are perfectly correlated, is $M_x \operatorname{SD}_y / \operatorname{SD}_x$. Therefore, the prior on the intercept will have a standard deviation that is large compared to that maximum. (p. 487)

With that in mind, we'll specify our `stanvars` as follows.

```{r}
beta_0_sigma <- 10 * abs(m_x * sd_y / sd_x)
beta_1_sigma <- 10 * abs(sd_y / sd_x) 

stanvars <- 
  stanvar(beta_0_sigma, name = "beta_0_sigma") + 
  stanvar(beta_1_sigma, name = "beta_1_sigma") +
  stanvar(sd_y, name = "sd_y") +
  stanvar(1/29, name = "one_over_twentynine")
```

As in Chapter 16, "set the priors to be extremely broad relative to the data" (p. 487). With our `stanvars` defined, we're ready to fit `fit17.3`.

```{r fit17.3}
fit17.3 <-
  brm(data = d,
      family = student,
      weight ~ 1 + height,
      prior = c(prior(normal(0, beta_0_sigma), class = Intercept),
                prior(normal(0, beta_1_sigma), class = b),
                prior(normal(0, sd_y), class = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvars,
      seed = 17,
      file = "fits/fit17.03")
```

Here's the model summary.

```{r}
print(fit17.3)
```

Now compare the histograms for these posterior draws to those we made, above, from those `fit17.1`. You'll see they're quite similar.

```{r, fig.width = 6, fig.height = 4, warning = F, message = F}
# here are the primary data
as_draws_df(fit17.3) %>% 
  transmute(Intercept = b_Intercept,
            Slope     = b_height,
            Scale     = sigma,
            Normality = nu %>% log10()) %>% 
  pivot_longer(everything()) %>% 
  
  # the plot
  ggplot() +
  geom_rect(data = rope,
            aes(xmin = xmin, xmax = xmax,
                ymin = -Inf, ymax = Inf),
            color = "transparent", fill = bp[9]) +
  stat_histinterval(aes(x = value, y = 0), 
                    point_interval = mode_hdi, .width = .95,
                    fill = bp[6], color = bp[1], slab_color = bp[5],
                    breaks = 40, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

### Stan or JAGS?

In this ebook we only fit the models with **brms**, which uses Stan under the hood. But since we fit the $N = 300$ model with both standardized and unstandardized data, we can compare their performance. For that, we'll want **bayesplot**.

```{r, message = F, warning = F}
library(bayesplot)
```

They had equally impressive autocorrelation plots.

```{r, fig.width = 5, fig.height = 3}
# set the bayesplot color scheme
color_scheme_set(scheme = bp[c(1, 3, 8, 7, 5, 5)])

as_draws_df(fit17.1) %>% 
  mutate(chain = .chain) %>% 
  mcmc_acf(pars = vars(b_Intercept:nu),
           lags = 10) +
  ggtitle("fit17.1")

as_draws_df(fit17.3) %>% 
  mutate(chain = .chain) %>% 
  mcmc_acf(pars = vars(b_Intercept:nu),
           lags = 10) +
  ggtitle("fit17.3")
```

Their $N_{eff}/N$ ratios were pretty similar. Both were reasonable. You'd probably want to run a simulation to contrast them with any rigor.

```{r, fig.width = 6, fig.height = 3.25, warning = F, message = F}
# change the bayesplot color scheme
color_scheme_set(scheme = bp[c(1, 3, 4, 6, 7, 9)])

p1 <-
  neff_ratio(fit17.1) %>% 
  mcmc_neff() +
  yaxis_text(hjust = 0) +
  ggtitle("fit17.1")

p2 <-
  neff_ratio(fit17.3) %>% 
  mcmc_neff() +
  yaxis_text(hjust = 0) +
  ggtitle("fit17.3")

p1 / p2 + plot_layout(guide = "collect")
```

### Interpreting the posterior distribution.

Halfway through the prose, Kruschke mentioned how the models provide entire posteriors for the `weight` of a 50-inch-tall person. **brms** offers a few ways to do so.

> In some applications, there is interest in extrapolating or interpolating trends at $x$ values sparsely represented in the current data. For instance, we might want to predict the weight of a person who is 50 inches tall. A feature of Bayesian analysis is that we get an entire distribution of credible predicted values, not only a point estimate. (p. 489)

Since this is such a simple model, one way is to work directly with the posterior draws Here we use the model formula $y_i = \beta_0 + \beta_1 x_i$ by adding the transformed intercept `b_0` to the product of $50$ and the transformed coefficient for `height`, `b_1`.

```{r, fig.width = 3.5, fig.height = 2.5}
draws %>% 
  mutate(weight_at_50 = b_0 + b_1 * 50) %>% 
  
  ggplot(aes(x = weight_at_50, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = bp[6], color = bp[1], slab_color = bp[5],
                    breaks = 40, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("lbs")
```

Looks pretty wide, doesn’t it? Hopefully this isn't a surprise. Recall that this `post` is from `fit17.2`, the posterior based on the $n = 30$ data. With so few cases, most predictions from that model are uncertain. But also, 50 inches is way out of the bounds of the data the model was based on, so we should be uncertain in this range.

Let's practice a second method. With the `brms::fitted()` function, we can specify the desired `height` value into a tibble, which we'll then feed into the `newdata` argument. Fitted will then return the model-implied criterion value for that predictor variable. To warm up, we'll first to it with `fit17.3`, the model based on the untransformed data.

```{r}
nd <- tibble(height = 50)

fitted(fit17.3,
       newdata = nd)
```

The code returned a typical **brms**-style summary of the posterior mean, standard deviation, and 95% percentile-based intervals. The same basic method will work for the standardized models, `fit17.1` or `fit17.2`. But that will take a little more wrangling. First, we'll need to transform our desired value 50 into its standardized version.

```{r}
nd <- tibble(height_z = (50 - mean(d$height)) / sd(d$height))
```

When we feed this value into `fitted()`, it will return the corresponding posterior within the standardized metric. But we want unstandardized, so we'll need to transform. That'll be a few-step process. First, to do the transformation properly, we'll want to work with the poster draws themselves, rather than summary values. So we'll set `summary = F`. We'll then convert the draws into a tibble format. Then we'll use the `transmute()` function to do the conversion. In the final step, we'll use `mean_qi()` to compute the summary values.

```{r, warning = F, message = F}
fitted(fit17.1,
       newdata = nd,
       summary = F) %>% 
  as_tibble() %>% 
  transmute(weight = V1 * sd(d$weight) + mean(d$weight)) %>% 
  mean_qi()
```

If you look above, you'll see the results are within rounding error of those from `fit3`.

## Hierarchical regression on individuals within groups

> In the previous applications, the $j$th individual contributed a single $x_j, y_j$ pair. But suppose instead that every individual, $j$, contributes multiple observations of $x_{i|j}, y_{i|j}$ pairs. (The subscript notation $i|j$ means the $i$th observation within the $j$th individual.) With these data, we can estimate a regression curve for every individual. If we also assume that the individuals are mutually representative of a common group, then we can estimate group-level parameters too. (p. 490)

Load the fictitious data and take a `glimpse()`.

```{r, message = F}
my_data <- read_csv("data.R/HierLinRegressData.csv")

glimpse(my_data)
```

> Our goal is to describe each individual with a linear regression, and simultaneously to estimate the typical slope and intercept of the group overall. A key assumption for our analysis is that each individual is representative of the group. Therefore, every individual informs the estimate of the group slope and intercept, which in turn inform the estimates of all the individual slopes and intercepts. Thereby we get sharing of information across individuals, and shrinkage of individual estimates toward the overarching mode. (p. 491)

### The model and implementation in ~~JAGS~~ brms.

Kruschke described the model diagram in Figure 17.6 as "a bit daunting" (p. 491). The code to make our version of the diagram is "a bit daunting," too. Just like the code for any other diagram, it's modular. If you're following along with me and making these on your own, just build it up, step by step.

```{r, fig.width = 8.5, fig.height = 6, message = F}
# normal density
p1 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# half-normal density
p2 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma][0]", 
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# a second normal density
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)[1]", "italic(S)[1]"), 
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# a second half-normal density
p4 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma][1]", 
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# four annotated arrows
p5 <-
  tibble(x    = c(.05, .35, .65, .95),
         y    = c(1, 1, 1, 1),
         xend = c(.32, .4, .65, .72),
         yend = c(.2, .2, .2, .2)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bp[1]) +
  annotate(geom = "text",
           x = c(.15, .35, .625, .78), y = .55,
           label = "'~'",
           size = 10, color = bp[1], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# third normal density
p6 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("mu[0]", "sigma[0]"), 
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# fourth normal density
p7 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("mu[1]", "sigma[1]"), 
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# two annotated arrows
p8 <-
  tibble(x    = c(.18, .82),
         y    = c(1, 1),
         xend = c(.36, .55),
         yend = c(0, 0)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bp[1]) +
  annotate(geom = "text",
           x = c(.18, .33, .64, .77), y = .55,
           label = c("'~'", "italic(j)", "'~'", "italic(j)"),
           size = c(10, 7, 10, 7), 
           color = bp[1], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# exponential density
p9 <-
  tibble(x = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "exp",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(K)",
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# likelihood formula
p10 <-
  tibble(x = .5,
         y = .25,
         label = "beta[0][italic(j)]+beta[1][italic(j)]*italic(x)[italic(i)*'|'*italic(j)]") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = bp[1], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# half-normal density
p11 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma]", 
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# four annotated arrows
p12 <-
  tibble(x    = c(.43, .43, 1.5, 2.5),
         y    = c(1, .55, 1, 1),
         xend = c(.43, 1.225, 1.5, 1.75),
         yend = c(.8, .15, .2, .2)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bp[1]) +
  annotate(geom = "text",
           x = c(.3, .7, 1.38, 2), y = c(.92, .22, .65, .6),
           label = c("'~'", "'='", "'='", "'~'"),
           size = 10, 
           color = bp[1], family = "Times", parse = T) +
  annotate(geom = "text",
           x = .43, y = .7,
           label = "nu*minute+1",
           size = 7, color = bp[1], family = "Times", parse = T) +
  xlim(0, 3) +
  theme_void()

# student-t density
p13 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) +
  geom_area(fill = bp[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "student t",
           size = 7, color = bp[1]) +
  annotate(geom = "text",
           x = 0, y = .6,
           label = "nu~~mu[italic(i)*'|'*italic(j)]~~sigma",
           size = 7, color = bp[1], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bp[1]))

# the final annotated arrow
p14 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)*'|'*italic(j)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = bp[1], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               color = bp[1], arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p15 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y)[italic(i)*'|'*italic(j)]") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = bp[1], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 1, r = 3),
  area(t = 1, b = 2, l = 5, r = 7),
  area(t = 1, b = 2, l = 9, r = 11),
  area(t = 1, b = 2, l = 13, r = 15),
  area(t = 4, b = 5, l = 5, r = 7),
  area(t = 4, b = 5, l = 9, r = 11),
  area(t = 3, b = 4, l = 1, r = 15),
  area(t = 7, b = 8, l = 3, r = 5),
  area(t = 7, b = 8, l = 7, r = 9),
  area(t = 7, b = 8, l = 11, r = 13),
  area(t = 6, b = 7, l = 5, r = 11),
  area(t = 10, b = 11, l = 7, r = 9),
  area(t = 9, b = 10, l = 3, r = 13),
  area(t = 12, b = 12, l = 7, r = 9),
  area(t = 13, b = 13, l = 7, r = 9)
)

# combine and plot!
(p1 + p2 + p3 + p4 + p6 + p7 + p5 + p9 + p10 + p11 + p8 + p13 + p12 + p14 + p15) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Just look at that sweet thing! If you made you version, here; have a piece of cake. `r emo::ji("cake")` You earned it.

Now let's standardize the data and define our `stanvars`. I should note that standardizing and mean centering, more generally, becomes complicated with multilevel models. Here we're just standardizing based on the grand mean and grand standard deviation. But there are other ways to standardize, such as within groups. Craig Enders has a good [-@endersCenteringPredictorsContextual2013] [book chapter](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C1&q=Centering+Predictors+and+Contextual+Effects&btnG=) that touched on the topic, as well as an earlier [-@enders2007centering] [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.928.9848&rep=rep1&type=pdf) with Tofighi.

```{r}
my_data <-
  my_data %>% 
  mutate(x_z = standardize(X),
         y_z = standardize(Y))
```

In my experience, you typically use the `(|)` syntax when fitting a hierarchical model with the`brm()` function. The terms before the `|` are those varying by group and you tell `brm()` what the grouping variable is after the `|`. In the case of multiple group-level parameters--which is the case with this model (i.e., both intercept and the `x_z` slope--, this syntax also estimates correlations among the group-level parameters. Kruschke's model doesn't appear to include such a correlation. Happily, we can use the `(||)` syntax instead, which omits correlations among the group-level parameters. If you're curious about the distinction, fit the model both ways and explore the differences in the `print()` output. For more on the topic, see the *Group-level terms* subsection of the `brmsformula` section of the [**brms** reference manual](https://CRAN.R-project.org/package=brms/brms.pdf) [@brms2022RM].

```{r fit17.4}
fit17.4 <-
  brm(data = my_data,
      family = student,
      y_z ~ 1 + x_z + (1 + x_z || Subj),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(normal(0, 1), class = sigma),
                # the next line is new
                prior(normal(0, 1), class = sd),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      seed = 17,
      stanvars = stanvar(1/29, name = "one_over_twentynine"),
      file = "fits/fit17.04")
```

Did you catch that `prior(normal(0, 1), class = sd)` line in the code? That's the prior we used for our hierarchical variance parameters, $\sigma_0$ and $\sigma_1$. Just like with the scale parameter, $\sigma$, we used the zero-mean half-normal distribution. By default, **brms** sets their left boundary to zero, which keeps the HMC algorithm from exploring negative variance values.

Anyway, here's the model `summary()`.

```{r}
summary(fit17.4)
```

### The posterior distribution: Shrinkage and prediction.

Keeping in the same spirit of [Section 17.2.4][Interpreting the posterior distribution.], we'll make the plots of Figure 17.5 in two ways. First, we'll use our `make_beta_0()` and  `make_beta_1()` functions to transform the model coefficients.

```{r, warning = F}
draws <- as_draws_df(fit17.4)

sd_x <- sd(my_data$X)
sd_y <- sd(my_data$Y)
m_x  <- mean(my_data$X)
m_y  <- mean(my_data$Y)

draws <-
  draws %>% 
  mutate(b_0 = make_beta_0(zeta_0 = b_Intercept,
                           zeta_1 = b_x_z,
                           sd_x   = sd_x,
                           sd_y   = sd_y,
                           m_x    = m_x,
                           m_y    = m_y),
         b_1 = make_beta_1(zeta_1 = b_x_z,
                           sd_x   = sd_x,
                           sd_y   = sd_y)) %>% 
  select(.draw, b_0, b_1)

head(draws)
```

Here's the top panel of Figure 17.4.
  
```{r, fig.width = 4.25, fig.height = 4}
# how many posterior lines would you like?
n_lines <- 250

my_data %>% 
  mutate(Subj = factor(Subj, levels = 25:1)) %>% 
  
  ggplot(aes(x = X, y = Y)) +
  geom_abline(data = draws %>% slice(1:n_lines),
              aes(intercept = b_0, slope = b_1, group = .draw),
              color = "grey50", size = 1/4, alpha = 1/5) +
  geom_point(aes(color = Subj),
             alpha = 1/2) +
  geom_line(aes(group = Subj, color = Subj),
            size = 1/4) +
  scale_color_manual(values = beyonce_palette(41, n = 25, type = "continuous"), breaks = NULL) +
  scale_y_continuous(breaks = seq(from = 50, to = 250, by = 50)) +
  labs(subtitle = eval(substitute(paste("Data from all units with", n_lines, "credible population-level\nregression lines")))) +
  coord_cartesian(xlim = c(40, 95),
                  ylim = c(30, 270))
```

Recall how we can use `coef()` to extract the `Subj`-specific parameters. But we'll want posterior draws rather than summaries, which requires `summary = F`. It'll take a bit of wrangling to get the output in a tidy format. Once we're there, the plot code will be fairly simple.

```{r, fig.width = 7, fig.height = 6}
c <-
  # first collect and wrangle the draws for the Subj-level intercept and slopes
  rbind(coef(fit17.4, summary = F)$Subj[, , "Intercept"],
        coef(fit17.4, summary = F)$Subj[, , "x_z"]) %>% 
  data.frame() %>% 
  set_names(1:25) %>% 
  mutate(draw = rep(1:4000, times = 2),
         param = rep(c("Intercept", "Slope"), each = 4000)) %>% 
  pivot_longer(`1`:`25`, names_to = "Subj") %>% 
  pivot_wider(names_from = param, values_from = value) %>% 
  # now we're ready to un-standardize the standardized coefficients
  mutate(b_0 = make_beta_0(zeta_0 = Intercept,
                           zeta_1 = Slope,
                           sd_x   = sd_x,
                           sd_y   = sd_y,
                           m_x    = m_x,
                           m_y    = m_y),
         b_1 = make_beta_1(zeta_1 = Slope,
                           sd_x   = sd_x,
                           sd_y   = sd_y))

# how many lines would you like?
n_lines <- 250

# the plot:
my_data %>% 
  mutate(Subj = factor(Subj, levels = 25:1)) %>% 
  
  ggplot(aes(x = X, y = Y)) +
  geom_abline(data = c %>% filter(draw <= n_lines),
              aes(intercept = b_0, slope = b_1), 
              color = "grey50", size = 1/4, alpha = 1/5) +
  geom_point(aes(color = Subj)) +
  scale_color_manual(values = beyonce_palette(41, n = 25, type = "continuous"), breaks = NULL) +
  scale_x_continuous(breaks = seq(from = 50, to = 90,  by = 20)) +
  scale_y_continuous(breaks = seq(from = 50, to = 250, by = 100)) +
  labs(subtitle = "Each unit now has its own bundle of credible regression lines") +
  coord_cartesian(xlim = c(45, 90),
                  ylim = c(50, 270)) +
  facet_wrap(~ Subj %>% factor(., levels = 1:25))
```

There's some good pedagogy in that method. But I like having options and in this case `fitted()` affords a simpler workflow. Here's the preparatory data wrangling step.

```{r}
# how many posterior lines would you like?
n_lines <- 250

nd <- 
  # since we're working with straight lines, we only need two x-values
  tibble(x_z = c(-5, 5)) %>% 
  mutate(X    = x_z * sd(my_data$X) + mean(my_data$X),
         name = str_c("V", 1:n()))

f <-
  fitted(fit17.4,
         newdata = nd,
         # since we only want the fixed effects, we'll use `re_formula` 
         # to maginalize over the random effects
         re_formula = Y_z ~ 1 + X_z,
         summary = F,
         # here we use `ndraws` to subset right from the get go
         ndraws = n_lines) %>% 
  as_tibble() %>% 
  mutate(draw = 1:n()) %>% 
  pivot_longer(-draw) %>% 
  # transform the `y_z` values back into the `Y` metric
  mutate(Y = value * sd(my_data$Y) + mean(my_data$Y)) %>% 
  # now attach the predictor values to the output
  left_join(nd, by = "name")

head(f)  
```

For the second time, here's the top panel of Figure 17.4, this time based off of `fitted()`.

```{r, fig.width = 4.25, fig.height = 4}
p1 <-
  my_data %>% 
  mutate(Subj = factor(Subj, levels = 25:1)) %>%
  
  ggplot(aes(x = X, y = Y)) +
  geom_line(data = f,
            aes(group = draw),
            color = "grey50", size = 1/4, alpha = 1/5) +
  geom_point(aes(color = Subj),
             alpha = 1/2) +
  geom_line(aes(group = Subj, color = Subj),
            size = 1/4) +
  scale_color_manual(values = beyonce_palette(41, n = 25, type = "continuous")) +
  scale_y_continuous(breaks = seq(from = 50, to = 250, by = 50)) +
  labs(subtitle = eval(substitute(paste("Data from all units with", n_lines, "credible population-level\nregression lines")))) +
  coord_cartesian(xlim = c(40, 95),
                  ylim = c(30, 270)) +
  theme(legend.position = "none")

p1
```

The whole process is quite similar for the `Subj`-specific lines. There are two main differences. First, we need to specify which `Subj` values we'd like to get `fitted()` points for. That goes into our `nd` tibble. Second, we omit the `re_formula` argument. There are other subtleties, like with the contents of the `bind_cols()` function. But hopefully those are self-evident.

```{r}
# how many posterior lines would you like?
n_lines <- 250

nd <- 
  tibble(x_z = c(-5, 5)) %>% 
  mutate(X = x_z * sd(my_data$X) + mean(my_data$X)) %>% 
  expand(nesting(x_z, X),
         Subj = distinct(my_data, Subj) %>% pull()) %>% 
  mutate(name = str_c("V", 1:n()))

f <-
  fitted(fit17.4,
         newdata = nd,
         summary = F,
         ndraws = n_lines) %>% 
  as_tibble() %>% 
  mutate(draw = 1:n()) %>% 
  pivot_longer(-draw) %>% 
  mutate(Y = value * sd(my_data$Y) + mean(my_data$Y)) %>% 
  left_join(nd, by = "name")

head(f)  
```

And now for the second time, here's the bottom panel of Figure 17.4, this time based off of `fitted()`.

```{r, fig.width = 7, fig.height = 10}
p2 <-
  my_data %>% 
  mutate(Subj = factor(Subj, levels = 25:1)) %>%
  
  ggplot(aes(x = X, y = Y)) +
  geom_line(data = f,
            aes(group = draw),
            color = "grey50", size = 1/4, alpha = 1/5) +
  geom_point(aes(color = Subj)) +
  scale_color_manual(values = beyonce_palette(41, n = 25, type = "continuous"), breaks = NULL) +
  scale_x_continuous(breaks = seq(from = 50, to = 90,  by = 20)) +
  scale_y_continuous(breaks = seq(from = 50, to = 250, by = 100)) +
  labs(subtitle = "Each unit now has its own bundle of credible regression lines") +
  coord_cartesian(xlim = c(45, 90),
                  ylim = c(50, 270)) +
  facet_wrap(~ Subj %>% factor(., levels = 1:25))

# combine with patchwork
p3 <- plot_spacer()

p4 <-
  (p3 | p1 | p3) + 
  plot_layout(widths = c(1, 4, 1))

(p4 / p2) + plot_layout(heights = c(0.6, 1))
```

Especially if you're new to these kinds of models, it's easy to get lost in all that code. And for real--the wrangling required for those plots was no joke. The primary difficulty was that we had to convert standardized solutions to unstandardized solutions, which leads to an important distinction. When we used the first method of working with the `as_draws_df()` and `coef()` output, we focused on **transforming the model parameters**. In contrast, when we used the second method of working with the `fitted()` output, we focused instead on **transforming the model predictions and predictor values**. This distinction can be really confusing, at first. Stick with it! There will be times one method is more convenient or intuitive than the other. It's good to have both methods in your repertoire.

## Quadratic trend and weighted data

Quadratic models follow the general form

$$y = \beta_0 + \beta_1 x + \beta_2 x^2,$$

where $\beta_2$ is the quadratic term which, when 0, reduces the results to a simple linear model. That's right; the linear model is a special case of the quadratic.

This time the data come from the American Community Survey and Puerto Rico Community Survey. In his footnote #3, Kruschke indicated "Data are from [http://www.census.gov/hhes/www/income/data/Fam_Inc_SizeofFam1.xls](http://www.census.gov/hhes/www/income/data/Fam_Inc_SizeofFam1.xls), retrieved December 11, 2013. Median family income for years 2009-2011." As to our `read.csv()` code, note the `comment.char` argument.

```{r, message = F}
my_data <- read.csv("data.R/IncomeFamszState3yr.csv", comment.char = "#")

glimpse(my_data)
```

Here we'll standardize all variables but `State`, our grouping variable. It'd be silly to try to standardize that.

```{r}
my_data <-
  my_data %>% 
  mutate(family_size_z   = standardize(FamilySize),
         median_income_z = standardize(MedianIncome),
         se_z            = SampErr / (mean(SampErr)))

glimpse(my_data)
```

With **brms**, there are a couple ways to handle measurement error on a variable (e.g., see [Chapter 14](https://bookdown.org/content/3890/missing-data-and-other-opportunities.html) of my ebook, [*Statistical rethinking with brms, ggplot2, and the tidyverse*](https://bookdown.org/content/3890/) [@kurzStatisticalRethinkingBrms2020]. Here we'll use the `se()` syntax, following the form `response | se(se_response, sigma = TRUE)`. In this form, `se` stands for standard error, the loose frequentist analogue to the Bayesian posterior $SD$. Unless you're fitting a meta-analysis on summary information, make sure to specify `sigma = TRUE`. Without that you'll have no estimate for $\sigma$! For more information on the `se()` method, go to the [**brms** reference manual](https://CRAN.R-project.org/package=brms/brms.pdf) and find the *Additional response information* subsection of the `brmsformula` section [@brms2022RM, p. 42].

```{r fit17.5, warning = F}
fit17.5 <-
  brm(data = my_data,
      family = student,
      median_income_z | se(se_z, sigma = TRUE) ~ 1 + family_size_z +  I(family_size_z^2) + 
        (1 + family_size_z +  I(family_size_z^2) || State),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(normal(0, 1), class = sigma),
                prior(normal(0, 1), class = sd),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvar(1/29, name = "one_over_twentynine"),
      seed = 17,
      file = "fits/fit17.05")
```

Did you notice the `I(family_size_z^2)` part of the `formula`? The **brms** package follows a typical convention in **R** statistical functions in that if you want to multiply a variable by itself as in a quadratic model, you nest the `family_size_z^2` part within the `I()` function.

Take a look at the model summary.

```{r}
print(fit17.5)
```

Do see that `Ifamily_size_zE2` row? That's the summary of our quadratic term.

### Results and interpretation.

A new model type requires a different approach to un-standardizing our standardized coefficients. Based on Equation 17.3, we can convert our coefficients using the formulas

\begin{align*}
\beta_0 & = \zeta_0 \operatorname{SD}_y + M_y - \frac{\zeta_1 M_x \operatorname{SD}_y}{\operatorname{SD}_x} + \frac{\zeta_2 M^{2}_x \operatorname{SD}_y}{\operatorname{SD}^{2}_x}, \\
\beta_1 & = \frac{\zeta_1 \operatorname{SD}_y}{\operatorname{SD}_x} - \frac{2 \zeta_2 M_x \operatorname{SD}_y}{\operatorname{SD}^{2}_x}, \text{and} \\
\beta_2 & = \frac{\zeta_2 \operatorname{SD}_y}{\operatorname{SD}^{2}_x}.
\end{align*}

We'll make new custom functions to use them.

```{r}
make_beta_0 <- function(zeta_0, zeta_1, zeta_2, sd_x, sd_y, m_x, m_y) {
  zeta_0 * sd_y + m_y - zeta_1 * m_x * sd_y / sd_x + zeta_2 * m_x^2 * sd_y / sd_x^2
}

make_beta_1 <- function(zeta_1, zeta_2, sd_x, sd_y, m_x) {
  zeta_1 * sd_y / sd_x - 2 * zeta_2 * m_x * sd_y / sd_x^2
}

make_beta_2 <- function(zeta_2, sd_x, sd_y) {
  zeta_2 * sd_y / sd_x^2
}

# may as well respecify these, too
m_x  <- mean(my_data$FamilySize)
m_y  <- mean(my_data$MedianIncome)
sd_x <- sd(my_data$FamilySize)
sd_y <- sd(my_data$MedianIncome)
```

Now we'll extract our posterior draws and make the conversions.

```{r, fig.width = 7, fig.height = 6, warning = F}
draws <- 
  as_draws_df(fit17.5) %>% 
  mutate(b_0 = make_beta_0(zeta_0 = b_Intercept,
                           zeta_1 = b_family_size_z,
                           zeta_2 = b_Ifamily_size_zE2,
                           sd_x   = sd_x,
                           sd_y   = sd_y,
                           m_x    = m_x,
                           m_y    = m_y),
         b_1 = make_beta_1(zeta_1 = b_family_size_z,
                           zeta_2 = b_Ifamily_size_zE2,
                           sd_x   = sd_x,
                           sd_y   = sd_y,
                           m_x    = m_x),
         b_2 = make_beta_2(zeta_2 = b_Ifamily_size_zE2,
                           sd_x   = sd_x,
                           sd_y   = sd_y)) %>% 
  select(.draw, b_0:b_2)
```

Our `geom_abline()` approach from before won't work with curves. We'll have to resort to `geom_line()`. With the `geom_line()` approach, we'll need many specific values of model-implied `MedianIncome` across a densely-packed range of `FamilySize`. We want to use a lot of `FamilySize` values, like 30 or 50 or so, to make sure the curves look smooth. Below, we'll use 50 (i.e., `length.out = 50`). But if it's still not clear why, try plugging in a lesser value, like 5 or so. You'll see.

```{r}
# how many posterior lines would you like?
n_lines <- 200

set.seed(17)
draws <-
  draws %>% 
  slice_sample(n = n_lines) %>% 
  rownames_to_column(var = "draw") %>% 
  expand(nesting(.draw, b_0, b_1, b_2),
         FamilySize = seq(from = 1, to = 9, length.out = 50)) %>% 
  mutate(MedianIncome = b_0 + b_1 * FamilySize + b_2 * FamilySize^2)

head(draws)
```

Now we're ready to make the top panel of Figure 17.7.
                   
```{r, fig.width = 4.25, fig.height = 4 }
my_data %>%
  ggplot(aes(x = FamilySize, y = MedianIncome)) +
  geom_line(data = draws,
            aes(group = .draw),
            size  = 1/4, alpha = 1/5, color = "grey67") +
  geom_line(aes(group = State, color = State),
            alpha = 2/3, size = 1/4) +
  geom_point(aes(color = State),
             alpha = 2/3, size = 1/2) +
  scale_color_manual(values = beyonce_palette(41, n = 52, type = "continuous"), breaks = NULL) +
  scale_x_continuous("Family size", breaks = 1:8) +
  labs(title = "All states",
       y = "Median income") +
  coord_cartesian(xlim = c(1, 8),
                  ylim = c(0, 150000))
```

Like before, we'll extract the group-level coefficients (i.e., those specific to the `State`s) with the `coef()` function. And also like before, the `coef()` output will require a little wrangling.

```{r}
c <-
  # first collect and wrangle the draws for the State-level intercept and slopes
  rbind(coef(fit17.5, summary = F)$State[, , "Intercept"],
        coef(fit17.5, summary = F)$State[, , "family_size_z"],
        coef(fit17.5, summary = F)$State[, , "Ifamily_size_zE2"]) %>% 
  data.frame() %>% 
  mutate(draw  = rep(1:4000, times = 3),
         param = rep(c("Intercept", "family_size_z", "Ifamily_size_zE2"), each = 4000)) %>% 
  pivot_longer(Alabama:Wyoming, names_to = "State") %>% 
  pivot_wider(names_from = param, values_from = value)  %>% 
  # let's go ahead and make the standardized-to-unstandardized conversions, here
  mutate(b_0 = make_beta_0(zeta_0 = Intercept,
                           zeta_1 = family_size_z,
                           zeta_2 = Ifamily_size_zE2,
                           sd_x   = sd_x,
                           sd_y   = sd_y,
                           m_x    = m_x,
                           m_y    = m_y),
         b_1 = make_beta_1(zeta_1 = family_size_z,
                           zeta_2 = Ifamily_size_zE2,
                           sd_x   = sd_x,
                           sd_y   = sd_y,
                           m_x    = m_x),
         b_2 = make_beta_2(zeta_2 = Ifamily_size_zE2,
                           sd_x   = sd_x,
                           sd_y   = sd_y)) %>% 
  # we just want the first 25 states, from Alabama through Mississippi, so we'll `filter()`
  filter(State <= "Mississippi")

str(c)
```

Now we'll subset by `n_lines`, `expand()` by `FamilySize`, and use the model formula to compute the expected `MedianIncome` values.

```{r}
# how many posterior lines would you like?
n_lines <- 200

set.seed(17)
c <-
  c %>% 
  group_by(State) %>%
  slice_sample(n = n_lines) %>%
  ungroup() %>% 
  expand(nesting(draw, State, b_0, b_1, b_2),
         FamilySize = seq(from = 1, to = 9, length.out = 50)) %>% 
  mutate(MedianIncome = b_0 + b_1 * FamilySize + b_2 * FamilySize^2)

head(c)
```

Finally, we're ready for the `State`-specific miniatures in Figure 17.7.

```{r, fig.width = 7, fig.height = 6}
my_data %>%
  filter(State <= "Mississippi") %>% 
 
  ggplot(aes(x = FamilySize, y = MedianIncome)) +
  geom_line(data = c,
            aes(group = draw),
            size  = 1/4, alpha = 1/5, color = "grey67") +
  geom_point(aes(color = State)) +
  geom_line(aes(color = State)) +
  scale_color_manual(values = beyonce_palette(41, n = 52, type = "continuous"), breaks = NULL) +
  scale_x_continuous("Family size", breaks = 1:8) +
  labs(subtitle = "Each State now has its own bundle of credible regression curves.",
       y = "Median income") +
  coord_cartesian(xlim = c(1, 8),
                  ylim = c(0, 150000)) +
  theme(legend.position = "none") +
  facet_wrap(~ State)
```

Magic! As our model coefficients proliferate, the `fitted()` approach from above starts to look more and more appetizing. Check it out for yourself.

Although "almost all of the posterior distribution [was] below $\nu = 4$" in the text (p. 500), the bulk of our $\nu$ distribution spanned across much larger values.

```{r, fig.width = 3.5, fig.height = 2.75}
as_draws_df(fit17.5) %>%
  ggplot(aes(x = nu, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = bp[6], color = bp[1], slab_color = bp[5],
                    breaks = 40, normalize = "panels") +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = expression(Our~big~nu),
       x = NULL)
```

I'm guessing the distinction in our $\nu$ distribution and that in the text is our use of the `se()` syntax in the `brm()` `formula`. If you have a better explanation, [share it](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues).

### Further extensions.

Kruschke discussed the ease with which users of Bayesian software might specify nonlinear models. Check out Bürkner's [-@Bürkner2022Non_linear] vignette, [*Estimating non-linear models with brms*](https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html), for more on the topic. Though I haven't used it, I believe it is also possible to use the $t$ distribution to model group-level variation in **brms** (see [this GitHub discussion](https://github.com/paul-buerkner/brms/issues/231) for details).

## Procedure and perils for expanding a model

Across several chapters, we've already dipped our toes into posterior predictive checks. For more on the PPC "double dipping" issue, check out Gelman's [*Discussion with Sander Greenland on posterior predictive checks*](http://andrewgelman.com/2014/08/11/discussion-sander-greenland-posterior-predictive-checks/) or Simpson's [*Touch me, I want to feel your data*](http://andrewgelman.com/2017/09/07/touch-want-feel-data/), which is itself connected to @gabry2019visualization, [*Visualization in Bayesian workflow*](https://arxiv.org/abs/1709.01449).

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
# remove the objects
rm(n_draw, d, curves, beyonce_palettes, bp, p1, p2, my_arrow, p3, p4, p5, p6, p7, p8, p9, p10, layout, HtWtDataGenerator, standardize, fit17.1, stanvars, fit17.2, draws, make_beta_0, make_beta_1, sd_x, sd_y, m_x, m_y, n_lines, rope, text, beta_0_sigma, beta_1_sigma, fit17.3, nd, my_data, p11, p12, p13, p14, p15, fit17.4, c, f, fit17.5, make_beta_2)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:17.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

# Metric Predicted Variable with Multiple Metric Predictors

> We will consider models in which the predicted variable is an additive combination of predictors, all of which have proportional influence on the prediction. This kind of model is called *multiple linear regression*. We will also consider nonadditive combinations of predictors, which are called *interactions*. [@kruschkeDoingBayesianData2015, p. 509, *emphasis* in the original]

## Multiple linear regression

Say we have one criterion $y$ and two predictors, $x_1$ and $x_2$. If $y \sim \operatorname{Normal}(\mu, \sigma)$ and $\mu = \beta_0 + \beta_1 x_1 + \beta_2 x_2$, then it's also the case that we can rewrite the formula for $y$ as

$$y \sim \operatorname{Normal}(\beta_0 + \beta_1 x_1 + \beta_2 x_2, \sigma).$$

As Kruschke pointed out, the basic model "assumes homogeneity of variance, which means that at all values of $x_1$ and $x_2$, the variance $\sigma^2$ of $y$ is the same" (p. 510).

If we presume the data for the two $x$ variables are uniformly distributed within 0 and 10, we can make the data for Figure 18.1 like this.

```{r, message = F, warning = F}
library(tidyverse)

n <- 300

set.seed(18)
d <-
  tibble(x_1 = runif(n = n, min = 0, max = 10),
         x_2 = runif(n = n, min = 0, max = 10)) %>% 
  mutate(y = rnorm(n = n, mean = 10 + x_1 + 2 * x_2, sd = 1))

head(d)  
```

Before we plot those `d` data, we'll want to make a data object containing the information necessary to make the grid lines for Kruschke's 3D regression plane. To my mind, this will be easier to do in stages. If you look at the top upper panel of Figure 18.1 as a reference, our first step will be to make the vertical lines. Save them as `d1`.

```{r, fig.width = 3.5, fig.height = 3.25, warning = F, message = F}
theme_set(
  theme_linedraw() +
    theme(panel.grid = element_blank())
)

d1 <-
  tibble(index = 1:21,
         x_1   = seq(from = 0, to = 10, length.out = 21)) %>% 
  expand(nesting(index, x_1),
         x_2 = c(0, 10)) %>% 
  mutate(y = 10 + 1 * x_1 + 2 * x_2)

d1 %>% 
  ggplot(aes(x = x_1, y = y, group = index)) +
  geom_path(color = "grey85") +
  ylim(0, 50)
```

You may have noticed our `theme_set()` lines at the top. Though we'll be using a different default theme later in the project, this is the best theme to use for these initial few plots. Okay, now let's make the more horizontally-oriented grid lines and save them as `d2`.

```{r, fig.width = 3.5, fig.height = 3.25, warning = F, message = F}
d2 <-
  tibble(index = 1:21 + 21,
         x_2   = seq(from = 0, to = 10, length.out = 21)) %>% 
  expand(nesting(index, x_2),
         x_1 = c(0, 10)) %>% 
  mutate(y = 10 + 1 * x_1 + 2 * x_2)

d2 %>% 
  ggplot(aes(x = x_1, y = y, group = index)) +
  geom_path(color = "grey85") +
  ylim(0, 50)
```

Now combine the two and save them as `grid`.

```{r, fig.width = 3.5, fig.height = 3.25, warning = F, message = F}
grid <-
  bind_rows(d1, d2) 

grid %>%
  ggplot(aes(x = x_1, y = y, group = index)) +
  geom_path(color = "grey85") +
  ylim(0, 50)

grid %>%
  ggplot(aes(x = x_2, y = y, group = index)) +
  geom_path(color = "grey85") +
  ylim(0, 50)

grid %>%
  ggplot(aes(x = x_1, y = x_2, group = index)) +
  geom_path(color = "grey85")
```

We're finally ready combine `d` and `grid` to make the three 2D scatter plots from Figure 18.1.

```{r, fig.width = 3.5, fig.height = 3.25, warning = F, message = F}
d %>% 
  ggplot(aes(x = x_1, y = y)) +
  geom_path(data = grid,
            aes(group = index),
            color = "grey85") +
  geom_point(shape = 21, stroke = 1/10,
             color = "white", fill = "steelblue4") +
  scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) +
  scale_y_continuous(limits = c(0, 50), expand = c(0, 0))

d %>% 
  ggplot(aes(x = x_2, y = y)) +
  geom_path(data = grid,
            aes(group = index),
            color = "grey85") +
  geom_point(shape = 21, stroke = 1/10,
             color = "white", fill = "steelblue4") +
  scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) +
  scale_y_continuous(limits = c(0, 50), expand = c(0, 0))

d %>% 
  ggplot(aes(x = x_1, y = x_2)) +
  geom_path(data = grid,
            aes(group = index),
            color = "grey85") +
  geom_point(shape = 21, stroke = 1/10,
             color = "white", fill = "steelblue4") +
  scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) +
  scale_y_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2)
```

As in previous chapters, I'm not aware that **ggplot2** allows for three-dimensional wireframe plots of the kind in the upper left panel. If you'd like to make one in base **R**, have at it.

For Figure 18.2, the $x$ variables look to be multivariate normal with a correlation of about -.95. We can simulate such data with help from the [**MASS** package](https://CRAN.R-project.org/package=MASS) [@R-MASS; @MASS2002].

Sven Hohenstein's [answer to this stats.stackexchange.com question](https://stats.stackexchange.com/questions/164471/generating-a-simulated-dataset-from-a-correlation-matrix-with-means-and-standard) provides the steps for simulating the data. First, we'll need to specify the desired means and standard deviations for our variables. Then we'll make a correlation matrix with 1s on the diagonal and the desired correlation coefficient, $\rho$ on the off-diagonal. Since the correlation matrix is symmetric, both off-diagonal positions are the same. Then we convert the correlation matrix to a covariance matrix.

```{r}
mus <- c(5, 5)
sds <- c(2, 2)

cors <- matrix(c(1, -.95,
                 -.95, 1), 
               ncol = 2)
cors

covs <- sds %*% t(sds) * cors
covs
```

Now we've defined our means, standard deviations, and covariance matrix, we're ready to simulate the data with the `MASS::mvrnorm()` function.

```{r, warning = F, message = F}
# how many data points would you like to simulate?
n <- 300

set.seed(18.2)

d <- 
  MASS::mvrnorm(n = n, 
                mu = mus, 
                Sigma = covs, 
                empirical = T) %>% 
  as_tibble() %>% 
  set_names("x_1", "x_2") %>%  
  mutate(y = rnorm(n = n, mean = 10 + x_1 + 2 * x_2))
```

Now we have our simulated data in hand, we're ready for three of the four panels of Figure 18.2.

```{r, fig.width = 6, fig.height = 5.5, warning = F, message = F}
p1 <-
  d %>% 
  ggplot(aes(x = x_1, y = y)) +
  geom_path(data = grid,
            aes(group = index),
            color = "grey85") +
  geom_point(shape = 21, stroke = 1/10,
             color = "white", fill = "steelblue4") +
  scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) +
  scale_y_continuous(limits = c(0, 50), expand = c(0, 0))

p2 <-
  d %>% 
  ggplot(aes(x = x_2, y = y)) +
  geom_path(data = grid,
            aes(group = index),
            color = "grey85") +
  geom_point(shape = 21, stroke = 1/10,
             color = "white", fill = "steelblue4") +
  scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) +
  scale_y_continuous(limits = c(0, 50), expand = c(0, 0))

p3 <-
  d %>% 
  ggplot(aes(x = x_1, y = x_2)) +
  geom_path(data = grid,
            aes(group = index),
            color = "grey85") +
  geom_point(shape = 21, stroke = 1/10,
             color = "white", fill = "steelblue4") +
  scale_x_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2) +
  scale_y_continuous(limits = c(0, 10), expand = c(0, 0), breaks = 0:5 * 2)

# bind them together with patchwork
library(patchwork)

plot_spacer() + p1 + p2 + p3
```

We came pretty close.

### The perils of correlated predictors.

> Figures 18.1 and 18.2 show data generated from the same model. In both figures, $\sigma = 2, \beta_0 = 10, \beta_1 = 1, \text{ and } \beta_2 = 2$. All that differs between the two figures is the distribution of the $\langle x_1, x_2 \rangle$ values, which is not specified by the model. In Figure 18.1, the $\langle x_1, x_2 \rangle$ values are distributed independently. In Figure 18.2, the $\langle x_1, x_2 \rangle$ values are negatively correlated: When $x_1$ is small, $x_2$ tends to be large, and when $x_1$ is large, $x_2$ tends to be small. (p. 510)

If you look closely at our simulation code from above, you'll see we have done so, too.

> Real data often have correlated predictors. For example, consider trying to predict a state's average high-school SAT score on the basis of the amount of money the state spends per pupil. If you plot only mean SAT against money spent, there is actually a *decreasing* trend... (p. 513, *emphasis* in the original)

Before we remake Figure 18.3 to examine that decreasing trend, we'll need to load the data from [@guber1999getting].

```{r, message = F}
my_data <- read_csv("data.R/Guber1999data.csv")

glimpse(my_data)
```

Before we get all excited and try to plot those data as in Figure 18.3, we'll need to redefine the 3D grid of our regression plane, this time based on the equation at the top of Figure 18.3.

```{r}
d1 <-
  tibble(index = 1:21,
         Spend = seq(from = 3.4, to = 10.1, length.out = 21)) %>% 
  expand(nesting(index, Spend),
         PrcntTake = c(0, 85))

d2 <-
  tibble(index     = 1:21 + 21,
         PrcntTake = seq(from = 0, to = 85, length.out = 21)) %>% 
  expand(nesting(index, PrcntTake),
         Spend = c(3.4, 10.1))

grid <-
  bind_rows(d1, d2) %>% 
  mutate(SATT = 993.8 + -2.9 * PrcntTake + 12.3 * Spend)

grid %>% glimpse()
```

Now we have our updated `grid` object, we're ready to plot the data in our version of Figure 18.3.

```{r, fig.width = 6, fig.height = 6, warning = F, message = F}
p1 <-
  my_data %>% 
  ggplot(aes(x = Spend, y = SATT)) +
  geom_path(data = grid,
            aes(group = index),
            color = "grey85") +
  geom_point(shape = 21, stroke = 1/10,
             color = "white", fill = "steelblue4") +
  scale_x_continuous(limits = c(3.4, 10.1), expand = c(0, 0), breaks = 2:5 * 2) +
  scale_y_continuous(limits = c(785, 1120))

p2 <-
  my_data %>% 
  ggplot(aes(x = PrcntTake, y = SATT)) +
  geom_path(data = grid,
            aes(group = index),
            color = "grey85") +
  geom_point(shape = 21, stroke = 1/10,
             color = "white", fill = "steelblue4") +
  scale_x_continuous("% Take", limits = c(0, 85), expand = c(0, 0)) +
  scale_y_continuous(limits = c(785, 1120))

p3 <-
  my_data %>% 
  ggplot(aes(x = PrcntTake, y = Spend)) +
  geom_path(data = grid,
            aes(group = index),
            color = "grey85") +
  geom_point(shape = 21, stroke = 1/10,
             color = "white", fill = "steelblue4") +
  scale_x_continuous("% Take", limits = c(0, 85), expand = c(0, 0)) +
  scale_y_continuous(limits = c(3.4, 10.1), expand = c(0, 0))

# bind them together and add a title
wrap_elements(grid::textGrob('No 3D wireframe plots for us')) + 
  p1 + p2 + p3 + 
  plot_annotation(title = "SATT ~ N(m,sd=31.5), m = 993.8 + −2.9 %Take + 12.3 Spend")
```

You can learn more about how we added that title to our plot ensemble from Pedersen's [-@Pedersen2020AddingAnnotation] vignette, [*Adding annotation and style*](https://patchwork.data-imaginist.com/articles/guides/annotation.html), and more about how we added that text in place of a wireframe plot from another of his [-@Pedersen2020PlotAssembly] vignettes, [*Plot assembly*](https://patchwork.data-imaginist.com/articles/guides/assembly.html).

> The separate influences of the two predictors could be assessed in this example because the predictors had only mild correlation with each other. There was enough independent variation of the two predictors that their distinct relationships to the outcome variable could be detected. In some situations, however, the predictors are so tightly correlated that their distinct effects are difficult to tease apart. Correlation of predictors causes the estimates of their regression coefficients to trade-off, as we will see when we examine the posterior distribution. (p. 514)

### The model and implementation.

Let's make our version of the model diagram in Figure 18.4 to get a sense of where we're going. If you look back to [Section 17.2][Robust linear regression], you'll see this is just a minor reworking of the code from Figure 17.2.

```{r, fig.width = 6.75, fig.height = 5, message = F}
# normal density
p1 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = "steelblue4", color = "steelblue4", alpha = .6) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# a second normal density
p2 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = "steelblue4", color = "steelblue4", alpha = .6) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)[italic(j)]", "italic(S)[italic(j)]"), 
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

## two annotated arrows
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")
p3 <-
  tibble(x    = c(.33, 1.67),
         y    = c(1, 1),
         xend = c(.67, 1.2),
         yend = c(0, 0)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = "text",
           x = c(.35, 1.3), y = .5,
           label = "'~'",
           size = 10, family = "Times", parse = T) +
  xlim(0, 2) +
  theme_void()

# exponential density
p4 <-
  tibble(x = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) +
  geom_area(fill = "steelblue4", color = "steelblue4", alpha = .6) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "exp",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(K)",
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# likelihood formula
p5 <-
  tibble(x = .5,
         y = .25,
         label = "beta[0]+sum()[italic(j)]*beta[italic(j)]*italic(x)[italic(ji)]") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()
  
  # half-normal density
p6 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = "steelblue4", color = "steelblue4", alpha = .6) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma]", 
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# four annotated arrows
p7 <-
  tibble(x    = c(.43, .43, 1.5, 2.5),
         y    = c(1, .55, 1, 1),
         xend = c(.43, 1.225, 1.5, 1.75),
         yend = c(.8, .15, .2, .2)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = "text",
           x = c(.3, .7, 1.38, 2), y = c(.92, .22, .65, .6),
           label = c("'~'", "'='", "'='", "'~'"),
           size = 10, family = "Times", parse = T) +
  annotate(geom = "text",
           x = .43, y = .7,
           label = "nu*minute+1",
           size = 7, family = "Times", parse = T) +
  xlim(0, 3) +
  theme_void()

# student-t density
p8 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) +
  geom_area(fill = "steelblue4", color = "steelblue4", alpha = .6) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "student t",
           size = 7) +
  annotate(geom = "text",
           x = 0, y = .6,
           label = "nu~~~mu[italic(i)]~~~sigma",
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# the final annotated arrow
p9 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p10 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 3, r = 5),
  area(t = 1, b = 2, l = 7, r = 9),
  area(t = 4, b = 5, l = 1, r = 3),
  area(t = 4, b = 5, l = 5, r = 7),
  area(t = 4, b = 5, l = 9, r = 11),
  area(t = 3, b = 4, l = 3, r = 9),
  area(t = 7, b = 8, l = 5, r = 7),
  area(t = 6, b = 7, l = 1, r = 11),
  area(t = 9, b = 9, l = 5, r = 7),
  area(t = 10, b = 10, l = 5, r = 7)
)

# combine and plot!
(p1 + p2 + p4 + p5 + p6 + p3 + p8 + p7 + p9 + p10) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

"As with the model for simple linear regression, the Markov Chain Monte Carlo (MCMC) sampling can be more efficient if the data are mean-centered or standardized" (p. 515). We'll make a custom function to standardize the criterion and predictor values.

```{r}
standardize <- function(x) {
  (x - mean(x)) / sd(x)
}

my_data <-
  my_data %>% 
  mutate(prcnt_take_z = standardize(PrcntTake),
         spend_z      = standardize(Spend),
         satt_z       = standardize(SATT))
```

Let's open **brms**.

```{r, message = F, warning = F}
library(brms)
```

Now we're ready to fit the model. As Kruschke pointed out, the priors on the standardized predictors are set with

> an arbitrary standard deviation of 2.0. This value was chosen because standardized regression coefficients are algebraically constrained to fall between −1 and +1 in least-squares regression[^5], and therefore, the regression coefficients will not exceed those limits by much. A normal distribution with standard deviation of 2.0 is reasonably flat over the range from −1 to +1. (p. 516)

With data like this, even a `prior(normal(0, 1), class = b)` would be only mildly regularizing.

This is a good place to emphasize how priors in **brms** are given classes. If you'd like all parameters within a given class to have the prior, you can just specify one prior argument within that class. For our `fit8.1`, both parameters of `class = b` have a `normal(0, 2)` prior. So we can just include one statement to handle both. Had we wanted different priors for the coefficients for `spend_z` and `prcnt_take_z`, we'd need to include two `prior()` arguments with at least one including a `coef` argument.

```{r fit18.1}
fit18.1 <-
  brm(data = my_data,
      family = student,
      satt_z ~ 1 + spend_z + prcnt_take_z,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 2), class = b),
                prior(normal(0, 1), class = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvar(1/29, name = "one_over_twentynine"),
      seed = 18,
      file = "fits/fit18.01")
```

Check the model summary.

```{r}
print(fit18.1)
```

So when we use a multivariable model, increases in spending now appear associated with *increases* in SAT scores.

### The posterior distribution.

Based on Equation 18.1, we can convert the standardized coefficients from our multivariable model back to their original metric as follows:

\begin{align*}
\beta_0 & = \operatorname{SD}_y \zeta_0 + M_y - \operatorname{SD}_y \sum_j \frac{\zeta_j M_{x_j}}{\operatorname{SD}_{x_j}} \;\;\; \text{and} \\
\beta_j & = \frac{\operatorname{SD}_y \zeta_j}{\operatorname{SD}_{x_j}}.
\end{align*}

To use them, we'll first extract the posterior draws

```{r}
draws <- as_draws_df(fit18.1)

head(draws)
```

Like we did in [Chapter 17][Metric Predicted Variable with One Metric Predictor], let's wrap the consequences of Equation 18.1 into two functions.

```{r}
make_beta_0 <- function(zeta_0, zeta_1, zeta_2, sd_x_1, sd_x_2, sd_y, m_x_1, m_x_2, m_y) {
  sd_y * zeta_0 + m_y - sd_y * ((zeta_1 * m_x_1 / sd_x_1) + (zeta_2 * m_x_2 / sd_x_2))
}

make_beta_j <- function(zeta_j, sd_j, sd_y) {
  sd_y * zeta_j / sd_j
}
```

After saving a few values, we're ready to use our custom functions.

```{r}
sd_x_1 <- sd(my_data$Spend)
sd_x_2 <- sd(my_data$PrcntTake)
sd_y   <- sd(my_data$SATT)
m_x_1  <- mean(my_data$Spend)
m_x_2  <- mean(my_data$PrcntTake)
m_y    <- mean(my_data$SATT)

draws <-
  draws %>% 
  mutate(b_0 = make_beta_0(zeta_0 = b_Intercept,
                           zeta_1 = b_spend_z,
                           zeta_2 = b_prcnt_take_z,
                           sd_x_1 = sd_x_1,
                           sd_x_2 = sd_x_2,
                           sd_y   = sd_y,
                           m_x_1  = m_x_1,
                           m_x_2  = m_x_2,
                           m_y    = m_y),
         b_1 = make_beta_j(zeta_j = b_spend_z,
                           sd_j   = sd_x_1,
                           sd_y   = sd_y),
         b_2 = make_beta_j(zeta_j = b_prcnt_take_z,
                           sd_j   = sd_x_2,
                           sd_y   = sd_y))

glimpse(draws)
```

Before we make the figure, we'll update our overall plot theme to `cowplot::theme_minimal_grid()`. Our overall color scheme and plot aesthetic will be based on some of the plots in [Chapter 16, *Visualizing uncertainty*](https://clauswilke.com/dataviz/visualizing-uncertainty.html), of @wilkeFundamentalsDataVisualization2019. As we'll be making a lot of customized density plots in this chapter, we may as well save those settings, here. We'll call the function with those settings `stat_wilke()`.

```{r, warning = F, message = F}
library(tidybayes)
library(ggdist)
library(cowplot)

# update the default theme setting
theme_set(theme_minimal_grid())

# define the function
stat_wilke <- function(height = 1.25, point_size = 5, ...) {
  
  list(
    # for the graded fill
    stat_slab(aes(fill_ramp = stat(
      cut_cdf_qi(cdf, 
                 .width = c(.8, .95, .99),
                 labels = scales::percent_format(accuracy = 1)))), 
      height = height, slab_alpha = .75, fill = "steelblue4", 
      ...),
    # for the top outline and the mode dot
    stat_halfeye(.width = 0, point_interval = mode_qi,
                 height = height, size = point_size, slab_size = 1/3,
                 slab_color = "steelblue4", fill = NA, color = "chocolate3", 
                 ...),
    # fill settings
    scale_fill_ramp_discrete(range = c(1, .4), na.translate = F),
    # adjust the guide_legend() settings
    guides(fill_ramp = 
             guide_legend(
               direction = "horizontal",
               keywidth = unit(0.925, "cm"),
               label.hjust = 0.5,
               label.position = "bottom",
               title = "posterior prob.",
               title.hjust = 0.5,
               title.position = "top")),
    # ensure we're using `cowplot::theme_minimal_hgrid()` as a base theme
    theme_minimal_hgrid(),
    # adjust the legend settings
    theme(legend.background = element_rect(fill = "white"),
          legend.text = element_text(margin = margin(-0.2, 0, -0.2, 0, "cm")),
          legend.title = element_text(margin = margin(-0.2, 0, -0.2, 0, "cm")))
  )
  
}
```

```{r, warning = F, message = F, eval = F, echo = F}
library(tidybayes)
library(cowplot)

# update the default theme setting
theme_set(theme_minimal_grid())

# keep a lookout at https://github.com/mjskay/ggdist/issues/11

stat_wilke <- function(height = 1.25, ...) {
  
  list(
    stat_halfeye(.width = 0, height = height, fill = "transparent", slab_color = "steelblue4", slab_size = 1/2, ...),
    stat_halfeye(.width = 0, height = height, fill = "steelblue4", alpha = .6, ...),
    stat_pointinterval(.width = .95, color = "chocolate3", size = 2, point_size = 4),
    theme_minimal_hgrid()
  )
  
}
```

Here's the top panel of Figure 18.5.

```{r, fig.width = 6, fig.height = 4, warning = F, message = F}
# here are the primary data
draws %>% 
  transmute(Intercept      = b_0,
            Spend          = b_1,
            `Percent Take` = b_2,
            Scale          = sigma * sd_y,
            Normality      = nu %>% log10()) %>% 
  pivot_longer(everything()) %>% 
  
  # the plot
  ggplot(aes(x = value)) +
  stat_wilke(normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(ylim = c(-0.01, NA)) +
  panel_border() +
  theme(legend.position = c(.72, .2)) +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

> The slope on spending has a mode of about 13, which suggests that SAT scores rise by about 13 points for every extra $1000 spent per pupil. The slope on percentage taking the exam (PrcntTake) is also credibly non-zero, with a mode around −2.8, which suggests that SAT scores fall by about 2.8 points for every additional 1% of students who take the test. (p. 517)

If you want those exact modes and, say, 50% intervals around them, you can just use `tidybayes::mode_hdi()`.

```{r, warning = F}
draws %>% 
  transmute(Spend          = b_1,
            `Percent Take` = b_2) %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  mode_hdi(value, .width = .5)
```

The `brms::bayes_R2()` function makes it easy to compute a Bayesian $R^2$. Simply feed a `brm()` fit object into `bayes_R2()` and you'll get back the posterior mean, $SD$, and 95% intervals.

```{r}
bayes_R2(fit18.1)
```

I'm not going to go into the technical details here, but you should be aware that the Bayeisan $R^2$ returned from the `bayes_R2()` function is not calculated the same as it is with OLS. If you want to dive in, check out the paper by @gelmanRsquaredBayesianRegression2019, [*R-squared for Bayesian regression models*](https://stat.columbia.edu/~gelman/research/published/bayes_R2_v3.pdf). Anyway, if you'd like to view the Bayesian $R^2$ distribution rather than just get the summaries, specify `summary = F`, convert the output to a tibble, and plot as usual.

```{r, fig.width = 3.5, fig.height = 2.25}
bayes_R2(fit18.1, summary = F) %>% 
  as_tibble() %>% 
  
  ggplot(aes(x = R2, y = 0)) +
  stat_wilke() +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = expression(paste("Bayesian ", italic(R)^2)),
       x = NULL) +
  coord_cartesian(xlim = c(0, 1),
                  ylim = c(-0.01, NA)) +
  theme(legend.position = c(.01, .8))
```

Since the `brms::bayes_R2()` function is not identical with Kruschke's method in the text, the results might differ a bit.

We can get a sense of the scatter plots with `bayesplot::mcmc_pairs()`.

```{r, fig.width = 6, fig.height = 5.5, warning = F, message = F}
library(bayesplot)

color_scheme_set(c("steelblue4", "steelblue4", "steelblue4", "steelblue4", "steelblue4", "steelblue4"))

draws %>% 
  transmute(Intercept      = b_0,
            Spend          = b_1,
            `Percent Take` = b_2,
            Scale          = sigma * sd_y,
            Normality      = nu %>% log10()) %>% 
  mcmc_pairs(diag_fun = "dens",
             off_diag_args = list(size = 1/8, alpha = 1/8))
```

One way to get the Pearson's correlation coefficients among the parameters is with `psych::lowerCor()`.

```{r, warning = F}
draws %>% 
  transmute(Intercept      = b_0,
            Spend          = b_1,
            `Percent Take` = b_2,
            Scale          = sigma * sd_y,
            Normality      = nu %>% log10()) %>% 
  psych::lowerCor(digits = 3)
```

If you like more control for customizing your pairs plots, you'll find a friend in the `ggpairs()` function from the [**GGally** package](https://cran.r-project.org/package=GGally) [@R-GGally]. We're going to blow past the default settings and customize the format for the plots in the upper triangle, the diagonal, and the lower triangle.

```{r, warning = F, message = F}
library(GGally)

my_upper <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) + 
    geom_point(size = 1/2, shape = 21, stroke = 1/10,
               color = "white", fill = "steelblue4") +
    panel_border()
}

my_diag <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) + 
    stat_wilke(point_size = 2) +
    scale_x_continuous(NULL, breaks = NULL) +
    scale_y_continuous(NULL, breaks = NULL) +
    coord_cartesian(ylim = c(-0.01, NA)) +
    panel_border()
}

my_lower <- function(data, mapping, ...) {
  
  # get the x and y data to use the other code
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  
  # compute the correlations
  corr <- cor(x, y, method = "p", use = "pairwise")
  
  # plot the cor value
  ggally_text(
    label = formatC(corr, digits = 2, format = "f") %>% str_replace(., "0\\.", "."),
    mapping = aes(),
    color = "black",
    size = 4) +
    scale_x_continuous(NULL, breaks = NULL) +
    scale_y_continuous(NULL, breaks = NULL) +
    panel_border()
}
```

Let's see what we've done.

```{r, fig.width = 6, fig.height = 5.5, warning = F, message = F}
draws %>% 
  transmute(`Intercept~(beta[0])`    = b_0,
            `Spend~(beta[1])`        = b_1,
            `Percent~Take~(beta[2])` = b_2,
            sigma                    = sigma * sd_y,
            `log10(nu)`              = nu %>% log10()) %>% 
  ggpairs(upper = list(continuous = my_upper),
          diag  = list(continuous = my_diag),
          lower = list(continuous = my_lower),
          labeller = label_parsed) +
  theme(strip.text = element_text(size = 8))
```

For more ideas on customizing a `ggpairs()` plot, go [here](https://ggobi.github.io/ggally/articles/ggpairs.html) or [here](https://stackoverflow.com/questions/30858337/how-to-customize-lines-in-ggpairs-ggally) or [here](https://stackoverflow.com/questions/45873483/ggpairs-plot-with-heatmap-of-correlation-values).

Kruschke finished the subsection with the observation: "Sometimes we are interested in using the linear model to predict $y$ values for $x$ values of interest. It is straight forward to generate a large sample of credible $y$ values for specified $x$ values" (p. 519).

Like we practiced with in the last chapter, the simplest way to do so in **brms** is with the `fitted()` function. For a quick example, say we wanted to know what the model would predict if we were to have a standard-score increase in spending and a simultaneous standard-score decrease in the percent taking the exam. We'd just specify those values in a tibble and feed that tibble into `fitted()` along with the model.

```{r}
nd <-
  tibble(prcnt_take_z = -1,
         spend_z      =  1)

fitted(fit18.1,
       newdata = nd)
```

### Redundant predictors.

> As a simplified example of correlated predictors, think of just two data points: Suppose $y = 1 \text{ for }  \langle x_1, x_2 \rangle = \langle 1, 1 \rangle \text{ and } y = 2 \text{ for } \langle x_1, x_2 \rangle = \langle 2, 2 \rangle$. The linear model, $y = \beta_1 x_1 + \beta_2 x_2$ is supposed to satisfy both data points, and in this case both are satisfied by $1 = \beta_1 + \beta_2$. Therefore, many different combinations of $\beta_1$ and $\beta_2$ satisfy the data. For example, it could be that $\beta_1 = 2$ and $\beta_2 = -1$, or $\beta_1 = 0.5$ and $\beta_2 = 0.5$, or $\beta_1 = 0$ and $\beta_2 = 1$. In other words, the credible values of $\beta_1$ and $\beta_2$ are anticorrelated and trade-off to fit the data. (p. 519)

Here are what those data look like. You would not want to fit a regression model with these data.

```{r}
tibble(x_1 = 1:2,
       x_2 = 1:2,
       y   = 1:2)
```

We can take percentages and turn them into their inverse re-expressed as a proportion.

```{r}
percent_take <- 37

(100 - percent_take) / 100
```

Let's make a redundant predictor and then `standardize()` it.

```{r}
my_data <-
  my_data %>% 
  mutate(prop_not_take   = (100 - PrcntTake) / 100) %>% 
  mutate(prop_not_take_z = standardize(prop_not_take))
  
glimpse(my_data)
```

We're ready to fit the redundant-predictor model.

```{r fit18.2}
fit18.2 <-
  brm(data = my_data,
      family = student,
      satt_z ~ 0 + Intercept + spend_z + prcnt_take_z + prop_not_take_z,
      prior = c(prior(normal(0, 2), class = b, coef = "Intercept"),
                prior(normal(0, 2), class = b, coef = "spend_z"),
                prior(normal(0, 2), class = b, coef = "prcnt_take_z"),
                prior(normal(0, 2), class = b, coef = "prop_not_take_z"),
                prior(normal(0, 1), class = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvar(1/29, name = "one_over_twentynine"),
      seed = 18,
      # this will let us use `prior_samples()` later on
      sample_prior = "yes",
      file = "fits/fit18.02")
```

You might notice a few things about the `brm()` code. First, we have used the `~ 0 + Intercept + ...` syntax instead of the default syntax for intercepts. In normal situations, we would have been in good shape using the typical `~ 1 + ...` syntax for the intercept, especially given our use of standardized data. However, since **brms** version 2.5.0, using the `sample_prior` argument to draw samples from the prior distribution will no longer allow us to return samples from the typical **brms** intercept. Bürkner addressed the issue on the [Stan forums](https://discourse.mc-stan.org/t/prior-intercept-samples-no-longer-saved-in-brms-2-5-0/6107). As he pointed out, if you want to get prior samples from an intercept, you'll have to use the alternative syntax. The other thing to point out is that even though we used the same prior on all the predictors, including the intercept, we still explicitly spelled each out with the `coef` argument. If we hadn't been explicit like this, we would only get a single `b` vector from the `prior_samples()` function. But since we want separate vectors for each of our predictors, we used the verbose code. If you're having a difficult time understanding these two points, experiment. Fit the model in a few different ways with either the typical or the alternative intercept syntax and with either the verbose prior code or the simplified `prior(normal(0, 2), class = b)` code. And after each, execute `prior_samples(fit18.2)`. You'll see.

Let's move on. Kruschke mentioned high autocorrelations in the prose. Here are the autocorrelation plots for our $\beta$'s.

```{r, fig.width = 6.5, fig.height = 4}
color_scheme_set(c("steelblue4", "steelblue4", "chocolate3", "steelblue4", "steelblue4", "steelblue4"))

draws <- as_draws_df(fit18.2)

draws %>% 
  mutate(chain = .chain) %>% 
  mcmc_acf(pars = vars(b_Intercept:b_prop_not_take_z),
           lags = 10)
```

Looks like HMC made a big difference. The $N_{eff}/N$ ratios weren't terrible, either.

```{r, fig.width = 6, fig.height = 1.75}
color_scheme_set(c("steelblue4", "steelblue4", "chocolate3", "steelblue4", "chocolate3", "chocolate3"))

neff_ratio(fit18.2)[1:6] %>% 
  mcmc_neff() +
  yaxis_text(hjust = 0)
```

The `brms::vcov()` function returns a variance/covariance matrix--or a correlation matrix when you set `correlation = T`--of the population-level parameters (i.e., the fixed effects). It returns the values to a decadent level of precision, so we'll simplify the output with `round()`.

```{r}
vcov(fit18.2, correlation = T) %>% 
  round(digits = 3)
```

Notice how much lower our `Spend_z` correlations are than those Kruschke displayed on page 520 of the text. However, it turns out the correlations among the redundant predictors were still very high.

> If any of the nondiagonal correlations are high (i.e., close to +1 or close to −1), be careful when interpreting the posterior distribution. Here, we can see that the correlation of PrcntTake and PropNotTake is −1.0, which is an immediate sign of redundant predictors. (p. 520)

You can really get a sense of the silliness of the parameters if you plot them. We'll use `stat_wilke()` to get a sense of densities and summaries of the $\beta$'s.

```{r, fig.width = 8, fig.height = 1.75, warning = F}
draws %>% 
  pivot_longer(b_Intercept:b_prop_not_take_z) %>% 
  # this line isn't necessary, but it does allow us to arrange the parameters on the y-axis
  mutate(name = factor(name, 
                       levels = c("b_prop_not_take_z", "b_prcnt_take_z", "b_spend_z", "b_Intercept"))) %>% 
  
  ggplot(aes(x = value, y = name)) +
  geom_vline(xintercept = 0, color = "white") +
  stat_wilke(normalize = "xy", point_size = 3) +
  labs(x = NULL, 
       y = NULL) +
  coord_cartesian(xlim = c(-5, 5),
                  ylim = c(1.4, NA)) +
  theme(axis.text.y = element_text(hjust = 0),
        legend.position = c(.76, .8))
```

Yeah, on the standardized scale those are some ridiculous estimates. Let's update our `make_beta_0()` function.

```{r}
make_beta_0 <- function(zeta_0, zeta_1, zeta_2, zeta_3, sd_x_1, sd_x_2, sd_x_3, sd_y, m_x_1, m_x_2, m_x_3,  m_y) {
  sd_y * zeta_0 + m_y - sd_y * ((zeta_1 * m_x_1 / sd_x_1) + (zeta_2 * m_x_2 / sd_x_2) + (zeta_3 * m_x_3 / sd_x_3))
}
```

```{r, warning = F}
sd_x_1 <- sd(my_data$Spend)
sd_x_2 <- sd(my_data$PrcntTake)
sd_x_3 <- sd(my_data$prop_not_take)
sd_y   <- sd(my_data$SATT)
m_x_1  <- mean(my_data$Spend)
m_x_2  <- mean(my_data$PrcntTake)
m_x_3  <- mean(my_data$prop_not_take)
m_y    <- mean(my_data$SATT)

draws <-
  draws %>% 
  transmute(Intercept = make_beta_0(zeta_0 = b_Intercept,
                                    zeta_1 = b_spend_z,
                                    zeta_2 = b_prcnt_take_z,
                                    zeta_3 = b_prop_not_take_z,
                                    sd_x_1 = sd_x_1,
                                    sd_x_2 = sd_x_2,
                                    sd_x_3 = sd_x_3,
                                    sd_y   = sd_y,
                                    m_x_1  = m_x_1,
                                    m_x_2  = m_x_2,
                                    m_x_3  = m_x_3,
                                    m_y    = m_y),
            Spend = make_beta_j(zeta_j = b_spend_z,
                                sd_j   = sd_x_1,
                                sd_y   = sd_y),
            `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z,
                                         sd_j   = sd_x_2,
                                         sd_y   = sd_y),
            `Proportion not Take` = make_beta_j(zeta_j = b_prop_not_take_z,
                                                sd_j   = sd_x_3,
                                                sd_y   = sd_y),
            Scale     = sigma * sd_y,
            Normality = nu %>% log10())

glimpse(draws)
```

Now we've done the conversions, here are the histograms of Figure 18.6.

```{r, fig.width = 6, fig.height = 4}
draws %>% 
  pivot_longer(everything()) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_wilke(normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(ylim = c(-0.01, NA)) +
  panel_border() +
  theme(axis.text.x = element_text(size = 8),
        legend.position = "none") +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

Their scatter plots are as follows.

```{r, fig.width = 7, fig.height = 6.75, warning = F, message = F}
draws %>% 
  set_names("Intercept~(beta[0])", "Spend~(beta[1])", "Percent~Take~(beta[2])", "Percent~Not~Take~(beta[3])", "sigma", "log10(nu)") %>% 
  ggpairs(upper = list(continuous = my_upper),
          diag = list(continuous = my_diag),
          lower = list(continuous = my_lower),
          labeller = label_parsed) +
  theme(strip.text = element_text(size = 7))
```

Figure 18.7 is all about the prior predictive distribution. Here we'll extract the priors with `prior_samples()` and wrangle all in one step.

```{r}
prior_draws <- 
  prior_draws(fit18.2) %>% 
  transmute(Intercept = make_beta_0(zeta_0 = b_Intercept,
                                    zeta_1 = b_spend_z,
                                    zeta_2 = b_prcnt_take_z,
                                    zeta_3 = b_prop_not_take_z,
                                    sd_x_1 = sd_x_1,
                                    sd_x_2 = sd_x_2,
                                    sd_x_3 = sd_x_3,
                                    sd_y   = sd_y,
                                    m_x_1  = m_x_1,
                                    m_x_2  = m_x_2,
                                    m_x_3  = m_x_3,
                                    m_y    = m_y),
            Spend = make_beta_j(zeta_j = b_spend_z,
                                sd_j   = sd_x_1,
                                sd_y   = sd_y),
            `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z,
                                         sd_j   = sd_x_2,
                                         sd_y   = sd_y),
            `Proportion not Take` = make_beta_j(zeta_j = b_prop_not_take_z,
                                                sd_j   = sd_x_3,
                                                sd_y   = sd_y),
            Scale     = sigma * sd_y,
            Normality = nu %>% log10()) 

glimpse(prior_draws)
```

Now we've wrangled the priors, we're ready to make the histograms at the top of Figure 18.7.

```{r, fig.width = 6, fig.height = 4}
prior_draws %>% 
  pivot_longer(everything()) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_wilke(normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(ylim = c(-0.01, NA)) +
  panel_border() +
  theme(axis.text.x = element_text(size = 8),
        legend.position = "none") +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

Since we used the half-Gaussian prior for our $\sigma$, our `Scale` histogram looks different from Kruschke's. Otherwise, everything's on the up and up. Here are the pairs plots at the bottom of Figure 18.7.

```{r, fig.width = 7, fig.height = 6.75, warning = F, message = F}
prior_draws %>% 
  set_names("Intercept~(beta[0])", "Spend~(beta[1])", "Percent~Take~(beta[2])", "Percent~Not~Take~(beta[3])", "sigma", "log10(nu)") %>% 
  ggpairs(upper = list(continuous = my_upper),
          diag = list(continuous = my_diag),
          lower = list(continuous = my_lower),
          labeller = label_parsed) +
  theme(strip.text = element_text(size = 7))
```

At the top of page 523, Kruschke asked us to "notice that the posterior distribution in Figure 18.6 has ranges for the redundant parameters that are only a little smaller than their priors." With a little wrangling, we can compare the prior/posterior distributions for our redundant parameters more directly.

```{r, fig.width = 6, fig.height = 3}
draws %>% 
  pivot_longer(everything(),
               names_to = "parameter", 
               values_to = "posterior") %>% 
  bind_cols(
    prior_draws %>%
      pivot_longer(everything()) %>% 
      transmute(prior = value)
    ) %>% 
  pivot_longer(-parameter) %>% 
  filter(parameter %in% c("Percent Take", "Proportion not Take")) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_wilke(normalize = "panels") +
  scale_fill_viridis_d(option = "D", begin = .35, end = .65) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(ylim = c(-0.01, NA)) +
  panel_border() +
  theme(legend.position = "none") +
  facet_grid(name ~ parameter, scales = "free")
```

Kruschke was right. The posterior distributions are only slightly narrower than the priors for those two. With our combination of data and model, we learned virtually nothing beyond the knowledge we encoded in those priors.

Kruschke mentioned SEM as a possible solution to multicollinearity. **brms** isn't fully capable of SEM, at the moment (see [issue #304](https://github.com/paul-buerkner/brms/issues/304)), but its [multivariate syntax](https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html) [@Bürkner2022Multivariate] does allow for [path analysis](http://www.imachordata.com/2017/12/21/bayesian-sem-with-brms/) and [IRT models](https://github.com/paul-buerkner/brms/issues/203). However, you can currently fit a variety of Bayesian SEMs with the [**blavaan** package](https://CRAN.R-project.org/package=blavaan) [@Merkle2018blavaan; @R-blavaan]. I'm not aware of any textbooks highlighting **blavaan**. If you know of any, [please share](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues).

### Informative priors, sparse data, and correlated predictors.

It's worth reproducing some of Kruschke's prose from this subsection.

> The examples in this book tend to use mildly informed priors (e.g., using information about the rough magnitude and range of the data). But a benefit of Bayesian analysis is the potential for cumulative scientific progress by using priors that have been informed from previous research.
>
> Informed priors can be especially useful when the amount of data is small compared to the parameter space. A strongly informed prior essentially reduces the scope of the credible parameter space, so that a small amount of new data implies a narrow zone of credible parameter values. (p. 523)

## Multiplicative interaction of metric predictors

From page 526:

> Formally, interactions can have many different specific functional forms. We will consider multiplicative interaction. This means that the nonadditive interaction is expressed by multiplying the predictors. The predicted value is a weighted combination of the individual predictors and, additionally, the multiplicative product of the predictors. For two metric predictors, regression with multiplicative interaction has these algebraically equivalent expressions:

\begin{align*}
\mu & = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{1 \times 2} x_1 x_2 \\
    & = \beta_0 + \underbrace{(\beta_1 + \beta_{1 \times 2} x_2)}_{\text{slope of } x_1} x_1 + \beta_2 x_2 \\
    & = \beta_0 + \beta_1 x_1 + \underbrace{(\beta_2 + \beta_{1 \times 2} x_1)}_{\text{slope of } x_2} x_2.
\end{align*}

Figure 18.8 is out of our **ggplot2** repertoire. Even without it, we can still appreciate Kruschke's overall message that "Great care must be taken when interpreting the coefficients of a model that includes interaction terms [@braumoellerHypothesisTestingMultiplicative2004]. In particular, low-order terms are especially difficult to interpret when higher-order interactions are present" (p. 526). When in doubt, plot.

### An example.

Presuming we're still just modeling $\mu$ with two predictors, we can express the formula with the interaction term as

$$
\mu = \beta_0 + \beta_1+ \beta_2 x_2 + \underbrace{\beta_{1 \times 2}}_{\beta_3} \underbrace{x_2 x_1 }_{x_3}. 
$$

With **brms**, you can specify an interaction with either the `x_i*x_j` syntax or the `x_i:x_j` syntax. I typically use `x_i:x_j`. It's often the case that you can just make the interaction term right in the model formula. But since we're fitting the model with standardized predictors and then using Kruschke's equations to convert the parameters back to the unstandardized metric, it seems easier to make the interaction term in the data, first.

```{r}
my_data <-
  my_data %>% 
  # make x_3
  mutate(interaction   = Spend * PrcntTake) %>% 
  mutate(interaction_z = standardize(interaction))
```

Now we'll fit the model.

```{r fit18.3}
fit18.3 <-
  brm(data = my_data,
      family = student,
      satt_z ~ 1 + spend_z + prcnt_take_z + interaction_z,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 2), class = b),
                prior(normal(0, 1), class = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvar(1/29, name = "one_over_twentynine"),
      seed = 18,
      file = "fits/fit18.03")
```

Note that even though an interaction term might seem different kind from other regression terms, it's just another coefficient of `class = b` as far as the `prior()` statements are concerned. Anyway, let's inspect the `summary()`.

```{r}
summary(fit18.3)
```

The correlations among our parameters are about as severe as those in the text.

```{r}
vcov(fit18.3, correlation = T) %>% 
  round(digits = 3)
```

> We can see that the interaction variable is strongly correlated with both predictors. Therefore, we know that there will be strong trade-offs among the regression coefficients, and the marginal distributions of single regression coefficients might be much wider than when there was no interaction included. (p. 528)

Let's convert the posterior draws to the unstandardized metric.

```{r, warning = F}
sd_x_3 <- sd(my_data$interaction)
m_x_3  <- mean(my_data$interaction)

draws <- 
  as_draws_df(fit18.3) %>% 
  transmute(Intercept = make_beta_0(zeta_0 = b_Intercept,
                                    zeta_1 = b_spend_z,
                                    zeta_2 = b_prcnt_take_z,
                                    zeta_3 = b_interaction_z,
                                    sd_x_1 = sd_x_1,
                                    sd_x_2 = sd_x_2,
                                    sd_x_3 = sd_x_3,
                                    sd_y   = sd_y,
                                    m_x_1  = m_x_1,
                                    m_x_2  = m_x_2,
                                    m_x_3  = m_x_3,
                                    m_y    = m_y),
            Spend = make_beta_j(zeta_j = b_spend_z,
                                sd_j   = sd_x_1,
                                sd_y   = sd_y),
            `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z,
                                         sd_j   = sd_x_2,
                                         sd_y   = sd_y),
            `Spend : Percent Take` = make_beta_j(zeta_j = b_interaction_z,
                                                 sd_j   = sd_x_3,
                                                 sd_y   = sd_y),
            Scale     = sigma * sd_y,
            Normality = nu %>% log10())

glimpse(draws)
```

Now we've done the conversions, here are our versions of the histograms of Figure 18.9.

```{r, fig.width = 6, fig.height = 4}
draws %>% 
  pivot_longer(everything()) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_wilke(normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(ylim = c(-0.01, NA)) +
  panel_border() +
  theme(legend.position = "none") +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

"To properly understand the credible slopes on the two predictors, we must consider the credible slopes on each predictor as a function of the value of the other predictor" (p. 528). This is our motivation for the middle panel of Figure 18.9. To make it, we'll need to `expand()` our `post`, wrangle a bit, and plot with `geom_pointrange()`.

```{r}
# this will come in handy in `expand()`
bounds <- range(my_data$PrcntTake)

p1 <-
  # wrangle
  draws %>% 
  expand(nesting(Spend, `Spend : Percent Take`),
         PrcntTake = seq(from = bounds[1], to = bounds[2], length.out = 20)) %>% 
  mutate(slope = Spend + `Spend : Percent Take` * PrcntTake) %>% 
  group_by(PrcntTake) %>% 
  median_hdi(slope) %>% 
  
  # plot
  ggplot(aes(x = PrcntTake, y = slope,
             ymin = .lower, ymax = .upper)) +
  geom_hline(yintercept = 0, color = "grey25", linetype = 2) +
  geom_pointrange(shape = 21, stroke = 1/10, fatten = 6, 
                  color = "steelblue4", fill = "chocolate3") +
  labs(title = expression("Slope on spend is "~beta[1]+beta[3]%.%prcnt_take),
       x = "Value of prcnt_take",
       y = "Slope on spend")
```

We'll follow the same basic order of operations for the final panel and then bind them together with **patchwork**.

```{r, fig.width = 6, fig.height = 5, warning = F, message = F}
# this will come in handy in `expand()`
bounds <- range(my_data$Spend)

p2 <-
  # wrangle
  draws %>% 
  expand(nesting(`Percent Take`, `Spend : Percent Take`),
         Spend = seq(from = bounds[1], to = bounds[2], length.out = 20)) %>% 
  mutate(slope = `Percent Take` + `Spend : Percent Take` * Spend) %>% 
  group_by(Spend) %>% 
  median_hdi(slope) %>% 
  
  # plot
  ggplot(aes(x = Spend, y = slope,
             ymin = .lower, ymax = .upper)) +
  geom_pointrange(shape = 21, stroke = 1/10, fatten = 6, 
                  color = "steelblue4", fill = "chocolate3") +
  labs(title = expression("Slope on prcnt_take is "~beta[2]+beta[3]%.%spend),
       x = "Value of spend",
       y = "Slope on prcnt_take")

p1 / p2
```

Kruschke outlined all this in the opening paragraphs of page 530. His parting words of this subsection warrant repeating: "if you include an interaction term, you cannot ignore it even if its marginal posterior distribution includes zero" (p. 530).

## Shrinkage of regression coefficients

> In some research, there are many candidate predictors which we suspect could possibly be informative about the predicted variable. For example, when predicting college GPA, we might include high-school GPA, high-school SAT score, income of student, income of parents, years of education of the parents, spending per pupil at the student’s high school, student IQ, student height, weight, shoe size, hours of sleep per night, distance from home to school, amount of caffeine consumed, hours spent studying, hours spent earning a wage, blood pressure, etc. We can include all the candidate predictors in the model, with a regression coefficient for every predictor. And this is not even considering interactions, which we will ignore for now.
>
> With so many candidate predictors of noisy data, there may be some regression coefficients that are spuriously estimated to be non-zero. We would like some protection against accidentally nonzero regression coefficients. (p. 530)

That's what this section is all about. Figure 18.10 will give us a sense of what a model like this might look like.

```{r, fig.width = 6.5, fig.height = 6, message = F}
# brackets
p1 <-
  tibble(x = .5,
         y = .5,
         label = "{_}     {_}") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

#  two annotated arrows
p2 <-
  tibble(x    = c(.15, .85),
         y    = c(1, 1),
         xend = c(.25, .75),
         yend = c(.2, .2)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# normal density
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = "steelblue4", color = "steelblue4", alpha = .6) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# a student-t density
p4 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) +
  geom_area(fill = "steelblue4", color = "steelblue4", alpha = .6) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "student t",
           size = 7) +
  annotate(geom = "text",
           x = 0, y = .6,
           label = "nu[beta]~~~0~~~sigma[beta]",
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# two more annotated arrows
p5 <-
  tibble(x    = c(.33, 1.67),
         y    = c(1, 1),
         xend = c(.63, 1.2),
         yend = c(0, 0)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = "text",
           x = c(.35, 1.35, 1.54), y = .5,
           label = c("'~'", "'~'", "italic(j)"),
           size = c(10, 10, 7), family = "Times", parse = T) +
  xlim(0, 2) +
  theme_void()

# exponential density
p6 <-
  tibble(x = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) +
  geom_area(fill = "steelblue4", color = "steelblue4", alpha = .6) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "exp",
           size = 7) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(K)",
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# likelihood formula
p7 <-
  tibble(x = .5,
         y = .25,
         label = "beta[0]+sum()[italic(j)]*beta[italic(j)]*italic(x)[italic(ji)]") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()
  
  # half-normal density
p8 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = "steelblue4", color = "steelblue4", alpha = .6) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma]", 
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# four annotated arrows
p9 <-
  tibble(x    = c(.43, .43, 1.5, 2.5),
         y    = c(1, .55, 1, 1),
         xend = c(.43, 1.225, 1.5, 1.75),
         yend = c(.8, .15, .2, .2)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = "text",
           x = c(.3, .7, 1.38, 2), y = c(.92, .22, .65, .6),
           label = c("'~'", "'='", "'='", "'~'"),
           size = 10, family = "Times", parse = T) +
  annotate(geom = "text",
           x = .43, y = .7,
           label = "nu*minute+1",
           size = 7, family = "Times", parse = T) +
  xlim(0, 3) +
  theme_void()

# a second student-t density
p10 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) +
  geom_area(fill = "steelblue4", color = "steelblue4", alpha = .6) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "student t",
           size = 7) +
  annotate(geom = "text",
           x = 0, y = .6,
           label = "nu~~~mu[italic(i)]~~~sigma",
           size = 7, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5))

# the final annotated arrow
p11 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p12 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 1, l = 7, r = 9),
  area(t = 3, b = 4, l = 3, r = 5),
  area(t = 3, b = 4, l = 7, r = 9),
  area(t = 2, b = 3, l = 7, r = 9),
  area(t = 6, b = 7, l = 1, r = 3),
  area(t = 6, b = 7, l = 5, r = 7),
  area(t = 6, b = 7, l = 9, r = 11),
  area(t = 5, b = 6, l = 3, r = 9),
  area(t = 9, b = 10, l = 5, r = 7),
  area(t = 8, b = 9, l = 1, r = 11),
  area(t = 11, b = 11, l = 5, r = 7),
  area(t = 12, b = 12, l = 5, r = 7)
)

# combine and plot!
(p1 + p3 + p4 + p2 + p6 + p7 + p8 + p5 + p10 + p9 + p11 + p12) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Make our random noise predictors with `rnorm()`.

```{r}
set.seed(18)
my_data <-
  my_data %>% 
  mutate(x_rand_1  = rnorm(n = n(), mean = 0, sd = 1),
         x_rand_2  = rnorm(n = n(), mean = 0, sd = 1),
         x_rand_3  = rnorm(n = n(), mean = 0, sd = 1),
         x_rand_4  = rnorm(n = n(), mean = 0, sd = 1),
         x_rand_5  = rnorm(n = n(), mean = 0, sd = 1),
         x_rand_6  = rnorm(n = n(), mean = 0, sd = 1),
         x_rand_7  = rnorm(n = n(), mean = 0, sd = 1),
         x_rand_8  = rnorm(n = n(), mean = 0, sd = 1),
         x_rand_9  = rnorm(n = n(), mean = 0, sd = 1),
         x_rand_10 = rnorm(n = n(), mean = 0, sd = 1),
         x_rand_11 = rnorm(n = n(), mean = 0, sd = 1),
         x_rand_12 = rnorm(n = n(), mean = 0, sd = 1))

glimpse(my_data)
```

Here's the naïve model.

```{r fit18.4}
fit18.4 <-
  update(fit18.1, 
         newdata = my_data,
         formula = satt_z ~ 1 + prcnt_take_z + spend_z + x_rand_1 + x_rand_2 + x_rand_3 + x_rand_4 + x_rand_5 + x_rand_6 + x_rand_7 + x_rand_8 + x_rand_9 + x_rand_10 + x_rand_11 + x_rand_12,
         seed = 18,
         file = "fits/fit18.04")
```

Here we'll examine the posterior with `posterior_summary()`.

```{r}
posterior_summary(fit18.4) %>% 
  round(digits = 2)
```

Before we can make Figure 18.11, we'll need to update our `make_beta_0()` function to accommodate this model.

```{r}
make_beta_0 <- 
  function(zeta_0, zeta_1, zeta_2, zeta_3, zeta_4, zeta_5, zeta_6, zeta_7, zeta_8, zeta_9, zeta_10, zeta_11, zeta_12, zeta_13, zeta_14, 
           sd_x_1, sd_x_2, sd_x_3, sd_x_4, sd_x_5, sd_x_6, sd_x_7, sd_x_8, sd_x_9, sd_x_10, sd_x_11, sd_x_12, sd_x_13, sd_x_14, sd_y, 
           m_x_1, m_x_2, m_x_3, m_x_4, m_x_5, m_x_6, m_x_7, m_x_8, m_x_9, m_x_10, m_x_11, m_x_12, m_x_13, m_x_14, m_y) {
    sd_y * zeta_0 + m_y - sd_y * ((zeta_1 * m_x_1 / sd_x_1) + 
                                    (zeta_2 * m_x_2 / sd_x_2) + 
                                    (zeta_3 * m_x_3 / sd_x_3) + 
                                    (zeta_4 * m_x_4 / sd_x_4) + 
                                    (zeta_5 * m_x_5 / sd_x_5) + 
                                    (zeta_6 * m_x_6 / sd_x_6) + 
                                    (zeta_7 * m_x_7 / sd_x_7) + 
                                    (zeta_8 * m_x_8 / sd_x_8) + 
                                    (zeta_9 * m_x_9 / sd_x_9) + 
                                    (zeta_10 * m_x_10 / sd_x_10) + 
                                    (zeta_11 * m_x_11 / sd_x_11) + 
                                    (zeta_12 * m_x_12 / sd_x_12) + 
                                    (zeta_13 * m_x_13 / sd_x_13) + 
                                    (zeta_14 * m_x_14 / sd_x_14))
}
```

*Sigh*, our poor `make_beta_0()` and `make_beta_1()` code is getting obscene. I don't have the energy to think of how to wrap this into a simpler function. Someone probably should. If that ends up as you, [do share your code](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues).

```{r, warning = F}
sd_x_1  <- sd(my_data$Spend)
sd_x_2  <- sd(my_data$PrcntTake)
sd_x_3  <- sd(my_data$x_rand_1)
sd_x_4  <- sd(my_data$x_rand_2)
sd_x_5  <- sd(my_data$x_rand_3)
sd_x_6  <- sd(my_data$x_rand_4)
sd_x_7  <- sd(my_data$x_rand_5)
sd_x_8  <- sd(my_data$x_rand_6)
sd_x_9  <- sd(my_data$x_rand_7)
sd_x_10 <- sd(my_data$x_rand_8)
sd_x_11 <- sd(my_data$x_rand_9)
sd_x_12 <- sd(my_data$x_rand_10)
sd_x_13 <- sd(my_data$x_rand_11)
sd_x_14 <- sd(my_data$x_rand_12)
sd_y    <- sd(my_data$SATT)

m_x_1  <- mean(my_data$Spend)
m_x_2  <- mean(my_data$PrcntTake)
m_x_3  <- mean(my_data$x_rand_1)
m_x_4  <- mean(my_data$x_rand_2)
m_x_5  <- mean(my_data$x_rand_3)
m_x_6  <- mean(my_data$x_rand_4)
m_x_7  <- mean(my_data$x_rand_5)
m_x_8  <- mean(my_data$x_rand_6)
m_x_9  <- mean(my_data$x_rand_7)
m_x_10 <- mean(my_data$x_rand_8)
m_x_11 <- mean(my_data$x_rand_9)
m_x_12 <- mean(my_data$x_rand_10)
m_x_13 <- mean(my_data$x_rand_11)
m_x_14 <- mean(my_data$x_rand_12)
m_y    <- mean(my_data$SATT)

draws <-
  as_draws_df(fit18.4) %>% 
  transmute(Intercept      = make_beta_0(zeta_0 = b_Intercept,
                                         zeta_1  = b_spend_z,
                                         zeta_2  = b_prcnt_take_z,
                                         zeta_3  = b_x_rand_1,
                                         zeta_4  = b_x_rand_2,
                                         zeta_5  = b_x_rand_3,
                                         zeta_6  = b_x_rand_4,
                                         zeta_7  = b_x_rand_5,
                                         zeta_8  = b_x_rand_6,
                                         zeta_9  = b_x_rand_7,
                                         zeta_10 = b_x_rand_8,
                                         zeta_11 = b_x_rand_9,
                                         zeta_12 = b_x_rand_10,
                                         zeta_13 = b_x_rand_11,
                                         zeta_14 = b_x_rand_12,
                                         sd_x_1  = sd_x_1,
                                         sd_x_2  = sd_x_2,
                                         sd_x_3  = sd_x_3,
                                         sd_x_4  = sd_x_4,
                                         sd_x_5  = sd_x_5,
                                         sd_x_6  = sd_x_6,
                                         sd_x_7  = sd_x_7,
                                         sd_x_8  = sd_x_8,
                                         sd_x_9  = sd_x_9,
                                         sd_x_10 = sd_x_10,
                                         sd_x_11 = sd_x_11,
                                         sd_x_12 = sd_x_12,
                                         sd_x_13 = sd_x_13,
                                         sd_x_14 = sd_x_14,
                                         sd_y    = sd_y,
                                         m_x_1   = m_x_1,
                                         m_x_2   = m_x_2,
                                         m_x_3   = m_x_3,
                                         m_x_4   = m_x_4,
                                         m_x_5   = m_x_5,
                                         m_x_6   = m_x_6,
                                         m_x_7   = m_x_7,
                                         m_x_8   = m_x_8,
                                         m_x_9   = m_x_9,
                                         m_x_10  = m_x_10,
                                         m_x_11  = m_x_11,
                                         m_x_12  = m_x_12,
                                         m_x_13  = m_x_13,
                                         m_x_14  = m_x_14,
                                         m_y     = m_y),
            Spend          = make_beta_j(zeta_j = b_spend_z,
                                         sd_j   = sd_x_1,
                                         sd_y   = sd_y),
            `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z,
                                         sd_j   = sd_x_2,
                                         sd_y   = sd_y),
            x_rand_1       = make_beta_j(zeta_j = b_x_rand_1,
                                         sd_j   = sd_x_3,
                                         sd_y   = sd_y),
            x_rand_2       = make_beta_j(zeta_j = b_x_rand_2,
                                         sd_j   = sd_x_4,
                                         sd_y   = sd_y),
            x_rand_3       = make_beta_j(zeta_j = b_x_rand_3,
                                         sd_j   = sd_x_5,
                                         sd_y   = sd_y),
            x_rand_4       = make_beta_j(zeta_j = b_x_rand_4,
                                         sd_j   = sd_x_6,
                                         sd_y   = sd_y),
            x_rand_5       = make_beta_j(zeta_j = b_x_rand_5,
                                         sd_j   = sd_x_7,
                                         sd_y   = sd_y),
            x_rand_6       = make_beta_j(zeta_j = b_x_rand_6,
                                         sd_j   = sd_x_8,
                                         sd_y   = sd_y),
            x_rand_7       = make_beta_j(zeta_j = b_x_rand_7,
                                         sd_j   = sd_x_9,
                                         sd_y   = sd_y),
            x_rand_8       = make_beta_j(zeta_j = b_x_rand_8,
                                         sd_j   = sd_x_10,
                                         sd_y   = sd_y),
            x_rand_9       = make_beta_j(zeta_j = b_x_rand_9,
                                         sd_j   = sd_x_11,
                                         sd_y   = sd_y),
            x_rand_10      = make_beta_j(zeta_j = b_x_rand_10,
                                         sd_j   = sd_x_12,
                                         sd_y   = sd_y),
            x_rand_11      = make_beta_j(zeta_j = b_x_rand_11,
                                         sd_j   = sd_x_13,
                                         sd_y   = sd_y),
            x_rand_12      = make_beta_j(zeta_j = b_x_rand_12,
                                         sd_j   = sd_x_14,
                                         sd_y   = sd_y),
            Scale          = sigma * sd_y,
            Normality      = nu %>% log10())

glimpse(draws)
```

Okay, here are our versions of the histograms of Figure 18.11.

```{r, fig.width = 6, fig.height = 7}
draws %>% 
  pivot_longer(cols = c(Intercept:x_rand_3, x_rand_10:Normality)) %>% 
  mutate(name = factor(name, 
                       levels = c("Intercept", "Spend", "Percent Take",
                                  "x_rand_1", "x_rand_2", "x_rand_3",
                                  "x_rand_10", "x_rand_11", "x_rand_12",
                                  "Scale", "Normality"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_wilke(normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(ylim = c(-0.01, NA)) +
  panel_border() +
  theme(axis.text.x = element_text(size = 9),
        legend.position = c(.74, .09)) +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

And here's the final density plot depicting the Bayesian $R^2$.

```{r, fig.width = 3.5, fig.height = 2.25}
bayes_R2(fit18.4, summary = F) %>% 
  as_tibble() %>% 
  
  ggplot(aes(x = R2, y = 0)) +
  stat_wilke() +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = expression(paste("Bayesian ", italic(R)^2)),
       x = NULL) +
  coord_cartesian(xlim = c(0, 1),
                  ylim = c(-0.01, NA)) +
  theme(legend.position = c(.01, .8))
```

Note that unlike the one Kruschke displayed in the text, our `brms::bayes_R2()`-based $R^2$ distribution did not exceed the logical right bound of 1.

Sometimes when you have this many parameters you'd like to compare, it's better to display their summaries with an ordered coefficient plot.

```{r, fig.width = 8, fig.height = 3}
draws %>% 
  pivot_longer(Spend:x_rand_12) %>% 
  mutate(name = fct_reorder(name, value)) %>% 
  
  ggplot(aes(x = value, y = name)) +
  geom_vline(xintercept = 0, color = "grey25", linetype = 2) +
  stat_pointinterval(point_interval = mode_hdi, .width = .95, point_size = 4,
                     color = "steelblue4", point_color = "chocolate3") +
  labs(x = NULL,
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0))
```

Now we can see that by chance alone, the coefficients for `x_rand_8` and `x_rand_9` are clearly distinct from zero. Our `stat_wilke()` function can be informative, too.

```{r, fig.width = 8, fig.height = 3, warning = F, message = F}
draws %>% 
  pivot_longer(Spend:x_rand_12) %>% 

  ggplot(aes(x = value, y = reorder(name, value), group = reorder(name, value))) +
  geom_vline(xintercept = 0, color = "grey25", linetype = 2) +
  stat_wilke(height = 10, point_size = 3) +
  labs(x = NULL,
       y = NULL) +
  coord_cartesian(ylim = c(1.25, 14.5)) +
  theme(axis.text.y = element_text(hjust = 0),
        legend.position = c(.785, .13)) 
```

With **brms**, we can fit something like the model Kruschke displayed in Figure 18.12 with the `horseshoe()` prior. From the `horseshoe` section of the [**brms** reference manual](https://CRAN.R-project.org/package=brms/brms.pdf):

> The horseshoe prior is a special shrinkage prior initially proposed by [@carvalho2009handling]. It is
symmetric around zero with fat tails and an infinitely large spike at zero. This makes it ideal for
sparse models that have many regression coefficients, although only a minority of them is non-zero.
The horseshoe prior can be applied on all population-level effects at once (excluding the intercept) by using `set_prior("horseshoe(1)")`. The `1` implies that the student-t prior of the local
shrinkage parameters has 1 degrees of freedom [@brms2022RM, p. 105]

Based on the quote, here's how to fit our horseshoe-prior model.

```{r fit18.5, message = F}
fit18.5 <-
  update(fit18.4, 
         newdata = my_data,
         formula = satt_z ~ 1 + prcnt_take_z + spend_z + x_rand_1 + x_rand_2 + x_rand_3 + x_rand_4 + x_rand_5 + x_rand_6 + x_rand_7 + x_rand_8 + x_rand_9 + x_rand_10 + x_rand_11 + x_rand_12,
         prior = c(prior(normal(0, 2), class = Intercept),
                   prior(horseshoe(1), class = b),
                   prior(normal(0, 1), class = sigma),
                   prior(exponential(one_over_twentynine), class = nu)),
         seed = 18,
         control = list(adapt_delta = .9),
      file = "fits/fit18.05")
```

Check the parameter summary.

```{r}
posterior_summary(fit18.5) %>% 
  round(digits = 2)
```

Our `make_beta_0()` and `make_beta_1()` code remains obscene.

```{r, warning = F}
draws <-
  as_draws_df(fit18.5) %>% 
  transmute(Intercept      = make_beta_0(zeta_0  = b_Intercept,
                                         zeta_1  = b_spend_z,
                                         zeta_2  = b_prcnt_take_z,
                                         zeta_3  = b_x_rand_1,
                                         zeta_4  = b_x_rand_2,
                                         zeta_5  = b_x_rand_3,
                                         zeta_6  = b_x_rand_4,
                                         zeta_7  = b_x_rand_5,
                                         zeta_8  = b_x_rand_6,
                                         zeta_9  = b_x_rand_7,
                                         zeta_10 = b_x_rand_8,
                                         zeta_11 = b_x_rand_9,
                                         zeta_12 = b_x_rand_10,
                                         zeta_13 = b_x_rand_11,
                                         zeta_14 = b_x_rand_12,
                                         sd_x_1  = sd_x_1,
                                         sd_x_2  = sd_x_2,
                                         sd_x_3  = sd_x_3,
                                         sd_x_4  = sd_x_4,
                                         sd_x_5  = sd_x_5,
                                         sd_x_6  = sd_x_6,
                                         sd_x_7  = sd_x_7,
                                         sd_x_8  = sd_x_8,
                                         sd_x_9  = sd_x_9,
                                         sd_x_10 = sd_x_10,
                                         sd_x_11 = sd_x_11,
                                         sd_x_12 = sd_x_12,
                                         sd_x_13 = sd_x_13,
                                         sd_x_14 = sd_x_14,
                                         sd_y    = sd_y,
                                         m_x_1   = m_x_1,
                                         m_x_2   = m_x_2,
                                         m_x_3   = m_x_3,
                                         m_x_4   = m_x_4,
                                         m_x_5   = m_x_5,
                                         m_x_6   = m_x_6,
                                         m_x_7   = m_x_7,
                                         m_x_8   = m_x_8,
                                         m_x_9   = m_x_9,
                                         m_x_10  = m_x_10,
                                         m_x_11  = m_x_11,
                                         m_x_12  = m_x_12,
                                         m_x_13  = m_x_13,
                                         m_x_14  = m_x_14,
                                         m_y     = m_y),
            Spend          = make_beta_j(zeta_j = b_spend_z,
                                         sd_j   = sd_x_1,
                                         sd_y   = sd_y),
            `Percent Take` = make_beta_j(zeta_j = b_prcnt_take_z,
                                         sd_j   = sd_x_2,
                                         sd_y   = sd_y),
            x_rand_1       = make_beta_j(zeta_j = b_x_rand_1,
                                         sd_j   = sd_x_3,
                                         sd_y   = sd_y),
            x_rand_2       = make_beta_j(zeta_j = b_x_rand_2,
                                         sd_j   = sd_x_4,
                                         sd_y   = sd_y),
            x_rand_3       = make_beta_j(zeta_j = b_x_rand_3,
                                         sd_j   = sd_x_5,
                                         sd_y   = sd_y),
            x_rand_4       = make_beta_j(zeta_j = b_x_rand_4,
                                         sd_j   = sd_x_6,
                                         sd_y   = sd_y),
            x_rand_5       = make_beta_j(zeta_j = b_x_rand_5,
                                         sd_j   = sd_x_7,
                                         sd_y   = sd_y),
            x_rand_6       = make_beta_j(zeta_j = b_x_rand_6,
                                         sd_j   = sd_x_8,
                                         sd_y   = sd_y),
            x_rand_7       = make_beta_j(zeta_j = b_x_rand_7,
                                         sd_j   = sd_x_9,
                                         sd_y   = sd_y),
            x_rand_8       = make_beta_j(zeta_j = b_x_rand_8,
                                         sd_j   = sd_x_10,
                                         sd_y   = sd_y),
            x_rand_9       = make_beta_j(zeta_j = b_x_rand_9,
                                         sd_j   = sd_x_11,
                                         sd_y   = sd_y),
            x_rand_10      = make_beta_j(zeta_j = b_x_rand_10,
                                         sd_j   = sd_x_12,
                                         sd_y   = sd_y),
            x_rand_11      = make_beta_j(zeta_j = b_x_rand_11,
                                         sd_j   = sd_x_13,
                                         sd_y   = sd_y),
            x_rand_12      = make_beta_j(zeta_j = b_x_rand_12,
                                         sd_j   = sd_x_14,
                                         sd_y   = sd_y),
            Scale          = sigma * sd_y,
            Normality      = nu %>% log10())

glimpse(draws)
```

And here are our `stat_wilke()` plot versions of the majority of the histograms of Figure 18.12.

```{r, fig.width = 6, fig.height = 7}
draws %>% 
  pivot_longer(cols = c(Intercept:x_rand_3, x_rand_10:Normality)) %>% 
  mutate(name = factor(name, 
                       levels = c("Intercept", "Spend", "Percent Take",
                                  "x_rand_1", "x_rand_2", "x_rand_3",
                                  "x_rand_10", "x_rand_11", "x_rand_12",
                                  "Scale", "Normality"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_wilke(normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(ylim = c(-0.01, NA)) +
  panel_border() +
  theme(axis.text.x = element_text(size = 10),
        legend.position = c(.74, .09)) +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

Based on the distributions for the random predictors, it looks like our **brms** horseshoe prior regularized more aggressively than Kruschke's hierarchical prior in the text. And interestingly, look how our marginal posterior for `Spend` is bimodal.

For kicks and giggles, here's the corresponding coefficient plot for $\beta_1$ through $\beta_{14}$.

```{r, fig.width = 8, fig.height = 3}
draws %>% 
  pivot_longer(Spend:x_rand_12) %>% 

  ggplot(aes(x = value, y = reorder(name, value))) +
  geom_vline(xintercept = 0, color = "grey25", linetype = 2) +
  stat_pointinterval(point_interval = mode_hdi, .width = .95, point_size = 4,
                     color = "steelblue4", point_color = "chocolate3") +
  labs(x = NULL,
       y = NULL) +
  theme(axis.text.y  = element_text(hjust = 0))
```

But anyways, here's that final Bayesian $R^2$ density for Figure 18.12.

```{r, fig.width = 3.5, fig.height = 2.25}
bayes_R2(fit18.5, summary = F) %>% 
  as_tibble() %>% 
  
  ggplot(aes(x = R2, y = 0)) +
  stat_wilke() +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = expression(paste("Bayesian ", italic(R)^2)),
       x = NULL) +
  coord_cartesian(xlim = c(0, 1),
                  ylim = c(-0.01, NA)) +
  theme(legend.position = c(.01, .8))
```

Just recall, though, that our `fit18.5` was not exactly like Kruschke's model. Whereas we hard coded the scale of our Student-$t$ horseshoe prior to be 1, Kruschke estimated it with help from the gamma distribution. I'm not aware that's currently possible in **brms**. If I'm at fault and you know how to do it, [please share your code](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues).

## Variable selection

We can rewrite the linear regression model to accommodate whether it includes a predictor as

$$\mu_i = \beta_0 + \sum_j \delta_j \beta_j x_{j, i},$$

where $\delta$ is a dummy for which 0 = *not included* 1 = *included*. I'm not aware of a way to use $\delta$ as an inclusion indicator in **brms** the way Kruschke implemented it in JAGS. And in fact, it appears this might be [unfeasible within the Stan framework](https://discourse.mc-stan.org/t/include-extra-coefficient-multiplied-with-regression-weight-inclusion-weight-a-la-kruschke/5768). But if you know of a way, please [share your code](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues). However, this issue can lead to a similar approach: information criteria. To do so, let's follow Kruschke's basic flow and use the first model from way back in [Section 18.1.1][The perils of correlated predictors.] as a starting point. The model formula was as follows.

```{r}
fit18.1$formula
```

Taking interactions off the table for a moment, we can specify four model types with various combinations of the two predictors, `prcnt_take_z` and `spend_z`. `fit18.1` was the first, which we might denote as $\langle 1, 1 \rangle$. That leads to the remaining possibilities as

* $\langle 1, 0 \rangle:$ `satt_z ~ 1 + spend_z`
* $\langle 0, 1 \rangle:$ `satt_z ~ 1 + prcnt_take_z`
* $\langle 0, 0 \rangle:$ `satt_z ~ 1`

Let's fit those models.

```{r fit18.6, message = F}
fit18.6 <-
  update(fit18.1,
         formula = satt_z ~ 1 + spend_z,
         seed = 18,
         file = "fits/fit18.06")

fit18.7 <-
  update(fit18.1,
         formula = satt_z ~ 1 + prcnt_take_z,
         seed = 18,
         file = "fits/fit18.07")

fit18.8 <-
  brm(data = my_data,
      family = student,
      satt_z ~ 1,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 1), class = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvar(1/29, name = "one_over_twentynine"),
      seed = 18,
      file = "fits/fit18.08")
```

We'll compare our models with the LOO information criterion. Like other information criteria, the LOO values aren't of interest in and of themselves. However, the values of one model's LOO relative to that of another is of great interest. We generally prefer models with lower estimates.

```{r, message = F}
fit18.1 <- add_criterion(fit18.1, "loo")
fit18.6 <- add_criterion(fit18.6, "loo")
fit18.7 <- add_criterion(fit18.7, "loo")
fit18.8 <- add_criterion(fit18.8, "loo")

loo_compare(fit18.1, fit18.6, fit18.7, fit18.8) %>% 
  print(simplify = F)
```

In this case, `fit18.1` and `fit18.7` clearly have the lowest estimates, but the standard error of their difference score is about the same size as their difference. So the LOO difference score puts them on similar footing. Recall that you can do a similar analysis with the `waic()` function.

Let's compare that with the insights from the `model_weights()` function.

```{r}
(mw <- model_weights(fit18.1, fit18.6, fit18.7, fit18.8))
```

If you don't like scientific notation, you can always wrangle and plot.

```{r, fig.width = 6, fig.height = 1.5}
mw %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  set_names("fit", "estimate") %>% 
  
  ggplot(aes(x = estimate, y = reorder(fit, estimate))) +
  geom_text(aes(label = estimate %>% round(3) %>% as.character())) +
  scale_x_continuous("stacking weight", limits = c(0, 1)) +
  ylab(NULL)
```

Based on this weighting scheme, almost all the weight went to the full model, `fit18.1`. But note, in the intro of their [vignette on the topic](https://CRAN.R-project.org/package=loo/vignettes/loo2-weights.html), @vehtariBayesianStackingPseudoBMA opined:

> Ideally, we would avoid the Bayesian model combination problem by extending the model to include the separate models as special cases, and preferably as a continuous expansion of the model space. For example, instead of model averaging over different covariate combinations, all potentially relevant covariates should be included in a predictive model (for causal analysis more care is needed) and a prior assumption that only some of the covariates are relevant can be presented with regularized horseshoe prior [@piironenSparsityInformationRegularization2017]. For variable selection we recommend projective predictive variable selection (Piironen and Vehtari, 2017; [projpred package](https://CRAN.R-project.org/package=projpred)).

Perhaps unsurprisingly, their thoughts on the topic are similar with the [Gelman et al](https://stat.columbia.edu/~gelman/book/) quotation Kruschke provided on page 536:

> Some prominent authors eschew the variable-selection approach for typical applications in their fields. For example, [@gelman2013bayesian, p. 369] said, "For the regressions we typically see, we do not believe any coefficients to be truly zero and we do not generally consider it a conceptual (as opposed to computational) advantage to get point estimates of zero—but regularized estimates such as obtained by lasso can be much better than those resulting from simple least squares and flat prior distributions ...we are not comfortable with an underlying model in which the coefficients can be exactly zero."

For more on some of these methods, check out Vehtari's GitHub repository, [Tutorial on model assessment, model selection and inference after model selection](https://github.com/avehtari/modelselection).

But anyways, our model weighting methods cohered with Kruschke's $\delta$-inclusion-indicator method in that both suggested the full model, `fit1`, and the model with `prcnt_take` as the sole predictor, `fit7`, were given the greatest weight. I'm not aware that our information criteria weighting/model stacking methods provide probability distributions of the type Kruschke displayed in the left portions of Figure 18.13. But we can at least recreate the plots in the other panels.

```{r, fig.width = 6, fig.height = 3, warning = F}
# first we'll get the posterior draws from `fit18.1` and wrangle them
as_draws_df(fit18.1) %>% 
  transmute(Spend     = make_beta_j(zeta_j = b_spend_z,
                                    sd_j   = sd_x_1,
                                    sd_y   = sd_y),
            PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z,
                                    sd_j   = sd_x_2,
                                    sd_y   = sd_y)) %>% 
  pivot_longer(everything()) %>% 
  # within `bind_rows()`, we extract and wrangle the posterior draws from `fit18.7` and them insert them below those from `fit18.1`
  bind_rows(
    as_draws_df(fit18.7) %>%
      mutate(value = make_beta_j(zeta_j = b_prcnt_take_z,
                                    sd_j   = sd_x_2,
                                    sd_y   = sd_y),
             name = "PrcntTake") %>% 
      select(name, value)
  ) %>% 
  # now we just need a little indexing and factor ordering
  mutate(model = rep(c("fit18.1", "fit18.7"), times = c(8000, 4000)), 
         name  = factor(name, levels = c("Spend", "PrcntTake"))) %>% 
  
  # we finally plot!
  ggplot(aes(x = value, y = 0)) +
  stat_wilke(normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(ylim = c(-0.01, NA)) +
  panel_border() +
  theme(legend.position = c(.13, .23)) +
  facet_grid(model ~ name, scales = "free")
```

### Inclusion probability is strongly affected by vagueness of prior.

To follow along, let's fit the models with the updated $SD = 1$ on the $\beta_{1+}$ priors code.

```{r fit18.9, message = F}
fit18.9 <-
  update(fit18.1,
         prior = c(prior(normal(0, 1), class = Intercept),
                   prior(normal(0, 1), class = b),
                   prior(normal(0, 1), class = sigma),
                   prior(exponential(one_over_twentynine), class = nu)),
         chains = 4, cores = 4,
         stanvars = stanvar(1/29, name = "one_over_twentynine"),
         seed = 18,
         file = "fits/fit18.09")

fit18.10 <-
  update(fit18.9,
         formula = satt_z ~ 1 + spend_z,
         seed = 18,
         file = "fits/fit18.10")

fit18.11 <-
  update(fit18.9,
         formula = satt_z ~ 1 + prcnt_take_z,
         seed = 18,
         file = "fits/fit18.11")

fit18.12 <-
  update(fit18.8,
         prior = c(prior(normal(0, 1), class = Intercept),
                   prior(normal(0, 1), class = sigma),
                   prior(exponential(one_over_twentynine), class = nu)),
         seed = 18,
         file = "fits/fit18.12")
```

And now we'll fit the models with the updated $SD = 10$.

```{r fit18.13, message = F}
fit18.13 <-
  update(fit18.9,
         prior = c(prior(normal(0, 10), class = Intercept),
                   prior(normal(0, 10), class = b),
                   prior(normal(0, 10), class = sigma),
                   prior(exponential(one_over_twentynine), class = nu)),
         seed = 18,
         file = "fits/fit18.13")

fit18.14 <-
  update(fit18.13,
         formula = satt_z ~ 1 + spend_z,
         seed = 18,
         file = "fits/fit18.14")

fit18.15 <-
  update(fit18.13,
         formula = satt_z ~ 1 + prcnt_take_z,
         seed = 18,
         file = "fits/fit18.15")

fit18.16 <-
  update(fit18.12,
         prior = c(prior(normal(0, 10), class = Intercept),
                   prior(normal(0, 10), class = sigma),
                   prior(exponential(one_over_twentynine), class = nu)),
         seed = 18,
         file = "fits/fit18.16")
```

Now we've fit the models, we're ready to examine how altering the $SD$s on the $\beta_j$ priors influenced the model comparisons via `model_weights()`. Here we'll use the default stacking method.

```{r, fig.width = 6, fig.height = 4.5}
mw %>% 
  rbind(model_weights(fit18.9, fit18.10, fit18.11, fit18.12),
        model_weights(fit18.13, fit18.14, fit18.15, fit18.16)) %>% 
  as_tibble() %>% 
  set_names("1, 1", "1, 0", "0, 1", "0, 0") %>% 
  pivot_longer(everything()) %>% 
  mutate(prior = rep(str_c("SD = ", c(10, 2, 1)), times = 4) %>% 
           factor(., levels = str_c("SD = ", c(10, 2, 1)))) %>% 
  
  ggplot(aes(x = value, y = reorder(name, value))) +
  geom_text(aes(label = value %>% round(3) %>% as.character())) +
  labs(x = "Stacking weight",
       y = expression(paste("Models defined by Kruschke's ", delta, " notation"))) +
  coord_cartesian(xlim = c(0, 1)) +
  theme(panel.grid = element_blank()) +
  panel_border() +
  facet_grid(prior ~ .)
```

So unlike in the depictions in Figure 18.14, the stacking method was *insensitive* to the $SD$s on our $\beta_j$ priors. We might compare LOO difference scores, too.

```{r loo_fit18.9, message = F}
fit18.9  <- add_criterion(fit18.9,  "loo")
fit18.10 <- add_criterion(fit18.10, "loo")
fit18.11 <- add_criterion(fit18.11, "loo")
fit18.12 <- add_criterion(fit18.12, "loo")

fit18.13 <- add_criterion(fit18.13, "loo")
fit18.14 <- add_criterion(fit18.14, "loo")
fit18.15 <- add_criterion(fit18.15, "loo")
fit18.16 <- add_criterion(fit18.16, "loo")

loo_compare(fit18.1, fit18.6, fit18.7, fit18.8) %>% 
  print(simplify = F)

loo_compare(fit18.9, fit18.10, fit18.11, fit18.12) %>% 
  print(simplify = F)

loo_compare(fit18.13, fit18.14, fit18.15, fit18.16) %>% 
  print(simplify = F)
```

The LOO difference score patterns were also about the same across the $SD$s on our $\beta_j$ priors. Let's finish up with our version of the histograms comparing the model predictors. Here's the code for those in the top portion of Figure 18.14.

```{r, fig.width = 6, fig.height = 3, warning = F}
# first we'll get the posterior draws from `fit18.9` and wrangle them
as_draws_df(fit18.9) %>% 
  transmute(Spend     = make_beta_j(zeta_j = b_spend_z,
                                    sd_j   = sd_x_1,
                                    sd_y   = sd_y),
            PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z,
                                    sd_j   = sd_x_2,
                                    sd_y   = sd_y)) %>% 
  pivot_longer(everything()) %>% 
  # within `bind_rows()`, we extract and wrangle the posterior draws from `fit18.11` and 
  # then insert them below those from `fit18.9`
  bind_rows(
    as_draws_df(fit18.11) %>% 
      transmute(value = make_beta_j(zeta_j = b_prcnt_take_z,
                                    sd_j   = sd_x_2,
                                    sd_y   = sd_y),
                name = "PrcntTake") %>% 
      select(name, value)
  ) %>% 
  # now we just need a little indexing and factor ordering
  mutate(model = rep(c("fit18.9", "fit18.11"), times = c(8000, 4000)) %>% 
           factor(., levels = c("fit18.9", "fit18.11")),
         name  = factor(name, levels = c("Spend", "PrcntTake"))) %>%
  
  # we finally plot!
  ggplot(aes(x = value, y = 0)) +
  stat_wilke(normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(ylim = c(-0.01, NA)) +
  panel_border() +
  theme(legend.position = c(.13, .23)) +
  facet_grid(model ~ name, scales = "free")
```

And now we'll do our version of the histograms for the bottom portion of Figure 18.14.

```{r, fig.width = 6, fig.height = 3, warning = F}
as_draws_df(fit18.13) %>% 
  transmute(Spend     = make_beta_j(zeta_j = b_spend_z,
                                    sd_j   = sd_x_1,
                                    sd_y   = sd_y),
            PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z,
                                    sd_j   = sd_x_2,
                                    sd_y   = sd_y)) %>% 
  pivot_longer(everything()) %>% 
  bind_rows(
    as_draws_df(fit18.15) %>% 
      transmute(value = make_beta_j(zeta_j = b_prcnt_take_z,
                                    sd_j   = sd_x_2,
                                    sd_y   = sd_y),
                name  = "PrcntTake") %>% 
      select(name, value)
  ) %>% 
  mutate(model = rep(c("fit18.13", "fit18.15"), times = c(8000, 4000)) %>% 
           factor(., levels = c("fit18.13", "fit18.15")),
         name  = factor(name, levels = c("Spend", "PrcntTake"))) %>%
  
  ggplot(aes(x = value, y = 0)) +
  stat_wilke(normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(ylim = c(-0.01, NA)) +
  theme(legend.position = c(.13, .23)) +
  panel_border() +
  facet_grid(model ~ name, scales = "free")
```

Kruschke concluded this subsection with

> Bayesian model comparison can be strongly affected by the degree of vagueness in the priors, even though explicit estimates of the parameter values may be minimally affected. Therefore, be very cautious when interpreting the results of Bayesian variable selection. The next section discusses a way to inform the prior by using concurrent data instead of previous data. (p. 542)

We should note that while the method in text was "strongly affected by the degree of vagueness in the priors", the information-criteria and model weighting methods, above, were not. If you're interested in comparing models within the Bayesian paradigm, choose your method with care.

### Variable selection with hierarchical shrinkage.

Kruschke opened the subsection with a few good points:

> If you have strong previous research that can inform the prior, then it should be used. But if previous knowledge is weak, then the uncertainty should be expressed in the prior. This is an underlying mantra of the Bayesian approach: Any uncertainty should be expressed in the prior. (p. 543)

Here we'll standardize our new predictors, `StuTeaRat` and `Salary`.

```{r}
my_data <-
  my_data %>% 
  mutate(stu_tea_rat_z = standardize(StuTeaRat),
         salary_z      = standardize(Salary))
```

We can use Kruschke's `gamma_s_and_r_from_mode_sd()` function to return the exact shape and rate parameters to make a gamma with a mode of 1 and an $SD$ of 10.

```{r}
gamma_s_and_r_from_mode_sd <- function(mode, sd) {
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate  <- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape <- 1 + mode * rate
  return(list(shape = shape, rate = rate))
}
```

Here are the values.

```{r}
(p <- gamma_s_and_r_from_mode_sd(mode = 1, sd = 10) %>% as.numeric())
```

That gamma distribution looks like this.

```{r, fig.width = 4, fig.height = 2}
tibble(x = seq(from = 0, to = 65, length.out = 1e3)) %>% 
  ggplot(aes(x = x, y = dgamma(x, shape = p[1], rate = p[2]))) +
  geom_area(color = "steelblue4", fill = "steelblue4", alpha = .6) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Our gamma prior") +
  coord_cartesian(xlim = c(0, 60))
```

We can code those values in with arbitrary precision with the `stanvar()` function.

```{r}
stanvars <- 
  stanvar(1/29, name = "one_over_twentynine") +
  stanvar(p[1], name = "my_shape") +
  stanvar(p[2], name = "my_rate") +
  stanvar(scode = "  real<lower=0> tau;", block = "parameters")
```

Note that last `stanvar()` line. Bürkner recently posted an [exemplar of how to set a hierarchical prior on a regression coefficient](https://github.com/paul-buerkner/brms/issues/459) in a `brm()` model:

```{r, eval = F}
# define a hierarchical prior on the regression coefficients
bprior <- set_prior("normal(0, tau)", class = "b") +
  set_prior("target += normal_lpdf(tau | 0, 10)", check = FALSE)

stanvars <- stanvar(scode = "  real<lower=0> tau;", block = "parameters")

make_stancode(count ~ Trt + log_Base4_c, epilepsy,
              prior = bprior, stanvars = stanvars)
```

Following the method, we tell `brm()` we'd like to estimate the $SD$ of our $\beta_{1+}$ priors with `prior(normal(0, tau), class = b)`, where the `tau` is a stand-in for the $SD$. In the next line, `set_prior("target += gamma_lpdf(tau | my_shape, my_rate)", check = FALSE)`, we tell `brm()` we'd like to estimate `tau` with a gamma(`my_shape`, `my_rate`), the values for which were saved in our `stanvars` object, above. And it's that `stanvar()` line in that code wherein we told `brm()` we'd like that parameter to have a lower bound of 0. Let's put it to use.

```{r fit18.1111, message = F, warning = F}
fit18.1111 <-
  brm(data = my_data,
      family = student,
      satt_z ~ 1 + spend_z + prcnt_take_z + stu_tea_rat_z + salary_z,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, tau), class = b),
                set_prior("target += gamma_lpdf(tau | my_shape, my_rate)", check = FALSE),
                prior(normal(0, 1), class = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      control = list(adapt_delta = .99),
      stanvars = stanvars,
      seed = 18,
      file = "fits/fit18.1111")

fit18.0111 <-
  update(fit18.1111,
         formula = satt_z ~ 1           + prcnt_take_z + stu_tea_rat_z + salary_z,
         file = "fits/fit18.0111")

fit18.1011 <-
  update(fit18.1111,
         formula = satt_z ~ 1 + spend_z                + stu_tea_rat_z + salary_z,
         file = "fits/fit18.1011")
 
fit18.1101 <-
  update(fit18.1111,
         formula = satt_z ~ 1 + spend_z + prcnt_take_z                 + salary_z,
         file = "fits/fit18.1101")

fit18.1110 <-
  update(fit18.1111,
         formula = satt_z ~ 1 + spend_z + prcnt_take_z + stu_tea_rat_z           ,
         file = "fits/fit18.1110")

fit18.0011 <-
  update(fit18.1111,
         formula = satt_z ~ 1                          + stu_tea_rat_z + salary_z,
         file = "fits/fit18.0011")

fit18.0101 <-
  update(fit18.1111,
         formula = satt_z ~ 1           + prcnt_take_z                 + salary_z,
         file = "fits/fit18.0101")

fit18.0110 <-
  update(fit18.1111,
         formula = satt_z ~ 1           + prcnt_take_z + stu_tea_rat_z           ,
         file = "fits/fit18.0110")

fit18.1001 <-
  update(fit18.1111,
         formula = satt_z ~ 1 + spend_z                                + salary_z,
         file = "fits/fit18.1001")

fit18.1010 <-
  update(fit18.1111,
         formula = satt_z ~ 1 + spend_z                + stu_tea_rat_z           ,
         file = "fits/fit18.1010")

fit18.1100 <-
  update(fit18.1111,
         formula = satt_z ~ 1 + spend_z + prcnt_take_z                           ,
         file = "fits/fit18.1100")

fit18.0001 <-
  update(fit18.1111,
         formula = satt_z ~ 1                                          + salary_z,
         file = "fits/fit18.0001")

fit18.0010 <-
  update(fit18.1111,
         formula = satt_z ~ 1                          + stu_tea_rat_z           ,
         control = list(adapt_delta = .99999),
         seed = 18,
         file = "fits/fit18.0010")

fit18.0100 <-
  update(fit18.1111,
         formula = satt_z ~ 1           + prcnt_take_z                           ,
         file = "fits/fit18.0100")

fit18.1000 <-
  update(fit18.1111,
         formula = satt_z ~ 1 + spend_z                                          ,
         file = "fits/fit18.1000")

fit18.0000 <-
  update(fit18.1111,
         formula = satt_z ~ 1                                                    ,
         file = "fits/fit18.0000")
```

In order to keep track of the next 16 models, we switched our usual naming convention. Instead of continuing on keeping on calling them `fit18.17` through `fit18.33`, we used Kruschke's $\delta$ 0/1 convention. If we set the formula for the full model as `satt_z ~ 1 + spend_z + prcnt_take_z + stu_tea_rat_z + salary_z`, the name becomes `fit18.1111`. Accordingly, we called the model omitting `spend_z`, the first predictor, `fit18.0111`, and so on.

Before we go any further, here are the correlations among the $\beta$'s for the full model, `fit18.1111`.

```{r}
vcov(fit18.1111, correlation = T) %>% 
  round(digits = 3)
```

Once again, the HMC correlations differ from Kruschke's JAGS correlations. Moving on--behold the model weights.

```{r mw_fit18.1111, cache = T, message = F, warning = F}
(
  mw <-
  model_weights(fit18.1111, 
                fit18.0111, fit18.1011, fit18.1101, fit18.1110,
                fit18.0011, fit18.0101, fit18.0110, fit18.1001, fit18.1010, fit18.1100, 
                fit18.0001, fit18.0010, fit18.0100, fit18.1000,
                fit18.0000)
 )
```

We'll plot our model weights like before.

```{r, fig.width = 6, fig.height = 4}
mw %>% 
  data.frame() %>% 
  rownames_to_column() %>% 
  set_names("fit", "weight") %>% 
  mutate(fit = str_remove(fit, "fit18.")) %>% 
  
  ggplot(aes(x = weight, y = reorder(fit, weight))) +
  geom_text(aes(label = weight %>% round(3) %>% as.character()),
            size = 3) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(x = "Stacking weight",
       y = "fit18.[xxxx]") +
  theme(panel.grid = element_blank()) +
  panel_border()
```

As you might notice, pattern among model weights is similar with but not identical to the one among the model probabilities Kruschke displayed in Figure 18.15. Here we'll plot the histograms for our top six.

```{r, fig.width = 7, fig.height = 7, warning = F}
# first, we need to redefine `sd_x_3` and `sd_x_4` in terms of our two new predictors
sd_x_3 <- sd(my_data$StuTeaRat)
sd_x_4 <- sd(my_data$Salary)

## Now we'll start extracting our posterior draws and wrangling them, by model
# fit18.0100
as_draws_df(fit18.0100) %>% 
  transmute(PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z,
                                    sd_j   = sd_x_2,
                                    sd_y   = sd_y)) %>% 
  pivot_longer(everything()) %>% 
  mutate(fit = "fit18.0100") %>% 
  # fit18.0111
  bind_rows(
    as_draws_df(fit18.0111) %>% 
      transmute(PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z,
                                        sd_j   = sd_x_2,
                                        sd_y   = sd_y),
                StuTeaRat = make_beta_j(zeta_j = b_stu_tea_rat_z,
                                        sd_j   = sd_x_3,
                                        sd_y   = sd_y),
                Salary    = make_beta_j(zeta_j = b_salary_z,
                                        sd_j   = sd_x_4,
                                        sd_y   = sd_y)) %>% 
      pivot_longer(everything()) %>% 
      mutate(fit = "fit18.0111")
    )  %>% 
  # fit18.1100
  bind_rows(
    as_draws_df(fit18.1100) %>% 
      transmute(Spend     = make_beta_j(zeta_j = b_spend_z,
                                        sd_j   = sd_x_1,
                                        sd_y   = sd_y),
                PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z,
                                        sd_j   = sd_x_2,
                                        sd_y   = sd_y)) %>% 
      pivot_longer(everything()) %>% 
      mutate(fit = "fit18.1100")
    )  %>% 
  # fit18.1000
  bind_rows(
    as_draws_df(fit18.1000) %>% 
      transmute(Spend     = make_beta_j(zeta_j = b_spend_z,
                                        sd_j   = sd_x_1,
                                        sd_y   = sd_y)) %>% 
      pivot_longer(everything()) %>% 
      mutate(fit = "fit18.1000")
    )  %>% 
  # fit18.0001
  bind_rows(
    as_draws_df(fit18.0001) %>% 
      transmute(Salary    = make_beta_j(zeta_j = b_salary_z,
                                        sd_j   = sd_x_4,
                                        sd_y   = sd_y)) %>% 
      pivot_longer(everything()) %>% 
      mutate(fit = "fit18.0001")
    )  %>% 
  # fit18.0110       # spend_z + prcnt_take_z + stu_tea_rat_z
  bind_rows(
    as_draws_df(fit18.0110) %>% 
      transmute(PrcntTake = make_beta_j(zeta_j = b_prcnt_take_z,
                                        sd_j   = sd_x_2,
                                        sd_y   = sd_y),
                StuTeaRat = make_beta_j(zeta_j = b_stu_tea_rat_z,
                                        sd_j   = sd_x_3,
                                        sd_y   = sd_y)) %>% 
      pivot_longer(everything()) %>% 
      mutate(fit = "fit18.0110")
    ) %>% 
  # the next two lines just help order the grid the plots appear in
  mutate(name = factor(name, levels = c("Spend", "PrcntTake", "StuTeaRat", "Salary")),
         fit = factor(fit, levels = c("fit18.0100", "fit18.0111", "fit18.1100", "fit18.1000", "fit18.0001", "fit18.0110"))) %>%
  # finally, the plot!
  ggplot(aes(x = value, y = 0)) +
  stat_wilke(normalize = "panels", point_size = 4) +
  scale_fill_viridis_d(option = "D", begin = .2, end = .8) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(ylim = c(-0.01, NA)) +
  panel_border() +
  theme(legend.position = "none") +
  facet_grid(fit ~ name, scales = "free")
```

Like Kruschke's results in the text, `PrcntTake` was the most prominent predictor.

### What to report and what to conclude.

Kruschke made a great point in the opening paragraph of this subsection.

> It might make sense to use the single most credible model, especially if it is notably more credible than the runner up, and if the goal is to have a parsimonious explanatory description of the data. But it is important to recognize that using the single best model, when it excludes some predictors, is concluding that the regression coefficients on the excluded predictors are exactly zero. (p. 546)

Later he added "A forthright report should state the posterior probabilities of the several top models. Additionally, it can be useful to report, for each model, the ratio of its posterior probability relative to that of the best model" (p. 546). With our information criteria and model weights approach, we don't have posterior probabilities for the models themselves. But we can report on their information criteria comparisons and weights.

In the final paragraph of the subsection, Kruschke wrote:

> When the goal is prediction of $y$ for interesting values of the predictors, as opposed to parsimonious explanation, then it is usually not appropriate to use only the single most probable model. Instead, predictions should be based on as much information as possible, using all models to the extent that they are credible. This approach is called Bayesian model averaging (BMA). (p. 547)

It's worth it to walk this out a bit. With **brms**, one can use `brms::pp_average()` to get the weighted posterior distributions for the model parameters. This is a natural extension of our model weights comparisons.

```{r 18_pp_average_1, cache = T, warning = F}
# how many points on the x-axis?
n_points <- 30

# what vales of the predictors would we like to evaluate the weighted posterior over?
nd <-
  tibble(spend_z       = seq(from = -3, to = 3, length.out = n_points),
         prcnt_take_z  = 0, 
         stu_tea_rat_z = 0, 
         salary_z      = 0)

pp <-
  # the first things we feed into `pp_average()` are the `brm()` fits we'd like to average over
  pp_average(fit18.1111, fit18.0111, fit18.1011, fit18.1101, fit18.1110, fit18.0011, fit18.0101, fit18.0110, fit18.1001, fit18.1010, fit18.1100, fit18.0001, fit18.0010, fit18.0100, fit18.1000, fit18.0000,
             # here we tell it to evaluate the posterior over these predictor values
             newdata = nd, 
             # we can get the mean trends using the "fitted" method
             method = "fitted",
             # by `robust`, we mean we'd like the Estimate in terms of posterior medians, rather than means
             robust = T)

str(pp)
```

The `pp` object will require a little wrangling before it's of use for **ggplot2**.

```{r, fig.width = 6, fig.height = 2}
pp %>% 
  as_tibble() %>%
  bind_cols(nd) %>% 

  ggplot(aes(x = spend_z, y = Estimate,
             ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = 0, color = "grey25", linetype = 2) +
  geom_pointrange(shape = 21, stroke = 1/10, fatten = 6, 
                  color = "steelblue4", fill = "chocolate3") +
  labs(x = "Value of Spend_z",
       y = "Standardized SATT")
```

We can build on this to make a plot considering each of the four predictors. But first that requires we make a new `nd` tibble to feed into `pp_average()`.

```{r 18_pp_average_2, cache = T, warning = F}
# how many points on the x-axis?
n_points <- 30

# what vales of the predictors would we like to evaluate the weighted posterior over?
nd <-
  tibble(spend_z       = c(seq(from = -3, to = 3, length.out = n_points),
                           rep(0, times = n_points * 3)),
         
         prcnt_take_z  = c(rep(0, times = n_points),
                           seq(from = -3, to = 3, length.out = n_points),
                           rep(0, times = n_points * 2)), 
         
         stu_tea_rat_z = c(rep(0, times = n_points * 2),
                           seq(from = -3, to = 3, length.out = n_points),
                           rep(0, times = n_points)), 
         
         salary_z      = c(rep(0, times = n_points * 3),
                           seq(from = -3, to = 3, length.out = n_points)))

pp <-
  pp_average(fit18.1111, fit18.0111, fit18.1011, fit18.1101, fit18.1110, fit18.0011, fit18.0101, fit18.0110, fit18.1001, fit18.1010, fit18.1100, fit18.0001, fit18.0010, fit18.0100, fit18.1000, fit18.0000,
             newdata = nd, 
             method = "fitted",
             robust = T,
             # note the `probs` argument
             probs = c(.025, .975, .1, .9, .25, .75))

str(pp)
```

In each panel of the plot, below, we focus on one predictor. For that predictor, we hold all other three at their mean, which, since they are all standardized, is zero. We consider the posterior predictions for standardized SAT scores across a range of values each focal predictor. The posterior predictions are depicted in terms of 95%, 80%, and 50% percentile-based interval bands and a line at the median.

```{r, fig.width = 6, fig.height = 7}
pp %>% 
  as_tibble() %>% 
  mutate(x = seq(from = -3, to = 3, length.out = n_points) %>% rep(., times = 4),
         predictor = rep(c("Spend_z", "PrcntTake_z", "StuTeaRat_z", "Salary_z"), each = n_points)) %>% 
  
  ggplot(aes(x = x)) +
  geom_hline(yintercept = 0, color = "grey25", linetype = 2) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),
              alpha = 1/5, fill = "steelblue4") +
  geom_ribbon(aes(ymin = Q10, ymax = Q90),
              alpha = 1/4, fill = "steelblue4") +
  geom_ribbon(aes(ymin = Q25, ymax = Q75),
              alpha = 1/3, fill = "steelblue4") +
  geom_line(aes(y = Estimate),
            size = 1, color = "chocolate3") +
  labs(x = "Standardized value of the focal predictor",
       y = "Standardized SATT") +
  theme_minimal_vgrid() +
  panel_border() +
  facet_grid(predictor ~ ., scales = "free")
```

Based on the weighted average across the models, the `PrcntTake_z` predictor was the most potent.

### Caution: Computational methods.

> To conclude this section regarding variable selection, it is appropriate to recapitulate the considerations at the beginning of the section. Variable selection is a reasonable approach only if it is genuinely plausible and meaningful that candidate predictors have zero relation to the predicted variable. The results can be surprisingly sensitive to the seemingly innocuous choice of prior for the regression coefficients, and, of course, the prior for the inclusion probability. Because of these limitations, hierarchical shrinkage priors may be a more meaningful approach. (p. 548)

### Caution: Interaction variables.

> When interaction terms are included in a model that also has hierarchical shrinkage on regression coefficients, the interaction coefficients should not be put under the same higher-level prior distribution as the individual component coefficients, because interaction coefficients are conceptually from a different class of variables than individual components. (pp. 548--549)

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
# here we'll remove our objects
rm(n, d, d1, d2, grid, mus, sds, cors, covs, p1, p2, p3, my_data, my_arrow, p4, p5, p6, p7, p8, p9, p10, layout, standardize, stanvars, fit18.1, draws, my_upper, my_diag, my_lower, make_beta_0, make_beta_j, sd_x_1, sd_x_2, sd_y, m_x_1, m_x_2, m_y, stat_wilke, percent_take, fit18.2, sd_x_3, m_x_3, bounds, prior_draws, fit18.3, p11, p12, fit18.4, sd_x_4, sd_x_5, sd_x_6, sd_x_7, sd_x_8, sd_x_9, sd_x_10, sd_x_11, sd_x_12, sd_x_13, sd_x_14, m_x_4, m_x_5, m_x_6, m_x_7, m_x_8, m_x_9, m_x_10, m_x_11, m_x_12, m_x_13, m_x_14, fit18.5, fit18.6, fit18.7, fit18.8, mw, fit18.9, fit18.10, fit18.11, fit18.12, fit18.13, fit18.14, fit18.15, fit18.16, gamma_s_and_r_from_mode_sd, p, fit18.1111, fit18.0111, fit18.1011, fit18.1101, fit18.1110, fit18.0011, fit18.0101, fit18.0110, fit18.1001, fit18.1010, fit18.1100, fit18.0001, fit18.0010, fit18.0100, fit18.1000, fit18.0000, n_points, nd, pp)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

## Footnote {-}

[^5]: In his [Corrigenda](https://sites.google.com/site/doingbayesiandataanalysis/corrigenda), Kruschke further clarified: "that is true only in case there is a single predictor, not for multiple predictors. The statement could have said that 'R^2 is algebraically constrained to fall between −1 and +1 in least-squares regression'. More relevantly, replace the statement with the following: 'In multiple linear regression, standardized regression coefficients tend to fall between -2 and +2 unless the predictors are very strongly correlated and have strongly opposing effects. If your data have strongly correlated predictors, consider widening the prior.'"


<!--chapter:end:18.Rmd-->


```{r, echo = FALSE, cache = FALSE}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

# Metric Predicted Variable with One Nominal Predictor

> This chapter considers data structures that consist of a metric predicted variable and a nominal predictor.... This type of data structure can arise from experiments or from observational studies. In experiments, the researcher assigns the categories (at random) to the experimental subjects. In observational studies, both the nominal predictor value and the metric predicted value are generated by processes outside the direct control of the researcher. In either case, the same mathematical description can be applied to the data (although causality is best inferred from experimental intervention).
>
> The traditional treatment of this sort of data structure is called single-factor analysis of variance (ANOVA), or sometimes one-way ANOVA. Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter will also consider the situation in which there is also a metric predictor that accompanies the primary nominal predictor. The metric predictor is sometimes called a covariate, and the traditional treatment of this data structure is called analysis of covariance (ANCOVA). The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups, etc. [@kruschkeDoingBayesianData2015, pp. 553--554]

## Describing multiple groups of metric data

> Figure 19.1 illustrates the conventional description of grouped metric data. Each group is represented as a position on the horizontal axis. The vertical axis represents the variable to be predicted by group membership. The data are assumed to be normally distributed within groups, with equal standard deviation in all groups. The group means are deflections from overall baseline, such that the deflections sum to zero. Figure 19.1 provides a specific numerical example, with data that were randomly generated from the model. (p. 554)

We'll want a custom data-generating function for our primary group data.

```{r, warning = F, message = F}
library(tidyverse)

generate_data <- function(seed, mean) {
  set.seed(seed)
  rnorm(n, mean = grand_mean + mean, sd = 2)
}

n          <- 20
grand_mean <- 101

d <-
  tibble(group     = 1:5,
         deviation = c(4, -5, -2, 6, -3)) %>% 
  mutate(d = map2(group, deviation, generate_data)) %>% 
  unnest(d) %>% 
  mutate(iteration = rep(1:n, times = 5))

glimpse(d)
```

Here we'll make a tibble containing the necessary data for the rotated Gaussians. As far as I can tell, Kruschke's Gaussians only span to the bounds of percentile-based 98% intervals. We partition off those bounds for each `group` by the `ll` and `ul` columns in the first `mutate()` function. In the second `mutate()`, we expand the dataset to include a sequence of 100 values between those lower- and upper-limit points. In the third `mutate()`, we feed those points into the `dnorm()` function, with group-specific means and a common `sd`.

```{r}
densities <-
  d %>% 
  distinct(group, deviation) %>% 
  mutate(ll = qnorm(.01, mean = grand_mean + deviation, sd = 2),
         ul = qnorm(.99, mean = grand_mean + deviation, sd = 2)) %>% 
  mutate(d = map2(ll, ul, seq, length.out = 100)) %>% 
  mutate(density = map2(d, grand_mean + deviation, dnorm, sd = 2)) %>% 
  unnest(c(d, density))

head(densities)
```

We'll need two more supplementary tibbles to add the flourishes to the plot. The `arrow` tibble will specify our light-gray arrows. The `text` tibble will contain our annotation information.

```{r}
arrow <-
  tibble(d         = grand_mean,
         group     = 1:5,
         deviation = c(4, -5, -2, 6, -3),
         offset    = .1)

head(arrow)

text <-
  tibble(d         = grand_mean,
         group     = c(0:5, 0),
         deviation = c(0, 4, -5, -2, 6, -3, 10),
         offset    = rep(c(1/4, 0), times = c(6, 1)),
         angle     = rep(c(90, 0), times = c(6, 1)),
         label     = c("beta[0]==101", "beta['[1]']==4","beta['[2]']==-5", "beta['[3]']==-2", "beta['[4]']==6", "beta['[5]']==3", "sigma['all']==2"))

head(text)
```

We're almost ready to plot. Before we do, let's talk color and theme. For this chapter, we'll take our color palette from the [**palettetown** package](https://CRAN.R-project.org/package=palettetown) [@R-palettetown], which provides an array of color palettes inspired by [Pokémon](https://www.pokemon.com/us/). Our color palette will be #17, which is based on [Pidgeotto](https://www.pokemon.com/us/pokedex/pidgeotto).

```{r, warning = F, message = F, fig.height = 4}
library(palettetown)

scales::show_col(pokepal(pokemon = 17))

pp <- pokepal(pokemon = 17)

pp
```

Our overall plot theme will be based on the default `theme_grey()` with a good number of adjustments.

```{r}
theme_set(
  theme_grey() +
    theme(text = element_text(color = pp[4]),
          axis.text = element_text(color = pp[4]),
          axis.ticks = element_line(color = pp[4]),
          legend.background = element_blank(),
          legend.box.background = element_blank(),
          legend.key = element_rect(fill = pp[9]),
          panel.background = element_rect(fill = pp[9], color = pp[9]),
          panel.grid = element_blank(),
          plot.background = element_rect(fill = pp[12], color = pp[12]),
          strip.background = element_rect(fill = alpha(pp[2], 1/3), color = "transparent"),
          strip.text = element_text(color = pp[4]))
)
```

Now make Figure 19.1.

```{r, fig.width = 6, fig.height = 2.75, warning = F, message = F}
library(ggridges)

d %>% 
  ggplot(aes(x = d, y = group, group = group)) +
  geom_vline(xintercept = grand_mean, color = pp[12]) +
  geom_jitter(height = .05, alpha = 4/4, shape = 1, color = pp[10]) +
  # the Gaussians
  geom_ridgeline(data = densities,
                 aes(height = -density),
                 min_height = NA, scale = 3/2, size = 3/4,
                 fill = "transparent", color = pp[7]) +
  # the small arrows
  geom_segment(data = arrow,
               aes(xend = d + deviation,
                   y = group + offset, yend = group + offset),
               color = pp[5], size = 1,
               arrow = arrow(length = unit(.2, "cm"))) +
  # the large arrow on the left
  geom_segment(aes(x = 80, xend = grand_mean,
                   y = 0, yend = 0),
               color = pp[5], size = 3/4,
               arrow = arrow(length = unit(.2, "cm"))) +
  # the text
  geom_text(data = text,
            aes(x = grand_mean + deviation, y = group + offset,
                label = label, angle = angle), 
            size = 4, color = pp[4], parse = T) +
  scale_y_continuous(NULL, breaks = 1:5,
                     labels = c("<1,0,0,0,0>", "<0,1,0,0,0>", "<0,0,1,0,0>", "<0,0,0,1,0>", "<0,0,0,0,1>")) +
  xlab(NULL) +
  coord_flip(xlim = c(90, 112),
             ylim = c(-0.2, 5.5))
```

> The descriptive model presented in Figure 19.1 is the traditional one used by classical ANOVA (which is described a bit more in the next section). More general models are straight forward to implement in Bayesian software. For example, outliers could be accommodated by using heavy-tailed noise distributions (such as a $t$ distribution) instead of a normal distribution, and different groups could be given different standard deviations. (p. 556)

## Traditional analysis of variance

> The terminology, "analysis of variance," comes from a decomposition of overall data variance into within-group variance and between-group variance [@fisherStatisticalMethodsResearch1925]. Algebraically, the sum of squared deviations of the scores from their overall mean equals the sum of squared deviations of the scores from their respective group means plus the sum of squared deviations of the group means from the overall mean. In other words, the total variance can be partitioned into within-group variance plus between-group variance. Because one definition of the word "analysis" is separation into constituent parts, the term ANOVA accurately describes the underlying algebra in the traditional methods. That algebraic relation is not used in the hierarchical Bayesian approach presented here. The Bayesian method can estimate component variances, however. Therefore, the Bayesian approach is not ANOVA, but is analogous to ANOVA. (p. 556)

## Hierarchical Bayesian approach

"Our goal is to estimate its parameters in a Bayesian framework. Therefore, all the parameters need to be given a meaningfully structured prior distribution" (p. 557). However, our approach will depart a little from the one in the text. All our parameters will **not** "have generic noncommittal prior distributions" (p. 557). Most importantly, we will not follow the example in [@gelmanPriorDistributionsVariance2006] of putting a broad uniform prior on $\sigma_y$. Rather, we will continue using the half-Gaussian prior, as [recommended by the Stan team](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations). However, we will follow Kruschke's lead for the overall intercept and use a Gaussian prior "made broad on the scale of the data" (p. 557). And like Kruschke, we will estimate $\sigma_\beta$ from the data. To further get a sense of this, let's make our version of the hierarchical model diagram of Figure 19.2.

```{r, fig.width = 6.5, fig.height = 5.75, message = F}
library(patchwork)

# bracket
p1 <-
  tibble(x = .99,
         y = .5,
         label = "{_}") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, hjust = 1, color = pp[8], family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

##  plain arrow
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")
p2 <-
  tibble(x    = .72,
         y    = 1,
         xend = .68,
         yend = .25) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# normal density
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# second normal density
p4 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("0", "sigma[beta]"), 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# two annotated arrows
p5 <-
  tibble(x    = c(.16, .81),
         y    = c(1, 1),
         xend = c(.47, .77),
         yend = c(0, 0)) %>%
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pp[4]) +
  annotate(geom = "text",
           x = c(.25, .74, .83), y = .5,
           label = c("'~'", "'~'", "italic(j)"),
           size = c(10, 10, 7), 
           color = pp[4], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p6 <-
  tibble(x = .99,
         y = .25,
         label = "beta[0]+sum()[italic(j)]*beta['['*italic(j)*']']*italic(x)['['*italic(j)*']'](italic(i))") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(hjust = 1, size = 7, color = pp[4], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# half-normal density
p7 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma]", 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# annotated arrow
p8 <-
  tibble(x     = .38,
         y     = .65,
         label = "'='") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, color = pp[4], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = .25, 
               arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# the third normal density
p9 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("mu[italic(i)]", "sigma[italic(y)]"), 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# another annotated arrow
p10 <-
  tibble(x     = .55,
         y     = .6,
         label = "'~'") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, color = pp[4], parse = T, family = "Times") +
  geom_segment(x = .82, xend = .38,
               y = 1, yend = .2, 
               arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# the final annotated arrow
p11 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), 
            color = pp[4], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# some text
p12 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pp[4], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()


# define the layout
layout <- c(
  area(t = 1, b = 1, l = 6, r = 7),
  area(t = 2, b = 3, l = 6, r = 7),
  area(t = 3, b = 4, l = 1, r = 3),
  area(t = 3, b = 4, l = 5, r = 7),
  area(t = 6, b = 7, l = 1, r = 7),
  area(t = 5, b = 6, l = 1, r = 7),
  area(t = 6, b = 7, l = 9, r = 11),
  area(t = 9, b = 10, l = 5, r = 7),
  area(t = 8, b = 9, l = 5, r = 7),
  area(t = 8, b = 9, l = 5, r = 11),
  area(t = 11, b = 11, l = 5, r = 7),
  area(t = 12, b = 12, l = 5, r = 7)
)

# combine and plot!
(p1 + p2 + p3 + p4 + p6 + p5 + p7 + p9 + p8 + p10 + p11 + p12) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Later on in the text, Kruschke opined:

> A crucial pre-requisite for estimating $\sigma_\beta$ from all the groups is an assumption that all the groups are representative and informative for the estimate. It only makes sense to influence the estimate of one group with data from the other groups if the groups can be meaningfully described as representative of a shared higher-level distribution. (p. 559)

Although I agree with him in spirit, this doesn't appear to strictly be the case. As odd and paradoxical as this sounds, partial pooling can be of use even when the some of the cases are of a different kind. For more on the topic, see Efron and Morris's classic [-@efronSteinParadoxStatistics1977] paper, [*Stein's paradox in statistics*](http://statweb.stanford.edu/~ckirby/brad/other/Article1977.pdf), and [my blog post](https://solomonkurz.netlify.com/post/stein-s-paradox-and-what-partial-pooling-can-do-for-you/) walking out one of their examples in **brms**.

### Implementation in ~~JAGS~~ brms.

The **brms** setup, of course, differs a bit from JAGS.

```{r, eval = F}
fit <- 
  brm(data = my_data, 
      family = gaussian,
      y ~ 1 + (1 | categirical_variable),
      prior = c(prior(normal(0, x), class = Intercept),
                prior(normal(0, x), class = b),
                prior(cauchy(0, x), class = sd),
                prior(cauchy(0, x), class = sigma)))
```

The noise standard deviation $\sigma_y$ is depicted in the prior statement including the argument `class = sigma`. The grand mean is depicted by the first `1` in the model formula and its prior is indicated by the `class = Intercept` argument. We indicate we'd like group-based deviations from the grand mean with the `(1 | categirical_variable)` syntax, where the `1` on the left side of the bar indicates we'd like our intercepts to vary by group and the `categirical_variable` part simply represents the name of a given categorical variable we'd like those intercepts to vary by. The **brms** default is to do this with deviance scores, the mean for which will be zero. Although it's not obvious in the formula syntax, the model presumes the group-based deviations are normally distributed with a mean of zero and a standard deviation, which Kruschke termed $\sigma_\beta$. There is no prior for the mean. It's set at zero. But there is a prior for $\sigma_\beta$, which is denoted by the argument `class = sd`. We, of course, are not using a uniform prior on any of our variance parameters. But in order to be weakly informative, we will use the half-Cauchy. Recall that since the **brms** default is to set the lower bound for any variance parameter to 0, there's no need to worry about doing so ourselves. So even though the syntax only indicates `cauchy`, it's understood to mean Cauchy with a lower bound at zero; since the mean is usually 0, that makes is a half-Cauchy.

Kruschke set the upper bound for his $\sigma_y$ to 10 times the standard deviation of the criterion variable. The tails of the half-Cauchy are sufficiently fat that, in practice, I've found it doesn't matter much what you set the $SD$ of its prior to. One is often a sensible default for reasonably-scaled data. But if we want to take a more principled approach, we can set it to the size of the criterion's $SD$ or perhaps even 10 times that.

Kruschke suggested using a gamma on $\sigma_\beta$, which is a sensible alternative to half-Cauchy often used within the Stan universe. Especially in situations in which you would like to (a) keep the variance parameter above zero, but (b) still allow it to be arbitrarily close to zero, and also (c) let the likelihood dominate the posterior, the Stan team recommends the gamma(2, 0) prior, based on the paper by Chung and colleagues [-@chungNondegeneratePenalizedLikelihood2013, click [here](http://www.stat.columbia.edu/~gelman/research/published/chung_etal_Pmetrika2013.pdf)]. But you should note that I don't mean a literal 0 for the second parameter in the gamma distribution, but rather some small value like 0.1 or so. This is all clarified in @chungNondegeneratePenalizedLikelihood2013. Here's what $\operatorname{Gamma}(2, 0.1)$ looks like.

```{r, fig.width = 6, fig.height = 2}
tibble(x = seq(from = 0, to = 110, by = .1)) %>% 
  
  ggplot(aes(x = x, y = dgamma(x, 2, 0.1))) +
  geom_area(fill = pp[10]) +
  annotate(geom = "text", 
           x = 14.25, y = 0.015, label = "'gamma'*(2*', '*0.1)", 
           parse = T, color = pp[1], size = 4.25) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 110)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05)))
```

If you'd like that prior be even less informative, just reduce it to like $\operatorname{Gamma}(2, 0.01)$ or so. Kruschke goes further to recommend "the shape and rate parameters of the gamma distribution are set so its mode is `sd(y)/2` and its standard deviation is `2*sd(y)`, using the function `gammaShRaFromModeSD` explained in [Section 9.2.2][A realistic model with MCMC.]." (pp. 560--561). Let's make that function.

```{r}
gamma_a_b_from_omega_sigma <- function(mode, sd) {
  
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate <- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape <- 1 + mode * rate
  return(list(shape = shape, rate = rate))
  
}
```

So in the case of standardized data where `sd(1)` = 1, we'd use our `gamma_a_b_from_omega_sigma()` function like so.

```{r}
sd_y  <- 1 

omega <- sd_y / 2
sigma <- 2 * sd_y

(s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma))
```

And that produces the following gamma distribution.

```{r, fig.width = 6, fig.height = 2}
tibble(x = seq(from = 0, to = 21, by = .01)) %>% 
  
  ggplot(aes(x = x, y = dgamma(x, s_r$shape, s_r$rate))) +
  geom_area(fill = pp[8]) +
  annotate(geom = "text", 
           x = 2.75, y = 0.02, label = "'gamma'*(1.283196*', '*0.5663911)", 
           parse = T, color = pp[7], size = 2.75) +
  scale_x_continuous(breaks = c(0, 1, 5, 10, 20), expand = c(0, 0), limits = c(0, 21)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05)))
```

In the parameter space that matters, from zero to one, that gamma is pretty noninformative. It peaks between the two, slopes very gently rightward, but has the nice steep slope on the left keeping the estimates off the zero boundary. And even though that right slope is very gentle given the scale of the data, it's aggressive enough that it should keep the MCMC chains from spending a lot of time in ridiculous parts of the parameter space. I.e., when working with finite numbers of iterations, we want our MCMC chains wasting exactly zero iterations investigating what the density might be for $\sigma_\beta \approx 1e10$ for standardized data.

### Example: Sex and death.

Let's load and `glimpse()` at Hanley and Shapiro's [-@hanleySexualActivityLifespan1994] fruit-fly data.

```{r, message = F}
my_data <- read_csv("data.R/FruitflyDataReduced.csv")

glimpse(my_data)
```

We can use `geom_density_ridges()` to help get a sense of how our criterion `Longevity` is distributed across groups of `CompanionNumber`.

```{r, fig.width = 6, fig.height = 2.5, message = F}
my_data %>% 
  group_by(CompanionNumber) %>% 
  mutate(group_mean = mean(Longevity)) %>% 
  ungroup() %>% 
  mutate(CompanionNumber = fct_reorder(CompanionNumber, group_mean)) %>% 
  
  ggplot(aes(x = Longevity, y = CompanionNumber, fill = group_mean)) +
  geom_density_ridges(scale = 3/2, size = .2, color = pp[9]) +
  scale_fill_gradient(low = pp[4], high = pp[2]) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +
  scale_y_discrete(NULL, expand = expansion(mult = c(0, 0.4))) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        legend.position = "none")
```

Let's fire up **brms**.

```{r, warning = F, message = F}
library(brms)
```

We'll want to do the preparatory work to define our `stanvars`.

```{r}
(mean_y <- mean(my_data$Longevity))
(sd_y <- sd(my_data$Longevity))

omega <- sd_y / 2
sigma <- 2 * sd_y

(s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma))
```

With the prep work is done, here are our `stanvars`.

```{r}
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta")
```

Now fit the model, our hierarchical Bayesian alternative to an ANOVA.

```{r fit19.1}
fit19.1 <-
  brm(data = my_data,
      family = gaussian,
      Longevity ~ 1 + (1 | CompanionNumber),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      seed = 19,
      control = list(adapt_delta = 0.99),
      stanvars = stanvars,
      file = "fits/fit19.01")
``` 

Much like Kruschke's JAGS chains, our **brms** chains are well behaved, but only after fiddling with the `adapt_delta` setting.

```{r, fig.width = 8, fig.height = 4, message = F, warning = F}
library(bayesplot)

color_scheme_set(scheme = pp[c(10, 8, 12, 5, 1, 4)])

plot(fit19.1, widths = c(2, 3))
```

Also like Kruschke, our chains appear moderately autocorrelated.

```{r, fig.width = 7, fig.height = 4}
# extract the posterior draws
draws <- as_draws_df(fit19.1)

# plot
draws %>% 
  mutate(chain = .chain) %>% 
  mcmc_acf(pars = vars(b_Intercept:sigma), lags = 10)
```

Here's the model summary.

```{r}
print(fit19.1)
```

With the `ranef()` function, we can get the summaries of the group-specific deflections.

```{r}
ranef(fit19.1)
```

And with the `coef()` function, we can get those same group-level summaries in a non-deflection metric.

```{r}
coef(fit19.1)
```

Those are all estimates of the group-specific means. Since it wasn't modeled, all have the same parameter estimates for $\sigma_y$.

```{r}
posterior_summary(fit19.1)["sigma", ]
```

To prepare for our version of the top panel of Figure 19.3, we'll use `slice_sample()` to randomly sample from the posterior draws, saving the subset as `draws_20`.

```{r}
# how many random draws from the posterior would you like?
n_draws <- 20

# subset
set.seed(19)

draws_20 <-
  draws %>% 
  slice_sample(n = n_draws, replace = F)

glimpse(draws_20)
```

Before we make our version of the top panel, let's make a corresponding plot of the fixed intercept, the grand mean. The most important lines in the code, below are the ones where we used `stat_function()` within `mapply()`.

```{r, fig.height = 2.75, fig.width = 4}
tibble(x = c(0, 150)) %>% 

  ggplot(aes(x = x)) +
  mapply(function(mean, sd) {
    stat_function(fun   = dnorm, 
                  args  = list(mean = mean, sd = sd), 
                  alpha = 2/3, 
                  size  = 1/3,
                  color = pp[4])
    }, 
    # enter means and standard deviations here
    mean = draws_20 %>% pull(b_Intercept),
    sd   = draws_20 %>% pull(sigma)
    ) +
  geom_jitter(data = my_data, aes(x = Longevity, y = -0.001),
              height = .001, 
              alpha = 3/4, color = pp[10]) +
  scale_x_continuous("Longevity", breaks = 0:4 * 25,
                     limits = c(0, NA), expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Posterior Predictive Distribution",
       subtitle = "The jittered dots are the ungrouped Longevity data. The\nGaussians are posterior draws depicting the overall\ndistribution, the grand mean.") +
  coord_cartesian(xlim = c(0, 110))
```

Unfortunately, we can't extend our `mapply(stat_function())` method to the group-level estimates. To my knowledge, there isn't a way to show the group estimates at different spots along the y-axis. And our `mapply(stat_function())` approach has other limitations, too. Happily, we have some great alternatives. To use them, we'll need a little help from **tidybayes**.

```{r, warning = F, message = F}
library(tidybayes)
```

For the first part, we'll take `tidybayes::add_epred_draws()` for a whirl.

```{r}
densities <-
  my_data %>% 
  distinct(CompanionNumber) %>% 
  add_epred_draws(fit19.1, ndraws = 20, seed = 19, dpar = c("mu", "sigma"))

glimpse(densities)
```

With the first two lines, we made a $5 \times 1$ tibble containing the five levels of the experimental grouping variable, `CompanionNumber`. The `add_epred_draws()` function comes from **tidybayes** [see the [Posterior fits](https://mjskay.github.io/tidybayes/articles/tidy-brms.html#posterior-fits) section of @kayExtractingVisualizingTidy2021]. The first argument of the `add_epred_draws()` is `newdata`, which works much like it does in `brms::fitted()`; it took our $5 \times 1$ tibble. The next argument took our **brms** model fit, `fit19.1`. With the `ndraws` argument, we indicated we just wanted 20 random draws from the posterior. The `seed` argument makes those random draws reproducible. With `dpar`, we requested distributional regression parameters in the output. In our case, those were the $\mu$ and $\sigma$ values for each level of `CompanionNumber`. Since we took 20 draws across 5 groups, we ended up with a 100-row tibble.

The next steps are a direct extension of the method we used to make our Gaussians for our version of Figure 19.1.

```{r}
densities <-
  densities %>% 
  mutate(ll = qnorm(.025, mean = mu, sd = sigma),
         ul = qnorm(.975, mean = mu, sd = sigma)) %>% 
  mutate(Longevity = map2(ll, ul, seq, length.out = 100)) %>% 
  unnest(Longevity) %>% 
  mutate(density = dnorm(Longevity, mu, sigma))

glimpse(densities)
```

If you look at the code we used to make `ll` and `ul`, you'll see we used 95% intervals, this time. Our second `mutate()` function is basically the same. After unnesting the tibble, we just needed to plug in the `Longevity`, `mu`, and `sigma` values into the `dnorm()` function to compute the corresponding density values.

```{r, fig.height = 4.5, fig.width = 4}
densities %>% 
  ggplot(aes(x = Longevity, y = CompanionNumber)) +
  # here we make our density lines
  geom_ridgeline(aes(height = density, group = interaction(CompanionNumber, .draw)),
                 fill = NA, color = adjustcolor(pp[4], alpha.f = 2/3),
                 size = 1/3, scale = 25) +
  # the original data with little jitter thrown in
  geom_jitter(data = my_data,
              height = .04, alpha = 3/4, color = pp[10]) +
  # pretty much everything below this line is aesthetic fluff
  scale_x_continuous(breaks = 0:4 * 25, limits = c(0, 110), 
                     expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Data with Posterior Predictive Distrib.", 
       y = NULL) +
  coord_cartesian(ylim = c(1.25, 5.25)) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())
```

Do be aware that when you use this method, you may have to fiddle around with the `geom_ridgeline()` `scale` argument to get the Gaussian's heights on reasonable-looking relative heights. Stick in different numbers to get a sense of what I mean. I also find that I'm often not a fan of the way the spacing on the y axis ends up with default `geom_ridgeline()`. It's easy to overcome this with a little `ylim` fiddling.

To return to the more substantive interpretation, the top panel of

> Figure 19.3 suggests that the normal distributions with homogeneous variances appear to be reasonable descriptions of the data. There are no dramatic outliers relative to the posterior predicted curves, and the spread of the data within each group appears to be reasonably matched by the width of the posterior normal curves. (Be careful when making visual assessments of homogeneity of variance because the visual spread of the data depends on the sample size; for a reminder see the [see the right panel of Figure 17.1, p. 478].) The range of credible group means, indicated by the peaks of the normal curves, suggests that the group Virgin8 is clearly lower than the others, and the group Virgin1 might be lower than the controls. To find out for sure, we need to examine the differences of group means, which we do in the next section. (p. 564)

For clarity, the "see the right panel of Figure 17.1, p. 478" part was changed following Kruschke's [Corrigenda](https://sites.google.com/site/doingbayesiandataanalysis/corrigenda).

### Contrasts.

> It is straight forward to examine the posterior distribution of credible differences. Every step in the MCMC chain provides a combination of group means that are jointly credible, given the data. Therefore, every step in the MCMC chain provides a credible difference between groups...
>
>To construct the credible differences of group 1 and group 2, at every step in the MCMC chain we compute
>
> \begin{align*}
> \mu_1 - \mu_2 & =  (\beta_0 + \beta_1) - (\beta_0 + \beta_2) \\
>               & =  (+1) \cdot \beta_1 + (-1) \cdot \beta_2
> \end{align*}
>
> In other words, the baseline cancels out of the calculation, and the difference is a sum of weighted group deflections. Notice that the weights sum to zero. To construct the credible differences of the average of groups 1-3 and the average of groups 4-5, at every step in the MCMC chain we compute
>
> \small{
> \begin{align*}
> (\mu_1 + \mu_2 + \mu_3) / 3 - (\mu_4 + \mu_5) / 2 & = ((\beta_0 + \beta_1)  + (\beta_0 + \beta_2)  + (\beta_0 + \beta_3) ) / 3 - ((\beta_0 + \beta_4) + (\beta_0 + \beta_5) ) / 2 \\
> & = (\beta_1 + \beta_2 + \beta_3) / 3 - (\beta_4 + \beta_5) / 2 \\
> & = (+ 1/3) \cdot \beta_1 + (+ 1/3) \cdot \beta_2 + (+ 1/3) \cdot \beta_3 + (- 1/2) \cdot \beta_4 + (- 1/2) \cdot \beta_5
> \end{align*}
> }
> 
> Again, the difference is a sum of weighted group deflections. The coefficients on the group deflections have the properties that they sum to zero, with the positive coefficients summing to +1 and the negative coefficients summing to −1. Such a combination is called a contrast. The differences can also be expressed in terms of effect size, by dividing the difference by $\sigma_y$ at each step in the chain. (pp. 565--566)

To warm up, here's how to compute the first contrast shown in the lower portion of Kruschke's Figure 19.3--the contrast between the two pregnant conditions and the none-control condition.

```{r, fig.width = 3, fig.height = 2.5, warning = F}
draws %>% 
  transmute(c = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant8,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) %>% 
  
  ggplot(aes(x = c, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_color = pp[5], slab_fill = pp[5], color = pp[4]) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Pregnant1.Pregnant8 vs None0",
       x = expression(Difference~(mu[1]+mu[2])/2-mu[3]))
```

Up to this point, our primary mode of showing marginal posterior distributions has either been minute variations on Kruschke's typical histogram approach or with densities. We'll use those again in the future, too. In this chapter and the next, we'll veer a little further from the source material and depict our marginal posteriors with dot plots and their very close relatives, quantile plots. In the dot plot, above, each of the 4,000 posterior draws is depicted by one of the stacked brown dots. To stack the dots in neat columns like that, **tidybayes** has to round a little. Though we lose something in the numeric precision, we gain a lot in interpretability. We'll have more to say in just a moment.

In case you were curious, here are the HMC-based posterior mode and 95% HDIs.

```{r, warning = F}
draws %>% 
  transmute(difference = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant8,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) %>% 
  
  mode_hdi(difference)
```

Little difference, there. Now let's quantify the same contrast as a standardized mean difference effect size.

```{r, fig.width = 3, fig.height = 2.5, warning = F}
draws %>% 
  transmute(es = ((`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant8,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) / sigma) %>% 
  
  ggplot(aes(x = es, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4], 
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Pregnant1.Pregnant8 vs None0",
       x = expression(Effect~Size~(Difference/sigma[italic(y)])))
```

From a standardized-mean-difference perspective, that's tiny. Also note that because our model `fit19.1` did not allow the standard deviation parameter $\sigma_y$ to vary across groups, $\sigma_y$ is effectively a pooled standard deviation ($\sigma_p$).

Did you notice the `quantiles = 100` argument within `stat_dotsinterval()`? Instead of a dot plot with 4,000 tiny little dots, that argument converted the output to a quantile plot. The 4,000 posterior draws are now summarized by 100 dots, each of which represents $1\%$ of the total sample [see @kayWhenIshMy2016, @fernandesUncertaintyDisplaysUsing2018]. This quantile dot-plot method will be our main approach for the rest of the chapter.

Okay, now let's do the rest in bulk. First we'll do the difference scores.

```{r, fig.width = 8, fig.height = 2.5, warning = F}
differences <-
  draws %>% 
  transmute(`Pregnant1.Pregnant8.None0 vs Virgin1` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant8,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - `r_CompanionNumber[Virgin1,Intercept]`,
            
            `Virgin1 vs Virgin8` = `r_CompanionNumber[Virgin1,Intercept]` - `r_CompanionNumber[Virgin8,Intercept]`,
            
            `Pregnant1.Pregnant8.None0 vs Virgin1.Virgin8` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant8,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - (`r_CompanionNumber[Virgin1,Intercept]` + `r_CompanionNumber[Virgin8,Intercept]`) / 2)

differences %>% 
  pivot_longer(everything()) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4], 
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  facet_wrap(~ name, scales = "free")
```

Because we save our data wrangling labor from above as `differences`, it won't take much more effort to compute and plot the corresponding effect sizes as displayed in the bottom row of Figure 19.3.

```{r, fig.width = 8, fig.height = 2.5}
differences %>% 
  mutate_all(.funs = ~ . / draws$sigma) %>% 
  pivot_longer(everything()) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4], 
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Effect Size (Standardized mean difference)") +
  facet_wrap(~ name, scales = "free_x")
```

> In traditional ANOVA, analysts often perform a so-called omnibus test that asks whether it is plausible that all the groups are simultaneously exactly equal. I find that the omnibus test is rarely meaningful, however.... In the hierarchical Bayesian estimation used here, there is no direct equivalent to an omnibus test in ANOVA, and the emphasis is on examining all the meaningful contrasts. (p. 567)

Speaking of all meaningful contrasts, if you'd like to make all pairwise comparisons in a hierarchical model of this form, **tidybayes** offers a convenient way to do so [see the [Comparing levels of a factor](https://mjskay.github.io/tidybayes/articles/tidy-brms.html#comparing-levels-of-a-factor) section of @kayExtractingVisualizingTidy2021]. Here we'll demonstrate with `stat_dotsinterval()`.

```{r, fig.height = 4.5, fig.width = 6, warning = F}
fit19.1 %>%
  # these two lines are where the magic is at
  spread_draws(r_CompanionNumber[CompanionNumber,]) %>%
  compare_levels(r_CompanionNumber, by = CompanionNumber) %>%
  
  ggplot(aes(x = r_CompanionNumber, y = CompanionNumber)) +
  geom_vline(xintercept = 0, color = pp[12]) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4], 
                    slab_size = 0, quantiles = 100) +
  labs(x = "Contrast",
       y = NULL) +
  coord_cartesian(ylim = c(1.5, 10.5)) +
  theme(axis.text.y = element_text(hjust = 0))
```

But back to that omnibus test notion. If you really wanted to, I suppose one rough analogue would be to use information criteria to compare the hierarchical model to one that includes a single intercept with no group-level deflections. Here's what the simpler model would look like.

```{r fit19.2}
fit19.2 <-
  brm(data = my_data,
      family = gaussian,
      Longevity ~ 1,
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      seed = 19,
      stanvars = stanvars,
      file = "fits/fit19.02")
``` 

Here's the model summary.

```{r}
print(fit19.2)
```

Here are their LOO values and their difference score.

```{r, message = F}
fit19.1 <- add_criterion(fit19.1, criterion = "loo")
fit19.2 <- add_criterion(fit19.2, criterion = "loo")

loo_compare(fit19.1, fit19.2) %>% 
  print(simplify = F)
```

The hierarchical model has a better LOO. Here are the stacking-based model weights.

```{r}
(mw <- model_weights(fit19.1, fit19.2))
```

If you don't like scientific notation, just `round()`.

```{r}
mw %>% 
  round(digits = 3)
```

Yep, in complimenting the LOO difference, virtually all the stacking weight went to the hierarchical model. You might think of this another way. The conceptual question we're asking is *Does it make sense to say that the* $\sigma_\beta$ *parameter is zero? Is zero a credible value?* We'll, I suppose we could just look at the posterior to assess for that.

```{r, fig.width = 4, fig.height = 2.5}
draws %>% 
  ggplot(aes(x = sd_CompanionNumber__Intercept, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4], 
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 50)) +
  labs(title = expression("Behold the fit19.1 posterior for "*sigma[beta]*"."),
       subtitle = "This parameter's many things, but zero isn't one of them.",
       x = NULL)
```

Yeah, zero and other values close to zero don't look credible for that parameter. 95% of the mass is between 5 and 30, with the bulk hovering around 10. We don't need an $F$-test or even a LOO model comparison to see the writing on wall.

### Multiple comparisons and shrinkage.

> The previous section suggested that an analyst should investigate all contrasts of interest. This recommendation can be thought to conflict with traditional advice in the context on null hypothesis significance testing, which instead recommends that a minimal number of comparisons should be conducted in order to maximize the power of each test while keeping the overall false alarm rate capped at 5% (or whatever maximum is desired).... Instead, a Bayesian analysis can mitigate false alarms by incorporating prior knowledge into the model. In particular, hierarchical structure (which is an expression of prior knowledge) produces shrinkage of estimates, and shrinkage can help rein in estimates of spurious outlying data. For example, in the posterior distribution from the fruit fly data, the modal values of the posterior group means have a range of 23.2. The sample means of the groups have a range of 26.1. Thus, there is some shrinkage in the estimated means. The amount of shrinkage is dictated only by the data and by the prior structure, not by the intended tests. (p. 568)

We may as well compute those ranges by hand. Here's the range of the observed data.

```{r, message = F}
my_data %>% 
  group_by(CompanionNumber) %>% 
  summarise(mean  = mean(Longevity)) %>% 
  summarise(range = max(mean) - min(mean))
```

For our hierarchical model `fit19.1`, the posterior means are rank ordered in the same way as the empirical data.

```{r}
coef(fit19.1)$CompanionNumber[, , "Intercept"] %>% 
  data.frame() %>% 
  rownames_to_column(var = "companion_number") %>% 
  arrange(Estimate) %>% 
  mutate_if(is.double, round, digits = 1)
```

If we compute the range by a difference of the point estimates of the highest and lowest posterior means, we can get a quick number.

```{r}
coef(fit19.1)$CompanionNumber[, , "Intercept"] %>% 
  as_tibble() %>% 
  summarise(range = max(Estimate) - min(Estimate))
```

Note that wasn't fully Bayesian of us. Those means and their difference carry uncertainty and that uncertainty can be fully expressed if we use all the posterior draws (i.e., use `summary = F` and wrangle).

```{r}
coef(fit19.1, summary = F)$CompanionNumber[, , "Intercept"] %>% 
  as_tibble() %>% 
  transmute(range = Pregnant1 - Virgin8) %>% 
  mode_hdi(range)
```

Happily, the central tendency of the range is near equivalent with both methods, but now we have 95% intervals, too. Do note how wide they are. This is why we work with the full set of posterior draws.

### The two-group case.

> A special case of our current scenario is when there are only two groups. The model of the present section could, in principle, be applied to the two-group case, but the hierarchical structure would do little good because there is virtually no shrinkage when there are so few groups (and the top-level prior on $\sigma_\beta$ is broad as assumed here). (p. 568)

For kicks and giggles, let's practice. Since `Pregnant1` and `Virgin8` had the highest and lowest empirical means—making them the groups best suited to define our range, we'll use them to fit the 2-group hierarchical model. To fit it with haste, just use `update()`.

```{r fit19.3}
fit19.3 <-
  update(fit19.1,
         newdata = my_data %>% 
           filter(CompanionNumber %in% c("Pregnant1", "Virgin8")),
         seed = 19,
         file = "fits/fit19.03")
```

Even with just two groups, there were no gross issues with fitting the model.

```{r}
print(fit19.3)
```

If you compare the posteriors for $\sigma_\beta$ across the two models, you'll see how the one for `fit19.3` is substantially larger.

```{r}
posterior_summary(fit19.1)["sd_CompanionNumber__Intercept", ]
posterior_summary(fit19.3)["sd_CompanionNumber__Intercept", ]
```

Here that is in a coefficient plot using `tidybayes::stat_interval()`.

```{r, fig.width = 6, fig.height = 1.25, warning = F}
bind_rows(as_draws_df(fit19.1) %>% select(sd_CompanionNumber__Intercept),
          as_draws_df(fit19.3) %>% select(sd_CompanionNumber__Intercept)) %>% 
  mutate(fit = rep(c("fit19.1", "fit19.3"), each = n() / 2)) %>% 
  
  ggplot(aes(x = sd_CompanionNumber__Intercept, y = fit)) +
  stat_interval(point_interval = mode_hdi, .width = c(.5, .8, .95)) +
  scale_color_manual(values = pp[c(11, 5, 7)], 
                     labels = c("95%", "80%", "50%")) +
  scale_x_continuous(expression(sigma[beta]), 
                     limits = c(0, NA), expand = expansion(mult = c(0, 0.05))) +
  ylab(NULL) +
  theme(legend.key.size = unit(0.45, "cm"))
```

This all implies less shrinkage and a larger range.

```{r, warning = F, message = F}
coef(fit19.3, summary = F)$CompanionNumber[, , "Intercept"] %>% 
  as_tibble() %>% 
  transmute(range = Pregnant1 - Virgin8) %>% 
  mode_hdi(range)
```

And indeed, the range between the two groups is larger. Now the posterior mode for their difference has almost converged to that of the raw data. Kruschke then went on to recommend using a single-level model in such situations, instead.

>  That is why the two-group model in [Section 16.3][Two groups] did not use hierarchical structure, as illustrated in Figure 16.11 (p. 468). That model also used a $t$ distribution to accommodate outliers in the data, and that model allowed for heterogeneous variances across groups. Thus, for two groups, it is more appropriate to use the model of Section 16.3. The hierarchical multi-group model is generalized to accommodate outliers and heterogeneous variances in [Section 19.5][Heterogeneous variances and robustness against outliers]. (p. 568)

As a refresher, here's what the **brms** code for that Chapter 16 model looked like.

```{r, eval = F}
fit16.3 <-
  brm(data = my_data,
      family = student,
      bf(Score ~ 0 + Group, 
         sigma ~ 0 + Group),
      prior = c(prior(normal(mean_y, sd_y * 100), class = b),
                prior(normal(0, log(sd_y)), class = b, dpar = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvars,
      seed = 16,
      file = "fits/fit16.03")
```

Let's adjust it for our data. Since we have a reduced data set, we'll need to re-compute our `stanvars` values, which were based on the raw data.

```{r}
# it's easier to just make a reduced data set
my_small_data <-
  my_data %>% 
  filter(CompanionNumber %in% c("Pregnant1", "Virgin8"))
  
(mean_y <- mean(my_small_data$Longevity))
(sd_y <- sd(my_small_data$Longevity))

omega <- sd_y / 2
sigma <- 2 * sd_y

(s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma))
```

Here we update `stanvars`.

```{r}
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta") +
  stanvar(1/29,      name = "one_over_twentynine")
```

Note that our priors, here, are something of a blend of those from Chapter 16 and those from our hierarchical model, `fit19.1`.

```{r, fit19.4}
fit19.4 <-
  brm(data = my_small_data,
      family = student,
      bf(Longevity ~ 0 + CompanionNumber, 
         sigma ~ 0 + CompanionNumber),
      prior = c(prior(normal(mean_y, sd_y * 10), class = b),
                prior(normal(0, log(sd_y)), class = b, dpar = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      seed = 19,
      stanvars = stanvars,
      file = "fits/fit19.04")
```

Here's the model summary.

```{r}
print(fit19.4)
```

Man, look at those `Bulk_ESS` values! As it turns out, they can be [greater than the number of post-warmup samples](https://andrewgelman.com/2018/01/18/measuring-speed-stan-incorrectly-faster-thought-cases-due-antithetical-sampling/). And here's the range in posterior means.

```{r, warning = F, message = F}
fixef(fit19.4, summary = F) %>% 
  as_tibble() %>% 
  transmute(range = CompanionNumberPregnant1 - CompanionNumberVirgin8) %>% 
  mode_hdi(range)
```

The results are pretty much the same as that of the two-group hierarchical model, maybe a touch larger. Yep, Kruschke was right. Hierarchical models with two groups and permissive priors on $\sigma_\beta$ don't shrink the estimates to the grand mean all that much.

## Including a metric predictor

"In Figure 19.3, the data within each group have a large standard deviation. For example, longevities in the Virgin8 group range from 20 to 60 days" (p. 568). Turns out Kruschke's slightly wrong on this. Probably just a typo.

```{r, message = F}
my_data %>% 
  group_by(CompanionNumber) %>% 
  summarise(min   = min(Longevity),
            max   = max(Longevity),
            range = max(Longevity) - min(Longevity))
```

But you get the point. For each group, there was quite a range. We might add predictors to the model to help account for those ranges.

> The additional metric predictor is sometimes called a covariate. In the experimental setting, the focus of interest is usually on the nominal predictor (i.e., the experimental treatments), and the covariate is typically thought of as an ancillary predictor to help isolate the effect of the nominal predictor. But mathematically the nominal and metric predictors have equal status in the model. Let’s denote the value of the metric covariate for subject $i$ as $x_\text{cov}(i)$. Then the expected value of the predicted variable for subject $i$ is
>
> $$\mu (i) = \beta_0 + \sum_j \beta_{[j]} x_{[j]} (i) + \beta_\text{cov}  x_\text{cov}(i)$$
>
with the usual sum-to-zero constraint on the deflections of the nominal predictor stated in Equation 19.2. In words, Equation 19.5 says that the predicted value for subject $i$ is a baseline plus a deflection due to the group of $i$ plus a shift due to the value of $i$ on the covariate. (p. 569)

And the $j$ subscript, recall, denotes group membership. In this context, it often

> makes sense to set the intercept as the mean of predicted values if the covariate is re-centered at its mean value, which is denoted $\overline x_\text{cov}$. Therefore Equation 19.5 is algebraically reformulated to make the baseline respect those constraints.... The first equation below is simply Equation 19.5 with $x_\text{cov}$ recentered on its mean, $\overline x_\text{cov}$. The second line below merely algebraically rearranges the terms so that the nominal deflections sum to zero and the constants are combined into the overall baseline:
>
> \begin{align*}
> \mu & = \alpha_0 + \sum_j \alpha_{[j]} x_{[j]} + \alpha_\text{cov} (x_\text{cov} - \overline{x}_\text{cov}) \\
>     & = \underbrace{\alpha_0 + \overline{\alpha} - \alpha_\text{cov} \overline{x}_\text{cov}}_{\beta_0} + \sum_j \underbrace{(\alpha_{[j]} - \overline{\alpha})}_{\beta_[j]} x_{[j]} + \underbrace{\alpha_\text{cov}}_{\beta_{\text{cov}}} x_\text{cov} \\
> & \text{where } \overline{\alpha} = \frac{1}{J} \sum^J_{j = 1} \alpha_{[j]}
> \end{align*}
> (pp. 569--570)

We have a visual depiction of all this in the hierarchical model diagram of Figure 19.4.

```{r, fig.width = 7.5, fig.height = 6, message = F}
# bracket
p1 <-
  tibble(x = .99,
         y = .5,
         label = "{_}") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, hjust = 1, color = pp[8], family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

#  plain arrow
p2 <-
  tibble(x    = .71,
         y    = 1,
         xend = .68,
         yend = .25) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# normal density
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# second normal density
p4 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("0", "sigma[beta]"), 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# third density
p5 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("italic(M)[italic(c)]", "italic(S)[italic(c)]"), 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# three annotated arrows
p6 <-
  tibble(x    = c(.09, .49, .9),
         y    = c(1, 1, 1),
         xend = c(.20, .40, .64),
         yend = c(0, 0, 0)) %>%
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pp[4]) +
  annotate(geom = "text",
           x = c(.11, .42, .47, .74), y = .5,
           label = c("'~'", "'~'", "italic(j)", "'~'"),
           size = c(10, 10, 7, 10), 
           color = pp[4], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p7 <-
  tibble(x = .99,
         y = .25,
         label = "beta[0]+sum()[italic(j)]*beta['['*italic(j)*']']*italic(x)['['*italic(j)*']'](italic(i))+beta[italic(cov)]*italic(x)[italic(cov)](italic(i))") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(hjust = 1, size = 7, color = pp[4], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# half-normal density
p8 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma]", 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# annotated arrow
p9 <-
  tibble(x     = .38,
         y     = .65,
         label = "'='") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, color = pp[4], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = .25, 
               arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# the fourth normal density
p10 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("mu[italic(i)]", "sigma[italic(y)]"), 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# another annotated arrow
p11 <-
  tibble(x     = .5,
         y     = .6,
         label = "'~'") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, color = pp[4], parse = T, family = "Times") +
  geom_segment(x = .85, xend = .27,
               y = 1, yend = .2, 
               arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# the final annotated arrow
p12 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), 
            color = pp[4], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# some text
p13 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pp[4], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()


# define the layout
layout <- c(
  area(t = 1, b = 1, l = 6, r = 7),
  area(t = 2, b = 3, l = 6, r = 7),
  area(t = 3, b = 4, l = 1, r = 3),
  area(t = 3, b = 4, l = 5, r = 7),
  area(t = 3, b = 4, l = 9, r = 11),
  area(t = 6, b = 7, l = 1, r = 9),
  area(t = 5, b = 6, l = 1, r = 11),
  area(t = 6, b = 7, l = 11, r = 13),
  area(t = 9, b = 10, l = 5, r = 7),
  area(t = 8, b = 9, l = 5, r = 7),
  area(t = 8, b = 9, l = 5, r = 13),
  area(t = 11, b = 11, l = 5, r = 7),
  area(t = 12, b = 12, l = 5, r = 7)
)

# combine and plot!
(p1 + p2 + p3 + p4 + p5 + p7 + p6 + p8 + p10 + p9 + p11 + p12 + p13) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

### Example: Sex, death, and size.

Kruschke recalled `fit19.1`'s estimate for $\sigma_y$ had a posterior mode around 14.8. Let's confirm with a plot.

```{r, fig.width = 4, fig.height = 2}
as_draws_df(fit19.1) %>% 
  ggplot(aes(x = sigma, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4], 
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(sigma[italic(y)])) +
  theme(panel.grid = element_blank())
```

Yep, that looks about right. That large of a difference in days would indeed make it difficult to detect between-group differences if those differences were typically on the scale of just a few days. Since `Thorax` is moderately correlated with `Longevity`, including `Thorax` in the statistical model should help shrink that $\sigma_y$ estimate, making it easier to compare group means. Following the sensibilities from the equations just above, here we'll mean-center our covariate, first.

```{r}
my_data <-
  my_data %>% 
  mutate(thorax_c = Thorax - mean(Thorax))

head(my_data)
```

Our model code follows the structure of that in Kruschke's `Jags-Ymet-Xnom1met1-MnormalHom-Example.R` and `Jags-Ymet-Xnom1met1-MnormalHom.R` files. As a preparatory step, we redefine the values necessary for `stanvars`.

```{r}
(mean_y <- mean(my_data$Longevity))
(sd_y <- sd(my_data$Longevity))
(sd_thorax_c <- sd(my_data$thorax_c))

omega <- sd_y / 2
sigma <- 2 * sd_y

(s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma))

stanvars <- 
  stanvar(mean_y,      name = "mean_y") + 
  stanvar(sd_y,        name = "sd_y") +
  stanvar(sd_thorax_c, name = "sd_thorax_c") +
  stanvar(s_r$shape,   name = "alpha") +
  stanvar(s_r$rate,    name = "beta")
```

Now we're ready to fit the `brm()` model, our hierarchical alternative to ANCOVA.

```{r fit19.5}
fit19.5 <-
  brm(data = my_data,
      family = gaussian,
      Longevity ~ 1 + thorax_c + (1 | CompanionNumber),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(normal(0, 2 * sd_y / sd_thorax_c), class = b),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      seed = 19,
      control = list(adapt_delta = 0.99),
      stanvars = stanvars,
      file = "fits/fit19.05")
``` 

Here's the model summary.

```{r}
print(fit19.5)
```

Let's see if that $\sigma_y$ posterior shrank.

```{r, fig.width = 4, fig.height = 2}
as_draws_df(fit19.5) %>% 
  ggplot(aes(x = sigma, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4], 
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(sigma[italic(y)]))
```

Yep, sure did! Now our between-group comparisons should be more precise. Heck, if we wanted to we could even make a difference plot.

```{r}
tibble(sigma1 = as_draws_df(fit19.1) %>% pull(sigma),
       sigma5 = as_draws_df(fit19.5) %>% pull(sigma))
```

```{r, fig.width = 4, fig.height = 2.25, warning = F}
tibble(sigma1 = as_draws_df(fit19.1) %>% pull(sigma),
       sigma5 = as_draws_df(fit19.5) %>% pull(sigma)) %>% 
  transmute(dif = sigma1 - sigma5) %>% 
  
  ggplot(aes(x = dif, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4], 
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "This is a difference distribution",
       x = expression(sigma[italic(y)][" | fit19.1"]-sigma[italic(y)][" | fit19.5"]))
```

If you want a quick and dirty plot of the relation between `thorax_c` and `Longevity`, you might employ `brms::conditional_effects()`.

```{r, fig.width = 3, fig.height = 3}
conditional_effects(fit19.5) %>% 
  plot(line_args = list(color = pp[5], fill = pp[11]))
```

But to make plots like the ones at the top of Figure 19.5, we'll have to work a little harder. First, we need some intermediary values marking off the three values along the `Thorax`-axis Kruschke singled out in his top panel plots. As far as I can tell, they were the `min()`, the `max()`, and their `mean()`.

```{r}
(r <- range(my_data$Thorax))
mean(r)
```

Next, we'll make the data necessary for our side-tipped Gaussians. For kicks and giggles, we'll choose 80 draws instead of 20. But do note how we used our `r` values, from above, to specify both `Thorax` and `thorax_c` values in addition to the `CompanionNumber` categories for the `newdata` argument. Otherwise, this workflow is very much the same as in previous plots.

```{r}
n_draws <- 80

densities <-
  my_data %>% 
  distinct(CompanionNumber) %>% 
  expand(CompanionNumber, Thorax = c(r[1], mean(r), r[2])) %>% 
  mutate(thorax_c  = Thorax - mean(my_data$Thorax)) %>% 
  add_epred_draws(fit19.5, ndraws = n_draws, seed = 19, dpar = c("mu", "sigma")) %>% 
  mutate(ll = qnorm(.025, mean = mu, sd = sigma),
         ul = qnorm(.975, mean = mu, sd = sigma)) %>% 
  mutate(Longevity = map2(ll, ul, seq, length.out = 100)) %>% 
  unnest(Longevity) %>% 
  mutate(density = dnorm(Longevity, mu, sigma))

glimpse(densities)
```

Here, we'll use a simplified workflow to extract the `fitted()` values in order to make the regression lines. Since these are straight lines, all we need are two values for each draw, one at the extremes of the `Thorax` axis.

```{r}
f <-
  my_data %>% 
  distinct(CompanionNumber) %>% 
  expand(CompanionNumber, Thorax = c(r[1], mean(r), r[2])) %>% 
  mutate(thorax_c = Thorax - mean(my_data$Thorax)) %>% 
  add_epred_draws(fit19.5, ndraws = n_draws, seed = 19, value = "Longevity")

glimpse(f)
```

Now we're ready to make our plots for the top row of Figure 19.3.

```{r, fig.width = 8, fig.height = 2.5}
densities %>% 
  ggplot(aes(x = Longevity, y = Thorax)) +
  # the Gaussians
  geom_ridgeline(aes(height = -density, group = interaction(Thorax, .draw)),
                 fill = NA, size = 1/5, scale = 5/3,
                 color = adjustcolor(pp[4], alpha.f = 1/5),
                 min_height = NA) +
  # the vertical lines below the Gaussians
  geom_line(aes(group = interaction(Thorax, .draw)),
            color = pp[4], alpha = 1/5, size = 1/5) +
  # the regression lines
  geom_line(data = f,
            aes(group = .draw),
            alpha = 1/5, size = 1/5, color = pp[4]) +
  # the data
  geom_point(data = my_data,
             alpha = 3/4, color = pp[10]) +
  coord_flip(xlim = c(0, 110),
             ylim = c(.58, 1)) +
  facet_wrap(~ CompanionNumber, ncol = 5)
```

Now we have a covariate in the model, we have to decide on which of its values we want to base our group comparisons. Unless there's a substantive reason for another value, the mean is a good standard choice. And since the covariate `thorax_c` is already mean centered, that means we can effectively leave it out of the equation. Here we make and save them in the simple difference metric.

```{r, warning = F}
draws <- as_draws_df(fit19.5)

differences <-
  draws %>% 
  transmute(`Pregnant1.Pregnant8 vs None0` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`,
            
            `Pregnant1.Pregnant8.None0 vs Virgin1` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - `r_CompanionNumber[Virgin1,Intercept]`,
            
            `Virgin1 vs Virgin8` = `r_CompanionNumber[Virgin1,Intercept]` - `r_CompanionNumber[Virgin8,Intercept]`,
            
            `Pregnant1.Pregnant8.None0 vs Virgin1.Virgin8` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - (`r_CompanionNumber[Virgin1,Intercept]` + `r_CompanionNumber[Virgin8,Intercept]`) / 2)

p1 <-
  differences %>% 
  pivot_longer(everything()) %>%   
  
  ggplot(aes(x = value, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4], 
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  theme(strip.text = element_text(size = 6.4)) +
  facet_wrap(~ name, scales = "free_x", ncol = 4)
```

Now we'll look at the differences in the effect size metric. Since we saved our leg work above, it's really easy to just convert the differences in bulk with `mutate_all()`. After the conversion, we'll bind the two rows of subplots together with a little **patchwork** and display the results.

```{r, fig.width = 8, fig.height = 5}
p2 <-
  differences %>% 
  mutate_all(.funs = ~. / draws$sigma) %>% 
  pivot_longer(everything()) %>%   
  
  ggplot(aes(x = value, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4], 
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Effect Size") +
  theme(strip.text = element_text(size = 6.4)) +
  facet_wrap(~ name, scales = "free_x", ncol = 4)

# combine
p1 / p2
```

"The HDI widths of all the contrasts have gotten smaller by virtue of including the covariate in the analysis" (p. 571).

### Analogous to traditional ANCOVA.

In contrast with ANCOVA,

> Bayesian methods do not partition the least-squares variance to make estimates, and therefore the Bayesian method is analogous to ANCOVA but is not ANCOVA. Frequentist practitioners are urged to test (with $p$ values) whether the assumptions of (a) equal slope in all groups, (b) equal standard deviation in all groups, and (c) normally distributed noise can be rejected. In a Bayesian approach, the descriptive model is generalized to address these concerns, as will be discussed in [Section 19.5][Heterogeneous variances and robustness against outliers]. (p. 572)

### Relation to hierarchical linear regression.

Here Kruschke contrasts our last model with the one from way back in [Section 17.3][Hierarchical regression on individuals within groups]. As a refresher, here's what that code looked like.

```{r, eval = F}
fit17.4 <-
  brm(data = my_data,
      family = student,
      y_z ~ 1 + x_z + (1 + x_z || Subj),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(normal(0, 1), class = sigma),
                # the next line is new
                prior(normal(0, 1), class = sd),
                prior(exponential(one_over_twentynine) + 1, class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvar(1/29, name = "one_over_twentynine"),
      seed = 17,
      file = "fits/fit17.04")
```

And for convenience, here's the code from the model we just fit.

```{r eval = F}
fit19.5 <-
  brm(data = my_data,
      family = gaussian,
      Longevity ~ 1 + thorax_c + (1 | CompanionNumber),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(normal(0, 2 * sd_y / sd_thorax_c), class = b),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      seed = 19,
      control = list(adapt_delta = 0.99),
      stanvars = stanvars,
      file = "fits/fit19.05")
``` 

It's easy to get lost in the differences in the priors and the technical details with the model chains and such. The main thing to notice, here, is the differences in the model formulas (i.e., the likelihoods). Both models had intercepts and slopes. But whereas the model from 17.3 set both parameters to random, only the intercept in our last model was random. The covariate `thorax_c` was fixed--it did not vary by group. Had we wanted it to, our `formula` syntax would have been something like `Longevity ~ 1 + thorax_c + (1 + thorax_c || CompanionNumber)`. And again, as noted in [Section 17.3.1][The model and implementation in ~~JAGS~~ brms.], the `||` portion of the syntax set the random intercepts and slopes to be orthogonal (i.e., correlate exactly at zero). As we'll see, this will often not be the case. But let's not get ahead of ourselves.

> Conceptually, the main difference between the models is merely the focus of attention. In the hierarchical linear regression model, the focus was on the slope coefficient. In that case, we were trying to estimate the magnitude of the slope, simultaneously for individuals and overall. The intercepts, which describe the levels of the nominal predictor, were of ancillary interest. In the present section, on the other hand, the focus of attention is reversed. We are most interested in the intercepts and their differences between groups, with the slopes on the covariate being of ancillary interest. (p. 573)

## Heterogeneous variances and robustness against outliers

In Figure 19.6 on page 574, Kruschke laid out the diagram for a hierarchical Student's-$t$ model in for which both the $\mu$ and $\sigma$ parameters are random. If you recall, @Bürkner2022Distributional calls these [distributional models](https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html) and they are indeed available within the **brms** framework. But there's a catch. Though we can model $\sigma$ all day long and we can even make it hierarchical, **brms** limits us to modeling the hierarchical $\sigma$ parameters within the typical Gaussian framework. That is, we will depart from Kruschke's schematic in that we will be

* modeling the log of $\sigma$,
* indicating its grand mean with the `sigma ~ 1` syntax,
* modeling the group-level deflections as Gaussian with a mean of 0 and standard deviation $\sigma_\sigma$ estimated from the data,
* and choosing a sensible prior for $\sigma_\sigma$ that is left-bound at 0 and gently slopes to the right (i.e., a folded $t$ or gamma distribution).

Thus, here's our **brms**-centric variant of the diagram in Figure 19.6.

```{r, fig.width = 8, fig.height = 6, message = F}
# bracket
p1 <-
  tibble(x = .99,
         y = .5,
         label = "{_}") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, hjust = 1, color = pp[8], family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

#  plain arrow
p2 <-
  tibble(x    = .68,
         y    = 1,
         xend = .68,
         yend = .25) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# normal density
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# second normal density
p4 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("0", "sigma[beta]"), 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# third normal density
p5 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("italic(M)[mu[sigma]]", "italic(S)[mu[sigma]]"), 
           hjust = c(.5, 0),
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# half-normal density
p6 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 6, color = pp[4]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma[sigma]]", 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# exponential density
p7 <-
  tibble(x = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "exp",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(K)",
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# likelihood formula
p8 <-
  tibble(x = .5,
         y = .25,
         label = "beta[0]+sum()[italic(j)]*beta['['*italic(j)*']']*italic(x)['['*italic(j)*']'](italic(i))") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pp[4], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# normal density
p9 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(0, 1.2), y = .6,
           hjust = c(.5, 0),
           label = c("mu[sigma]", "sigma[sigma]"), 
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# four annotated arrows
p10 <-
  tibble(x    = c(.06, .37, .67, .95),
         y    = c(1, 1, 1, 1),
         xend = c(.15, .31, .665, .745),
         yend = c(0, 0, .2, .2)) %>%
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pp[4]) +
  annotate(geom = "text",
           x = c(.065, .31, .36, .64, .79), y = .5,
           label = c("'~'", "'~'", "italic(j)", "'~'", "'~'" ),
           size = c(10, 10, 7, 10, 10), 
           color = pp[4], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# student-t density
p11 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) +
  geom_area(fill = pp[9]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "student t",
           size = 7, color = pp[4]) +
  annotate(geom = "text",
           x = c(-1.4, 0), y = .6,
           label = c("nu", "mu[italic(i)]"),
           size = 7, color = pp[4], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pp[4]))

# log sigma
p12 <-
  tibble(x = .65,
         y = .6,
         label = "log~sigma[italic(j)*'('*italic(i)*')']") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(hjust = 0, size = 7, color = pp[4], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) +
  ylim(0, 1) +
  theme_void()

# two annotated arrows
p13 <-
  tibble(x    = c(.15, .15),
         y    = c(1, .47),
         xend = c(.15, .72),
         yend = c(.75, .1)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pp[4]) +
  annotate(geom = "text",
           x = c(.1, .15, .28), y = c(.92, .64, .22),
           label = c("'~'", "nu*minute+1", "'='"),
           size = c(10, 7, 10), color = pp[4], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# one annotated arrow
p14 <-
  tibble(x     = .38,
         y     = .65,
         label = "'='") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, color = pp[4], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = .15, 
               arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# another annotated arrow
p15 <-
  tibble(x     = c(.58, .71),
         y     = .6,
         label = c("'~'", "italic(j)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = pp[4], parse = T, family = "Times") +
  geom_segment(x = .85, xend = .42,
               y = 1, yend = .18, 
               arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# the final annotated arrow
p16 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), 
            color = pp[4], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               arrow = my_arrow, color = pp[4]) +
  xlim(0, 1) +
  theme_void()

# some text
p17 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pp[4], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 1, l = 8, r = 9),
  area(t = 2, b = 3, l = 8, r = 9),
  area(t = 3, b = 4, l = 3, r = 5),
  area(t = 3, b = 4, l = 7, r = 9),
  area(t = 3, b = 4, l = 11, r = 13),
  area(t = 3, b = 4, l = 15, r = 17),
  area(t = 6, b = 7, l = 1, r = 3),
  area(t = 6, b = 7, l = 5, r = 9),
  area(t = 6, b = 7, l = 11, r = 13),
  area(t = 5, b = 6, l = 3, r = 17),
  area(t = 10, b = 11, l = 6, r = 8),
  area(t = 10, b = 11, l = 7, r = 9),
  area(t = 8, b = 10, l = 1, r = 8),
  area(t = 8, b = 10, l = 6, r = 8),
  area(t = 8, b = 10, l = 6, r = 13),
  area(t = 12, b = 12, l = 6, r = 8),
  area(t = 13, b = 13, l = 6, r = 8)
)

# combine and plot!
(p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10 + p11 + p12 + p13 + p14 + p15 + p16 + p17) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Since we're modeling $\log \left (\sigma_{j(i)} \right )$, we might use Gaussian prior centered on `sd(my_data$y) %>% log()` and a reasonable spread like 1. We can simulate a little to get a sense of what those distributions look like.

```{r, fig.width = 7.5, fig.height = 3}
n_draws <- 1e3

set.seed(19)
tibble(prior = rnorm(n_draws, mean = log(1), sd = 1)) %>% 
  mutate(prior_exp = exp(prior)) %>% 
  pivot_longer(everything()) %>% 

  ggplot(aes(x = value)) +
  stat_dots(slab_fill = pp[5], slab_color = pp[5]) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +
  facet_wrap(~ name, scales = "free")
```

Here's what is looks like with `sd = 2`.

```{r, fig.width = 3.75, fig.height = 2.75}
set.seed(19)
tibble(prior = rnorm(n_draws, mean = log(1), sd = 2)) %>% 
  mutate(prior_exp = exp(prior)) %>% 

  ggplot(aes(x = prior_exp)) +
  stat_dots(slab_fill = pp[5], slab_color = pp[5]) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +
  coord_cartesian(xlim = c(0, 17))
```

Though we're still peaking around 1, there's more mass in the tail, making it easier for the likelihood to pull away from the prior mode.

But all this is the prior on the fixed effect, the grand mean of $\log (\sigma)$. Keep in mind we're also estimating group-level deflections using a hierarchical model. The good old folded $t$ on the unit scale is already pretty permissive for an estimate that is itself on the log scale. To make it more conservative, set $\nu$ to infinity and go with a folded Gaussian. Or keep your regularization loose and go with a low-$\nu$ folded $t$ or even a folded Cauchy. And, of course, one could even go with a gamma.

Consider we have data `my_data` for which our primary variable of interest is `y`. Starting from preparing our `stanvars` values, here's what the model code might look like.

```{r, eval = F}
# get ready for `stanvars`
mean_y <- mean(my_data$y)
sd_y   <- sd(my_data$y)

omega  <- sd_y / 2
sigma  <- 2 * sd_y

s_r    <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)

# define `stanvars`
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta") +
  stanvar(1/29,      name = "one_over_twentynine")

# fit the model
fit <-
  brm(data = my_data,
      family = student,
      bf(Longevity ~ 1 + (1 | CompanionNumber), 
         sigma     ~ 1 + (1 | CompanionNumber)),
      prior = c(# grand means
                prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(normal(log(sd_y), 1), class = Intercept, dpar = sigma),
                
                # the priors controlling the spread for our hierarchical deflections
                prior(gamma(alpha, beta), class = sd),
                prior(normal(0, 1), class = sd, dpar = sigma),
                
                # don't forget our student-t nu
                prior(exponential(one_over_twentynine), class = nu)),
      stanvars = stanvars)
``` 

### Example: Contrast of means with different variances.

Let's load and take a look at Kruschke's simulated group data.

```{r, message = F}
my_data <- read_csv("data.R/NonhomogVarData.csv")

head(my_data)
```

Here are the means and $SD$s for each `Group`.

```{r, message = F}
my_data %>% 
  group_by(Group) %>% 
  summarise(mean = mean(Y),
            sd   = sd(Y))
```

First we'll fit the model with homogeneous variances. To keep things simple, here we'll fit a conventional model following the form of our original `fit1`. Here are our `stanvars`.

```{r}
(mean_y <- mean(my_data$Y))
(sd_y <- sd(my_data$Y))

omega <- sd_y / 2
sigma <- 2 * sd_y

(s_r  <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma))

# define `stanvars`
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta")
```

Now fit the ANOVA-like homogeneous-variances model.

```{r fit19.6}
fit19.6 <-
  brm(data = my_data,
      family = gaussian,
      Y ~ 1 + (1 | Group),
      prior = c(prior(normal(mean_y, sd_y * 10), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      seed = 19,
      control = list(adapt_delta = .995),
      stanvars = stanvars,
      file = "fits/fit19.06")
``` 

Here's the model summary.

```{r}
print(fit19.6)
```

Let's get ready to make our version of the top of Figure 19.7. First we wrangle.

```{r}
# how many model-implied Gaussians would you like?
n_draws <- 20

densities <-
  my_data %>% 
  distinct(Group) %>% 
  add_epred_draws(fit19.6, ndraws = n_draws, seed = 19, dpar = c("mu", "sigma")) %>% 
  mutate(ll = qnorm(.025, mean = mu, sd = sigma),
         ul = qnorm(.975, mean = mu, sd = sigma)) %>% 
  mutate(Y = map2(ll, ul, seq, length.out = 100)) %>% 
  unnest(Y) %>% 
  mutate(density = dnorm(Y, mu, sigma)) %>% 
  group_by(.draw) %>% 
  mutate(density = density / max(density))

glimpse(densities)
```

In our wrangling code, the main thing to notice is those last two lines. If you look closely to Kruschke's Gaussians, you'll notice they all have the same maximum height. Up to this point, ours haven't. This has to do with technicalities on how densities are scaled. In brief, the wider densities have been shorter. So those last two lines scaled all the densities within the same group to the same metric. Otherwise the code was business as usual.

Anyway, here's our version of the top panel of Figure 19.7.
  
```{r, fig.height = 3.5, fig.width = 3.75}
densities %>% 
  ggplot(aes(x = Y, y = Group)) +
  geom_ridgeline(aes(height = density, group = interaction(Group, .draw)),
                 fill = NA, color = adjustcolor(pp[7], alpha.f = 2/3),
                 size = 1/3, scale = 3/4) +
  geom_jitter(data = my_data,
              height = .04, alpha = 3/4, color = pp[10]) +
  scale_x_continuous(breaks = seq(from = 80, to = 120, by = 10)) +
  labs(title = "Data with Posterior Predictive Distrib.", 
       y = NULL) +
  coord_cartesian(xlim = c(75, 125),
                  ylim = c(1.25, 4.5)) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())
```

Here are the difference distributions in the middle of Figure 19.7.

```{r, fig.width = 4, fig.height = 2.5, warning = F}
draws <- as_draws_df(fit19.6)

differences <-
  draws %>% 
  transmute(`D vs A` = `r_Group[D,Intercept]` - `r_Group[A,Intercept]`,
            `C vs B` = `r_Group[C,Intercept]` - `r_Group[B,Intercept]`)

differences %>% 
  pivot_longer(everything()) %>%
  mutate(name = factor(name, levels = c("D vs A", "C vs B"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4],
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = "Difference") +
  facet_wrap(~ name, scales = "free_x", ncol = 4)
```

Now here are the effect sizes at the bottom of the figure.

```{r, fig.width = 4, fig.height = 2.5}
differences %>% 
  mutate_all(.funs = ~ . / draws$sigma) %>% 
  pivot_longer(everything()) %>%
  mutate(name = factor(name, levels = c("D vs A", "C vs B"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4],
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = "Effect Size") +
  facet_wrap(~ name, scales = "free_x", ncol = 4)
```

Oh and remember, if you'd like to get all the possible contrasts in bulk, **tidybayes** has got your back.

```{r, fig.height = 4, fig.width = 6}
fit19.6 %>%
  spread_draws(r_Group[Group,]) %>%
  compare_levels(r_Group, by = Group) %>%
  # these next two lines allow us to reorder the contrasts along the y
  ungroup() %>% 
  mutate(Group = reorder(Group, r_Group)) %>%
  
  ggplot(aes(x = r_Group, y = Group)) +
  geom_vline(xintercept = 0, color = pp[12]) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4],
                    slab_size = 0, quantiles = 100) +
  labs(x = "Contrast",
       y = NULL) +
  coord_cartesian(ylim = c(1.5, 6.5)) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())
```

But to get back on track, here are the `stanvars` for the robust hierarchical variances model.

```{r}
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta") +
  stanvar(1/29,      name = "one_over_twentynine")
```

Now fit that robust better-than-ANOVA model.

```{r fit19.7}
fit19.7 <-
  brm(data = my_data,
      family = student,
      bf(Y     ~ 1 + (1 | Group), 
         sigma ~ 1 + (1 | Group)),
      prior = c(# grand means
                prior(normal(mean_y, sd_y * 10), class = Intercept),
                prior(normal(log(sd_y), 1), class = Intercept, dpar = sigma),
                
                # the priors controlling the spread for our hierarchical deflections
                prior(gamma(alpha, beta), class = sd),
                prior(normal(0, 1), class = sd, dpar = sigma),
                
                # don't forget our student-t nu
                prior(exponential(one_over_twentynine), class = nu)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      seed = 19,
      control = list(adapt_delta = 0.99,
                     max_treedepth = 12),
      stanvars = stanvars,
      file = "fits/fit19.07")
``` 

The chains look good.

```{r, fig.width = 8, fig.height = 5}
plot(fit19.7, widths = c(2, 3))
```

Here's the parameter summary.

```{r}
print(fit19.7)
```

Let's get ready to make our version of the top of Figure 19.7. First we wrangle.

```{r}
densities <-
  my_data %>% 
  distinct(Group) %>% 
  add_epred_draws(fit19.7, ndraws = n_draws, seed = 19, dpar = c("mu", "sigma", "nu")) %>% 
  mutate(ll = qt(.025, df = nu),
         ul = qt(.975, df = nu)) %>% 
  mutate(Y = map2(ll, ul, seq, length.out = 100)) %>% 
  unnest(Y) %>%
  mutate(density = dt(Y, nu)) %>% 
  # notice the conversion
  mutate(Y = mu + Y * sigma) %>% 
  group_by(.draw) %>% 
  mutate(density = density / max(density))

glimpse(densities)
```

If you look closely at our code, above, you'll note switching from the Gaussian to the Student $t$ required changes in our flow. Most obviously, we switched from `qnorm()` and `dnorm()` to `qt()` and `dt()`, respectively. The base **R** Student $t$ functions don't take arguments for $\mu$ and $\sigma$. Rather, they're presumed to be 0 and 1, respectively. That means that for our first three `mutate()` functions, the computations were all based on the standard Student $t$, with only the $\nu$ parameter varying according to the posterior. The way we corrected for that was with the fourth `mutate()`.

Now we're ready to make and save our version of the top panel of Figure 19.7.

```{r}
p1 <-
  densities %>% 
  ggplot(aes(x = Y, y = Group)) +
  geom_ridgeline(aes(height = density, group = interaction(Group, .draw)),
                 fill = NA, color = adjustcolor(pp[7], alpha.f = 2/3),
                 size = 1/3, scale = 3/4) +
  geom_jitter(data = my_data,
              height = .04, alpha = 3/4, color = pp[10]) +
  scale_x_continuous(breaks = seq(from = 80, to = 120, by = 10)) +
  labs(title = "Data with Posterior Predictive Distrib.", 
       y = NULL) +
  coord_cartesian(xlim = c(75, 125),
                  ylim = c(1.25, 4.5)) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())
```

Here we make the difference distributions in the middle of Figure 19.8.

```{r, fig.width = 4, fig.height = 2.5, warning = F}
draws <- as_draws_df(fit19.7)

p2 <-
  draws %>% 
  transmute(`D vs A` = `r_Group[D,Intercept]` - `r_Group[A,Intercept]`,
            `C vs B` = `r_Group[C,Intercept]` - `r_Group[B,Intercept]`) %>% 
  pivot_longer(everything()) %>%
  mutate(name = factor(name, levels = c("D vs A", "C vs B"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4],
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  facet_wrap(~ name, scales = "free_x", ncol = 4)
```

And here we make the plots of the corresponding effect sizes at the bottom of the Figure 19.8.

```{r, fig.width = 4, fig.height = 2.5, warning = F}
# first compute and save the sigma_j's, which will come in handy later 
draws <-
  draws %>% 
  mutate(sigma_A = exp(b_sigma_Intercept + `r_Group__sigma[A,Intercept]`),
         sigma_B = exp(b_sigma_Intercept + `r_Group__sigma[B,Intercept]`),
         sigma_C = exp(b_sigma_Intercept + `r_Group__sigma[C,Intercept]`),
         sigma_D = exp(b_sigma_Intercept + `r_Group__sigma[D,Intercept]`))

p3 <-
  draws %>% 
  # note we're using pooled standard deviations to standardize our effect sizes, here
  transmute(`D vs A` = (`r_Group[D,Intercept]` - `r_Group[A,Intercept]`) / sqrt((sigma_A^2 + sigma_D^2) / 2),
            `C vs B` = (`r_Group[C,Intercept]` - `r_Group[B,Intercept]`) / sqrt((sigma_B^2 + sigma_C^2) / 2)) %>% 
  pivot_longer(everything()) %>%
  mutate(name = factor(name, levels = c("D vs A", "C vs B"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4],
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Effect Size") +
  facet_wrap(~ name, scales = "free_x", ncol = 4)
```

Combine them all and plot!

```{r, fig.width = 4.5, fig.height = 8}
p1 / p2 / p3 + plot_layout(heights = c(2, 1, 1))
```

Notice that because (a) the sigma parameters were heterogeneous and (b) they were estimated on the log scale, we had to do quite a bit more data processing before they effect size estimates were ready.

"Finally, because each group has its own estimated scale (i.e., $\sigma_j$), we can investigate differences in scales across groups" (p. 578). That's not a bad idea. Even though Kruschke didn't show this in the text, we may as well give it a go.

```{r, fig.width = 8, fig.height = 2.5, warning = F}
# recall we computed the sigma_j's a couple blocks up;
# now we put them to use
draws %>% 
  transmute(`D vs A` = sigma_D - sigma_A,
            `C vs B` = sigma_C - sigma_B,
            `D vs C` = sigma_D - sigma_C,
            `B vs A` = sigma_B - sigma_A) %>% 
  pivot_longer(everything()) %>%
  mutate(name = factor(name, levels = c("D vs A", "C vs B", "D vs C", "B vs A"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[5], color = pp[4],
                    slab_size = 0, quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(Differences~'in'~sigma[italic(j)])) +
  facet_wrap(~ name, scales = "free", ncol = 4)
```

For more on models including a hierarchical structure on both the mean and scale structures, check out [Donald Williams](https://twitter.com/wdonald_1985) and colleagues' work on what they call Mixed Effect Location and Scale Models [MELSM; e.g., @williamsBayesianMultivariateMixedeffects2021; @williamsBayesianNonlinearMixedeffects2019]. They're quite handy and I've begun using them in my applied work (e.g., [here](https://osf.io/vekpf/)). You can also find a brief introduction to MELSM's within the context of the multilevel growth model in [Section 14.6](https://bookdown.org/content/4857/adventures-in-covariance.html#summary-bonus-multilevel-growth-models-and-the-melsm) of my [-@kurzStatisticalRethinkingBrms2020] translation of McElreath's [-@mcelreathStatisticalRethinkingBayesian2020] second edition.

## ~~Exercises~~ Walk out an effect size

We computed a lot of effect sizes in this chapter. They were all standardized mean differences. @cohenStatisticalPowerAnalysis1988 discussed these kinds of effect sizes in this way:

> We need a "pure" number, one free of our original measurement unit, with which to index what can be alternately called the degree of departure from the null hypothesis of the alternate hypothesis, or the ES (effect size) we wish to detect. This is accomplished by standardizing the raw effect size as expressed in the measurement unit of the dependent variable by dividing it by the (common) standard deviation of the measures in their respective populations, the latter also in the original measurement unit. (p. 20)

Though Cohen framed his discussion in terms of null-hypothesis significance testing, we can just as easily apply it to our Bayesian modeling framework. The main thing is we can use his definitions from above to define a particular kind of effect size--the standardized mean difference between two groups. This is commonly referred to as a Cohen's $d$, which follows the formula

$$d = \frac{\bar y_A - \bar y_B}{s_p},$$

where the unstandardized means of the variable of interest $y$ are compared between two groups, A and B. From the raw data, we compute their two means, $\bar y_A$ and $\bar y_B$, and divide their difference by the common (i.e., pooled) standard deviation $s_p$. In practice, the empirically-derived means and standard deviations are stand-ins (i.e., estimates) of the population parameters. If we're willing to ignore uncertainty, we can do this all by hand.

Let's walk this out with the fruit-fly data from [Section 19.3.2][Example: Sex and death.].

```{r, message = F}
my_data <- read_csv("data.R/FruitflyDataReduced.csv")

glimpse(my_data)
```

Recall we have five groups indexed by `CompanionNumber`, each with $n = 25$.

```{r}
my_data %>% 
  count(CompanionNumber)
```

Let's focus on just two groups, the male fruit flies for which individual males were supplied access to one or with virgin female fruit flies per day. In the data, these are `CompanionNumber == Virgin1` and `CompanionNumber == Virgin8`, respectively. Here's a look at their mean `Longevity` values.

```{r, message = F}
my_data %>% 
  filter(str_detect(CompanionNumber, "Virgin")) %>% 
  group_by(CompanionNumber) %>% 
  summarise(mean = mean(Longevity))
```

If we're willing to treat the males in the `Virgin1` group as group "a" and those in the `Virgin8` group as group "b", we can save those mean values like so.

```{r}
y_bar_a <- filter(my_data, CompanionNumber == "Virgin1") %>% summarise(y_bar = mean(Longevity)) %>% pull()
y_bar_b <- filter(my_data, CompanionNumber == "Virgin8") %>% summarise(y_bar = mean(Longevity)) %>% pull()
```

Now we'll compute their pooled standard deviation.

```{r, message = F}
my_data %>% 
  filter(str_detect(CompanionNumber, "Virgin")) %>% 
  group_by(CompanionNumber) %>% 
  summarise(s = sd(Longevity)) %>% 
  summarise(s_p = sqrt(sum(s^2) / 2))
```

Save that value.

```{r}
s_a <- filter(my_data, CompanionNumber == "Virgin1") %>% summarise(s = sd(Longevity)) %>% pull()
s_b <- filter(my_data, CompanionNumber == "Virgin8") %>% summarise(s = sd(Longevity)) %>% pull()

s_p <- sqrt((s_a^2 + s_b^2) / 2)
```

If you're confused by how we computed the pooled standard deviation, recall that when comparing two groups that may have different group-level standard deviations, the formula for the $s_p$ is 

$$s_p = \sqrt{\frac{s_A^2 + s_B^2}{2}},$$

where $s_A$ and $s_B$ are the group-level standard deviations. Kruschke introduced this formula back in [Section 16.3][Two groups] and we briefly emphasized it in our [Bonus Section 16.3.0.1][Bonus: Pooled standard deviation.]. Now we have the sample $s_p$ in hand, computing Cohen's $d$ is just simple arithmetic.

```{r}
(y_bar_a - y_bar_b) / s_p
```

Though I'm not up on contemporary standards in fruit fly research, a Cohen's $d$ of that size would be considered [conspicuously] large in most areas of my field (psychology). If we'd like to compute the $d$ estimates for any other combination of experimental conditions, we'd just follow the corresponding arithmetic.

As I hinted at earlier, the problem with this approach is it ignores uncertainty. Frequentists use various formulas to express this in terms of 95% confidence intervals. Our approach will be to express it with the posterior distribution of a Bayesian model. We've already accomplished this with our `fit19.1` from above. Here we'll use three other approaches.

Instead of the Bayesian hierarchical alternative to the frequentist ANOVA, we can use a single-level model where we predict a metric variable with separate intercepts for the two levels of `CompanionNumber`. First, we subset the data and define our `stanvars`.

```{r}
my_data <-
  my_data %>% 
  filter(str_detect(CompanionNumber, "Virgin"))

mean_y <- (y_bar_a + y_bar_b) / 2

stanvars <- 
  stanvar(mean_y, name = "mean_y") + 
  stanvar(s_p,    name = "sd_y")
```

Fit the model with `brm()`.

```{r fit19.8}
fit19.8 <-
  brm(data = my_data,
      family = gaussian,
      Longevity ~ 0 + CompanionNumber,
      prior = c(prior(normal(mean_y, sd_y * 5), class = b),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 19,
      stanvars = stanvars,
      file = "fits/fit19.08")
```

Check the summary.

```{r}
print(fit19.8)
```

Extract the posterior draws.

```{r}
draws <- as_draws_df(fit19.8)
```

Here we'll plot the three dimensions of the posterior, each with the corresponding value from the Cohen's $d$ formula marked off as a vertical line in the foreground.

```{r, fig.width = 8, fig.height = 2.5, warning = F}
lines <-
  tibble(name  = c("b_CompanionNumberVirgin1", "b_CompanionNumberVirgin8", "sigma"),
         value = c(y_bar_a, y_bar_b, s_p))

draws %>% 
  pivot_longer(b_CompanionNumberVirgin1:sigma) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[2], color = pp[7],
                    slab_size = 0, quantiles = 100) +
  geom_vline(data = lines,
             aes(xintercept = value),
             color = pp[13], linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("posterior") +
  facet_wrap(~ name, scales = "free")
```

The model did a great job capturing all three parameters. If we would like to compute our Cohen's $d$ using the posterior iterations from `fit19.8`, we'd execute something like this.

```{r, fig.width = 3, fig.height = 2.5}
draws %>% 
  mutate(d = (b_CompanionNumberVirgin1 - b_CompanionNumberVirgin8) / sigma) %>% 
  
  ggplot(aes(x = d, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[2], color = pp[7],
                    slab_size = 0, quantiles = 100) +
  geom_vline(xintercept = (y_bar_a - y_bar_b) / s_p,
             color = pp[13], linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression("Cohen's"~italic(d)~"expressed as a posterior"))
```

Similar to the previous plots, this time we superimposed the posterior density with the sample estimate for $d$ we computed above, `(y_bar_a - y_bar_b) / s_p`. Happily, the hand-calculated estimate coheres nicely with the central tendency of our posterior distribution. But now we get a full measure of uncertainty. Notice how wide those 95% HDIs are. Hopefully this isn't a surprise given our noncommittal priors and only $n = 25$ for both groups. There's a lot of uncertainty in that posterior.

A second way we might use a single-level model to compute a Cohen's $d$ effect size is using a dummy variable. We'll convert our nominal variable `CompanionNumber` into a binary variable `Virgin1` for which 1 corresponds to `CompanionNumber == Virgin1` and 0 corresponds to `CompanionNumber == Virgin8`. Compute the dummy.

```{r}
my_data <-
  my_data %>% 
  mutate(Virgin1 = if_else(CompanionNumber == "Virgin1", 1, 0))
```

Now fit the dummy-predictor model with `brm()`.

```{r fit19.9}
fit19.9 <-
  brm(data = my_data,
      family = gaussian,
      Longevity ~ 1 + Virgin1,
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(normal(0, sd_y * 5), class = b),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 19,
      stanvars = stanvars,
      file = "fits/fit19.09")
```

```{r}
print(fit19.9)
```

With this parameterization, our posterior for `Intercept` is the same, within simulation variation, as `CompanionNumberVirgin8` from `fit7`. The posterior for `sigma` is about the same for both models, too. But focus on `Virgin1`. This is the unstandardized mean difference, what we called $\bar y_A - \bar y_B$ when we computed Cohen's $d$ using sample statistics. Here's a look at its posterior distribution with its empirical estimate superimposed with a vertical line.

```{r, fig.width = 3, fig.height = 2.5}
draws <- as_draws_df(fit19.9)

draws %>% 
  ggplot(aes(x = b_Virgin1, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[2], color = pp[7],
                    slab_size = 0, quantiles = 100) +
  geom_vline(xintercept = y_bar_a - y_bar_b,
             color = pp[13], linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Unstandardized mean difference")
```

Here's how to standardize that unstandardized effect size into a Cohen's-$d$ metric.

```{r, fig.width = 3, fig.height = 2.5}
draws %>% 
  mutate(d = b_Virgin1 / sigma) %>% 
  
  ggplot(aes(x = d, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[2], color = pp[7],
                    slab_size = 0, quantiles = 100) +
  geom_vline(xintercept = (y_bar_a - y_bar_b) / s_p,
             color = pp[13], linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression("Cohen's"~italic(d)~"expressed as a posterior"))
```

Let's work this one more way. By simple algebra, a standardized mean difference is the same as the difference between two standardized means. The trick, though, is you have to use the pooled standard deviation ($s_p$) as your standardizer. Thus, if we standardize the criterion `Longevity` before fitting the model and continue using the dummy variable approach, the `Virgin1` posterior will be the same as a Cohen's $d$.

Standardize the criterion with `s_p`.

```{r}
my_data <-
  my_data %>% 
  mutate(Longevity_s = (Longevity - mean(Longevity)) / s_p)
```

Because our criterion in a standardized metric, we no longer need our `stanvars`.
      
```{r fit19.10}
fit19.10 <-
  brm(data = my_data,
      family = gaussian,
      Longevity_s ~ 1 + Virgin1,
      prior = c(prior(normal(0, 1 * 5), class = Intercept),
                prior(normal(0, 1 * 5), class = b),
                prior(cauchy(0, 1), class = sigma)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 19,
      file = "fits/fit19.10")
```

Behold our out-of-the-box Bayesian Cohen's $d$.

```{r, fig.width = 3, fig.height = 2.5}
# no transformation necessary
as_draws_df(fit19.10) %>% 
  
  ggplot(aes(x = b_Virgin1, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[2], color = pp[7],
                    slab_size = 0, quantiles = 100) +
  geom_vline(xintercept = (y_bar_a - y_bar_b) / s_p,
             color = pp[13], linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression("Cohen's"~italic(d)~"expressed as a posterior"))
```

If you work them through, the three approaches we just took can be generalized to models with more than two groups. You just need to be careful how to compute the $s_p$ for each comparison.

It's also the case the that standardized mean differences we computed for `fit19.1`, above, are not quite Cohen's $d$ effect sizes in the same way these have been. This is because the hierarchical approach we used partially pooled the estimates for each group toward the grand mean. You might say they were hierarchically-regularized Cohen's $d$s. But then again, Cohen's formula for his $d$ statistic did not account for Bayesian priors, either. So perhaps a purist would deny that any of the standardized mean differences we've computed in this chapter were proper Cohen's $d$ effect sizes. To be on the safe side, tell your readers exactly how you computed your models and what formulas you used to compute your effect sizes.

### Your sample sizes may differ.

In the examples, above, the two groups had equal sample sizes, which allowed us to be lazy with how we hand computed the sample estimate of the pooled standard deviation. When working with data for which $n_A \neq n_B$, it's a good idea to switch out the equation for the pooled standard deviation $s_p$ for $s_p^*$, which is robust to unequal sample sizes. We wan write the equation for the two-groups version of $s_p^*$ as

$$s_p^* = \sqrt{\frac{(n_A - 1)s_A^2 + (n_B - 1)s_B^2}{n_A + n_B - 2}},$$

which strategically weights the sample estimate for the pooled standard deviation by sample size.

We should practice with $s_p^*$, a bit, to see how it works together with our Bayesian modeling paradigm. Back in [Section 16.1.2][Approximation by ~~MCMC in JAGS~~ HMC in brms.], we saw an example of this with the `TwoGroupIQ` data. Let's load them, again.

```{r, message = F}
my_data <- read_csv("data.R/TwoGroupIQ.csv")

glimpse(my_data)
```

The data are IQ scores for participants in two groups. They look like this.

```{r, fig.width = 5, fig.height = 2.75}
my_data %>% 
  ggplot(aes(x = Score, Group)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[2], color = pp[7],
                    slab_size = 0, quantiles = 100) +
  xlab("IQ score") +
  coord_cartesian(ylim = c(1.5, 2.25))
```

Unlike the examples from the last section, the samples sizes are different for our two levels of `Group`.

```{r}
my_data %>% 
  count(Group)
```

Here's how we can use that information to hand compute the sample estimate for Cohen's $d$.

```{r}
# save the sample means for the groups
y_bar_a <- filter(my_data, Group == "Smart Drug") %>% summarise(m = mean(Score)) %>% pull()
y_bar_b <- filter(my_data, Group == "Placebo")    %>% summarise(m = mean(Score)) %>% pull()

# save the sample sizes for the groups 
n_a <- filter(my_data, Group == "Smart Drug") %>% count() %>% pull()
n_b <- filter(my_data, Group == "Placebo") %>% count() %>% pull()

# save the sample standard deviations
s_a <- filter(my_data, Group == "Smart Drug") %>% summarise(s = sd(Score)) %>% pull()
s_b <- filter(my_data, Group == "Placebo")    %>% summarise(s = sd(Score)) %>% pull()

# compute and save the sample pooled standard deviation
s_p <- sqrt(((n_a - 1) * s_a^2 + (n_b - 1) * s_b^2) / (n_a + n_b - 2))

# compute Cohen's d
(y_bar_a - y_bar_b) / s_p
```

Although it's a lot of work to compute a sample-size corrected Cohen's $d$ with unequally-sized sample data, it's straightforward to compute the effect size in a Bayesian model. We could use any of the three approaches, from above. Here we'll practice with the `Score ~ 0 + Group` syntax.

```{r fit19.11}
mean_y <- (y_bar_a + y_bar_b) / 2

stanvars <- 
  stanvar(mean_y, name = "mean_y") + 
  stanvar(s_p,    name = "sd_y")

fit19.11 <-
  brm(data = my_data,
      family = gaussian,
      Score ~ 0 + Group,
      prior = c(prior(normal(mean_y, sd_y * 5), class = b),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 19,
      stanvars = stanvars,
      file = "fits/fit19.11")
```

Review the model summary.

```{r}
print(fit19.11)
```

The $\sigma$ parameter within this model is an estimate of the population value for $\sigma_p$. Happily, it already accommodates the differences in sample sizes, which we tried to correct for, above, with $s_p^*$. To give a sense, here's a plot of the $\sigma$ posterior with our hand-computed `s_p` value superimposed as a dashed line.

```{r, fig.width = 3, fig.height = 2.5}
as_draws_df(fit19.11) %>% 
  
  ggplot(aes(x = sigma, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[2], color = pp[7],
                    slab_size = 0, quantiles = 100) +
  geom_vline(xintercept = s_p,
             color = pp[13], linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(sigma[italic(p)]))
```

Nailed it! Now here's how we might use the posterior samples to then compute the standardized mean difference.

```{r, fig.width = 3, fig.height = 2.5}
as_draws_df(fit19.11) %>% 
  mutate(d = (b_GroupSmartDrug - b_GroupPlacebo) / sigma) %>% 
  
  ggplot(aes(x = d, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[2], color = pp[7],
                    slab_size = 0, quantiles = 100) +
  geom_vline(xintercept = (y_bar_a - y_bar_b) / s_p,
             color = pp[13], linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression((mu[italic(B)]-mu[italic(A)])/sigma[italic(p)]*", where "*sigma[italic(p)]%~~%sqrt(((italic(n[A])-1)*italic(s)[italic(A)]^2+(italic(n[B])-1)*italic(s)[italic(B)]^2)/(italic(n[A])+italic(n[B])-2)))) +
  theme(axis.title.x = element_text(size = 7))
```

Sometimes fitting a model is easier than computing estimates, by hand.

### Populations and samples.

You may have noticed that in our equation for $d$, above, we defined our standardized mean differences in terms of sample statistics,

$$d = \frac{\bar y_A - \bar y_B}{s_p},$$

where $s_p$ can either assume equal sample sizes, or it can be replaced with $s_p^*$ when sample sizes differ. Sometimes we speak of the true population effect size $\delta$, which is correspondingly defined as

$$\delta = \frac{\mu_A - \mu_B}{\sigma},$$

where $\mu_A$ and $\mu_B$ are the population means for the two groups under consideration and $\sigma$ is the pooled standard deviation in the population. Often times we don't have access to these values, which is why we run experiments and fit statistical models. But sometimes we do have access to the exact population parameters. In those cases, we can just plug them into the formula rather than estimate them in our models or with our sample statistics.

In the case of our IQ score data from the last section, we actually know the population mean and standard deviation for IQ are 100 and 15, respectively. We know this because the people who make IQ tests design them that way. Let's see how well our sample statistics approximate the population parameters.

```{r, message = F}
my_data %>% 
  group_by(Group) %>% 
  summarise(mean = mean(Score),
            sd = sd(Score))

# pooled standard deviation
s_p
```

Unsurprisingly, the values for the `Smart Drug` group are notably different from the population parameters. But notice how close the values from the `Placebo` group are to the population parameters. If they weren't, we'd be concerned the `Placebo` condition was not a valid control. Looks like it was.

However, notice that the mean and standard deviation for the `Placebo` group are not the exact values of 100 and 15 the way they are in the population. If we wanted to compute a standardized mean difference between our `Smart Drug` group and folks in the population, we could just plug those values directly into our effect size equation. Here's what that would look like if we plug in the population mean for the control group.

```{r}
(y_bar_a - 100) / s_p
```

The result is very close to the one above. But this time our equation for $d$ was

$$d = \frac{\bar y_A - \mu_B}{s_p},$$

where we used the population mean $\mu_B$, but the other two terms were based on values from the sample. As long as you are defining the `Placebo` control as a stand-in for the population, this is a more precise way to compute $d$. Going further, we can also replace our sample estimate $s_p$ with the true value for $\sigma$, 15.

```{r}
(y_bar_a - 100) / 15
```

Now our hand-computed estimate for $d$ is quite different. *Why*? Recall that sample standard deviations for both groups were larger than 15, which therefore produced an $s_p$ value that was larger than 15. When working with fractions, larger denominators return smaller products.

Here's what this looks like if we work with the posterior from the last model, `fit19.11`.

```{r, fig.width = 3, fig.height = 2.5}
as_draws_df(fit19.11) %>% 
  mutate(d = (b_GroupSmartDrug - 100) / 15) %>% 
  
  ggplot(aes(x = d, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95,
                    slab_fill = pp[2], color = pp[7],
                    slab_size = 0, quantiles = 100) +
  geom_vline(xintercept = (y_bar_a - 100) / 15,
             color = pp[13], linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression((mu[italic(B)]-100)/15))
```

Note how we've now changed our effect size formula to

$$d = \frac{\bar y_A - \mu_B}{\sigma}.$$

### Report your effect sizes effectively.

Wrapping up, we've been practicing computing standardized mean differences ($d$'s) by hand with sample statistics, with posterior samples from our Bayesian models, and with combinations of the two. We've also seen that whereas unequal sample sizes can matter a lot for hand computing your $d$ estimates, the procedure is more straightforward when using the Bayesian posterior approach. Finally, we played around a bit with how we defined our formula for $d$, depending on whether we knew the true population values of any of the parameters.

Confusingly, you might see all these variants, and more, referred to as Cohen's $d$ within the literature. As with all the other decisions you make with experimental design and data analysis, use careful reasoning to decide on how you'd like to compute your effect sizes. To stave off confusion, report the formulas for your effect sizes transparently in your work. When possible, use equations, prose, and authoritative citations.

Though we've followed Kruschke's lead and focused on the Cohen's $d$ approach to effect sizes, there are many other ways to express effect sizes. Furthermore, $d$-type effect sizes aren't appropriate for some model types or for some research questions. To expand your effect size repertoire, you might brush up on Cohen's [-@cohenStatisticalPowerAnalysis1988] authoritative text or Cummings newer [-@cummingUnderstandingTheNewStatistics2012] text. For nice conceptual overview on effect sizes, I recommend Kelley and Preacher's [-@kelley2012effect] paper, [*On effect size*](https://www3.nd.edu/~kkelley/publications/articles/Kelley_and_Preacher_Psychological_Methods_2012.pdf). Also see Pek and Flora's [-@pekReportingEffectSizes2018] handy paper, [*Reporting effect sizes in original psychological research: A discussion and tutorial*](https://doi.apa.org/fulltext/2017-10871-001.html).

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
# remove our objects
rm(generate_data, n, grand_mean, d, densities, arrow, text, pp, p1, p2, my_arrow, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, layout, gamma_a_b_from_omega_sigma, sd_y, omega, sigma, s_r, my_data, mean_y, stanvars, fit19.1, draws, n_draws, draws_20, fit19.2, mw, fit19.3, fit19.4, p13, my_small_data, sd_thorax_c, fit19.5, r, f, differences, p14, p15, p16, p17, fit19.6, fit19.7, y_bar_a, y_bar_b, s_a, s_b, s_p, fit19.8, lines, fit19.9, fit19.10, n_a, n_b, fit19.11)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:19.Rmd-->


```{r, echo = FALSE, cachse = FALSE}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

# Metric Predicted Variable with Multiple Nominal Predictors

> This chapter considers data structures that consist of a metric predicted variable and two (or more) nominal predictors. This chapter extends ideas introduced in the previous chapter, so please read the previous chapter if you have not already...
>
> The traditional treatment of this sort of data structure is called multifactor analysis of variance (ANOVA). Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups. [@kruschkeDoingBayesianData2015, pp. 583--584]

## Describing groups of metric data with multiple nominal predictors

Quick reprise:

> Suppose we have two nominal predictors, denoted $\overrightarrow x_1$ and $\overrightarrow x_2$. A datum from the $j$th level of $\overrightarrow x_1$ is denoted $x_{1[j]}$, and analogously for the second factor. The predicted value is a baseline plus a deflection due to the level of factor 1 plus a deflection due to the level of factor 2 plus a residual deflection due to the interaction of factors:
>
> \begin{align*}
> \mu & = \beta_0 + \overrightarrow \beta_1 \cdot \overrightarrow x_1 + \overrightarrow \beta_2 \cdot \overrightarrow x_2 + \overrightarrow \beta_{1 \times 2} \cdot \overrightarrow x_{1 \times 2} \\
> & = \beta_0 + \sum_j \beta_{1[j]} x_{1[j]} + \sum_k \beta_{2[k]} x_{2[k]} + \sum_{j, k} \beta_{1 \times 2[j, k]} x_{1 \times 2[j, k]}
> \end{align*}
>
> The deflections within factors and within the interaction are constrained to sum to zero:
>
> \begin{align*}
> \sum_j \beta_{1[j]} = 0                                &&& \text{and} && \sum_k \beta_{2[k]} = 0 \;\;\; \text{and} \\
> \sum_j \beta_{1 \times 2[j, k]} = 0 \text{ for all } k &&& \text{and} && \sum_k \beta_{1 \times 2[j, k]} = 0 \text{ for all } j
> \end{align*}
>
> ([these equations] are repetitions of Equations 15.9 and 15.10, p. 434). The actual data are assumed to be randomly distributed around the predicted value. (pp. 584--585)

### Interaction.

> An important concept of models with multiple predictors is interaction. Interaction means that the effect of a predictor depends on the level of another predictor. A little more technically, interaction is what is left over after the main effects of the factors are added: interaction is the nonadditive influence of the factors. (p. 585)

Here are the data necessary for our version of Figure 20.1, which displays an interaction of two 2-level factors.

```{r, message = F, warning = F}
library(tidyverse)

grand_mean            <- 5
deflection_1          <- 1.8
deflection_2          <- 0.2
nonadditive_component <- -1

(
  d <-
  tibble(x1 = rep(c(-1, 1), each = 2),
         x2 = rep(c(1, -1), times = 2)) %>% 
  mutate(mu_additive = grand_mean + (x1 * deflection_1) + (x2 * deflection_2)) %>% 
  mutate(mu_multiplicative = mu_additive + (x1 * x2 * nonadditive_component),
         # we'll need this to accommodate `position = "dodge"` within `geom_col()`
         x1_offset         = x1 + x2 * -.45,
         # we'll need this for the fill
         x2_c              = factor(x2, levels = c(1, -1)))
)
```

There's enough going on with the lines, arrows, and titles across the three panels that to my mind it seems easier to make three distinct plots and them join them at the end with syntax from the **patchwork** package. But enough of the settings are common among the panels that it also makes sense to keep from repeating that part of the code. So we'll take a three-step solution. For the first step, we'll make the baseline or foundational plot, which we'll call `p`.

Before we make `p`, let's talk color and theme. For this chapter, we'll carry forward our practice from [Chapter 19][Metric Predicted Variable with One Nominal Predictor] and take our color palette from the **palettetown** package. Our color palette will be #15, which is based on [Beedrill](https://www.pokemon.com/us/pokedex/beedrill).

```{r, warning = F, message = F, fig.height = 3.5}
library(palettetown)

scales::show_col(pokepal(pokemon = 15))

bd <- pokepal(pokemon = 15)

bd
```

Also like in the last chapter, our overall plot theme will be based on the default `theme_grey()` with a good number of adjustments. This time, it will have more of a [`theme_black()`](https://jonlefcheck.net/2013/03/11/black-theme-for-ggplot2-2/) vibe. 

```{r}
theme_set(
  theme_grey() +
    theme(text = element_text(color = bd[3]),
          axis.text = element_text(color = bd[3]),
          axis.ticks = element_line(color = bd[3]),
          legend.background = element_blank(),
          legend.box.background = element_blank(),
          legend.key = element_rect(fill = bd[1]),
          panel.background = element_rect(fill = bd[1], color = bd[3]),
          panel.grid = element_blank(),
          plot.background = element_rect(fill = bd[1], color = bd[1]),
          strip.background = element_rect(fill = alpha(bd[5], 1/3), color = alpha(bd[5], 1/3)),
          strip.text = element_text(color = bd[3]))
)
```

Okay, it's time to make `p`, the baseline or foundational plot for our Figure 20.1.

```{r, fig.height = 3, fig.width = 2.75}
p <-
  d %>% 
  ggplot(aes(x = x1, y = mu_multiplicative)) +
  geom_col(aes(fill = x2_c),
           position = "dodge") +
  scale_fill_manual(NULL, values = bd[c(11, 6)], labels = c("x2[1]", "x2[2]")) +
  scale_x_continuous(breaks = c(-1, 1), labels = c("x1[1]", "x1[2]")) +
  scale_y_continuous(expression(mu), breaks = seq(from = 0, to = 10, by = 2), 
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(ylim = c(0, 10)) +
  theme(axis.ticks.x = element_blank(),
        legend.position = c(.17, .875))

p
```

Now we have `p`, we'll add panel-specific elements to it, which we'll save as individual objects, `p1`, `p2`, and `p3`. That's step 2. Then for step 3, we'll bring them all together with **patchwork**.

```{r, fig.height = 3, fig.width = 8.25, warning = F, message = F}
# deflection from additive
p1 <-
  p +
  geom_segment(aes(x = x1_offset, xend = x1_offset,
                   y = mu_additive, yend = mu_multiplicative),
               size = 1.25, color = bd[5],
               arrow = arrow(length = unit(.275, "cm"))) +
  geom_line(aes(x = x1_offset, y = mu_additive, group = x2),
            linetype = 2, color = bd[5]) +
  geom_line(aes(x = x1_offset, y = mu_additive, group = x1),
            linetype = 2, color = bd[5]) +
  coord_cartesian(ylim = c(0, 10)) +
  ggtitle("Deflection from additive")
  
# effect of x1 depends on x2
p2 <-
  p +
  geom_segment(aes(x = x1_offset, xend = x1_offset,
                   y = mu_additive, yend = mu_multiplicative),
               size = .5, color = bd[5],
               arrow = arrow(length = unit(.15, "cm"))) +
  geom_line(aes(x = x1_offset, y = mu_additive, group = x2),
            linetype = 2, color = bd[5]) +
  geom_line(aes(x = x1_offset, y = mu_multiplicative, group = x2),
            size = 1.25, color = bd[5]) +
  ggtitle("Effect of x1 depends on x2")

# effect of x2 depends on x1
p3 <-
  p +
  geom_segment(aes(x = x1_offset, xend = x1_offset,
                   y = mu_additive, yend = mu_multiplicative),
               size = .5, color = bd[5],
               arrow = arrow(length = unit(.15, "cm"))) +
  geom_line(aes(x = x1_offset, y = mu_multiplicative, group = x1),
            size = 1.25, color = bd[5]) +
  geom_line(aes(x = x1_offset, y = mu_additive, group = x1),
            linetype = 2, color = bd[5]) +
  ggtitle("Effect of x2 depends on x1")

library(patchwork)

p1 + p2 + p3
```

And in case it's not clear, "the average deflection from baseline due to a predictor... is called the main effect of the predictor. The main effects of the predictors correspond to the dashed lines in the left panel of Figure 20.1" (p. 587). And further

> The left panel of Figure 20.1 highlights the interaction as the nonadditive component, emphasized by the heavy vertical arrows that mark the departure from additivity. The middle panel of Figure 20.1 highlights the interaction by emphasizing that the effect of $x_1$ depends on the level of $x_2$. The heavy lines mark the effect of $x_1$, that is, the changes from level 1 of $x_1$ to level 2 of $x_1$. Notice that the heavy lines have different slopes: The heavy line for level 1 of $x_2$ has a shallower slope than the heavy line for level 2 of $x_2$. The right panel of Figure 20.1 highlights the interaction by emphasizing that the effect of $x_2$ depends on the level of $x_1$. (p. 587)

### Traditional ANOVA.

> As was explained in [Section 19.2][Traditional analysis of variance] (p. 556), the terminology, "analysis of variance," comes from a decomposition of overall data variance into within-group variance and between-group variance... The Bayesian approach is not ANOVA, but is analogous to ANOVA. Traditional ANOVA makes decisions about equality of groups (i.e., null hypotheses) on the basis of $p$ values using a null hypothesis that assumes (i) the data are normally distributed within groups, and (ii) the standard deviation of the data within each group is the same for all groups. The second assumption is sometimes called "homogeneity of variance." The entrenched precedent of ANOVA is why basic models of grouped data make those assumptions, and why the basic models presented in this chapter will also make those assumptions. Later in the chapter, those constraints will be relaxed. (pp. 587--588)

## Hierarchical Bayesian approach

"Our goal is to estimate the main and interaction deflections, and other parameters, based on the observed data" (p. 588).

Figure 20.2 will provides a generic model diagram of how this can work.

```{r, fig.width = 8, fig.height = 6, message = F}
# bracket
p1 <-
  tibble(x = .99,
         y = .5,
         label = "{_}") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, hjust = 1, color = bd[2], family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

##  plain arrow
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")
p2 <-
  tibble(x    = .68,
         y    = 1,
         xend = .68,
         yend = .25) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()
  
# normal density
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p4 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("0", "sigma[beta][1]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p5 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("0", "sigma[beta][2]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p6 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 0.67), y = .6,
           hjust = c(.5, 0),
           label = c("0", "sigma[beta][1%*%2]"), 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# four annotated arrows
p7 <-
  tibble(x    = c(.05, .34, .64, .945),
         y    = c(1, 1, 1, 1),
         xend = c(.05, .18, .45, .74),
         yend = c(0, 0, 0, 0)) %>%
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = c(.025, .23, .30, .52, .585, .81, .91), y = .5,
           label = c("'~'", "'~'", "italic(j)", "'~'", "italic(k)", "'~'", "italic(jk)"),
           size = c(10, 10, 7, 10, 7, 10, 7), 
           color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p8 <-
  tibble(x = .5,
         y = .25,
         label = "beta[0]+sum()[italic(j)]*beta[1]['['*italic(j)*']']*italic(x)[1]['['*italic(j)*']'](italic(i))+sum()[italic(k)]*beta[2]['['*italic(k)*']']*italic(x)[2]['['*italic(k)*']'](italic(i))+sum()[italic(jk)]*beta[1%*%2]['['*italic(jk)*']']*italic(x)[1%*%2]['['*italic(jk)*']'](italic(i))") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(hjust = .5, size = 7, color = bd[3], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# half-normal density
p9 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma]", 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# the final normal density
p10 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("mu[italic(i)]", "sigma[italic(y)]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# an annotated arrow
p11 <-
  tibble(x     = .4,
         y     = .5,
         label = "'='") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, color = bd[3], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = .1, 
               arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()

# another annotated arrow
p12 <-
  tibble(x     = .49,
         y     = .55,
         label = "'~'") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, color = bd[3], parse = T, family = "Times") +
  geom_segment(x = .79, xend = .4,
               y = 1, yend = .2, 
               arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()

# the final annotated arrow
p13 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), 
            color = bd[3], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()

# some text
p14 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = bd[3], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 1, l = 6, r = 7),
  area(t = 1, b = 1, l = 10, r = 11),
  area(t = 1, b = 1, l = 14, r = 15),
  area(t = 3, b = 4, l = 1, r = 3),
  area(t = 3, b = 4, l = 5, r = 7),
  area(t = 3, b = 4, l = 9, r = 11),
  area(t = 3, b = 4, l = 13, r = 15),
  area(t = 2, b = 3, l = 6, r = 7),
  area(t = 2, b = 3, l = 10, r = 11),
  area(t = 2, b = 3, l = 14, r = 15),
  area(t = 6, b = 7, l = 1, r = 15),
  area(t = 5, b = 6, l = 1, r = 15),
  area(t = 9, b = 10, l = 10, r = 12),
  area(t = 12, b = 13, l = 7, r = 9),
  area(t = 8, b = 12, l = 7, r = 9),
  area(t = 11, b = 12, l = 7, r = 12),
  area(t = 14, b = 14, l = 7, r = 9),
  area(t = 15, b = 15, l = 7, r = 9)
)

# combine and plot!
(p1 + p1 + p1 + p3 + p4 + p5 + p6 + p2 + p2 + p2 + p8 + p7 + p9 + p10 + p11 + p12 + p13 + p14) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Wow that plot has a lot of working parts! `r emo::ji("wow")`

### Implementation in ~~JAGS~~ brms.

Below is how to implement the model based on the code from Kruschke's `Jags-Ymet-Xnom2fac-MnormalHom.R` and `Jags-Ymet-Xnom2fac-MnormalHom-Example.R` scripts. With **brms**, we'll need to specify the `stanvars`.

```{r, eval = F}
mean_y <- mean(my_data$y)
sd_y   <- sd(my_data$y)

omega <- sd_y / 2
sigma <- 2 * sd_y

s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)

stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta")
```

And before that, of course, make sure you've defined the `gamma_a_b_from_omega_sigma()` function. E.g.,

```{r}
gamma_a_b_from_omega_sigma <- function(mode, sd) {
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate <- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape <- 1 + mode * rate
  return(list(shape = shape, rate = rate))
}
```

With the preparatory work done, now all we'd need to do is run the `brm()` code.

```{r, eval = F}
fit <-
  brm(data = my_data,
      family = gaussian,
      y ~ 1 + (1 | factor_1) + (1 | factor_2) + (1 | factor_1:factor_2),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      stanvars = stanvars)
``` 

If you have reason to use different priors for the random effects, you can always specify multiple lines of `class = sd`, each with the appropriate `coef` argument.

The big new element is multiple `(|)` parts in the `formula`. In this simple model type, we're only working random intercepts, in this case with two factors and their interaction. The `formula` above presumes the interaction is not itself coded within the data. But consider the case you have data including a term for the interaction of the two lower-level factors, called `interaction`. In that case, you'd have that last part of the `formula` read `(1 | interaction)`, instead.

### Example: It's only money.

Load the salary data[^6].

```{r, message = F, warning = F}
my_data <- read_csv("data.R/Salary.csv")

glimpse(my_data)
```

We'll follow Kruschke's example on page 593 and modify the `Pos` variable a bit.

```{r}
my_data <-
  my_data %>% 
  mutate(Pos = factor(Pos,
                      levels = c("FT3", "FT2", "FT1", "NDW", "DST") ,
                      ordered = T,
                      labels = c("Assis", "Assoc", "Full", "Endow", "Disting")))
```

With 1080 cases, two factors, and a criterion, these data are a little too unwieldy to look at the individual case level. But if we're tricky on how we aggregate, we can get a good sense of their structure with a `geom_tile()` plot. Here our strategy is to aggregate by our two factors, `Pos` and `Org`. Since our criterion is `Salary`, we'll compute the mean value of the cases within each unique paring, encoded as `m_salary`. Also, we'll get a sense of how many cases there are within each factor pairing with `n`.

```{r, fig.width = 9, fig.height = 2.5, message = F}
my_data %>% 
  group_by(Pos, Org) %>% 
  summarise(m_salary = mean(Salary),
            n        = n()) %>% 
  ungroup() %>% 
  mutate(Org = fct_reorder(Org, m_salary),
         Pos = fct_reorder(Pos, m_salary)) %>% 
  
  ggplot(aes(x = Org, y = Pos, fill = m_salary, label = n)) +
  geom_tile() +
  geom_text(size = 2.75) +
  # everything below this is really just aesthetic flourish
  scale_fill_gradient(low = bd[9], high = bd[12],
                      breaks = c(55e3, 15e4, 26e4), 
                      labels = c("$55K", "$150K", "$260K")) +
  scale_x_discrete("Org", expand = c(0, 0)) +
  scale_y_discrete("Pos", expand = c(0, 0)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0),
        axis.text.y = element_text(hjust = 0),
        axis.ticks = element_blank(),
        legend.position = "top")
```

Hopefully it's clear that each cell is a unique pairing of `Org` and `Pos`. The cells are color coded by the mean `Salary`. The numbers in the cells give the $n$ cases they represent. When there's no data for a unique combination of `Org` and `Pos`, the cells are left light gray and blank.

Load **brms**.

```{r, warning = F, message = F}
library(brms)
```

Define our `stanvars`.

```{r}
mean_y <- mean(my_data$Salary)
sd_y   <- sd(my_data$Salary)

omega <- sd_y / 2
sigma <- 2 * sd_y

s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)

stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta")
```

Now fit the model.

```{r fit20.1}
fit20.1 <-
  brm(data = my_data,
      family = gaussian,
      Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 20,
      control = list(adapt_delta = 0.999,
                     max_treedepth = 13),
      stanvars = stanvars,
      file = "fits/fit20.01")
``` 

The chains look fine.

```{r, fig.width = 8, fig.height = 6.5}
bayesplot::color_scheme_set(scheme = bd[c(13, 7:9, 11:12)])

plot(fit20.1, widths = c(2, 3))
```

Here's the model summary.

```{r}
print(fit20.1)
```

This was a difficult model to fit with **brms**. Stan does well when the criteria are on or close to a standardized metric and these `Salary` data are a far cry from that. Tuning `adapt_delta` and `max_treedepth` went a long way to help the model out.

Okay, let's get ready for our version of Figure 20.3. First, we'll use `tidybayes::add_epred_draws()` to help organize the necessary posterior draws.

```{r, warning = F, message = F}
library(tidybayes)

# how many draws would you like?
n_draw <- 20

# wrangle
f <-
  my_data %>% 
  distinct(Pos) %>% 
  expand(Pos, 
         Org = c("BFIN", "CHEM", "PSY", "ENG")) %>% 
  add_epred_draws(fit20.1, ndraws = n_draw, seed = 20,
                  allow_new_levels = T,
                  dpar = c("mu", "sigma")) %>% 
  mutate(ll = qnorm(.025, mean = mu, sd = sigma),
         ul = qnorm(.975, mean = mu, sd = sigma)) %>% 
  mutate(Salary = map2(ll, ul, seq, length.out = 100)) %>% 
  unnest(Salary) %>% 
  mutate(density = dnorm(Salary, mu, sigma)) %>% 
  group_by(.draw) %>% 
  mutate(density = density / max(density)) %>% 
  mutate(Org = factor(Org, levels = c("BFIN", "CHEM", "PSY", "ENG")))

glimpse(f)
```

We're ready to plot.

```{r, warning = F, message = F ,fig.height = 5.5, fig.width = 8}
library(ggridges)

f %>% 
  ggplot(aes(x = Salary, y = Pos)) +
  geom_vline(xintercept = fixef(fit20.1)[, 1], color = bd[5]) +
  geom_ridgeline(aes(height = density, group = interaction(Pos, .draw),
                     color = Pos),
                 fill = NA, show.legend = F,
                 size = 1/4, scale = 3/4) +
  geom_jitter(data = my_data %>% 
                filter(Org %in% c("BFIN", "CHEM", "PSY", "ENG")) %>% 
                mutate(Org = factor(Org, levels = c("BFIN", "CHEM", "PSY", "ENG"))),
              height = .025, alpha = 1/2, size = 2/3, color = bd[11]) +
  scale_color_manual(values = bd[c(14, 13, 8, 12, 3)]) +
  scale_x_continuous(breaks = seq(from = 0, to = 300000, length.out = 4),
                     labels = c("$0", "$100K", "200K", "$300K")) +
  coord_cartesian(xlim = c(0, 35e4),
                  ylim = c(1.25, 5.5)) +
  labs(title = "Data with Posterior Predictive Distributions", 
       subtitle = "The white vertical line is the model-implied grand mean.",
       y = "Pos") +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank()) +
  facet_wrap(~ Org, ncol = 2)
```

The **brms** package doesn't have a convenience function that returns output quite like what Kruschke displayed in his Table 20.2. But we can get close. The `posterior_summary()` will return posterior means, $SD$s, and percentile-based 95% intervals for all model parameters. Due to space concerns, I'll just show the first ten lines.

```{r}
posterior_summary(fit20.1)[1:10,]
```

The `summarise_draws()` function from the **posterior** package [@R-posterior], however, will take us a long ways towards making Table 20.2. 

```{r, warning = F, message = F}
library(posterior)

as_draws_df(fit20.1) %>% 
  select(b_Intercept, 
         starts_with("r_Pos["),
         `r_Org[ENG,Intercept]`,
         `r_Org[PSY,Intercept]`,
         `r_Org[CHEM,Intercept]`,
         `r_Org[BFIN,Intercept]`,
         `r_Pos:Org[Assis_PSY,Intercept]`,
         `r_Pos:Org[Full_PSY,Intercept]`,
         `r_Pos:Org[Assis_CHEM,Intercept]`,
         `r_Pos:Org[Full_CHEM,Intercept]`,
         sigma) %>% 
  summarise_draws(mean, median, mode = Mode, ess_bulk, ess_tail, ~ quantile(.x, probs = c(.025, .975))) %>% 
  mutate_if(is.double, round, digits = 0)
```

Note how we've used by kinds of effective-sample size estimates available for **brms** and note that we've used percentile-based intervals. 

As Kruschke then pointed out, "individual salaries vary tremendously around the predicted cell mean" (p. 594), which you can quantify using $\sigma_y$. Here it is using `posterior_summary()`.

```{r}
posterior_summary(fit20.1)["sigma", ]
```

And we can get a better sense of the distribution with a dot plot.

```{r, fig.width = 4, fig.height = 2}
# extract the posterior draws
draws <- as_draws_df(fit20.1)

# plot
draws %>%
  ggplot(aes(x = sigma, y = 0)) +
  stat_dotsinterval(point_interval = mode_hdi, .width = .95, 
                    justification = -0.04,
                    shape = 23, stroke = 1/4, point_size = 3, slab_size = 1/4,
                    color = bd[2], point_color = bd[1], slab_color = bd[1],
                    point_fill = bd[2], slab_fill = bd[6], 
                    quantiles = 100) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(sigma[y]))
```

As Kruschke pointed out, this parameter is held constant across all subgroups. That is, the subgroups are homogeneous with respect to their variances. We'll relax this constraint later on.

Before we move on to the next section, look above at how many arguments we fiddled with to configure `stat_dotsinterval()`. Given how many more dot plots we have looming in our not-too-distant future, we might go ahead and save these settings as a new function. We'll call it `stat_beedrill()`.

```{r}
stat_beedrill <- function(point_size = 3, 
                          slab_color = bd[1],
                          quantiles = 100, ...) {
  
  stat_dotsinterval(point_interval = mode_hdi, .width = .95, 
                    shape = 23, stroke = 1/4, 
                    point_size = point_size, slab_size = 1/4,
                    color = bd[2], 
                    point_color = bd[1], point_fill = bd[2],
                    slab_color = slab_color, slab_fill = bd[6], 
                    quantiles = quantiles,
                    # learn more about this at https://github.com/mjskay/ggdist/issues/93
                    justification = -0.04,
                    ...)
}
```

Note how we hard coded the settings for some of the parameters within the function (e.g., `point_interval`) but allows others to be adjustable with new default settings (e.g., `point_size`).

### Main effect contrasts.

> In applications with multiple levels of the factors, it is virtually always the case that we are interested in comparing particular levels with each other.... These sorts of comparisons, which involve levels of a single factor and collapse across the other factor(s), are called main effect comparisons or contrasts.(p. 595)

The `fitted()` function provides a versatile framework for contrasts among the main effects. In order to follow Kruschke's aim to compare "levels of a single factor and collapse across the other factor(s)" when those factors are modeled in a hierarchical structure, one will have to make use of the `re_formula` argument within `fitted()`. Here's how to do that for the first contrast.

```{r, fig.width = 3, fig.height = 2.5, warning = F, message = F}
# define the new data
nd <- tibble(Pos = c("Assis", "Assoc"))

# feed the new data into `fitted()`
f <-
  fitted(fit20.1,
         newdata = nd,
         # this part is crucial
         re_formula = ~ (1 | Pos),
         summary = F) %>% 
  as_tibble() %>% 
  set_names("Assis", "Assoc") %>% 
  mutate(`Assoc vs Assis` = Assoc - Assis)

# plot
f %>% 
  ggplot(aes(x = `Assoc vs Assis`, y = 0)) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlim(0, NA) +
  labs(title = "Assoc vs Assis",
       x = "Difference")
```

In case you were curious, here are the summary statistics.

```{r, warning = F, message = F}
f %>% 
  mode_hdi(`Assoc vs Assis`) %>% 
  select(`Assoc vs Assis`:.upper) %>% 
  mutate_if(is.double, round, digits = 0)
```

Now make the next two contrasts.

```{r, fig.width = 6, fig.height = 2.5}
nd <- tibble(Org = c("BFIN", "CHEM", "ENG", "PSY"))

f <-
  fitted(fit20.1,
         newdata = nd,
         # note the change from above
         re_formula = ~ (1 | Org),
         summary = F) %>% 
  as_tibble() %>% 
  set_names(pull(nd, Org)) %>% 
  transmute(`CHEM vs PSY`     = CHEM - PSY,
            `BFIN vs other 3` = BFIN - ((CHEM + ENG + PSY) / 3))

# plot
f %>% 
  pivot_longer(everything()) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  expand_limits(x = 0) +
  facet_wrap(~ name, scales = "free")
```

And here are their numeric summaries.

```{r}
f %>% 
  pivot_longer(everything(),
               names_to = "contrast", 
               values_to = "mode") %>% 
  group_by(contrast) %>% 
  mode_hdi(mode) %>% 
  select(contrast:.upper) %>% 
  mutate_if(is.double, round, digits = 0)
```

For more on marginal contrasts in **brms**, see the discussion in [issue #552]((https://github.com/paul-buerkner/brms/issues/552)) in the **brms** GitHub repo and [this discussion thread](https://discourse.mc-stan.org/t/how-do-i-get-marginal-effects-for-categorical-variables-to-condition-on-an-average-rather-than-a-category/5323) on the Stan forums.

### Interaction contrasts and simple effects.

If we'd like to make the simple effects and interaction contrasts like Kruschke displayed in Figure 20.5 within our **tidyverse**/**brms** paradigm, it'll be simplest to just redefine our `nd` data and use `fitted()`, again. This time, however, we won't be using the `re_formula` argument.

```{r}
# define our new data
nd <-
  crossing(Pos = c("Assis", "Full"),
           Org = c("CHEM", "PSY")) %>% 
  # we'll need to update our column names
  mutate(col_names = str_c(Pos, "_", Org))

# get the draws with `fitted()`
f1 <-
  fitted(fit20.1, 
         newdata = nd,
         summary = F) %>% 
  # wrangle
  as_tibble() %>% 
  set_names(nd %>% pull(col_names)) %>% 
  mutate(`Full - Assis @ PSY`  = Full_PSY - Assis_PSY,
         `Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM) %>% 
  mutate(`Full.v.Assis\n(x)\nCHEM.v.PSY` = `Full - Assis @ CHEM` - `Full - Assis @ PSY`)

# what have we done?
head(f1)
```

It'll take just a tiny bit more wrangling before we're ready to plot.

```{r, fig.width = 8, fig.height = 2.5}
# save the levels
levels <- c("Full - Assis @ PSY", "Full - Assis @ CHEM", "Full.v.Assis\n(x)\nCHEM.v.PSY")

# rope annotation
text <-
  tibble(name  = "Full - Assis @ PSY",
         value = 15500,
         y     = .95,
         label = "ROPE") %>% 
  mutate(name = factor(name, levels = levels))

# wrangle
f1 %>% 
  pivot_longer(-contains("_")) %>% 
  mutate(name = factor(name, levels = levels)) %>% 
  
  # plot!
  ggplot(aes(x = value, y = 0)) +
  # for kicks and giggles we'll throw in the ROPE
  geom_rect(xmin = -1e3, xmax = 1e3,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = bd[7]) +
  stat_beedrill() +
  geom_text(data = text,
            aes(y = y, label = label),
            color = bd[7], size = 5) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  expand_limits(x = 0) +
  facet_wrap(~ name, scales = "free")
```

If it was really important that the labels in the $x$-axes were different, like they are in Kruschke's Figure 20.5, you could always make the three plots separately and then bind them together with **patchwork** syntax.

Though he didn't show the results, on page 598 Kruschke mentioned a few other contrasts we might consider. The example entailed comparing the differences within `BFIN` to the average of the other three. Let's walk that out.

```{r}
# define our new data
nd <-
  crossing(Pos = c("Assis", "Full"),
           Org = c("BFIN", "CHEM", "ENG", "PSY")) %>% 
  # we'll need to update our column names
  mutate(col_names = str_c(Pos, "_", Org))

# get the draws with `fitted()`
f2 <-
  fitted(fit20.1, 
         newdata = nd,
         summary = F) %>% 
  # wrangle
  as_tibble() %>% 
  set_names(nd %>% pull(col_names)) %>% 
  mutate(`Full - Assis @ BFIN` = Full_BFIN - Assis_BFIN,
         `Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM,
         `Full - Assis @ ENG`  = Full_ENG - Assis_ENG,
         `Full - Assis @ PSY`  = Full_PSY - Assis_PSY) %>% 
  mutate(`Full.v.Assis\n(x)\nBFIN.v.the rest` = `Full - Assis @ BFIN` - (`Full - Assis @ CHEM` + `Full - Assis @ ENG` + `Full - Assis @ PSY`) / 3)

# what have we done?
glimpse(f2)
```

Now plot.

```{r, fig.width = 8, fig.height = 4.5}
f2 %>% 
  pivot_longer(-contains("_")) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  geom_rect(xmin = -1e3, xmax = 1e3,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = bd[7]) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  expand_limits(x = 0) +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

So while the overall pay averages for those in `BFIN` were larger than those in the other three departments, the differences between full and associate professors within `BFIN` wasn't substantially different from the differences within the other three departments. To be sure, the interquartile range of that last difference distribution fell below both zero and the ROPE, but there's still a lot of spread in the rest of the distribution.

#### Interaction effects: High uncertainty and shrinkage.

"It is important to realize that the estimates of interaction contrasts are typically much more uncertain than the estimates of simple effects or main effects" (p. 598).

If we start with our `fitted()` object `f1`, we can wrangle a bit, compute the HDIs with `tidybayes::mode_hdi()` and then use simple subtraction to compute the interval range for each difference.

```{r, warning = F, message = F}
f1 %>% 
  pivot_longer(-contains("_")) %>% 
  mutate(name = factor(name, levels = c("Full - Assis @ PSY", "Full - Assis @ CHEM", "Full.v.Assis\n(x)\nCHEM.v.PSY"))) %>% 
  group_by(name) %>% 
  mode_hdi(value) %>% 
  select(name:.upper) %>% 
  mutate(`interval range` = .upper - .lower)
```

Just like Kruschke pointed out in the text, the interval for the interaction estimate was quite larger than the intervals for the simple contrasts.

> This large uncertainty of an interaction contrast is caused by the fact that it involves at least four sources of uncertainty (i.e., at least four groups of data), unlike its component simple effects which each involve only half of those sources of uncertainty. In general, interaction contrasts require a lot of data to estimate accurately. (p. 598)

Gelman has blogged on this, a bit (e.g., [*You need 16 times the sample size to estimate an interaction than to estimate a main effect*](https://andrewgelman.com/2018/03/15/need-16-times-sample-size-estimate-interaction-estimate-main-effect/)).

There is also shrinkage.

> The interaction contrasts also can experience notable shrinkage from the hierarchical model. In the present application, for example, there are 300 interaction deflections (5 levels of seniority times 60 departments) that are assumed to come from a higher- level distribution that has an estimated standard deviation, denoted $\sigma_{\beta 1 \times 2}$ in Figure 20.2. Chances are that most of the 300 interaction deflections will be small, and therefore the estimated standard deviation of the interaction deflections will be small, and therefore the estimated deflections themselves will be shrunken toward zero. This shrinkage is inherently neither good nor bad; it is simply the correct consequence of the model assumptions. The shrinkage can be good insofar as it mitigates false alarms about interactions, but the shrinkage can be bad if it inappropriately obscures meaningful interactions. (p. 598)

Here's that $\sigma_{\beta_{1 \times 2}}$.

```{r, fig.width = 3.75, fig.height = 2.5}
draws %>% 
  ggplot(aes(x = `sd_Pos:Org__Intercept`, y = 0)) +
  stat_beedrill() +
  xlab(expression(sigma[beta[1%*%2]])) +
  scale_y_continuous(NULL, breaks = NULL)
```

## Rescaling can change interactions, homogeneity, and normality

> When interpreting interactions, it can be important to consider the scale on which the data are measured. This is because an interaction means non-additive effects when measured on the current scale. If the data are nonlinearly transformed to a different scale, then the non-additivity can also change. (p. 599)

Here is Kruschke's initial example of a possible interaction effect of sex and political party with respect to wages.

```{r, fig.width = 3.25, fig.height = 3}
d <-
  tibble(monetary_units = c(10, 12, 15, 18),
         politics       = rep(c("democrat", "republican"), each = 2),
         sex            = rep(c("women", "men"), times = 2)) %>% 
  mutate(sex_number = if_else(sex == "women", 1, 2),
         politics   = factor(politics, levels = c("republican", "democrat")))

d %>% 
  ggplot(aes(x = sex_number, y = monetary_units, color = politics)) +
  geom_line(size = 3) +
  scale_color_manual(NULL, values = bd[8:7]) +
  scale_x_continuous("sex", breaks = 1:2, labels = c("women", "men")) +
  coord_cartesian(ylim = c(0, 20)) +
  theme(legend.position = c(.2, .15))
```

Because the pay discrepancy between men and women is not equal between Democrats and Republicans, in this example, it can be tempting to claim there is a subtle interaction. Not necessarily so.

```{r}
tibble(politics      = c("democrat", "republican"),
       female_salary = c(10, 15)) %>% 
  mutate(male_salary = 1.2 * female_salary)
```

If we take female salary as the baseline and then add 20% to it for the men, the salary difference between Republican men and women will be larger than that between Democratic men and women. Even though the rate increase from women to men was the same, the increase in absolute value was greater within Republicans because Republican women made more than Democratic women.

Look what happens to our original plot when we transform `monetary_units` with `log10()`.

```{r, fig.width = 3.25, fig.height = 3}
d %>% 
  ggplot(aes(x = sex_number, y = log10(monetary_units), color = politics)) +
  geom_line(size = 3) +
  scale_color_manual(NULL, values = bd[8:7]) +
  scale_x_continuous("sex", breaks = 1:2, labels = c("women", "men")) +
  theme(legend.position = c(.2, .4))
```

"Equal ratios are transformed to equal distances by a logarithmic transformation" (p. 599).

We can get a better sense of this with our version of Figure 20.6. Probably the easiest way to inset the offset labels will be with help from the `geom_dl()` function from the [**directlabels** package](https://CRAN.R-project.org/package=directlabels) [@R-directlabels].

```{r, fig.width = 7.5, fig.height = 4.5, warning = F, message = F}
library(directlabels)

# define the data
d <-
  crossing(int = c("Non−crossover Interaction", "Crossover Interaction"),
           x1  = factor(1:2),
           x2  = factor(1:2)) %>% 
  mutate(y = c(6, 2, 15, 48, 2, 6, 15, 48)) %>% 
  mutate(ly = log(y))

# save the subplots
p1 <-
  d %>% 
  filter(int == "Non−crossover Interaction") %>% 
  ggplot(aes(x = x1, y = y, group = x2, label = x2)) +
  geom_line(aes(color = x2),
            size = 1) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[8:7]) +
  scale_x_discrete(NULL, breaks = NULL, expand = expansion(mult = 0.1)) +
  labs(subtitle = "Non−crossover Interaction") +
  theme(legend.position = c(.15, .8),
        legend.key.size = unit(1/3, "cm"))

p2 <-
  d %>% 
  filter(int == "Crossover Interaction") %>% 
  ggplot(aes(x = x1, y = y, group = x2, label = x2)) +
  geom_line(aes(color = x2),
            size = 1) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[8:7], breaks = NULL) +
  scale_x_discrete(NULL, breaks = NULL, expand = expansion(mult = 0.1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Crossover Interaction")

p3 <-
  d %>% 
  filter(int == "Crossover Interaction") %>% 
  ggplot(aes(x = x2, y = y, group = x1, label = x1)) +
  geom_line(aes(color = x1),
            size = 2) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[12:11]) +
  scale_x_discrete(NULL, breaks = NULL, expand = expansion(mult = 0.1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Crossover Interaction") +
  theme(legend.position = c(.15, .8),
        legend.key.size = unit(1/3, "cm"))

p4 <-
  d %>% 
  filter(int == "Non−crossover Interaction") %>% 
  ggplot(aes(x = x1, y = ly, group = x2, label = x2)) +
  geom_line(aes(color = x2),
            size = 1) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[8:7], breaks = NULL) +
  scale_x_discrete(expand = expansion(mult = 0.1)) +
  ylab(expression(log(y)))

p5 <-
  d %>% 
  filter(int == "Crossover Interaction") %>% 
  ggplot(aes(x = x1, y = ly, group = x2, label = x2)) +
  geom_line(aes(color = x2),
            size = 1) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[8:7], breaks = NULL) +
  scale_x_discrete(expand = expansion(mult = 0.1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ylab(expression(log(y)))

p6 <-
  d %>% 
  filter(int == "Crossover Interaction") %>% 
  ggplot(aes(x = x2, y = ly, group = x1, label = x1)) +
  geom_line(aes(color = x1),
            size = 2) +
  geom_dl(method = list(dl.combine("first.points", "last.points")), 
          color = bd[3]) +
  scale_color_manual(values = bd[12:11], breaks = NULL) +
  scale_x_discrete(expand = expansion(mult = 0.1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ylab(expression(log(y)))

# combine
p1 + p2 + p3 + p4 + p5 + p6 
```

"The transformability from interaction to non-interaction is only possible for non-crossover interactions. This terminology, 'noncrossover,' is merely a description of the graph: The lines do not cross over each other and they have the same sign slope" (p. 601). The plots in the leftmost column are examples of non-crossover interactions. The plots in the center column are examples of crossover interactions.

Kruschke then pointed out that transforming data can have unexpected consequences for summary values, such as variances:

> Suppose one condition has data values of 100, 110, and 120, while a second condition has data values of 1100, 1110, and 1120. For both conditions, the variance is 66.7, so there is homogeneity of variance. When the data are logarithmically transformed, the variance of the first group becomes 1.05e−3, but the variance of the second group becomes two orders of magnitude smaller, namely 1.02e−5. In the transformed data there is not homogeneity of variance. (p. 601)

See for yourself.

```{r, message = F, warning = F}
tibble(x = c(100, 110, 120, 1100, 1110, 1120),
       y = rep(letters[1:2], each = 3)) %>% 
  group_by(y) %>% 
  summarise(variance_of_x     = var(x),
            variance_of_log_x = var(log(x)))
```

Though it looks like the numbers Kruschke reported in the text are off, his overall point stands. While we had homogeneity of variance for `x`, the variance is heterogeneous when working with `log(x)`.

## Heterogeneous variances and robustness against outliers

As we will see in just a moment, our approach to the variant of this model with heterogeneous variances and robustness against outliers will differ slightly from the one Kruschke presented in the text. The two are the same in spirit, but ours differs in how we model $\sigma_{[jk](i)}$. We'll get a sense of that difference in our version of the hierarchical model diagram of Figure 20.7.

```{r, fig.width = 8, fig.height = 7.25, message = F}
# bracket
p1 <-
  tibble(x     = .99,
         y     = .5,
         label = "{_}") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, hjust = 1, color = bd[2], family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

#  plain arrow
p2 <-
  tibble(x    = .73,
         y    = 1,
         xend = .68,
         yend = .25) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()
  
# normal density
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p4 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("0", "sigma[beta][1]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p5 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("0", "sigma[beta][2]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))
  
# normal density
p6 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 0.75), y = .6,
           hjust = c(.5, 0),
           label = c("0", "sigma[beta][1%*%2]"), 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# four annotated arrows
p7 <-
  tibble(x    = c(.05, .34, .64, .945),
         y    = c(1, 1, 1, 1),
         xend = c(.05, .18, .45, .75),
         yend = c(0, 0, 0, 0)) %>%
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = c(.03, .23, .30, .52, .585, .82, .90), y = .5,
           label = c("'~'", "'~'", "italic(j)", "'~'", "italic(k)", "'~'", "italic(jk)"),
           size = c(10, 10, 7, 10, 7, 10, 7), 
           color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p8 <-
  tibble(x = .5,
         y = .25,
         label = "beta[0]+sum()[italic(j)]*beta[1]['['*italic(j)*']']*italic(x)[1]['['*italic(j)*']'](italic(i))+sum()[italic(k)]*beta[2]['['*italic(k)*']']*italic(x)[2]['['*italic(k)*']'](italic(i))+sum()[italic(jk)]*beta[1%*%2]['['*italic(jk)*']']*italic(x)[1%*%2]['['*italic(jk)*']'](italic(i))") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(hjust = .5, size = 7, color = bd[3], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# normal density
p9 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.15), y = .6,
           label = c("italic(M)[mu[sigma]]", "italic(S)[mu[sigma]]"), 
           hjust = c(.5, 0),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# half-normal density
p10 <-
  tibble(x = seq(from = 0, to = 3, by = .01)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "half-normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = 1.5, y = .6,
           label = "0*','*~italic(S)[sigma[sigma]]", 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# exponential density
p11 <-
  tibble(x = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = x, y = (dexp(x, 2) / max(dexp(x, 2))))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "exp",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = .5, y = .6,
           label = "italic(K)",
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# normal density
p12 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(0, 1.2), y = .6,
           hjust = c(.5, 0),
           label = c("mu[sigma]", "sigma[sigma]"), 
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# student-t density
p13 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dt(x, 3) / max(dt(x, 3))))) +
  geom_area(fill = bd[6]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "student t",
           size = 7, color = bd[3]) +
  annotate(geom = "text",
           x = c(-1.4, 0), y = .6,
           label = c("nu", "mu[italic(i)]"),
           size = 7, color = bd[3], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = bd[3]))

# two annotated arrows
p14 <-
  tibble(x    = c(.18, .82),
         y    = c(1, 1),
         xend = c(.46, .66),
         yend = c(.28, .28)) %>%
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = c(.24, .69), y = .62,
           label = "'~'",
           size = 10, color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# log sigma
p15 <-
  tibble(x = .65,
         y = .6,
         label = "log(sigma['['*italic(jk)*']('*italic(i)*')'])") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(hjust = 0, size = 7, color = bd[3], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) +
  ylim(0, 1) +
  theme_void()

# two annotated arrows
p16 <-
  tibble(x    = c(.18, .18), # .2142857
         y    = c(1, .45),
         xend = c(.18, .67),
         yend = c(.75, .14)) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = c(.13, .18, .26), y = c(.92, .64, .22),
           label = c("'~'", "nu*minute+1", "'='"),
           size = c(10, 7, 10), color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# one annotated arrow
p17 <-
  tibble(x    = .5,
         y    = 1,
         xend = .5,
         yend = .03) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = .4, y = .5,
           label = "'='",
           size = 10, color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# another annotated arrow
p18 <-
  tibble(x    = .87,
         y    = 1,
         xend = .43,
         yend = .2) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = bd[3]) +
  annotate(geom = "text",
           x = c(.56, .7), y = .5,
           label = c("'~'", "italic(jk)"),
           size = c(10, 7), color = bd[3], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# the final annotated arrow
p19 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>%

  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7),
            color = bd[3], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow, color = bd[3]) +
  xlim(0, 1) +
  theme_void()

# some text
p20 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>%

  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = bd[3], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 1, l = 6, r = 7),
  area(t = 1, b = 1, l = 10, r = 11),
  area(t = 1, b = 1, l = 14, r = 15),
  area(t = 3, b = 4, l = 1, r = 3),
  area(t = 3, b = 4, l = 5, r = 7),
  area(t = 3, b = 4, l = 9, r = 11),
  area(t = 3, b = 4, l = 13, r = 15),
  area(t = 2, b = 3, l = 6, r = 7),
  area(t = 2, b = 3, l = 10, r = 11),
  area(t = 2, b = 3, l = 14, r = 15),
  area(t = 6, b = 7, l = 1, r = 15),
  area(t = 5, b = 6, l = 1, r = 15),
  area(t = 9, b = 10, l = 9, r = 11),
  area(t = 9, b = 10, l = 13, r = 15),
  area(t = 12, b = 13, l = 1, r = 3),
  area(t = 12, b = 13, l = 11, r = 13),
  area(t = 16, b = 17, l = 5, r = 7),
  area(t = 11, b = 12, l = 9, r = 15),
  area(t = 16, b = 17, l = 5, r = 10),
  area(t = 14, b = 16, l = 1, r = 7),
  area(t = 8, b = 16, l = 5, r = 7),
  area(t = 14, b = 16, l = 5, r = 13),
  area(t = 18, b = 18, l = 5, r = 7),
  area(t = 19, b = 19, l = 5, r = 7)
)

# combine and plot!
(p1 + p1 + p1 + p3 + p4 + p5 + p6 + p2 + p2 + p2 + p8 + p7 + p9 + p10 + p11 + p12 + p13 + p14 + p15 + p16 + p17 + p18 + p19 + p20) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Wow that's a lot! If you compare our diagram with the one in the text, you'll see the $\nu$ and $\mu$ structures are the same. If you look down toward the bottom of the diagram, the first big difference is that we're modeling the log of $\sigma_{[jk](i)}$, which is the typical **brms** strategy for avoiding negative values when modeling a $\sigma$ parameter. Then when you look up and to the right, you'll see that we're modeling $\log(\sigma_{[jk](i)})$ with a conventional hierarchical Gaussian structure. Keep this in mind when we get into our **brms** code, below.

Before we fit the robust hierarchical variances model, we need to define our `stanvars`.

```{r}
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta") +
  stanvar(1/29,      name = "one_over_twentynine")
```

Recall that to fit a robust hierarchical variances model, we need to wrap our two formulas within the `bf()` function. 

`r emo::ji("warning")` **Warning**: this one took a few hours fit. `r emo::ji("warning")`

```{r, echo = F, eval = F}
# Heads up: this took 2.850737 hours to fit (4-15-2022)
```

```{r fit20.2}
fit20.2 <-
  brm(data = my_data,
      family = student,
      bf(Salary ~ 1 + (1 | Pos) + (1 | Org) + (1 | Pos:Org), 
         sigma  ~ 1 + (1 | Pos:Org)),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(normal(log(sd_y), 1),     class = Intercept, dpar = sigma),
                prior(gamma(alpha, beta), class = sd),
                prior(normal(0, 1),       class = sd, dpar = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      seed = 20,
      control = list(adapt_delta = 0.9995,
                     max_treedepth = 15),
      stanvars = stanvars,
      file = "fits/fit20.02")
``` 

Behold the summary.

```{r}
print(fit20.2)
```

This time we'll just feed the results of the wrangling code right into the plotting code for our version of the top panels of Figure 20.8.

```{r, warning = F, message = F ,fig.height = 5.5, fig.width = 8}
# how many draws would you like?
n_draw <- 20

# wrangle
my_data %>% 
  distinct(Pos) %>% 
  expand(Pos, 
         Org = c("BFIN", "CHEM", "PSY", "ENG")) %>% 
  add_epred_draws(fit20.2, ndraws = n_draw, seed = 20,
                  allow_new_levels = T,
                  dpar = c("mu", "sigma", "nu")) %>% 
  mutate(ll = qt(.025, df = nu),
         ul = qt(.975, df = nu)) %>% 
  mutate(Salary = map2(ll, ul, seq, length.out = 100)) %>% 
  unnest(Salary) %>%
  mutate(density = dt(Salary, nu)) %>% 
  # notice the conversion
  mutate(Salary = mu + Salary * sigma) %>% 
  group_by(.draw) %>% 
  mutate(density = density / max(density)) %>% 
  mutate(Org = factor(Org, levels = c("BFIN", "CHEM", "PSY", "ENG"))) %>%
  
  # plot
  ggplot(aes(x = Salary, y = Pos)) +
  geom_vline(xintercept = fixef(fit20.1)[, 1], color = bd[5]) +
  geom_ridgeline(aes(height = density, group = interaction(Pos, .draw), color = Pos),
                 fill = NA, show.legend = F,
                 size = 1/4, scale = 3/4) +
  geom_jitter(data = my_data %>% 
                filter(Org %in% c("BFIN", "CHEM", "PSY", "ENG")) %>% 
                mutate(Org = factor(Org, levels = c("BFIN", "CHEM", "PSY", "ENG"))),
              height = .025, alpha = 1/2, size = 2/3, color = bd[11]) +
  scale_color_manual(values = bd[c(14, 13, 8, 12, 3)]) +
  scale_x_continuous(breaks = seq(from = 0, to = 300000, length.out = 4),
                     labels = c("$0", "$100K", "200K", "$300K")) +
  coord_cartesian(xlim = c(0, 35e4),
                  ylim = c(1.25, 5.5)) +
  labs(title = "Data with Posterior Predictive Distributions", 
       subtitle = "The white vertical line is the model-implied grand mean.",
       y = "Pos") +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank()) +
  facet_wrap(~ Org, ncol = 2)
```

Our results for the bottom panel of Figure 20.8 will differ substantially from Kruschke's. Recall that Kruschke modeled $\sigma_{[j, k](i)}$ with a hierarchical gamma distribution and using the $\omega + \sigma$ parameterization. We, however, modeled our hierarchical $\log (\sigma)$ with the typical normal distribution. As such, we have posteriors for $\sigma_\mu$ and $\sigma_\sigma$.

```{r, fig.width = 8, fig.height = 2.5, warning = F}
# wrangle
as_draws_df(fit20.2) %>% 
  transmute(Normality              = log10(nu), 
            `Mean of Cell Sigma's` = exp(b_sigma_Intercept), 
            `SD of Cell Sigma's`   = exp(`sd_Pos:Org__sigma_Intercept`)) %>%
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("Normality", "Mean of Cell Sigma's", "SD of Cell Sigma's"))) %>% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("param. value") +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

Though our $\sigma_\mu$ is on a similar metric to Kruschke's $\sigma_\omega$, our $\sigma_\sigma$ is just fundamentally different from his. So it goes. If you think I'm in error, here, [share your insights](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues). Even though our hierarchical $\sigma$ parameters look different from Kruschke's, it turns the contrast distributions are quite similar. Here's the necessary wrangling to make our version for Figure 20.9.

```{r}
# define our new data
nd <-
  crossing(Pos = c("Assis", "Full"),
           Org = c("CHEM", "PSY")) %>% 
  # we'll need to update our column names
  mutate(col_names = str_c(Pos, "_", Org))

# get the draws with `fitted()`
f <-
  fitted(fit20.2, 
         newdata = nd,
         summary = F) %>% 
  # wrangle
  as_tibble() %>% 
  set_names(nd %>% pull(col_names)) %>% 
  transmute(`Full - Assis @ CHEM` = Full_CHEM - Assis_CHEM,
            `Full - Assis @ PSY`  = Full_PSY - Assis_PSY) %>% 
  mutate(`Full.v.Assis\n(x)\nCHEM.v.PSY` = `Full - Assis @ CHEM` - `Full - Assis @ PSY`) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("Full - Assis @ PSY", "Full - Assis @ CHEM", "Full.v.Assis\n(x)\nCHEM.v.PSY")))

# what have we done?
head(f)
```

Now plot.

```{r, fig.width = 8, fig.height = 2.5}
f %>% 
  ggplot(aes(x = value, y = 0)) +
  geom_rect(xmin = -1e3, xmax = 1e3,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = bd[7]) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  expand_limits(x = 0) +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

See? Our contrast distributions are really close those in the text. Here are the numeric estimates.

```{r}
f %>% 
  group_by(name) %>% 
  mode_hdi(value) %>% 
  select(name:.upper) %>% 
  mutate_if(is.double, round, digits = 0)
```

In the second half of the middle paragraph on page 605, Kruschke contrasted the $\sigma_{\beta_{1 \times 2}}$ parameter in the two models (i.e., our `fit20.1` and `fit20.2`). Recall that in the **brms** output, these are termed `sd_Pos:Org__Intercept`. Here are the comparisons from our **brms** models.

```{r}
posterior_summary(fit20.1)["sd_Pos:Org__Intercept", ]
posterior_summary(fit20.2)["sd_Pos:Org__Intercept", ]
```

They are similar to the values in the text. And recall, of course, the `brms::posterior_summary()` function returns posterior means. If you really wanted the posterior modes, like Kruschke reported in the text, you'll have to work a little harder.

```{r, warning = F}
bind_rows(as_draws_df(fit20.1) %>% select(`sd_Pos:Org__Intercept`),
          as_draws_df(fit20.2) %>% select(`sd_Pos:Org__Intercept`)) %>% 
  mutate(fit = rep(c("fit20.1", "fit20.2"), times = c(8000, 12000))) %>% 
  group_by(fit) %>% 
  mode_hdi(`sd_Pos:Org__Intercept`) %>% 
  select(fit:.upper) %>% 
  mutate_if(is.double, round, digits = 0)
```

The curious might even look at those in a plot.

```{r, fig.width = 6, fig.height = 2, warning = F}
bind_rows(as_draws_df(fit20.1) %>% select(`sd_Pos:Org__Intercept`),
          as_draws_df(fit20.2) %>% select(`sd_Pos:Org__Intercept`)) %>% 
  mutate(fit = rep(c("fit20.1", "fit20.2"), times = c(8000, 12000))) %>% 
  
  ggplot(aes(x = `sd_Pos:Org__Intercept`, y = fit)) +
  stat_beedrill(size = 1/2, point_size = 2) +
  labs(x = expression(sigma[beta[1%*%2]]),
       y = NULL) +
  coord_cartesian(ylim = c(1.5, NA))
```

"Which model is a better description of the data?... In principle, an intrepid programmer could do a Bayesian model comparison..." (pp. 605--606). We could also examine information criteria, like the LOO.

```{r}
fit20.1 <- add_criterion(fit20.1, "loo")
fit20.2 <- add_criterion(fit20.2, "loo")
```

*Sigh*. Both models had high `pareto_k` values, suggesting there were outliers relative to what was expected by their likelihoods. Just a little further in the text, Kruschke gives us hints why this might be so:

> Moreover, both models assume that the data within cells are distributed symmetrically above and below their central tendency, either as a normal distribution or a $t$-distribution. The data instead seem to be skewed toward larger values, especially for advanced seniorities. (p. 606)

Here's the current LOO difference.

```{r}
loo_compare(fit20.1, fit20.2) %>% print(simplify = F)
```

But really, "we might want to create a model that describes the data within each cell as a skewed distribution such as a Weibull" (p. 606). Yes, **brms** can handle Weibull regression (e.g., [here](https://discourse.mc-stan.org/t/fitting-time-to-event-data-with-weibull-hazard-using-brm-function/4638)).

```{r, echo = F}
# these fits are sufficiently large that I'd like to clear them out before moving on
rm(fit20.1, fit20.2)
```

## Within-subject designs

> When every subject contributes many measurements to every cell, then the model of the situation is a straight-forward extension of the models we have already considered. We merely add "subject" as another nominal predictor in the model, with each individual subject being a level of the predictor. If there is one predictor other than subject, the model becomes
>
> $$ y = \beta_0 + \overrightarrow \beta_1 \overrightarrow x_1 + \overrightarrow \beta_S \overrightarrow x_S + \overrightarrow \beta_{1 \times S} \overrightarrow x_{1 \times S} $$
>
> This is exactly the two-predictor model we have already considered, with the second predictor being subject. When there are two predictors other than subject, the model becomes
> 
> \begin{align*}
> y = & \; \beta_0 & \text{baseline} \\
> & + \overrightarrow \beta_1 \overrightarrow x_1 + \overrightarrow \beta_2 \overrightarrow x_2 + \overrightarrow \beta_S \overrightarrow x_S  & \text{main effects} \\
> & + \overrightarrow \beta_{1 \times 2} \overrightarrow x_{1 \times 2} + \overrightarrow \beta_{1 \times S} \overrightarrow x_{1 \times S} + \overrightarrow \beta_{2 \times S} \overrightarrow x_{2 \times S} & \text{two-way interactions} \\
> & + \overrightarrow \beta_{1 \times 2 \times S} \overrightarrow x_{1 \times 2 \times S} & \text{three-way interactions}
> \end{align*}
> 
> This model includes all the two-way interactions of the factors, plus the three-way interaction. (p. 607)

In situations in which subjects only contribute one observation per condition/cell, we simplify the model to

\begin{align*}
y = & \; \beta_0 \\
& + \overrightarrow \beta_1 \overrightarrow x_1 + \overrightarrow \beta_2 \overrightarrow x_2 + \overrightarrow \beta_{1 \times 2} \overrightarrow x_{1 \times 2} \\
& + \overrightarrow \beta_S \overrightarrow x_S
\end{align*}

"In other words, we assume a main effect of subject, but no interaction of subject with other predictors. In this model, the subject effect (deflection) is constant across treatments, and the treatment effects (deflections) are constant across subjects" (p. 608).

### Why use a within-subject design? And why not?

Kruschke opined "the primary reason to use a within-subject design is that you can achieve greater precision in the estimates of the effects than in a between-subject design" (p. 608). Well, to that I counterpoint: "No one goes to the circus to see the average dog jump through the hoop significantly oftener than untrained does raised under the same circumstances" [@skinnerCaseHistoryScientific1956, p. 228]. And it's unlikely you’ll make a skillful jumper of your dog without repeated trials. There's also the related issue that between- and within-person processes aren't necessarily the same. For an introduction to the issue, see Hamaker's [-@hamakerWhyResearchersShould2012] chapter, [*Why researchers should think "within-person": A paradigmatic rationale*](https://www.researchgate.net/publication/266896375_Why_researchers_should_think_within-person_A_paradigmatic_rationale), or the paper from @bolgerCausalProcessesPsychology2019, [*Causal processes in psychology are heterogeneous*](https://www.researchgate.net/profile/Niall_Bolger/publication/332358948_Causal_processes_in_psychology_are_heterogeneous/links/5cd9b471a6fdccc9ddaa7879/Causal-processes-in-psychology-are-heterogeneous.pdf).

But we digress.

Here's the 4-subject response time data.

```{r}
(
  d <-
  tibble(response_time = c(300, 320, 350, 370, 400, 420, 450, 470),
         subject       = rep(1:4, each = 2),
         hand          = rep(c("dominant", "nondominant"), times = 4))
)
```

"For every subject, the difference between dominant and nondominant hands is exactly 20 ms, but there are big differences across subjects in overall response times" (p. 608). Here's what that looks like.

```{r, fig.width = 6, fig.height = 1.5}
d %>% 
  mutate(subject = factor(subject)) %>% 
  
  ggplot(aes(x = response_time, y = subject)) +
  geom_line(aes(group = subject), 
            color = bd[3], linetype = 3) +
  geom_point(aes(color = hand), size = 3) +
  scale_color_manual(values = bd[8:9])
```

Here there is more variability between subjects than within them, which you'd never detect without a within-subject design including multiple subjects.

### Split-plot design.

"Split-plot experiments were invented by @fisherStatisticalMethodsResearch1925 (p. 610)."

Kruschke then wrote this to set the stage for the next subsection:

> Consider an agricultural experiment investigating the productivity of different soil tilling methods and different fertilizers. It is relatively easy to provide all the farmers with the several different fertilizers. But it might be relatively difficult to provide all farmers with all the machinery for several different tilling methods. Therefore, any particular farmer will use a single (randomly assigned) tilling method on his whole plot, and tilling methods will differ between whole plots. Each farmer will split his field into subplots and apply all the fertilizers to different (randomly assigned) split plots, and fertilizers will differ across split plots within whole plots. This type of experiment inspires the name, split-plot design. The generic experiment-design term for the farmer's field is "block." Then, the factor that varies within every field is called the within-block factor and the factor that varies between fields is called the between-block factor. Notice also that each split plot yields a single measurement (in this case the productivity measured in bushels per acre), not multiple measurements. (p. 610)

#### Example: Knee high by the fourth of July.

Load the agronomy data.

```{r, warning = F, message = F}
my_data <- read_csv("data.R/SplitPlotAgriData.csv")

glimpse(my_data)
```

We might use `geom_tile()` to visualize the data like this.

```{r, fig.width = 6, fig.height = 4}
my_data %>% 
  mutate(Fert = str_c("Fert: ", Fert)) %>% 
  
  ggplot(aes(x = Till, y = Field)) +
  geom_tile(aes(fill = Yield)) +
  scale_fill_gradient(low = bd[1], high = bd[12]) +
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  theme(panel.background = element_rect(fill = bd[7])) +
  facet_wrap(~ Fert)
```

As Kruschke pointed out in the text, notice how each `Field` has only one level of `Till`, but three levels of `Fert`.

#### The descriptive model.

> In the classical ANOVA-style model for a split-plot design, the overall variance is conceptually decomposed into five components: the main effect of the between-subjects factor, the main effect of the within-subjects factor, the interaction of the two factors, the effect of subject within levels of the between-subject factor, and the interaction of subject with the within-subject factor. Unfortunately, because there is only a single datum per cell, the five components exactly match the data, which is to say that there are as many parameters as there are data points. (If every subject contributed multiple data points to every cell then the five-component model could be used.) Because there is no residual noise within cells, the classical approach is to treat the final component as noise, that is, treat the interaction of subject with the within-subject factor as noise. That component is not included in the model (at least, not distinct from noise). We will do the same for the descriptive model in our Bayesian analysis. (p. 612)

#### Implementation in ~~JAGS~~ brms.

Define the `stanvars`.

```{r}
mean_y <- mean(my_data$Yield)
sd_y   <- sd(my_data$Yield)

omega <- sd_y / 2
sigma <- 2 * sd_y

s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)

stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta")
```

Here's how to fit the model with `brm()`.

```{r fit20.3}
fit20.3 <-
  brm(data = my_data,
      family = gaussian,
      Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Field) + (1 | Till:Fert),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 20,
      control = list(adapt_delta = 0.9999,
                     max_treedepth = 12),
      stanvars = stanvars,
      file = "fits/fit20.03")
``` 

#### Results.

Check the summary.

```{r}
print(fit20.3)
```

We might compare the $\sigma$ posteriors with a plot.

```{r, fig.width = 8, fig.height = 2.75, warning = F}
as_draws_df(fit20.3) %>% 
  pivot_longer(c(sigma, starts_with("sd"))) %>% 
    
  ggplot(aes(x = value, y = name)) +
  stat_beedrill(slab_color = bd[6], size = 1/2, point_size = 3/2) +
  labs(x = NULL,
       y = NULL) +
  coord_cartesian(xlim = c(0, 50),
                  ylim = c(1.5, NA)) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())
```

Now we have the results of `fit20.3`, we are ready to make our version if Figure 20.10. Note that how within `add_fitted_draws()`, we used the `re_formula` argument to average over the random effects of `Field` (i.e., we left `(1 | Field)` out of the formula). That's our equivalent to when Kruschke wrote "The predictive normal distributions are plotted with means at $\beta_0 + \beta_B + \beta_W + \beta_{B \times W}$ (collapsed across $\beta_S$) and with standard deviation $\sigma$" (pp. 614--615).

```{r, fig.width = 6, fig.height = 3.5}
# wrangle
my_data %>% 
  distinct(Till, Fert) %>% 
  add_epred_draws(fit20.3, 
                  ndraws = 20,
                  re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert),
                  dpar = c("mu", "sigma")) %>% 
  mutate(ll = qnorm(.025, mean = mu, sd = sigma),
         ul = qnorm(.975, mean = mu, sd = sigma)) %>% 
  mutate(Yield = map2(ll, ul, seq, length.out = 100)) %>% 
  unnest(Yield) %>% 
  mutate(density = dnorm(Yield, mu, sigma)) %>% 
  group_by(.draw) %>% 
  mutate(density = density / max(density)) %>% 
  
  # plot
  ggplot(aes(x = Yield, y = Fert)) +
  geom_path(data = my_data,
            aes(group = Field %>% as.factor()),
            size = 1/4, color = bd[5]) +
  geom_ridgeline(aes(height = -density, group = interaction(Fert, .draw), color = Fert),
                 fill = NA, size = 1/3, scale = 3/4,
                 min_height = NA) +
  geom_jitter(data = my_data,
              height = .025, alpha = 1/2, color = bd[5]) +
  scale_color_manual(values = bd[c(9, 6, 12)]) +
  scale_x_continuous(breaks = seq(from = 100, to = 180, by = 20)) +
  coord_flip(xlim = c(90, 190),
             ylim = c(0.5, 2.75)) +
  ggtitle("Data with Posterior Predictive Distribution") +
  theme(axis.ticks.x = element_blank(),
        legend.position = "none") +
  facet_wrap(~ Till)
```

Now let's make our Figure 20.11 contrasts.

```{r, fig.width = 8, fig.height = 2.5}
nd <-
  my_data %>% 
  distinct(Till, Fert) %>% 
  # we'll need to update our column names
  mutate(col_names = str_c(Till, "_", Fert))

fitted(fit20.3,
       newdata = nd,
       summary = F,
       re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert)) %>% 
  as_tibble() %>% 
  set_names(nd %>% pull(col_names)) %>% 
  transmute(
    `Moldbrd\nvs\nRidge` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 3) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3),
            
    `Moldbrd.Ridge\nvs\nChisel` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface + Ridge_Broad + Ridge_Deep + Ridge_Surface) / 6) - ((Chisel_Broad + Chisel_Deep + Chisel_Surface) / 3),
    
    `Deep.Surface\nvs\nBroad` = ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6) - ((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3),
    
    `Chisel.Moldbrd.v.Ridge\n(x)\nBroad.v.Deep.Surface` = (((Chisel_Broad + Chisel_Deep + Chisel_Surface + Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 6) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3)) - (((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3) - ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6))
    ) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("Moldbrd\nvs\nRidge", "Moldbrd.Ridge\nvs\nChisel", "Deep.Surface\nvs\nBroad", "Chisel.Moldbrd.v.Ridge\n(x)\nBroad.v.Deep.Surface"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  geom_rect(xmin = -5, xmax = 5,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = bd[7]) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  expand_limits(x = -5) +
  theme(strip.text = element_text(size = 6)) +
  facet_wrap(~ name, scales = "free", ncol = 4)
```

As far as I can tell, it appears that our contrasts indicate our variance parameter for `Till` ended up larger than Kruschke's.

Kruschke then posed a model "with field/subject coding suppressed, hence no lines connecting data from the same field/subject" (p. 616). Here's how to fit that model.

```{r fit20.4}
fit20.4 <-
  brm(data = my_data,
      family = gaussian,
      Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert),
      prior = c(prior(normal(mean_y, sd_y * 5), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 20,
      control = list(adapt_delta = 0.999,
                     max_treedepth = 12),
      stanvars = stanvars,
      file = "fits/fit20.04")
``` 

Behold the summary.

```{r}
print(fit20.4)
```

Look at how much larger the posterior is for $\sigma_y$ in this model compared to `fit20.3`.

```{r}
posterior_summary(fit20.3)["sigma", ]
posterior_summary(fit20.4)["sigma", ]
```

Here's the top portion of Figure 20.12.

```{r, fig.width = 6, fig.height = 3.5}
# wrangle
my_data %>% 
  distinct(Till, Fert) %>% 
  add_epred_draws(fit20.4, 
                  ndraws = 20,
                  re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert),
                  dpar = c("mu", "sigma")) %>% 
  mutate(ll = qnorm(.025, mean = mu, sd = sigma),
         ul = qnorm(.975, mean = mu, sd = sigma)) %>% 
  mutate(Yield = map2(ll, ul, seq, length.out = 200)) %>% 
  unnest(Yield) %>% 
  mutate(density = dnorm(Yield, mu, sigma)) %>% 
  group_by(.draw) %>% 
  mutate(density = density / max(density)) %>% 
  
  # plot
  ggplot(aes(x = Yield, y = Fert)) +
  geom_ridgeline(aes(height = -density, group = interaction(Fert, .draw), color = Fert),
                 fill = NA, size = 1/3, scale = 3/4,
                 min_height = NA) +
  geom_jitter(data = my_data,
              height = .025, alpha = 1/2, color = bd[5]) +
  scale_color_manual(values = bd[c(9, 6, 12)]) +
  scale_x_continuous(breaks = seq(from = 100, to = 180, by = 20)) +
  coord_flip(xlim = c(90, 190),
             ylim = c(0.5, 2.75)) +
  ggtitle("Data with Posterior Predictive Distribution") +
  theme(axis.ticks.x = element_blank(),
        legend.position = "none") +
  facet_wrap(~ Till)
```

Now make the plots for the contrast distributions.

```{r, fig.width = 8, fig.height = 2.5}
fitted(fit20.4,
       newdata = nd,
       summary = F,
       re_formula = Yield ~ 1 + (1 | Till) + (1 | Fert) + (1 | Till:Fert)) %>% 
  as_tibble() %>% 
  set_names(nd %>% pull(col_names)) %>% 
  transmute(
    `Moldbrd\nvs\nRidge` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 3) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3),
            
    `Moldbrd.Ridge\nvs\nChisel` = ((Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface + Ridge_Broad + Ridge_Deep + Ridge_Surface) / 6) - ((Chisel_Broad + Chisel_Deep + Chisel_Surface) / 3),
    
    `Deep.Surface\nvs\nBroad` = ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6) - ((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3),
    
    `Chisel.Moldbrd.v.Ridge\n(x)\nBroad.v.Deep.Surface` = (((Chisel_Broad + Chisel_Deep + Chisel_Surface + Moldbrd_Broad + Moldbrd_Deep + Moldbrd_Surface) / 6) - ((Ridge_Broad + Ridge_Deep + Ridge_Surface) / 3)) - (((Chisel_Broad + Moldbrd_Broad + Ridge_Broad) / 3) - ((Chisel_Deep + Moldbrd_Deep + Ridge_Deep + Chisel_Surface + Moldbrd_Surface + Ridge_Surface) / 6))
    ) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("Moldbrd\nvs\nRidge", "Moldbrd.Ridge\nvs\nChisel", "Deep.Surface\nvs\nBroad", "Chisel.Moldbrd.v.Ridge\n(x)\nBroad.v.Deep.Surface"))) %>% 
  
  ggplot(aes(x = value, y = 0)) + 
  geom_rect(xmin = -5, xmax = 5,
            ymin = -Inf, ymax = Inf,
            color = "transparent", fill = bd[7]) +
  stat_beedrill() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference") +
  expand_limits(x = -5) +
  theme(strip.text = element_text(size = 6)) +
  facet_wrap(~ name, scales = "free", ncol = 4)
```

Though not identical, these were closer to those Kruschke displayed in the text.

#### Model comparison approach.

Like we covered in Chapter 10, I'm not aware that Stan/**brms** will allow for $\delta$ factor-inclusion parameters the way JAGS allows. However, if you'd like to compare models with different parameters, you can always use information criteria.

```{r, eval = F}
fit20.3 <- add_criterion(fit20.3, "loo")
fit20.4 <- add_criterion(fit20.4, "loo")
```

Executing that yielded the following warning message:

> Found 4 observations with a pareto_k > 0.7 in model 'fit20.3'. It is recommended to set 'moment_match = TRUE' in order to perform moment matching for problematic observations. 

To use the `moment_match` approach, the model in question must have been fit with the setting `save_pars = save_pars(all = TRUE)` within the `brm()` call. The default is `save_pars = NULL` and we used the default settings, above. If you go through the trouble of refitting the model with those updated settings, you'll find that the `moment_match` approach didn't help in this case. For the sake of reducing clutter, I'm not going to show the code for refitting the model. However, the next warning message I got after executing `fit20.3 <- add_criterion(fit20.3, "loo", moment_match = T)` was:

> Warning: Found 1 observations with a pareto_k > 0.7 in model 'fit20.3'. It is recommended to set 'reloo = TRUE' in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model 1 times to compute the ELPDs for the problematic observations directly.

So it's time to bring in the big guns. Let's use `reloo`. Be warned that, because this requires refitting the model multiple times, this will take a few minutes.

```{r, eval = F}
fit20.3 <- add_criterion(fit20.3, criterion = "loo", reloo = TRUE)
```

Okay, we're *finally* ready for the LOO comparison.

```{r, eval = F}
loo_compare(fit20.3, fit20.4) %>% 
  print(simplify = F)
```

```{r, echo = F}
# For some unknown reason, brms doesn't seem to let me save the results of 
# reloo in the external model fit file. Executing reloo takes too long to do 
# each time I render this file. So I'm saving the results of the loo comparison 
# as an external object.

# lc <- loo_compare(fit20.3, fit20.4)
# 
# save(lc, file = "fits/loo_comparefit20.03fit20.04.rda")
load("fits/loo_comparefit20.03fit20.04.rda")

lc %>% print(simplify = F)
```

All good. Based on the LOO values, we should prefer the fuller model. This, of course, should be no surprise. The posterior for $\sigma_\text{Field}$ was a far cry from zero.

```{r}
posterior_summary(fit20.3)["sd_Field__Intercept", ]
```

Kruschke ended this chapter by mentioning Bayes' factors:

> Bayes' factor approaches to hypothesis tests in ANOVA were presented by @rouderDefaultBayesFactors2012 and Wetzels, Grasman, and Wagenmakers [-@wetzelsDefaultBayesianHypothesis2012]. Morey and Rouder's BayesFactor package for R is available at the Web site [http://bayesfactorpcl.r-forge.r-project.org/](http://bayesfactorpcl.r-forge.r-project.org/). (p. 618)

If you wanna go that route, you're on your own.

## Session info {-}

```{r}
sessionInfo()
```

```{r, message = F, echo = F}
# remove the objects
rm(grand_mean, deflection_1, deflection_2, nonadditive_component, d, bd, p, p1, p2, p3, my_arrow, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, p14, layout, gamma_a_b_from_omega_sigma, my_data, mean_y, sd_y, omega, sigma, s_r, stanvars, n_draw, f, draws, stat_beedrill, nd, f1, levels, text, f2, p15, p16, p17, p18, p19, p20, fit20.3, col_names, fit20.4, lc)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

## Footnote {-}

[^6]: It's not quite clear where these data come from. At the top of page 591, Kruschke wrote "in this section, we will be looking at some real-world salaries." Further down on the same page, Kruschke added: "The data are annual salaries of 1,080 tenure-track professors at a large-enrollment, small-city, Midwestern-American, research-oriented, state university. (Salaries at big- city and private universities tend to be higher, while salaries at liberal-arts colleges and teaching-oriented state universities tend to be lower.) The data span 60 academic departments that had at least seven members. The data also include the professor’s seniority." He did not provide a reference.


<!--chapter:end:20.Rmd-->


```{r, echo = FALSE, cache = FALSE}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

# Dichotomous Predicted Variable

> This chapter considers data structures that consist of a dichotomous predicted variable. The early chapters of the book were focused on this type of data, but now we reframe the analyses in terms of the generalized linear model...
> 
> The traditional treatment of these sorts of data structure is called "logistic regression." In Bayesian software it is easy to generalize the traditional models so they are robust to outliers, allow different variances within levels of a nominal predictor, and have hierarchical structure to share information across levels or factors as appropriate. [@kruschkeDoingBayesianData2015, pp. 621--622]

## Multiple metric predictors

"We begin by considering a situation with multiple metric predictors, because this case makes it easiest to visualize the concepts of logistic regression" (p. 623).

Figure 21.1 is beyond the scope of our current **ggplot2** paradigm. But we will discuss an alternative in the end of [Section 21.1.2][Example: Height, weight, and gender.].

### The model and implementation in ~~JAGS~~ brms.

Our statistical model will follow the form

\begin{align*}
\mu & = \operatorname{logistic}(\beta_0 + \beta_1 x_1 + \beta_2 x_2) \\
y & \sim \operatorname{Bernoulli}(\mu)
\end{align*}

where

$$\operatorname{logistic}(x) = \frac{1}{[1 + \exp (-x)]}.$$

The generic **brms** code for logistic regression using the Bernoulli likelihood looks like so.

```{r, eval = F}
fit <-
  brm(data = my_data, 
      family = bernoulli,
      y ~ 1 + x1 + x2,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 2), class = b)))
```

Note that this syntax presumes the predictor variables have already been standardized.

We'd be remiss not to point out that you can also specify the model using the binomial distribution. That code would look like this.

```{r, eval = F}
fit <-
  brm(data = my_data, 
      family = binomial,
      y | trials(1) ~ 1 + x1 + x2,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 2), class = b)))
```

As long as the data are not aggregated, the results of these two should be the same within simulation variance. In **brms**, the default link for both `family = bernoulli` and `family = binomial` models is `logit`, which is exactly what we want, here. Also, note the additional `| trials(1) ` syntax on the left side of the model formula. You could get away with omitting this in older versions of **brms**. But newer versions prompt users to specify how many of trials each row in the data represents. This is because, as with the baseball data we'll use later in the chapter, the binomial distribution includes an $n$ parameter. When working with un-aggregated data like what we’re about to do, below, it's presumed that $n = 1$.

We won't be making our version of Figure 21.1 until a little later in the chapter. However, we can go ahead and make our version of the model diagram in Kruschke's Figure 21.2. Before we do, let's discuss the issues of plot colors and theme. For this chapter, we'll take our color palette from the [**PNWColors** package](https://CRAN.R-project.org/package=PNWColors) [@R-PNWColors], which provides color palettes inspired by the beauty of my birthplace,the US Pacific Northwest. Our color palette will be `"Mushroom"`.

```{r, warning = F, message = F, fig.width = 4, fig.height = 1}
library(PNWColors)

pm <- pnw_palette(name = "Mushroom", n = 8)

pm
```

Our overall plot theme will be a `"Mushroom"` infused extension of `theme_linedraw()`.

```{r, warning = F, message = F}
library(tidyverse)

theme_set(
  theme_linedraw() +
    theme(text = element_text(color = pm[1]),
          axis.text = element_text(color = pm[1]),
          axis.ticks = element_line(color = pm[1]),
          legend.background = element_blank(),
          legend.box.background = element_blank(),
          legend.key = element_rect(fill = pm[8]),
          panel.background = element_rect(fill = pm[8], color = pm[8]),
          panel.border = element_rect(colour = pm[1]),
          panel.grid = element_blank(),
          strip.background = element_rect(fill = pm[1], color = pm[1]),
          strip.text = element_text(color = pm[8]))
)
```

Now make Figure 21.2.

```{r, fig.width = 4, fig.height = 4.25, warning = F, message = F}
library(patchwork)

# normal density
p1 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pm[5]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pm[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = pm[1], hjust = 0, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5),
        plot.background = element_rect(fill = pm[8], color = "white", size = 1))

# second normal density
p2 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pm[5]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pm[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M[j])", "italic(S[j])"), 
           size = 7, color = pm[1], hjust = 0, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5),
        plot.background = element_rect(fill = pm[8], color = "white", size = 1))

## an annotated arrow
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")
p3 <-
  tibble(x    = .5,
         y    = 1,
         xend = .85,
         yend = 0) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pm[1]) +
  annotate(geom = "text",
           x = .55, y = .4,
           label = "'~'",
           size = 10, color = pm[1], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

## another annotated arrow
p4 <-
  tibble(x    = .5,
         y    = 1,
         xend = 1/3,
         yend = 0) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pm[1]) +
  annotate(geom = "text",
           x = c(.3, .48), y = .4,
           label = c("'~'", "italic(j)"),
           size = c(10, 7), color = pm[1], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p5 <-
  tibble(x = .5,
         y = .5,
         label = "logistic(beta[0]+sum()[italic(j)]~beta[italic(j)]~italic(x)[italic(ji)])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pm[1], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# a third annotated arrow
p6 <-
  tibble(x     = c(.375, .6),
         y     = c(1/2, 1/2),
         label = c("'='", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = pm[1], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# bar plot of Bernoulli data
p7 <-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = pm[5], width = .4) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "Bernoulli",
           size = 7, color = pm[1]) +
  annotate(geom = "text",
           x = .5, y = .94,
           label = "mu[italic(i)]", 
           size = 7, color = pm[1], family = "Times", parse = T) +
  xlim(-.75, 1.75) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5),
        plot.background = element_rect(fill = pm[8], color = pm[8]))

# the final annotated arrow
p8 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = pm[1], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               color = pm[1], arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p9 <-
  tibble(x     = 1,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pm[1], parse = T, family = "Times") +
  xlim(0, 2) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 1, r = 2),
  area(t = 1, b = 2, l = 3, r = 4),
  area(t = 3, b = 3, l = 1, r = 2),
  area(t = 3, b = 3, l = 3, r = 4),
  area(t = 4, b = 4, l = 1, r = 4),
  area(t = 5, b = 5, l = 2, r = 3),
  area(t = 6, b = 7, l = 2, r = 3),
  area(t = 8, b = 8, l = 2, r = 3),
  area(t = 9, b = 9, l = 2, r = 3)
)

# combine and plot!
(p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

### Example: Height, weight, and gender.

Load the height/weight data.

```{r, warning = F, message = F}
library(tidyverse)

my_data <- read_csv("data.R/HtWtData110.csv")

glimpse(my_data)
```

Let's standardize our predictors.

```{r}
my_data <-
  my_data %>% 
  mutate(height_z = (height - mean(height)) / sd(height),
         weight_z = (weight - mean(weight)) / sd(weight))
```

Before we fit a model, we might take a quick look at the data to explore the relations among the continuous variables `weight` and `height` and the dummy variable `male`. The `ggMarginal()` function from the [**ggExtra** package](https://github.com/daattali/ggExtra) [@R-ggExtra] will help us get a sense of the multivariate distribution by allowing us to add marginal densities to a scatter plot.

```{r, fig.width = 4.25, fig.height = 4}
library(ggExtra)

p <-
  my_data %>% 
  ggplot(aes(x = weight, y = height, fill = male == 1)) +
  geom_point(aes(color = male == 1), 
             alpha = 3/4) +
  scale_color_manual(values = pm[c(3, 6)]) +
  scale_fill_manual(values = pm[c(3, 6)]) +
  theme(legend.position = "none")

p %>% 
  ggMarginal(data = my_data,  
             colour = pm[1],
             groupFill = T,
             alpha = .8,
             type = "density")
```

Looks like the data for which `male == 1` are concentrated in the upper right and those for which `male == 0` are more so in the lower left. What we'd like is a model that would tell us the optimal dividing line(s) between our `male` categories with respect to those predictor variables.

Open **brms**.

```{r, warning = F, message = F}
library(brms)
```

Our first logistic model with `family = bernoulli` uses only `weight_z` as a predictor.

```{r fit21.1}
fit21.1 <-
  brm(data = my_data, 
      family = bernoulli,
      male ~ 1 + weight_z,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 2), class = b)),
      iter = 2500, warmup = 500, chains = 4, cores = 4,
      seed = 21,
      file = "fits/fit21.01")
```

Here's the model summary.

```{r}
print(fit21.1)
```

Now let's get ready to make our version of Figure 21.3. First, we extract the posterior draws.

```{r}
draws <- as_draws_df(fit21.1)
```

Now we wrangle a bit to make the top panel of Figure 21.3.

```{r, fig.width = 4.5, fig.height = 3.75}
length <- 200
n_draw <- 20

draws %>% 
  tibble() %>% 
  # take 20 random samples of the posterior draws
  slice_sample(n = n_draw) %>% 
  # add in a sequence of weight_z
  expand(nesting(.draw, b_Intercept, b_weight_z),
         weight_z = seq(from = -2, to = 3.5, length.out = length)) %>% 
  # compute the estimates of interest
  mutate(male   = inv_logit_scaled(b_Intercept + b_weight_z * weight_z),
         weight = weight_z * sd(my_data$weight) + mean(my_data$weight),
         thresh = -b_Intercept / b_weight_z * sd(my_data$weight) + mean(my_data$weight)) %>% 
  
  # plot!
  ggplot(aes(x = weight)) +
  geom_hline(yintercept = .5, color = pm[7], size = 1/2) +
  geom_vline(aes(xintercept = thresh, group = .draw),
             color = pm[6], size = 2/5, linetype = 2) +
  geom_line(aes(y = male, group = .draw),
            color = pm[1], size = 1/3, alpha = 2/3) +
  geom_point(data = my_data,
             aes(y = male),
             alpha = 1/3, color = pm[1]) +
  labs(title = "Data with Post. Pred.", 
       y = "male") +
  coord_cartesian(xlim = range(my_data$weight))
```

We should discuss those thresholds (i.e., the vertical lines) a bit. Kruschke:

> The spread of the logistic curves indicates the uncertainty of the estimate; the steepness of the logistic curves indicates the magnitude of the regression coefficient. The 50% probability threshold is marked by arrows that drop down from the logistic curve to the $x$-axis, near a weight of approximately 160 pounds. The threshold is the $x$ value at which $\mu = 0.5$, which is $x = -\beta_0 / \beta_1$. (p. 626)

It's important to realize that when you compute the thresholds with $-\beta_0 / \beta_1$, this returns the values *on the scale of the predictor*. In our case, the predictor was `weight_z`. But since we wanted to plot the data on the scale of the unstandardized variable, `weight`, we have to convert the thresholds to that metric by multiplying their values by $s_\text{weight}$ and then add the product to $\overline{\text{weight}}$. If you study it closely, you'll see that's what we did when computing the `thresh` values, above.

Now here we show the marginal distributions in our versions of the lower panels of Figure 21.3.

```{r, fig.width = 6, fig.height = 2.5, warning = F, message = F}
library(tidybayes)

draws <-
  draws %>% 
  # convert the parameter draws to their natural metric following Equation 21.1 (pp. 624--625)
  transmute(Intercept = b_Intercept - (b_weight_z * mean(my_data$weight) / sd(my_data$weight)),
            weight    = b_weight_z / sd(my_data$weight)) %>% 
  pivot_longer(everything())

# plot
draws %>% 
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95, 
               shape = 15, point_size = 2.5, point_color = pm[4],
               slab_color = pm[1], fill = pm[7], color = pm[1],
               slab_size = 1/2, size = 2, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

And here are those exact posterior mode and 95% HDI values.

```{r}
draws %>% 
  group_by(name) %>% 
  mode_hdi() %>% 
  mutate_if(is.double, round, digits = 3)
```

If you look back at our code for the lower marginal plots for Figure 21.3, you'll notice we did a whole lot of argument tweaking within `tidybayes::stat_halfeye()`. We have a lot of marginal densities ahead of us in this chapter, so we might streamline our code with those settings saved in a custom function. As the vibe I'm going for in those settings is based on some of the plots in [Chapter 16](https://clauswilke.com/dataviz/visualizing-uncertainty.html) of Wilke's [-@wilkeFundamentalsDataVisualization2019], *Fundamentals of data visualization*, we'll call our function `stat_wilke()`.

```{r}
stat_wilke <- function(.width = .95, 
                       shape = 15, point_size = 2.5, point_color = pm[4],
                       slab_color = pm[1], fill = pm[7], color = pm[1],
                       slab_size = 1/2, size = 2, ...) {
  
  stat_halfeye(point_interval = mode_hdi, .width = .width, 
               shape = shape, point_size = point_size, point_color = point_color,
               slab_color = slab_color, fill = fill, color = color, 
               slab_size = slab_size, size = size,
               normalize = "panels",
               ...)
  
}
```

Now fit the two-predictor model using both `weight_z` and `height_z`.

```{r fit21.2}
fit21.2 <-
  brm(data = my_data, 
      family = bernoulli,
      male ~ 1 + weight_z + height_z,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 2), class = b)),
      iter = 2500, warmup = 500, chains = 4, cores = 4,
      seed = 21,
      file = "fits/fit21.02")
```

Here's the model summary.

```{r}
print(fit21.2)
```

Before we make our plots for Figure 21.4, we'll need to extract the posterior samples and transform a little.

```{r, warning = F}
draws <-
  as_draws_df(fit21.2) %>% 
  mutate(b_weight  = b_weight_z / sd(my_data$weight),
         b_height  = b_height_z / sd(my_data$height),
         Intercept = b_Intercept - ((b_weight_z * mean(my_data$weight) / sd(my_data$weight)) +
                                         (b_height_z * mean(my_data$height) / sd(my_data$height)))) %>% 
  select(.draw, b_weight:Intercept)

head(draws)
```

Here's our version of Figure 21.4.a.

```{r, fig.width = 4.25, fig.height = 4}
set.seed(21)  # we need this for the `slice_sample()` function

draws %>% 
  slice_sample(n = 20) %>% 
  expand(nesting(.draw, Intercept, b_weight, b_height),
         weight = c(80, 280)) %>% 
  # this follows the Equation near the top of p. 629
  mutate(height = (-Intercept / b_height) + (-b_weight / b_height) * weight) %>% 
  
  # now plot
  ggplot(aes(x = weight, y = height)) +
  geom_line(aes(group = .draw),
            color = pm[7], size = 2/5, alpha = 2/3) +
  geom_text(data = my_data,
            aes(label = male, color = male == 1)) +
  scale_color_manual(values = pm[c(4, 1)]) +
  ggtitle("Data with Post. Pred.") +
  coord_cartesian(xlim = range(my_data$weight),
                  ylim = range(my_data$height)) +
  theme(legend.position = "none")
```

With just a tiny bit more wrangling, we'll be ready to make the bottom panels of Figure 21.4.

```{r, fig.width = 8, fig.height = 2.5, warning = F, message = F}
draws %>% 
  pivot_longer(-.draw) %>% 
  mutate(name = factor(str_remove(name, "b_"),
                       levels = c("Intercept", "weight", "height"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_wilke() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

Did you notice our use of `stat_wilke()`? By now, you know how to use `mode_hdi()` to return those exact summary values if you'd like them.

Now remember how we backed away from Figure 21.1? Well, when you have a logistic regression with two predictors, there is a reasonable way to express those three dimensions on a two-dimensional grid. Now we have the results from `fit21.2`, let's try it out.

First, we need a grid of values for our two predictors, `weight_z` and `height_z`.

```{r}
length <- 100

nd <- 
  crossing(weight_z = seq(from = -3.5, to = 3.5, length.out = length),
           height_z = seq(from = -3.5, to = 3.5, length.out = length))
```

Second, we plug those values into `fitted()` and wrangle.

```{r}
f <-
  fitted(fit21.2,
         newdata = nd,
         scale = "linear") %>% 
  as_tibble() %>% 
  # note we're only working with the posterior mean, here
  transmute(prob = Estimate %>% inv_logit_scaled()) %>% 
  bind_cols(nd) %>% 
  mutate(weight = (weight_z * sd(my_data$weight) + mean(my_data$weight)),
         height = (height_z * sd(my_data$height) + mean(my_data$height)))

glimpse(f)
```

Third, we're ready to plot. Here we'll express the third dimension, probability, on a color spectrum.

```{r, fig.width = 4.75, fig.height = 4}
f %>% 
  ggplot(aes(x = weight, y = height)) +
  geom_raster(aes(fill = prob),
              interpolate = T) +
  geom_text(data = my_data,
            aes(label = male, color = male == 1),
            show.legend = F) +
  scale_color_manual(values = pm[c(8, 1)]) +
  scale_fill_gradientn(colours = pnw_palette(name = "Mushroom", n = 101),
                       limits = c(0, 1)) +
  scale_y_continuous(position = "right") +
  coord_cartesian(xlim = range(my_data$weight),
                  ylim = range(my_data$height)) +
  theme(legend.position = "left")
```

If you look way back to Figure 21.1 (p. 623), you'll see the following formula at the top:

$$y \sim \operatorname{dbern}(m), m = \operatorname{logistic}(0.018 x_1 + 0.7 x_2 - 50).$$

Now while you keep your finger on that equation, take another look at the last line in Kruschke's Equation 21.1,

$$
\operatorname{logit}(\mu) = 
\underbrace{\zeta_0 - \sum_j \frac{\zeta_j}{s_{x_j}} \overline x_j}_{\beta_0} +
\sum_j \underbrace{\frac{\zeta_j}{s_{x_j}} \overline x_j}_{\beta_j},
$$

where the $\zeta$s are the parameters from the model based on standardized predictors. Our `fit21.2` was based on standardized `weight` and `height` values (i.e., `weight_z` and `height_z`), yielding model coefficients in the $\zeta$ metric. Here we use the formula above to convert our `fit21.2` estimates to their unstandardized $\beta$ metric. For simplicity, we'll just take their means.

```{r, warning = F}
as_draws_df(fit21.2) %>% 
  transmute(beta_0 = b_Intercept - ((b_weight_z * mean(my_data$weight) / sd(my_data$weight)) + 
                                      ((b_height_z * mean(my_data$height) / sd(my_data$height)))),
            beta_1 = b_weight_z / sd(my_data$weight),
            beta_2 = b_height_z / sd(my_data$height)) %>% 
  summarise_all(~ mean(.) %>% round(., digits = 3))
```

Within rounding error, those values are the same ones in the formula at the top of Kruschke's Figure 21.1! That is, our last plot was a version of Figure 21.1.

Hopefully this helps make sense of what the thresholds in Figure 21.4.a represented. But do note a major limitation of this visualization approach. By expressing the threshold with multiple lines drawn from the posterior in Figure 21.4.a, we expressed the uncertainty inherent in the posterior distribution. However, for this probability plane approach, we've taken a single value from the posterior, the mean (i.e., the `Estimate`), to compute the probabilities. Though beautiful, our probability-plane plot does a poor job expressing the uncertainty in the model. If you're curious how one might include uncertainty into a plot like this, check out the intriguing blog post by Adam Pearce, [*Communicating model uncertainty over space*](https://pair-code.github.io/interpretability/uncertainty-over-space/).

## Interpreting the regression coefficients

> In this section, I'll discuss how to interpret the parameters in logistic regression. The first subsection explains how to interpret the numerical magnitude of the slope coefficients in terms of "log odds." The next subsection shows how data with relatively few 1's or 0's can yield ambiguity in the parameter estimates. Then an example with strongly correlated predictors reveals tradeoffs in slope coefficients. Finally, I briefly describe the meaning of multiplicative interaction for logistic regression. (p. 629)

### Log odds.

> When the logistic regression formula is written using the logit function, we have $\operatorname{logit}(\mu) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$. The formula implies that whenever $x_1$ goes up by 1 unit (on the $x_1$ scale), then $\operatorname{logit}(\mu)$ goes up by an amount $\beta_1$. And whenever $x_2$ goes up by 1 unit (on the $x_2$ scale), then $\operatorname{logit}(\mu)$ goes up by an amount $\beta_2$. Thus, the regression coefficients are telling us about increases in $\operatorname{logit}(\mu)$. To understand the regression coefficients, we need to understand $\operatorname{logit}(\mu)$. (pp. 629--630)

Given the logit function is the inverse of the logistic, which itself is

$$\operatorname{logistic}(x) = \frac{1}{1 + \exp (−x)},$$

and given the formula

$$\operatorname{logit}(\mu) = \log \left (\frac{\mu}{1 - \mu} \right),$$

where

$$0 < \mu < 1,$$

it may or may not be clear that the results of our logistic regression models have a nonlinear relation with the actual parameter of interest, $\mu$, which, recall, is the probability our criterion variable is 1 (e.g., `male == 1`). To get a sense of that nonlinear relation, we might make a plot.

```{r, fig.width = 3.25, fig.height = 3}
tibble(mu = seq(from = 0, to = 1, length.out = 300)) %>% 
  mutate(logit_mu = log(mu / (1 - mu))) %>% 

  ggplot(aes(x = mu, y = logit_mu)) +
  geom_line(color = pm[3], size = 1.5) +
  labs(x = expression(mu~"(i.e., the probability space)"),
       y = expression(logit~mu~"(i.e., the parameter space)")) +
  theme(legend.position = "none")
```

So whereas our probability space is bound between 0 and 1, the parameter space shoots off into negative and positive infinity. Also,

$$\operatorname{logit}(\mu) = \log \left (\frac{p(y = 1)}{p(y = 0)} \right ).$$

Thus, "the ratio, $p(y = 1) / p(y = 0)$, is called the odds of outcome 1 to outcome 0, and therefore $\operatorname{logit}(\mu)$ is the log odds of outcome 1 to outcome 0" (p. 630).

Here's a table layout of the height/weight examples in the middle of page 630.

```{r}
tibble(b0     = -50,
       b1     = .02,
       b2     = .7,
       weight = 160,
       inches = c(63:64, 67:68)) %>% 
  mutate(logit_mu = b0 + b1 * weight + b2 * inches) %>%
  mutate(log_odds = logit_mu) %>% 
  mutate(p_male = 1 / (1 + exp(-log_odds))) %>% 
  knitr::kable()
```

> Thus, a regression coefficient in logistic regression indicates how much a 1 unit change of the predictor increases the log odds of outcome 1. A regression coefficient of 0.5 corresponds to a rate of probability change of about 12.5 percentage points per $x$-unit at the threshold $x$ value. A regression coefficient of 1.0 corresponds to a rate of probability change of about 24.4 percentage points per $x$-unit at the threshold $x$ value. When $x$ is much larger or smaller than the threshold $x$ value, the rate of change in probability is smaller, even though the rate of change in log odds is constant. (pp. 630--631)

### When there are few 1's or 0's in the data.

> In logistic regression, you can think of the parameters as describing the boundary between the 0's and the 1's. If there are many 0's and 1's, then the estimate of the boundary parameters can be fairly accurate. But if there are few 0’s or few 1's, the boundary can be difficult to identify very accurately, even if there are many data points overall. (p. 631)

As far as I can tell, Kruschke must have used $n = 500$ to simulate the data he displayed in Figure 21.5. Using the coefficient values he displayed in the middle of page 631, here's an attempt at replicating them.

```{r}
b0 <- -3
b1 <- 1

n <- 500

set.seed(21)
d_rare <-
  tibble(x = rnorm(n, mean = 0, sd = 1)) %>% 
  mutate(mu = b0 + b1 * x) %>% 
  mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu))))

glimpse(d_rare)
```

We're ready to fit the model. So far, we've been following along with Kruschke by using the Bernoulli distribution (i.e., `family = bernoulli`) in our **brms** models. Let's get frisky and use the $n = 1$ binomial distribution, here. You'll see it yields the same results.

```{r fit21.3}
fit21.3 <-
  brm(data = d_rare, 
      family = binomial,
      y | trials(1) ~ 1 + x,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 2), class = b)),
      iter = 2500, warmup = 500, chains = 4, cores = 4,
      seed = 21,
      file = "fits/fit21.03")
```

Recall that when you use the binomial distribution in newer versions of **brms**, you need to use the `trials()` syntax to tell `brm()` how many trials each row in the data corresponds to. Anyway, behold the summary.

```{r}
print(fit21.3)
```

Looks like the model did a good job recapturing those data-generating `b0` and `b1` values. Now make the top left panel of Figure 21.5.

```{r, fig.width = 4.5, fig.height = 3.75}
draws <- as_draws_df(fit21.3)

# unclear if Kruschke still used 20 draws or not
# perhaps play with the `n_draw` values
n_draw <- 20
length <- 100

set.seed(21)

draws %>% 
  # take 20 random samples of the posterior draws
  slice_sample(n = n_draw) %>% 
  # add in a sequence of x
  expand(nesting(.draw, b_Intercept, b_x),
         x = seq(from = -3.5, to = 3.5, length.out = length)) %>% 
  # compute the estimates of interest
  mutate(y      = inv_logit_scaled(b_Intercept + b_x * x),
         thresh = -b_Intercept / b_x) %>% 
  
  # plot!
  ggplot(aes(x = x)) +
  geom_hline(yintercept = .5, color = pm[7], size = 1/2) +
  geom_vline(aes(xintercept = thresh, group = .draw),
             color = pm[6], size = 2/5, linetype = 2) +
  geom_line(aes(y = y, group = .draw),
            color = pm[1], size = 1/3, alpha = 2/3) +
  geom_point(data = d_rare,
             aes(y = y),
             alpha = 1/3, color = pm[1]) +
  ggtitle("Data with Post. Pred.") +
  coord_cartesian(xlim = range(d_rare$x))
```

Here are the two subplots at the bottom, left.

```{r, fig.width = 6, fig.height = 2.5, warning = F, message = F}
draws %>% 
  mutate(Intercept = b_Intercept,
         x         = b_x) %>% 
  pivot_longer(Intercept:x) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_wilke() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

Since our data were simulated without the benefit of knowing how Kruschke set his seed and such, our results will only approximate those in the text.

Okay, now we need to simulate the complimentary data, those for which $y = 1$ is a less-rare event.

```{r}
b0 <- 0
b1 <- 1

n <- 500

set.seed(21)
d_not_rare <-
  tibble(x = rnorm(n, mean = 0, sd = 1)) %>% 
  mutate(mu = b0 + b1 * x) %>% 
  mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu))))

glimpse(d_not_rare)
```

Fitting this model is just like before.

```{r fit21.4}
fit21.4 <-
  update(fit21.3,
         newdata = d_not_rare,
         iter = 2500, warmup = 500, chains = 4, cores = 4,
         seed = 21,
         file = "fits/fit21.04")
```

Behold the summary.

```{r}
print(fit21.4)
```

Here's the code for the main plot in Figure 21.5.b.

```{r}
draws <- as_draws_df(fit21.4)

set.seed(21)

p1 <-
  draws %>% 
  slice_sample(n = n_draw) %>% 
  expand(nesting(.draw, b_Intercept, b_x),
         x = seq(from = -3.5, to = 3.5, length.out = length)) %>% 
  mutate(y      = inv_logit_scaled(b_Intercept + b_x * x),
         thresh = -b_Intercept / b_x) %>% 
  
  ggplot(aes(x = x, y = y)) +
  geom_hline(yintercept = .5, color = pm[7], size = 1/2) +
  geom_vline(aes(xintercept = thresh, group = .draw),
             color = pm[6], size = 2/5, linetype = 2) +
  geom_line(aes(group = .draw),
            color = pm[1], size = 1/3, alpha = 2/3) +
  geom_point(data = d_rare,
             alpha = 1/3, color = pm[1]) +
  ggtitle("Data with Post. Pred.") +
  coord_cartesian(xlim = c(-3, 3))
```

Now make the two subplots at the bottom.

```{r, warning = F}
p2 <-
  draws %>% 
  mutate(Intercept = b_Intercept,
         x         = b_x) %>% 
  pivot_longer(Intercept:x) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_wilke() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

This time we'll combine them with **patchwork**.

```{r, fig.width = 5.5, fig.height = 6}
p3 <- plot_spacer()

p1 / (p2 + p3 + plot_layout(widths = c(2, 1))) +
  plot_layout(height = c(4, 1))
```

> You can see in Figure 21.5 that the estimate of the slope (and of the intercept) is more certain in the right panel than in the left panel. The 95% HDI on the slope, $\beta_1$, is much wider in the left panel than in the right panel, and you can see that the logistic curves in the left panel have greater variation in steepness than the logistic curves in the right panel. The analogous statements hold true for the intercept parameter.
>
> Thus, if you are doing an experimental study and you can manipulate the $x$ values, you will want to select $x$ values that yield about equal numbers of 0's and 1's for the $y$ values overall. If you are doing an observational study, such that you cannot control any independent variables, then you should be aware that the parameter estimates may be surprisingly ambiguous if your data have only a small proportion of 0's or 1's. (pp. 631--632)

### Correlated predictors.

"Another important cause of parameter uncertainty is correlated predictors. This issue was previously discussed at length, but the context of logistic regression provides novel illustration in terms of level contours" (p. 632).

As far as I can tell, Kruschke chose about $n = 200$ for the data in this example. After messing around with correlations for a bit, it seems $\rho_{x_1, x_2} = .975$ looks about right. To my knowledge, the best way to simulate multivariate Gaussian data with a particular correlation is with the [`MASS::mvrnorm()` function](https://www.rdocumentation.org/packages/MASS/versions/7.3-51.1/topics/mvrnorm). Since we'll be using standardized $x$-variables, we'll need to specify our $n$, the desired correlation matrix, and a vector of means. Then we'll be ready to do the actual simulation with `mvrnorm()`.

```{r}
n <- 200

# correlation matrix
s <- matrix(c(1, .975, 
              .975, 1), 
            nrow = 2, ncol = 2)

# mean vector
m <- c(0, 0)

# simulate
set.seed(21)
d <- 
  MASS::mvrnorm(n = n, mu = m, Sigma = s) %>%
  data.frame() %>% 
  set_names(str_c("x", 1:2))
```

Let's confirm the correlation coefficient.

```{r}
cor(d)
```

Solid. Now we'll use the $\beta$ values from page 633 to simulate the data set by including our dichotomous criterion variable, `y`.

```{r}
b0 <- 0
b1 <- 1
b2 <- 1

set.seed(21)
d <-
  d %>% 
  mutate(mu = b0 + b1 * x1 + b2 * x2) %>% 
  mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu))))
```

Fit the model with the highly-correlated predictors.

```{r fit21.5}
fit21.5 <-
  brm(data = d, 
      family = binomial,
      y | trials(1) ~ 1 + x1 + x2,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 2), class = b)),
      iter = 2500, warmup = 500, chains = 4, cores = 4,
      seed = 21,
      file = "fits/fit21.05")
```

Behold the summary.

```{r}
print(fit21.5)
```

We did a good job recapturing Kruschke's $\beta$s in terms of our posterior means, but notice how large those posterior $SD$s are for $\beta_1$ and $\beta_2$. To get a better sense, let's look at them in a coefficient plot before continuing on with the text.

```{r, fig.width = 6, fig.height = 1, warning = F}
as_draws_df(fit21.5) %>% 
  pivot_longer(b_Intercept:b_x2) %>% 
  
  ggplot(aes(x = value, y = name)) +
  stat_gradientinterval(point_interval = mode_hdi, .width = c(.5, .95), 
                        color = pm[1], fill = pm[4]) +
  labs(x = NULL, y = NULL) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())
```

Them are some sloppy estimates! But we digress. Here's our version of Figure 21.6.a.

```{r, fig.width = 4.25, fig.height = 4}
set.seed(21)
as_draws_df(fit21.5) %>% 
  slice_sample(n = 20) %>% 
  expand(nesting(.draw, b_Intercept, b_x1, b_x2),
         x1 = c(-4, 4)) %>% 
  # this follows the equation near the top of p. 629
  mutate(x2 = (-b_Intercept / b_x2) + (-b_x1 / b_x2) * x1) %>% 

  # now plot
  ggplot(aes(x = x1, y = x2)) +
  geom_line(aes(group = .draw),
            color = pm[7], size = 2/5, alpha = 2/3) +
  geom_text(data = d,
            aes(label = y, color = y == 1),
            size = 2.5) +
  scale_color_manual(values = pm[c(4, 1)]) +
  ggtitle("Data with Post. Pred.") +
  coord_cartesian(xlim = c(-3, 3),
                  ylim = c(-3, 3)) +
  theme(legend.position = "none")
```

It can be easy to under-appreciate how sensitive this plot is to the seed you set for `sample_n()`. To give a better sense of the uncertainty in the posterior for the threshold, here we show the plot for several different seeds.

```{r, fig.width = 6, fig.height = 6}
# make a custom function
different_seed <- function(i) {
  
  set.seed(i)
  
  as_draws_df(fit21.5) %>% 
  slice_sample(n = 20) %>% 
  expand(nesting(.draw, b_Intercept, b_x1, b_x2),
         x1 = c(-4, 4)) %>% 
  mutate(x2 = (-b_Intercept / b_x2) + (-b_x1 / b_x2) * x1)
  
}

# specify your seeds
tibble(seed = 1:9) %>% 
  # pump those seeds into the `different_seed()` function
  mutate(sim = map(seed, different_seed)) %>% 
  unnest(sim) %>% 
  mutate(seed = str_c("seed: ", seed)) %>% 
  
  # plot
  ggplot(aes(x = x1, y = x2)) +
  geom_line(aes(group = .draw),
            color = pm[7], size = 2/5, alpha = 2/3) +
  geom_text(data = d,
            aes(label = y, color = y == 1),
            size = 1.5) +
  scale_color_manual(values = pm[c(4, 1)]) +
  ggtitle("Data with Post. Pred.") +
  coord_cartesian(xlim = c(-3, 3),
                  ylim = c(-3, 3)) +
  theme(legend.position = "none") +
  facet_wrap(~ seed)
```

To make our version of the pairs plots in Figure 21.6.b, we'll bring back some of our old tricks with **GGally**. First, we define our custom settings for the upper triangle, the diagonal, and lower triangle.

```{r, warning = F, message = F}
library(GGally)

my_upper <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) + 
    geom_point(size = 1/2, shape = 1, alpha = 1/4, color = pm[2])
    # geom_point(size = 1/4, alpha = 1/4, color = pm[2])
}

my_diag <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) + 
    stat_wilke() +
    scale_x_continuous(NULL, breaks = NULL) +
    scale_y_continuous(NULL, breaks = NULL) +
    coord_cartesian(ylim = c(-0.01, NA))
}

my_lower <- function(data, mapping, ...) {
  
  # get the x and y data to use the other code
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  
  # compute the correlations
  corr <- cor(x, y, method = "p", use = "pairwise")
  abs_corr <- abs(corr)
  
  # plot the cor value
  ggally_text(
    label = formatC(corr, digits = 2, format = "f") %>% str_replace(., "0.", "."),
    mapping = aes(),
    color = pm[1],
    size = 4) +
    scale_x_continuous(NULL, breaks = NULL) +
    scale_y_continuous(NULL, breaks = NULL) +
    theme(panel.background = element_rect(fill = pm[8]))
}
```

Now we wrangle and plot.

```{r, fig.width = 4, fig.height = 3.75, warning = F, message = F}
as_draws_df(fit21.5) %>% 
  transmute(`Intercept~(beta[0])` = b_Intercept,
            `x1~(beta[1])`        = b_x1,
            `x2~(beta[2])`        = b_x2) %>% 
  ggpairs(upper = list(continuous = my_upper),
          diag = list(continuous = my_diag),
          lower = list(continuous = my_lower),
          labeller = label_parsed)
```

### Interaction of metric predictors.

"There may be applications in which it is meaningful to consider a multiplicative interaction of metric predictors" (p. 633). Kruschke didn't walk through an analysis in this section, but it's worth the practice. Let's simulate data based on the formula he gave in Figure 21.7, top right.

```{r}
n  <- 500

b0 <- 0
b1 <- 0
b2 <- 0
b3 <- 4

set.seed(21)
d <-
  tibble(x1 = rnorm(n, mean = 0, sd = 1),
         x2 = rnorm(n, mean = 0, sd = 1)) %>% 
  mutate(mu = b1 * x1 + b2 * x2 + b3 * x1 * x2 - b0) %>% 
  mutate(y = rbinom(n, size = 1, prob = 1 / (1 + exp(-mu))))
```

To fit the interaction model, let's go back to the Bernoulli likelihood.

```{r fit21.6}
fit21.6 <-
  brm(data = d, 
      family = bernoulli,
      y ~ 1 + x1 + x2 + x1:x2,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 2), class = b)),
      iter = 2500, warmup = 500, chains = 4, cores = 4,
      seed = 21,
      file = "fits/fit21.06")
```

Looks like the model did a nice job recapturing the data-generating parameters.

```{r}
print(fit21.6)
```

I'm not quite sure how to expand Kruschke's equation from the top of page 629 to our interaction model. But no worries. We can take a slightly different approach to show the consequences of our interaction model on the probability $y = 1$. First, we define our `newdata` and then get the `Estimate`s from `fitted()`. Then we wrangle as usual.

```{r}
length <- 100

nd <- 
  crossing(x1 = seq(from = -3.5, to = 3.5, length.out = length),
           x2 = seq(from = -3.5, to = 3.5, length.out = length))

f <-
  fitted(fit21.6,
         newdata = nd,
         scale = "linear") %>% 
  as_tibble() %>% 
  transmute(prob = Estimate %>% inv_logit_scaled())
```

Now all we have to do is integrate our `f` results with the `nd` and original `d` data and then we can plot.

```{r, fig.width = 4.75, fig.height = 4}
f %>% 
  bind_cols(nd) %>% 
  
  ggplot(aes(x = x1, y = x2)) +
  geom_raster(aes(fill = prob),
              interpolate = T) +
  geom_text(data = d,
            aes(label = y, color = y == 1),
            size = 2.75, show.legend = F) +
  scale_color_manual(values = pm[c(8, 1)]) +
  scale_fill_gradientn("m", colours = pnw_palette(name = "Mushroom", n = 101),
                       limits = c(0, 1)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_cartesian(xlim = range(nd$x1),
                  ylim = range(nd$x2))
```

Instead of a simple threshold line, we get to visualize our interaction as a checkerboard-like probability plane. If you look back to Figure 21.7, you'll see this is our 2D version of Kruschke's wireframe plot on the top right.

## Robust logistic regression

With the robust logistic regression approach,

> we will describe the data as being a mixture of two different sources. One source is the logistic function of the predictor(s). The other source is sheer randomness or "guessing," whereby the $y$ value comes from the flip of a fair coin: $y \sim \operatorname{Bernoulli}(\mu = 1/2)$. We suppose that every data point has a small chance, $\alpha$, of being generated by the guessing process, but usually, with probability $1 - \alpha$, the $y$ value comes from the logistic function of the predictor. With the two sources combined, the predicted probability that $y = 1$ is
>
> $$\mu = \alpha \cdot \frac{1}{2} + (1 - \alpha) \cdot \operatorname{logistic} \bigg ( \beta_0  + \sum_j \beta_j x_j \bigg )$$
>
> Notice that when the guessing coefficient is zero, then the conventional logistic model is completely recovered. When the guessing coefficient is one, then the y values are completely random. (p. 635)

Here's what Kruschke's `beta(1, 9)` prior for $\alpha$ looks like.

```{r, fig.width = 3, fig.height = 2.75}
tibble(x = seq(from = 0, to = 1, length.out = 200)) %>% 
  
  ggplot(aes(x = x, y = dbeta(x, 1, 9))) +
  geom_area(fill = pm[4]) +
  scale_x_continuous(NULL, breaks = 0:5 / 5, expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  ggtitle(expression("beta"*(1*", "*9)))
```

To fit the **brms** analogue to Kruschke's robust logistic regression model, we'll need to adopt what  Bürkner calls the non-linear syntax, which you can learn about in detail with his [-@Bürkner2022Non_linear] vignette, [*Estimating non-linear models with brms*](https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html).

```{r fit21.7}
fit21.7 <-
  brm(data = my_data, 
      family = bernoulli(link = identity),
      bf(male ~ a * .5 + (1 - a) * 1 / (1 + exp(-1 * (b0 + b1 * weight_z))),
         a + b0 + b1  ~ 1,
         nl = TRUE),
      prior = c(prior(normal(0, 2), nlpar = "b0"),
                prior(normal(0, 2), nlpar = "b1"),
                prior(beta(1, 9),   nlpar = "a", lb = 0, ub = 1)),
      iter = 2500, warmup = 500, chains = 4, cores = 4,
      seed = 21,
      file = "fits/fit21.07")
```

And just for clarity, you can do the same thing with `family = binomial(link = identity)`, too. Just don't forget to specify the number of trials with `trials()`. But to explain what's going on with our syntax, above, I think it's best to quote Bürkner at length:

> When looking at the above code, the first thing that becomes obvious is that we changed the `formula` syntax to display the non-linear formula including predictors (i.e., [`weight_z`]) and parameters (i.e., [`a`, `b0`, and `b1`]) wrapped in a call to [the `bf()` function]. This stands in contrast to classical **R** formulas, where only predictors are given and parameters are implicit. The argument [`a + b0 + b1 ~ 1`] serves two purposes. First, it provides information, which variables in `formula` are parameters, and second, it specifies the linear predictor terms for each parameter. In fact, we should think of non-linear parameters as placeholders for linear predictor terms rather than as parameters themselves (see also the following examples). In the present case, we have no further variables to predict [`a`,  `b0`, and `b1`] and thus we just fit intercepts that represent our estimates of [$\alpha$, $\beta_0$, and  $\beta_1$]. The formula [`a + b0 + b1 ~ 1`] is a short form of [`a ~ 1, b0 ~ 1, b1 ~ 1`] that can be used if multiple non-linear parameters share the same formula. Setting `nl = TRUE` tells **brms** that the formula should be treated as non-linear.
>
> In contrast to generalized linear models, priors on population-level parameters (i.e., 'fixed effects') are often mandatory to identify a non-linear model. Thus, **brms** requires the user to explicitely specify these priors. In the present example, we used a [`beta(1, 9)` prior on (the population-level intercept of) `a`, while we used a `normal(0, 4)` prior on both (population-level intercepts of) `b0` and `b1`]. Setting priors is a non-trivial task in all kinds of models, especially in non-linear models, so you should always invest some time to think of appropriate priors. Quite often, you may be forced to change your priors after fitting a non-linear model for the first time, when you observe different MCMC chains converging to different posterior regions. This is a clear sign of an idenfication problem and one solution is to set stronger (i.e., more narrow) priors. (**emphasis** in the original)
 
Now, behold the summary.

```{r}
print(fit21.7)
```

Here's a quick and dirty look at the conditional effects for `weight_z`.

```{r, fig.width = 4, fig.height = 3}
conditional_effects(fit21.7) %>% 
  plot(points = T,
       line_args = list(color = pm[2], fill = pm[6]),
       point_args = list(color = pm[1], alpha = 3/4))
```

The way we prep for our version of Figure 21.8 is a minor extension what we did for Figure 21.3, above. Here we make the top panel. The biggest change, from before, is our adjusted formula for computing `male`.

```{r, fig.width = 4.5, fig.height = 3.75}
draws <- as_draws_df(fit21.7)

p1 <- 
  draws %>% 
  slice_sample(n = n_draw) %>% 
  expand(nesting(.draw, b_a_Intercept, b_b0_Intercept, b_b1_Intercept),
         weight_z = seq(from = -2, to = 3.5, length.out = length)) %>% 
  # compare this to Kruschke's mu[i] code at the top of page 636
  mutate(male   = 0.5 * b_a_Intercept + (1 - b_a_Intercept) * inv_logit_scaled(b_b0_Intercept + b_b1_Intercept * weight_z),
         weight = weight_z * sd(my_data$weight) + mean(my_data$weight),
         thresh = -b_b0_Intercept / b_b1_Intercept * sd(my_data$weight) + mean(my_data$weight)) %>% 
  
  ggplot(aes(x = weight, y = male)) +
  geom_hline(yintercept = .5, color = pm[7], size = 1/2) +
  geom_vline(aes(xintercept = thresh, group = .draw),
             color = pm[6], size = 2/5, linetype = 2) +
  geom_line(aes(group = .draw),
            color = pm[1], size = 1/3, alpha = 2/3) +
  geom_point(data = my_data,
             alpha = 1/3, color = pm[1]) +
  labs(title = "Data with Post. Pred.", 
       y = "male") +
  coord_cartesian(xlim = range(my_data$weight))
```


Here we make the marginal-distribution plots for our versions of the lower panels of Figure 21.8.

```{r, fig.width = 8, fig.height = 2.5, warning = F, message = F}
p2 <-
  draws %>% 
  mutate(Intercept = b_b0_Intercept - (b_b1_Intercept * mean(my_data$weight) / sd(my_data$weight)),
         weight    = b_b1_Intercept / sd(my_data$weight),
         guessing  = b_a_Intercept) %>% 
  pivot_longer(Intercept:guessing) %>% 
  mutate(name = factor(name, levels = c("Intercept", "weight", "guessing"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_wilke() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

Now combine them with **patchwork** and behold the glory.

```{r, fig.width = 5.5, fig.height = 6}
p1 / p2 +
  plot_layout(height = c(4, 1))
```

Here are the `ggpairs()` plots.

```{r, fig.width = 4, fig.height = 3.75, warning = F, message = F}
as_draws_df(fit21.7) %>% 
  transmute(`Intercept~(beta[0])` = b_b0_Intercept,
            `weight~(beta[1])`    = b_b1_Intercept,
            guessing              = b_a_Intercept) %>% 
  ggpairs(upper = list(continuous = my_upper),
          diag = list(continuous = my_diag),
          lower = list(continuous = my_lower),
          labeller = label_parsed)
```

Look at how that sweet Stan-based HMC reduced the correlation between $\beta_0$ and $\beta_1$.

For more on this approach to robust logistic regression in **brms** and an alternative suggested by Andrew Gelman, check out a thread from the Stan Forums, [*Translating robust logistic regression from rstan to brms*](https://discourse.mc-stan.org/t/translating-robust-logistic-regression-from-rstan-to-brms/5244). Yet do note that subsequent work suggests Gelman's alternative approach is not as robust as originally thought (see [here](https://discourse.mc-stan.org/t/robit-regression-not-robust/21245)).

## Nominal predictors

"We now turn our attention from metric predictors to nominal predictors" (p. 636).

### Single group.

If we have just a single group and no other predictors, that's just an intercept-only model. Back in the earlier chapters we thought of such a model as

\begin{align*}
y & \sim \operatorname{Bernoulli}(\theta) \\
\theta & \sim \operatorname{dbeta}(a, b).
\end{align*}

Now we're expressing the model as

\begin{align*}
y   & \sim \operatorname{Bernoulli}(\mu) \\
\mu & \sim \operatorname{logistic}(\beta_0).
\end{align*}

For that $\beta_0$, we typically use a Gaussian prior of the form

$$\beta_0 \sim \operatorname{Normal}(M_0, S_0).$$

To further explore what this means, we might make the model diagram in Figure 21.10.

```{r, fig.width = 2.5, fig.height = 4.25, warning = F, message = F}
p1 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pm[5]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pm[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = pm[1], hjust = 0, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5),
        plot.background = element_rect(fill = pm[8], color = pm[8]))

# an annotated arrow
p2 <-
  tibble(x    = .5,
         y    = 1,
         xend = .5,
         yend = 0) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pm[1]) +
  annotate(geom = "text",
           x = .4, y = .4,
           label = "'~'",
           size = 10, color = pm[1], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p3 <-
  tibble(x = .5,
         y = .5,
         label = "logistic(beta[0])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pm[1], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# a second annotated arrow
p4 <-
  tibble(x     = .375,
         y     = 1/2,
         label = "'='") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, color = pm[1], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow, color = pm[1]) +
  xlim(0, 1) +
  theme_void()

# bar plot of Bernoulli data
p5 <-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = pm[5], width = .4) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "Bernoulli",
           size = 7, color = pm[1]) +
  annotate(geom = "text",
           x = .5, y = .94,
           label = "mu", 
           size = 7, color = pm[1], family = "Times", parse = T) +
  xlim(-.75, 1.75) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5),
        plot.background = element_rect(fill = pm[8], color = pm[8]))

# another annotated arrow
p6 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = pm[1], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow, color = pm[1]) +
  xlim(0, 1) +
  theme_void()

# some text
p7 <-
  tibble(x     = 1,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pm[1], parse = T, family = "Times") +
  xlim(0, 2) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 2, r = 7),
  area(t = 3, b = 3, l = 2, r = 7),
  area(t = 4, b = 4, l = 1, r = 6),
  area(t = 5, b = 5, l = 1, r = 6),
  area(t = 6, b = 7, l = 1, r = 6),
  area(t = 8, b = 8, l = 1, r = 6),
  area(t = 9, b = 9, l = 1, r = 6)
)

# combine and plot!
(p1 + p2 + p3 + p4 + p5 + p6 + p7) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

In a situation where we don't have strong prior substantive knowledge, we often set $M_0 = 0$, which puts the probability mass around $\theta = .5$, a reasonable default hypothesis. Often times $S_0$ is some modest single-digit integer like 2 or 4. To get a sense of how different Gaussians translate to the beta distribution, we'll recreate Figure 21.11.

```{r, fig.width = 8, fig.height = 4}
# this will help streamline the conversion
logistic <- function(x) {
  1 / (1 + exp(-x))
}

# wrangle
crossing(m_0 = 0:1,
         s_0 = c(.5, 1, 2)) %>% 
  mutate(key = str_c("mu == logistic(beta %~%", " N(", m_0, ", ", s_0, "))"),
         sim = pmap(list(2e6, m_0, s_0), rnorm)) %>% 
  unnest(sim) %>% 
  mutate(sim = logistic(sim)) %>% 
  
  # plot
  ggplot(aes(x = sim, y = ..density..)) +
  geom_histogram(color = pm[8], fill = pm[7],
                 size = 1/3, bins = 20, boundary = 0) +
  geom_line(stat = "density", size = 1, color = pm[3]) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu)) +
  facet_wrap(~ key, scales = "free_y", labeller = label_parsed)
```

So the prior distribution doesn't even flatten out until you're somewhere between $S_0 = 1$ and $S_0 = 2$. Just for kicks, here we break that down a little further.

```{r, fig.width = 8, fig.height = 2}
# wrangle
tibble(m_0 = 0,
       s_0 = c(1.25, 1.5, 1.75)) %>% 
  mutate(key = str_c("mu == logistic(beta %~%", " N(", m_0, ", ", s_0, "))"),
         sim = pmap(list(1e7, m_0, s_0), rnorm)) %>% 
  unnest(sim) %>% 
  mutate(sim = logistic(sim)) %>% 
  
  # plot
  ggplot(aes(x = sim, y = ..density..)) +
  geom_histogram(color = pm[8], fill = pm[7],
                 size = 1/3, bins = 20, boundary = 0) +
  geom_line(stat = "density", size = 1, color = pm[3]) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu)) +
  facet_wrap(~ key, scales = "free_y", labeller = label_parsed, ncol = 3)
```

Here's the basic **brms** analogue to Kruschke's JAGS code from the bottom of page 639.

```{r, eval = F}
fit <-
  brm(data = my_data, 
      family = bernoulli(link = identity),
      y ~ 1,
      prior(beta(1, 1), class = Intercept))
```

Here's the **brms** analogue to Kruschke's JAGS code at the top of page 641.

```{r, eval = F}
fit <-
  brm(data = my_data, 
      family = bernoulli,
      y ~ 1,
      prior(normal(0, 2), class = "Intercept"))
```

### Multiple groups.

If there's only one group, we don't need a grouping variable. But that's a special case. Now we show the more general approach with multiple groups.

#### Example: Baseball again.

Load the baseball data.

```{r, warning = F, message = F}
my_data <- read_csv("data.R/BattingAverage.csv")

glimpse(my_data)
```

#### The model.

I'm not aware that Kruschke's modeling approach for this example will work well within the **brms** paradigm. Accordingly, our version of the hierarchical model diagram in Figure 21.12 will differ in important ways from the one in the text.

```{r, fig.width = 5, fig.height = 6.5}
# bracket
p1 <-
  tibble(x = .4,
         y = .25,
         label = "{_}") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, color = pm[3], family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# normal density
p2 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pm[5]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pm[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = pm[1], hjust = 0, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5),
        plot.background = element_rect(fill = pm[8], color = "white", size = 1))

# second normal density
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pm[5]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pm[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("0", "sigma[beta]"), 
           size = 7, color = pm[1], hjust = 0, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5),
        plot.background = element_rect(fill = pm[8], color = "white", size = 1))

#  plain arrow
p4 <-
  tibble(x    = .4,
         y    = 1,
         xend = .4,
         yend = .25) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pm[1]) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p5 <-
  tibble(x = .5,
         y = .3,
         label = "logistic(beta[0]+sum()[italic(j)]*beta['['*italic(j)*']']*italic(x)['['*italic(j)*']'](italic(i)))") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pm[1], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()
  
# two annotated arrows
p6 <-
  tibble(x    = c(.2, .8),
         y    = 1,
         xend = c(.37, .63),
         yend = .1) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pm[1]) +
  annotate(geom = "text",
           x = c(.22, .66, .77), y = .55,
           label = c("'~'", "'~'", "italic(j)"),
           size = c(10, 10, 7), color = pm[1], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# binomial density
p7 <-
tibble(x = 0:7) %>% 
  ggplot(aes(x = x, 
             y = (dbinom(x, size = 7, prob = .625)) / max(dbinom(x, size = 7, prob = .625)))) +
  geom_col(fill = pm[5], width = .4) +
  annotate(geom = "text",
           x = 3.5, y = .2,
           label = "binomial",
           size = 7) +
  annotate(geom = "text",
           x = 7, y = .85,
           label = "italic(N)[italic(i)*'|'*italic(j)]",
           size = 7, family = "Times", parse = TRUE) +
  coord_cartesian(xlim = c(-1, 8),
                  ylim = c(0, 1.2)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5),
        plot.background = element_rect(fill = pm[8], color = pm[8]))

# one annotated arrow
p8 <-
  tibble(x     = c(.375, .5),
         y     = c(.75, .3),
         label = c("'='", "mu[italic(i)*'|'*italic(j)]")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = pm[1], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = .425,
               arrow = my_arrow, color = pm[1]) +
  xlim(0, 1) +
  theme_void()

# another annotated arrow
p9 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)*'|'*italic(j)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = pm[1], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow, color = pm[1]) +
  xlim(0, 1) +
  theme_void()

# some text
p10 <-
  tibble(x     = 1,
         y     = .5,
         label = "italic(y[i])['|'][italic(j)]") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pm[1], parse = T, family = "Times") +
  xlim(0, 2) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 6, r = 8),
  area(t = 4, b = 5, l = 1, r = 3),
  area(t = 4, b = 5, l = 5, r = 7),
  area(t = 3, b = 4, l = 6, r = 8),
  area(t = 7, b = 8, l = 1, r = 7),
  area(t = 6, b = 7, l = 1, r = 7),
  area(t = 11, b = 12, l = 3, r = 5),
  area(t = 9, b = 11, l = 3, r = 5),
  area(t = 13, b = 14, l = 3, r = 5),
  area(t = 15, b = 15, l = 3, r = 5)
)

# combine and plot!
(p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

I recommend we fit a hierarchical aggregated-binomial regression model in place of the one Kruschke used. With this approach, we might express the statistical model as

\begin{align*}
\text{Hits}_i               & \sim \operatorname{Binomial}(\text{AtBats}_i, p_i) \\
\operatorname{logit}(p_i)   & = \beta_0 + \beta_{\text{PriPos}_i} + \beta_{\text{PriPos:Player}_i} \\
\beta_0                     & \sim \operatorname{Normal}(0, 2) \\
\beta_\text{PriPos}         & \sim \operatorname{Normal}(0, \sigma_\text{PriPos}) \\
\beta_\text{PriPos:Player}  & \sim \operatorname{Normal}(0, \sigma_\text{PriPos:Player}) \\
\sigma_\text{PriPos}        & \sim \operatorname{HalfCauchy}(0, 1) \\
\sigma_\text{PriPos:Player} & \sim \operatorname{HalfCauchy}(0, 1).
\end{align*}

Here's how to fit the model with **brms**. Notice we're finally making good use of the `trials()` syntax. This is because we're fitting an aggregated binomial model. Instead of our criterion variable `Hits` being a vector of 0's and 1's, it's the number of successful trials given the total number of trials, which is listed in the `AtBats` vector. *Aggregated* binomial.

```{r fit21.8}
fit21.8 <-
  brm(data = my_data,
      family = binomial(link = "logit"),
      Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player),
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(cauchy(0, 1), class = sd)),
      iter = 2500, warmup = 500, chains = 4, cores = 4,
      seed = 21,
      control = list(adapt_delta = .99),
      file = "fits/fit21.08")
```

#### Results.

Before we start plotting, review the model summary.

```{r}
print(fit21.8)
```

If you look closely at our model versus the one in the text, you'll see ours has fewer parameters. As a down-the-line consequence, our model doesn't support a direct analogue to the plot at the top of Figure 21.13. However, we can come close. Rather than modeling the position-based probabilities as multiple draws of beta distributions, we can simply summarize our probabilities by their posterior distributions.

```{r, fig.width = 8, fig.height = 2.75, warning = F, message = F}
library(ggridges)

# define our new data, `nd`
nd <- 
  my_data %>% 
  group_by(PriPos) %>% 
  summarise(AtBats = mean(AtBats) %>% round(0)) %>% 
  # to help join with the draws
  mutate(name = str_c("V", 1:n()))

# push the model through `fitted()` and wrangle
fitted(fit21.8,
       newdata = nd,
       re_formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos),
       scale = "linear",
       summary = F) %>% 
  as_tibble() %>% 
  pivot_longer(everything()) %>% 
  mutate(probability = inv_logit_scaled(value)) %>% 
  left_join(nd, by = "name") %>% 
  
  # plot
  ggplot(aes(x = probability, y = PriPos)) +
  geom_vline(xintercept = fixef(fit21.8)[1] %>% inv_logit_scaled(), color = pm[6]) +
  geom_density_ridges(color = pm[1], fill = pm[7], size = 1/2,
                      rel_min_height = 0.005, scale = .9) +
  geom_jitter(data = my_data,
              aes(x = Hits / AtBats),
              height = .025, alpha = 1/6, size = 1/6, color = pm[1]) +
  scale_x_continuous("Hits / AtBats", breaks = 0:5 / 5, expand = c(0, 0)) +
  coord_cartesian(xlim = c(0, 1), 
                  ylim = c(1, 9.5)) +
  ggtitle("Data with Posterior Predictive Distrib.") +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())
```

For kicks and giggles, we depicted the grand mean probability as the vertical line in the background with the `geom_vline()` line.

However, we can make our plot more directly analogous to Kruschke's if we're willing to stretch a little. Recall that Kruschke used the beta distribution with the $\omega-\kappa$ parameterization in both his statistical model and his plot code--both of which you can find detailed in his `Jags-Ybinom-Xnom1fac-Mlogistic.R`. file. We didn't use the beta distribution in our `brm()` model and the parameters from that model didn't have as direct correspondences to the beta distribution the way those from Kruschke's JAGS model did. However, recall that we can re-parameterize the beta distribution in terms of its mean $\mu$ and sample size $n$, following the form

\begin{align*}
\alpha & = \mu n \\
\beta  & = (1 - \mu) n .
\end{align*}

When we take the inverse logit of our intercepts, we do get vales in a probability metric. We might consider inserting those probabilities into the $\mu$ parameter. Furthermore, we can take our `AtBats` sample sizes and insert them directly into $n$. As before, we'll use the average sample size per position.

```{r, fig.width = 8, fig.height = 3.75}
# wrangle like a boss
nd %>% 
  add_epred_draws(fit21.8, 
                  ndraws = 20,
                  re_formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos),
                  dpar = "mu") %>% 
  # use the equations from above
  mutate(alpha = mu * AtBats, 
         beta  = (1 - mu) * AtBats) %>% 
  mutate(ll = qbeta(.025, shape1 = alpha, shape2 = beta),
         ul = qbeta(.975, shape1 = alpha, shape2 = beta)) %>% 
  mutate(theta = map2(ll, ul, seq, length.out = 100)) %>% 
  unnest(theta) %>% 
  mutate(density = dbeta(theta, alpha, beta)) %>% 
  group_by(.draw) %>% 
  mutate(density = density / max(density)) %>% 
  
  # plot!
  ggplot(aes(y = PriPos)) +
  geom_ridgeline(aes(x = theta, height = -density, group = interaction(PriPos, .draw)),
                 fill = NA, color = adjustcolor(pm[1], alpha.f = 1/3),
                 size = 1/3, alpha = 2/3, min_height = NA) +
  geom_jitter(data = my_data,
              aes(x = Hits / AtBats, size = AtBats),
              height = .05, alpha = 1/6, shape = 1, color = pm[1]) +
  scale_size_continuous(range = c(1/4, 4)) +
  scale_x_continuous("Hits / AtBats", expand = c(0, 0)) +
  labs(title = "Data with Posterior Predictive Distrib.",
       y = NULL) +
  coord_flip(xlim = c(0, 1),
             ylim = c(0.67, 8.67)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        axis.ticks.x = element_blank(),
        legend.position = c(.956, .8))
```

`r emo::ji("warning")` Since we didn't actually presume the beta distribution anywhere in our `brm()` statistical model, *I would not attempt to present this workflow in a scientific outlet*. Go with the previous plot. This attempt seems dishonest. But it is kinda fun to see how far we can push our results. `r emo::ji("warning")`

Happily, our contrasts will be less contentious. Here's the initial wrangling.

```{r, fig.width = 8, fig.height = 2.75, warning = F, message = F}
# define our subset of positions
positions <- c("1st Base", "Catcher", "Pitcher")

# redefine `nd`
nd <- 
  my_data %>% 
  filter(PriPos %in% positions) %>% 
  group_by(PriPos) %>% 
  summarise(AtBats = mean(AtBats) %>% round(0))

# push the model through `fitted()` and wrangle
f <-
  fitted(fit21.8,
         newdata = nd,
         re_formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos),
         scale = "linear",
         summary = F) %>% 
  as_tibble() %>% 
  set_names(positions)

# what did we do?
head(f)
```  

Here we make are our versions of the middle two panels of Figure 21.13.

```{r}
p1 <-
  f %>% 
  # compute the differences and put the data in the long format
  mutate(`Pitcher vs. Catcher`  = Pitcher - Catcher,
         `Catcher vs. 1st Base` = Catcher - `1st Base`) %>% 
  pivot_longer(contains("vs.")) %>% 
  mutate(name = factor(name, levels = c("Pitcher vs. Catcher", "Catcher vs. 1st Base"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  geom_vline(xintercept = 0, color = pm[6]) +
  stat_wilke() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference (in b)") +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

Now make our versions of the bottom two panels of Figure 21.13.

```{r}
p2 <-
  f %>% 
  # do the transformation before computing the differences
  mutate_all(inv_logit_scaled) %>% 
  mutate(`Pitcher vs. Catcher`  = Pitcher - Catcher,
         `Catcher vs. 1st Base` = Catcher - `1st Base`) %>% 
  pivot_longer(contains("vs.")) %>% 
  mutate(name = factor(name, levels = c("Pitcher vs. Catcher", "Catcher vs. 1st Base"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  geom_vline(xintercept = 0, color = pm[6]) +
  stat_wilke() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Difference (in probability)") +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

Combine and plot.

```{r, fig.width = 6, fig.height = 4.5}
p1 / p2
```

Note how our distributions are described as differences in probability, rather than in $\omega$.

## Session info {-}

```{r}
sessionInfo()
```

```{r, message = F, echo = F}
# Here we'll remove our objects
rm(pm, p1, p2, my_arrow, p3, p4, p5, p6, p7, p8, p9, layout, my_data, p, fit21.1, length, n_draw, nd, draws, stat_wilke, fit21.2, f, b0, b1, n, d_rare, fit21.3, d_not_rare, fit21.4, s, m, d, b2, fit21.5, different_seed, my_upper, my_diag, my_lower, b3, fit21.6, fit21.7, logistic, positions, p10, fit21.8)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:21.Rmd-->


```{r, echo = FALSE, cachse = FALSE}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

# Nominal Predicted Variable

> This chapter considers data structures that have a nominal predicted variable. When the nominal predicted variable has only two possible values, this reduces to the case of the dichotomous predicted variable considered in the previous chapter. In the present chapter, we generalize to cases in which the predicted variable has three or more categorical values...
>
> The traditional treatment of this sort of data structure is called multinomial logistic regression or conditional logistic regression. We will consider Bayesian approaches to these methods. As usual, in Bayesian software it is easy to generalize the traditional models so they are robust to outliers, allow different variances within levels of a nominal predictor, and have hierarchical structure to share information across levels or factors as appropriate. [@kruschkeDoingBayesianData2015, p. 649]

## Softmax regression

"The key descriptor of the [models in this chapter is their] inverse-link function, which is the softmax function (which will be defined below). Therefore, [Kruschke] refer[ed] to the method as softmax regression instead of multinomial logistic regression" (p. 650)

Say we have a metric predictor $x$ and a multinomial criterion $y$ with $k$ categories. We can express the basic linear model as

$$\lambda_k = \beta_{0, k} + \beta_{1, k} x,$$

for which the subscripts $k$ indicate there's a linear model for each of the $k$ categories. We call the possible set of $k$ outcomes $S$. Taking the case where $k = 3$, we'd have

\begin{align*}
\lambda_{[1]} & = \beta_{0, [1]} + \beta_{1, [1]} x, \\
\lambda_{[2]} & = \beta_{0, [2]} + \beta_{1, [2]} x, \text{and} \\
\lambda_{[3]} & = \beta_{0, [3]} + \beta_{1, [3]} x.
\end{align*}

In this scenario, what we want to know is the probability of $\lambda_{[1]}$, $\lambda_{[2]}$, and $\lambda_{[3]}$. The probability of a given outcome $k$ follows the formula

$$\phi_k = \operatorname{softmax}_S (\{\lambda_k\}) = \frac{\exp (\lambda_k)}{\sum_{c \in S} \exp  (\lambda_c)}.$$

> In words, [the equation] says that the probability of outcome $k$ is the exponentiated linear propensity of outcome $k$ relative to the sum of exponentiated linear propensities across all outcomes in the set $S$. You may be wondering, Why exponentiate? Intuitively, we have to go from propensities that can have negative values to probabilities that can only have non-negative values, and we have to preserve order. The exponential function satisfies that need. (p. 650)

You may be wondering what happened to $y$ and where all those $\lambda$s came from. Here we're using $\lambda$ to describe the propensity of outcome $k$, as indexed within our criterion $y$. So, the output of these models, $\phi_k$, is the relative probability we'll see each of our $k$ categories within our criterion $y$. What we want is $\phi_k$. The way we parameterize that with the softmax function is with $\lambda_k$.

There are are indeterminacies in the system of equations Kruschke covered in this section, the upshot of which is we'll end up making one of the $k$ categories the reference category, which we term $r$. Continuing on with our univariable model, we choose convenient constants for our parameters for $r$: $\beta_{0, r} = 0$ and $\beta_{1, r} = 0$. As such, *the regression coefficients for the remaining categories are relative to those for* $r$.

Kruschke saved the data for Figure 22.1 in the `SoftmaxRegData1.csv` and `SoftmaxRegData2.csv` files.

```{r, warning = F, message = F}
library(tidyverse)

d1 <- read_csv("data.R/SoftmaxRegData1.csv")
d2 <- read_csv("data.R/SoftmaxRegData2.csv")

glimpse(d1)
glimpse(d2)
```

Before we explore these data in a plot, let's talk color and theme. For this chapter, we'll carry forward our practice from [Chapter 21][Dichotomous Predicted Variable] and take our color palette from the [**PNWColors** package](https://CRAN.R-project.org/package=PNWColors). This time, our color palette will be `"Lake"`.

```{r, warning = F, message = F, fig.width = 4, fig.height = 1}
library(PNWColors)

pl <- pnw_palette(name = "Lake")

pl
```

We'll base our overall plot theme on `cowplot::theme_minimal_grid()`, with many color adjustments from `PNWColors::pnw_palette(name = "Lake")`.

```{r, warning = F, message = F}
library(cowplot)

theme_set(
  theme_minimal_grid() +
    theme(text = element_text(color = pl[1]),
          axis.text = element_text(color = pl[1]),
          axis.ticks = element_line(color = pl[1]),
          legend.background = element_blank(),
          legend.box.background = element_blank(),
          legend.key = element_rect(fill = pl[8]),
          panel.background = element_rect(fill = pl[8], color = pl[8]),
          panel.grid = element_blank(),
          strip.background = element_rect(fill = pl[7], color = pl[7]),
          strip.text = element_text(color = pl[1]))
)
```

Now bind the two data frames together and plot our version of Figure 22.1.

```{r, fig.width = 5.5}
bind_rows(d1, d2) %>%
  mutate(data = rep(str_c("d", 1:2), each = n() / 2),
         Y = factor(Y)) %>% 
  
  ggplot(aes(x = X1, y = X2, label = Y, color = Y)) +
  geom_hline(yintercept = 0, color = "white") +
  geom_vline(xintercept = 0, color = "white") +
  geom_text(size = 3) +
  scale_color_manual(values = pl[2:5]) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  coord_equal() +
  theme(legend.position = "none") +
  facet_wrap(~ data, ncol = 2)
```

### Softmax reduces to logistic for two outcomes.

"When there are only two outcomes, the softmax formulation reduces to the logistic regression of Chapter 21" (p. 653).

### Independence from irrelevant attributes.

> An important property of the softmax function of Equation 22.2 is known as independence from irrelevant attributes [@luceIndividualChoiceBehavior2012; @luceLuceChoiceAxiom2008]. The model implies that the ratio of probabilities of two outcomes is the same regardless of what other possible outcomes are included in the set. Let $S$ denote the set of possible outcomes. Then, from the definition of the softmax function, the ratio of outcomes $j$ and $k$ is
>
> $$\frac{\phi_j}{\phi_k} = \frac{\exp (\lambda_j) / \sum_{c \in S} \exp (\lambda_c)}{\exp (\lambda_k) / \sum_{c \in S} \exp (\lambda_c)}$$
>
> The summation in the denominators cancels and has no effect on the ratio of probabilities. Obviously if we changed the set of outcomes $S$ to any other set $S^*$ that still contains outcomes $j$ and $k$, the summation $\sum_{c \in S^*}$ would still cancel and have no effect on the ratio of probabilities. (p. 654)

Just to walk out that denominators-canceling business a little further,

\begin{align*}
\frac{\phi_j}{\phi_k} & = \frac{\exp (\lambda_j) / \sum_{c \in S} \exp (\lambda_c)}{\exp (\lambda_k) / \sum_{c \in S} \exp (\lambda_c)} \\
& = \frac{\exp (\lambda_j)}{\exp (\lambda_k)}.
\end{align*}

Thus even in the case of a very different set of possible outcomes $S^\text{very different}$, it remains that $\frac{\phi_j}{\phi_k} = \frac{\exp (\lambda_j)}{\exp (\lambda_k)}$.

Getting more applied, here's a tibble presentation of Kruschke's commute example with three modes of transportation.

```{r}
tibble(mode       = c("walking", "bicycling", "bussing"),
       preference = 3:1) %>% 
  mutate(`chance %` = (100 * preference / sum(preference)) %>% round(digits = 1))
```

Sticking with the example, if we take bicycling out of the picture, the `preference` values remain, but the `chance %` values change.

```{r}
tibble(mode       = c("walking", "bussing"),
       preference = c(3, 1)) %>% 
  mutate(`chance %` = 100 * preference / sum(preference))
```

Though we retain the same walking/bussing ratio, we end up with a different model of relative probabilities.

## Conditional logistic regression

> Softmax regression conceives of each outcome as an independent change in log odds from the reference outcome, and a special case of that is dichotomous logistic regression. But we can generalize logistic regression another way, which may better capture some patterns of data. The idea of this generalization is that we divide the set of outcomes into a hierarchy of two-set divisions, and use a logistic to describe the probability of each branch of the two-set divisions. (p. 655)

The model follows the generic equation

\begin{align*}
\phi_{S^* | S} = \operatorname{logistic}(\lambda_{S^* | S}) \\
\lambda_{S^* | S} = \beta_{0, S^* | S} + \beta_{1, {S^* | S}} x,
\end{align*}

where the conditional response probability (i.e., the goal of the analysis) is $\phi_{S^* | S}$. $S^*$ and $S$ denote the subset of outcomes and larger set of outcomes, respectively, and $\lambda_{S^* | S}$ is the propensity based on some linear model. The overall point is these "regression coefficients refer to the conditional probability of outcomes for the designated subsets, not necessarily to a single outcome among the full set of outcomes" (p. 655).

In Figure 22.2 (p. 656), Kruschke depicted the two hierarchies of binary divisions of the models he fit to the data in his `CondLogistRegData1.csv` and `CondLogistRegData2.csv` files. Here we load those data, save them as `d3` and `d4`, and take a look at their structures.

```{r, warning = F, message = F}
d3 <- read_csv("data.R/CondLogistRegData1.csv")
d4 <- read_csv("data.R/CondLogistRegData2.csv")

glimpse(d3)
glimpse(d4)
```

In both data sets, the nominal criterion is `Y` and the two predictors are `X1` and `X2`. Though the data seem simple, the conditional logistic models are complex enough that it seems like we'll be better served by focusing on them one at a time, which means I'm going to break up Figure 22.2. Here's how to make the diagram in the left panel.

```{r, fig.height = 3, fig.width = 3.5}
# the big numbers
numbers <- tibble(
  x     = c(3, 5, 2, 4, 1, 3, 2),
  y     = c(0, 0, 1, 1, 2, 2, 3),
  label = c("3", "4", "2", "3,4", "1", "2,3,4", "1,2,3,4")
)

# the smaller Greek numbers
greek <- tibble(
  x     = c(3.4, 4.6, 2.4, 3.6, 1.4, 2.6),
  y     = c(0.5, 0.5, 1.5, 1.5, 2.5, 2.5),
  hjust = c(1, 0, 1, 0, 1, 0),
  label = c("phi['{3}|{3,4}']", "1-phi['{3}|{3,4}']",
            "phi['{2}|{2,3,4}']", "1-phi['{2}|{2,3,4}']",
            "phi['{1}|{1,2,3,4}']", "1-phi['{1}|{1,2,3,4}']")
)

# arrows
tibble(
  x    = c(4, 4, 3, 3, 2, 2),
  y    = c(0.85, 0.85, 1.85, 1.85, 2.85, 2.85),
  xend = c(3, 5, 2, 4, 1, 3),
  yend = c(0.15, 0.15, 1.15, 1.15, 2.15, 2.15)
) %>%  
  
  # plot!
  ggplot(aes(x = x, y = y)) +
  geom_segment(aes(xend = xend, yend = yend),
               size = 1/4,
               arrow = arrow(length = unit(0.08, "in"), type = "closed")) +
  geom_text(data = numbers,
            aes(label = label),
            size = 5, family = "Times")+
  geom_text(data = greek,
            aes(label = label, hjust = hjust),
            size = 4.25, family = "Times", parse = T) +
  xlim(-1, 7) +
  theme_void()
```

The large numbers are the four levels in the criterion `Y` and the smaller numbers in the curly braces are various sets of those numbers. The diagram shows three levels of outcome-set divisions:

* 1 versus 2, 3, or 4;
* 2 versus 3 or 4; and
* 3 versus 4.

The divisions in each of these levels can be expressed as linear models which we'll denote $\lambda$. Given our data with two predictors `X1` and `X2`, we can express the three linear models as

$$
\begin{align*}
\lambda_{\{ 1 \} | \{ 1,2,3,4 \}} & = \beta_{0,\{ 1 \} | \{ 1,2,3,4 \}} + \beta_{1,\{ 1 \} | \{ 1,2,3,4 \}} \text{X1} + \beta_{2,\{ 1 \} | \{ 1,2,3,4 \}} \text{X2} \\
\lambda_{\{ 2 \} | \{ 2,3,4 \}}   & = \beta_{0,\{ 2 \} | \{ 2,3,4 \}} + \beta_{1,\{ 2 \} | \{ 2,3,4 \}} \text{X1} + \beta_{2,\{ 2 \} | \{ 2,3,4 \}} \text{X2} \\
\lambda_{\{ 3 \} | \{ 3,4 \}}     & = \beta_{0,\{ 3 \} | \{ 3,4 \}} + \beta_{1,\{ 3 \} | \{ 3,4 \}} \text{X1} + \beta_{2,\{ 3 \} | \{ 3,4 \}} \text{X2},
\end{align*}
$$

where, for convenience, we're omitting the typical $i$ subscripts. As these linear models are all defined within the context of the logit link, we can express the conditional probabilities of the outcome sets as

$$
\begin{align*}
\phi_{\{ 1 \} | \{ 1,2,3,4 \}} & = \operatorname{logistic} \left (\lambda_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_{\{ 2 \} | \{ 2,3,4 \}}   & = \operatorname{logistic} \left (\lambda_{\{ 2 \} | \{ 2,3,4 \}} \right) \\
\phi_{\{ 3 \} | \{ 3,4 \}}     & = \operatorname{logistic} \left (\lambda_{\{ 3 \} | \{ 3,4 \}} \right),
\end{align*}
$$

where $\phi_{\{ 1 \} | \{ 1,2,3,4 \}}$ through $\phi_{\{ 3 \} | \{ 3,4 \}}$ are the conditional probabilities for the outcome sets. If, however, we want the conditional probabilities for the actual levels of the criterion `Y`, we define those with a series of (in this case) four equations:

$$
\begin{align*}
\phi_1 & = \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \\
\phi_2 & = \phi_{\{ 2 \} | \{ 2,3,4 \}} \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_3 & = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 & = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right),
\end{align*}
$$

where the sum of the probabilities $\phi_1$ through $\phi_4$ is $1$. To get a sense of what this all means in practice, let's visualize the data and the data-generating equations for our version of Figure 22.3. As with the previous figure, I'm going to break this figure up to focus on one model at a time. Thus, here's the left panel of Figure 22.3.

```{r, fig.width = 2.75}
## define the various population parameters
# lambda 1
b01 <- -4
b11 <- -5
b21 <- 0.01  # rounding up to avoid dividing by zero
# lambda 2
b02 <- -2
b12 <- 1
b22 <- -5
# lambda 3
b03 <- -1
b13 <- 3
b23 <- 3

# use the parameters to define the lines 
lines <- tibble(
  intercept = c(-b01 / b21, -b02 / b22, -b03 / b23),
  slope = c(-b11 / b21, -b12 / b22, -b13 / b23),
  label = c("1", "2","3")
)

# wrangle
d3 %>% 
  mutate(Y = factor(Y)) %>% 
  
  # plot!
  ggplot() +
  geom_hline(yintercept = 0, color = "white") +
  geom_vline(xintercept = 0, color = "white") +
  geom_text(aes(x = X1, y = X2, label = Y, color = Y),
            size = 3, show.legend = F) +
  geom_abline(data = lines,
              aes(intercept = intercept,
                  slope = slope,
                  linetype = label),
              color = pl[1]) +
  scale_color_manual(values = pl[2:5]) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   "lambda['{1}|{1,2,3,4}']==-4+-5*x[1]+0*x[2]", 
                   "lambda['{2}|{2,3,4}']==-2+1*x[1]+-5*x[2]", 
                   "lambda['{3}|{3,4}']==-1+3*x[1]+3*x[2]")),
                 guide = guide_legend(
                   direction = "vertical",
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = "top")
```

Recall back on page 629, Kruschke showed the equation for the 50% threshold of a logistic regression model given two continuous predictors was

$$x_2 = (-\beta_0 / \beta_2) + (-\beta_1 / \beta_2) x_1.$$

It was that equation that gave us the values for the `intercept` and `slope` arguments ($-\beta_0 / \beta_2$ and $-\beta_1 / \beta_2$, respectively) for the `geom_abline()` function.

It still might not be clear how the various $\phi_{S^* | S}$ values connect to the data. Though not in the text, here's an alternative way of expressing the relations in Figure 22.3. This time the plot is faceted by the three levels of $\phi_{S^* | S}$ and the background fill is based on those conditional probabilities.

```{r, width = 5.5}
# define a grid of X1 and X2 values
crossing(X1 = seq(from = -2, to = 2, length.out = 50),
         X2 = seq(from = -2, to = 2, length.out = 50)) %>% 
  # compute the lambda's
  mutate(`lambda['{1}|{1,2,3,4}']` = b01 + b11 * X1 + b21 * X2,
         `lambda['{2}|{2,3,4}']`   = b02 + b12 * X1 + b22 * X2,
         `lambda['{3}|{3,4}']`     = b03 + b13 * X1 + b23 * X2) %>% 
  # compute the phi's
  mutate(`phi['{1}|{1,2,3,4}']` = plogis(`lambda['{1}|{1,2,3,4}']`),
         `phi['{2}|{2,3,4}']`   = plogis(`lambda['{2}|{2,3,4}']`),
         `phi['{3}|{3,4}']`     = plogis(`lambda['{3}|{3,4}']`)) %>% 
  # wrangle
  pivot_longer(contains("phi"), values_to = "phi") %>% 
  
  # plot!
  ggplot(aes(x = X1, y = X2)) +
  geom_raster(aes(fill = phi),
              interpolate = T) +
  # note how we're subsetting the d3 data by facet
  geom_text(data = bind_rows(
    d3 %>% mutate(name = "phi['{1}|{1,2,3,4}']"),
    d3 %>% mutate(name = "phi['{2}|{2,3,4}']") %>% filter(Y > 1),
    d3 %>% mutate(name = "phi['{3}|{3,4}']") %>% filter(Y > 2)),
            aes(label = Y),
            size = 2.5) +
  scale_fill_gradientn(expression(phi[italic(S)*"*|"*italic(S)]),
                       colours = pnw_palette(name = "Lake", n = 101),
                       breaks = 0:2 / 2, limits = c(0, 1)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_equal() +
  theme(legend.position = c(0.8, 0.2)) +
  facet_wrap(~ name, labeller = label_parsed, ncol = 2)
```

Notice how because each of the levels of $\phi$ is defined by a different subset of the data, each of the facets contains a different subset of the `d3` data, too. For example, since $\phi_{\{ 1 \} | \{ 1,2,3,4 \}}$ is defined by the full subset of the possible values of `Y`, you see all the `Y` data displayed by `geom_text()` for that facet. In contrast, since $\phi_{\{ 3 \} | \{ 3,4 \}}$ is defined by a subset of the data for which `Y` is only `3` or `4`, those are the only values you see displayed within that facet of the plot.

Now we'll consider an alternative way to set up the binary-choices hierarchy, as seen in the right panel of Figure 22.2. First, here's that half of the figure.

```{r, fig.width = 5, fig.height = 2.25}
# the big numbers
numbers <- tibble(
  x = c(0, 2, 6, 8, 1, 7, 4),
  y = c(0, 0, 0, 0, 1, 1, 2),
  label = c("1", "2", "3", "4", "1,2", "3,4", "1,2,3,4")
)

# the smaller Greek numbers
greek <- tibble(
  x = c(0.4, 1.6, 6.4, 7.6, 2.1, 5.8),
  y = c(0.5, 0.5, 0.5, 0.5, 1.5, 1.5),
  hjust = c(1, 0, 1, 0, 1, 0),
  label = c("phi['{1}|{1,2}']", "1-phi['{1}|{1,2}']",
            "phi['{3}|{3,4}']", "1-phi['{3}|{3,4}']",
            "phi['{1,2}|{1,2,3,4}']", "1-phi['{1,2}|{1,2,3,4}']")
)

# arrows
tibble(
  x = c(1, 1, 7, 7, 4, 4),
  y = c(0.85, 0.85, 0.85, 0.85, 1.85, 1.85),
  xend = c(0, 2, 6, 8, 1, 7),
  yend = c(0.15, 0.15, 0.15, 0.15, 1.15, 1.15)
) %>%  
  
  # plot!
  ggplot(aes(x = x, y = y)) +
  geom_segment(aes(xend = xend, yend = yend),
               size = 1/4,
               arrow = arrow(length = unit(0.08, "in"), type = "closed")) +
  geom_text(data = numbers,
            aes(label = label),
            size = 5, family = "Times")+
  geom_text(data = greek,
            aes(label = label, hjust = hjust),
            size = 4.25, family = "Times", parse = T) +
  xlim(-1, 10) +
  theme_void()
```

This diagram shows three levels of outcome-set divisions:

* 1 or 2 versus 3 or 4;
* 1 versus 2; and
* 3 versus 4.

Given our data with two predictors `X1` and `X2`, we can express the three linear models as

$$
\begin{align*}
\lambda_{\{ 1,2 \} | \{ 1,2,3,4 \}} & = \beta_{0,\{ 1,2 \} | \{ 1,2,3,4 \}} + \beta_{1,\{ 1,2 \} | \{ 1,2,3,4 \}} \text{X1} + \beta_{2,\{ 1,2 \} | \{ 1,2,3,4 \}} \text{X2} \\
\lambda_{\{ 1 \} | \{ 1,2 \}}   & = \beta_{0,\{ 1 \} | \{ 1,2 \}} + \beta_{1,\{ 1 \} | \{ 1,2 \}} \text{X1} + \beta_{2,\{ 1 \} | \{ 1,2 \}} \text{X2} \\
\lambda_{\{ 3 \} | \{ 3,4 \}}     & = \beta_{0,\{ 3 \} | \{ 3,4 \}} + \beta_{1,\{ 3 \} | \{ 3,4 \}} \text{X1} + \beta_{2,\{ 3 \} | \{ 3,4 \}} \text{X2}.
\end{align*}
$$

We can then express the conditional probabilities of the outcome sets as

$$
\begin{align*}
\phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} & = \operatorname{logistic} \left (\lambda_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_{\{ 1 \} | \{ 1,2 \}}   & = \operatorname{logistic} \left (\lambda_{\{ 1 \} | \{ 1,2 \}} \right) \\
\phi_{\{ 3 \} | \{ 3,4 \}}     & = \operatorname{logistic} \left (\lambda_{\{ 3 \} | \{ 3,4 \}} \right).
\end{align*}
$$

For the conditional probabilities of the actual levels of the criterion `Y`, we define those with a series of (in this case) four equations:

$$
\begin{align*}
\phi_1 & = \phi_{\{ 1 \} | \{ 1,2 \}} \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_2 & = \left ( 1 - \phi_{\{ 1 \} | \{ 1,2 \}} \right) \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_3 & = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 & = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right),
\end{align*}
$$

where the sum of the probabilities $\phi_1$ through $\phi_4$ is $1$. To get a sense of what this all means, let's visualize the data and the data-generating equations in our version of the right panel of Figure 22.3.

```{r, fig.width = 2.75}
d4 %>% 
  mutate(Y = factor(Y)) %>% 
  
  ggplot() +
  geom_hline(yintercept = 0, color = "white") +
  geom_vline(xintercept = 0, color = "white") +
  geom_text(aes(x = X1, y = X2, label = Y, color = Y),
            size = 3, show.legend = F) +
  geom_abline(data = lines,
              aes(intercept = intercept,
                  slope = slope,
                  linetype = label),
              color = pl[1]) +
  scale_color_manual(values = pl[2:5]) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   "lambda['{1,2}|{1,2,3,4}']==-4+-5*x[1]+0*x[2]", 
                   "lambda['{1}|{1,2}']==-2+1*x[1]+-5*x[2]", 
                   "lambda['{3}|{3,4}']==-1+3*x[1]+3*x[2]")),
                 guide = guide_legend(
                   direction = "vertical",
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = "top")
```

Here's an alternative way of expression the relations in the right panel of Figure 22.3. This time the plot is faceted by the three levels of $\phi_{S^* | S}$ and the background fill is based on those conditional probabilities.

```{r, fig.width = 5.5}
# define a grid of X1 and X2 values
crossing(X1 = seq(from = -2, to = 2, length.out = 50),
         X2 = seq(from = -2, to = 2, length.out = 50)) %>% 
  # compute the lambda's
  mutate(`lambda['{1,2}|{1,2,3,4}']` = b01 + b11 * X1 + b21 * X2,
         `lambda['{1}|{1,2}']`       = b02 + b12 * X1 + b22 * X2,
         `lambda['{3}|{3,4}']`       = b03 + b13 * X1 + b23 * X2) %>% 
  # compute the phi's
  mutate(`phi['{1,2}|{1,2,3,4}']` = plogis(`lambda['{1,2}|{1,2,3,4}']`),
         `phi['{1}|{1,2}']`       = plogis(`lambda['{1}|{1,2}']`),
         `phi['{3}|{3,4}']`       = plogis(`lambda['{3}|{3,4}']`)) %>% 
  # wrangle
  pivot_longer(contains("phi"), values_to = "phi") %>% 
  
  # plot!
  ggplot(aes(x = X1, y = X2)) +
  geom_raster(aes(fill = phi),
              interpolate = T) +
  # note how we're subsetting the d3 data by facet
  geom_text(data = bind_rows(
    d4 %>% mutate(name = "phi['{1,2}|{1,2,3,4}']"),
    d4 %>% mutate(name = "phi['{1}|{1,2}']") %>% filter(Y < 3),
    d4 %>% mutate(name = "phi['{3}|{3,4}']") %>% filter(Y > 2)),
            aes(label = Y),
            size = 2.5) +
  scale_fill_gradientn(expression(phi[italic(S)*"*|"*italic(S)]),
                       colours = pnw_palette(name = "Lake", n = 101),
                       breaks = 0:2 / 2, limits = c(0, 1)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_equal() +
  theme(legend.position = c(0.8, 0.2)) +
  facet_wrap(~ name, labeller = label_parsed, ncol = 2)
```

It could be easy to miss due to the way we broke up our workflow, but if you look closely at the $\lambda$ equations at the top of both panels of Figure 22.3, you'll see the right-hand side of the equations are the same. But because of the differences in the two data hierarchies, those $\lambda$ equations had different consequences for how the `X1` and `X2` values generated the `Y` data. Also, 

> In general, conditional logistic regression requires that there is a linear division between two subsets of the outcomes, and then within each of those subsets there is a linear division of smaller subsets, and so on. This sort of linear division is not required of the softmax regression model... Real data can be extremely noisy, and there can be multiple predictors, so it can be challenging or impossible to visually ascertain which sort of model is most appropriate. The choice of model is driven primarily by theoretical meaningfulness. (p. 659)

## Implementation in ~~JAGS~~ brms

### Softmax model.

Kruschke pointed out in his Figure 22.4 and the surrounding prose that we speak of the *categorical distribution* when fitting softmax models. Our **brms** paradigm will be much the same. To fit a softmax model with the `brm()` function, you specify `family = categorical`. The default is to use the logit link. In his [-@Bürkner2022Parameterization] [*Parameterization of response distributions in brms*](https://CRAN.R-project.org/package=brms/vignettes/brms_families.html#ordinal-and-categorical-models) vignette, Bürkner clarified:

> The **categorical** family is currently only implemented with the multivariate logit link function and has density
>
> $$f(y) = \mu_y = \frac{\exp (\eta_y)}{\sum_{k = 1}^K \exp (\eta_k)}$$
>
> Note that $\eta$ does also depend on the category $k$. For reasons of identifiability, $\eta_1$ is set to $0$.

Though there's no explicit softmax talk in that vignette, you can find it documented in his code [here](https://github.com/paul-buerkner/brms/blob/bc550ff3a2d41656a6711737faf1049207657800/R/distributions.R), starting in line 1891.

Now onto our **ggplot2** + **patchwork** version of the model diagram in Figure 22.4. I'm not gonna lie. The requisite code is a slog. We'll take the task in bits. First, we make and save the elements for the diagram on the left.

```{r, message = F, warning = F}
library(patchwork)

# normal density
p1 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pl[6], color = pl[5]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pl[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = pl[1], hjust = 0, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pl[1]),
        plot.background = element_rect(fill = pl[8], color = "white", size = 1))

# second normal density
p2 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pl[6], color = pl[5]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pl[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M[jk])", "italic(S[jk])"), 
           size = 7, color = pl[1], hjust = 0, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pl[1]),
        plot.background = element_rect(fill = pl[8], color = "white", size = 1))

## an annotated arrow
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")
p3 <-
  tibble(x    = .5,
         y    = 1,
         xend = .73,
         yend = 0) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pl[1]) +
  annotate(geom = "text",
           x = c(.48, .72), y = .5,
           label = c("'~'", "italic(k)"),
           size = c(10, 7), color = pl[1], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

## another annotated arrow
p4 <-
  tibble(x    = .5,
         y    = 1,
         xend = .4,
         yend = 0) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pl[1]) +
  annotate(geom = "text",
           x = c(.34, .6), y = .5,
           label = c("'~'", "italic(jk)"),
           size = c(10, 7), color = pl[1], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p5 <-
  tibble(x = .5,
         y = .5,
         label = "softmax(beta[0]['['*italic(k)*']']+sum()[italic(j)]~beta[italic(j)]['['*italic(k)*']']~italic(x)[italic(ji)])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pl[1], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# a third annotated arrow
p6 <-
  tibble(x     = c(.375, .6),
         y     = c(1/2, 1/2),
         label = c("'='", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = pl[1], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow, color = pl[1]) +
  xlim(0, 1) +
  theme_void()

# bar plot of categorical data
p7 <-
  tibble(x = 0:3,
         d = c(.5, .85, .5, .85)) %>% 
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = pl[6], color = pl[5], width = .45) +
  annotate(geom = "text",
           x = 1.5, y = .2,
           label = "categorical",
           size = 7, color = pl[1]) +
  annotate(geom = "text",
           x = 1.25, y = .9, hjust = 0,
           label = "mu[italic(i)*'['*italic(k)*']']",
           size = 7, color = pl[1], family = "Times", parse = TRUE) +
  coord_cartesian(xlim = c(-.5, 3.5),
                  ylim = 0:1) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pl[1]),
        plot.background = element_rect(fill = pl[8], color = "white", size = 1))

# the final annotated arrow
p8 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), color = pl[1], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0, 
               color = pl[1], arrow = my_arrow) +
  xlim(0, 1) +
  theme_void()

# some text
p9 <-
  tibble(x     = 1,
         y     = .5,
         label = "italic(y[i])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pl[1], parse = T, family = "Times") +
  xlim(0, 2) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 2, l = 1, r = 2),
  area(t = 1, b = 2, l = 3, r = 4),
  area(t = 3, b = 3, l = 1, r = 2),
  area(t = 3, b = 3, l = 3, r = 4),
  area(t = 4, b = 4, l = 1, r = 4),
  area(t = 5, b = 5, l = 2, r = 3),
  area(t = 6, b = 7, l = 2, r = 3),
  area(t = 8, b = 8, l = 2, r = 3),
  area(t = 9, b = 9, l = 2, r = 3)
)

# combine and plot!
a <-
  (
    (p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9) + 
      plot_layout(design = layout) &
      ylim(0, 1) &
      theme(plot.margin = margin(0, 5.5, 0, 5.5))
  )
```

Now we make and save the elements for the diagram on the right.

```{r, warning = F, message = F}
# third normal density
p2 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = pl[6], color = pl[5]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = pl[1]) +
  annotate(geom = "text",
           x = c(0, 1.5), y = .6,
           label = c("italic(M[j])", "italic(S[j])"), 
           size = 7, color = pl[1], hjust = 0, family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pl[1]),
        plot.background = element_rect(fill = pl[8], color = "white", size = 1))

## an annotated arrow
p3 <-
  tibble(x    = .5,
         y    = 1,
         xend = .85,
         yend = 0) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pl[1]) +
  annotate(geom = "text",
           x = .49, y = .5,
           label = "'~'",
           size = 10, color = pl[1], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

## another annotated arrow
p4 <-
  tibble(x    = .5,
         y    = 1,
         xend = .4,
         yend = 0) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = pl[1]) +
  annotate(geom = "text",
           x = c(.35, .57), y = .5,
           label = c("'~'", "italic(j)"),
           size = c(10, 7), color = pl[1], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# likelihood formula
p5 <-
  tibble(x = .5,
         y = .5,
         label = "logistic(beta[0]+sum()[italic(j)]~beta[italic(j)]~italic(x)[italic(ji)])") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = pl[1], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# bar plot of Bernoulli data
p7 <-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = pl[6], color = pl[5], width = .4) +
  annotate(geom = "text",
           x = .5, y = .2,
           label = "Bernoulli",
           size = 7, color = pl[1]) +
  annotate(geom = "text",
           x = .5, y = .9,
           label = "mu[italic(i)]", 
           size = 7, color = pl[1], family = "Times", parse = T) +
  xlim(-.75, 1.75) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = pl[1]),
        plot.background = element_rect(fill = pl[8], color = "white", size = 1))

# combine and plot!
c <-
  (
    (p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9) + 
      plot_layout(design = layout) &
      ylim(0, 1) &
      theme(plot.margin = margin(0, 5.5, 0, 5.5))
  )
```

Here we combine the two model diagrams and plot!

```{r, fig.width = 8, fig.height = 5, warning = F, message = F}
b <- plot_spacer()

(a | b | c) + plot_layout(widths = c(4, 1, 4))
```

### Conditional logistic model.

The conditional logistic regression models are not natively supported in **brms** at this time. Based on [issue #560](https://github.com/paul-buerkner/brms/issues/560) in the **brms** GitHub, there are ways to fit them using the nonlinear syntax. If you compare the syntax Bürkner used in that thread on January 30^th^ to the JAGS syntax Kruschke showed on pages 661 and 662, you'll see they appear to follow contrasting parameterizations.

However, there are at least two other ways to fit conditional logistic models with **brms**. Based on insights from [Henrik Singmann](http://singmann.org/), we can define conditional logistic models using the custom family approach. In contrast, [Mattan Ben-Shachar](https://sites.google.com/view/mattansb) has shown we can also fit conditional logistic models using a tricky application of sequential ordinal regression. Rather than present them in the abstract, here, we will showcase both of these approaches in the sections below.

### Results: Interpreting the regression coefficients.

#### Softmax model.

Load **brms**.

```{r, warning = F, message = F}
library(brms)
```

Along with Kruschke, we'll be modeling the `d1` data. In case it's not clear, the `X1` and `X2` variables are already in a standardized metric.

```{r, message = F}
d1 %>% 
  pivot_longer(-Y) %>% 
  group_by(name) %>% 
  summarise(mean = mean(value), 
            sd   = sd(value)) %>% 
  mutate_if(is.double, round, digits = 2)
```

This will make it easier to set the priors. Here we'll just use the rather wide priors Kruschke indicated on page 662. Note our use of the `dpar` argument within the `prior()` function.

```{r fit22.1}
fit22.1 <-
  brm(data = d1, 
      family = categorical(link = logit),
      Y ~ 0 + Intercept + X1 + X2,
      prior = c(prior(normal(0, 20), class = b, dpar = mu2),
                prior(normal(0, 20), class = b, dpar = mu3),
                prior(normal(0, 20), class = b, dpar = mu4)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      file = "fits/fit22.01")
```

Since it's the default, we didn't have to include the `(link = logit)` bit in the `family` argument. I'm just being explicit for the sake of pedagogy. Take a look at the parameter summary.

```{r}
print(fit22.1)
```

As indicated in the formulas in [Section 22.1][Softmax regression], we get posteriors for each level of `Y`, except for `Y == 1`. That serves as the reference category. The values for $\beta_{i, k = 1}$ are all fixed at $0$.

Here's how we might make the histograms in Figure 22.5.

```{r, fig.width = 8, fig.height = 4.5, warning = F, message = F}
library(tidybayes)

# extract the posterior draws
draws <- as_draws_df(fit22.1)

# wrangle
draws %>% 
  pivot_longer(starts_with("b_")) %>% 
  mutate(name = str_remove(name, "b_")) %>% 
  mutate(lambda    = str_extract(name, "[2-4]+") %>% str_c("lambda==", .),
         parameter = case_when(str_detect(name, "Intercept") ~ "beta[0]",
                               str_detect(name, "X1")        ~ "beta[1]",
                               str_detect(name, "X2")        ~ "beta[2]")) %>% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = pl[4], slab_color = pl[3], color = pl[1], point_color = pl[2],
                    normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("marginal posterior") +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = "free_x")
```

Because the $\beta$ values for when $\lambda = 1$ are all fixed to 0, we left those plots out of our version of the figure. If you really wanted them, you'd have to enter the corresponding cells into the data before plotting. If you summarize each parameter by it's posterior mean, `round()`, and wrangle a little, you can arrange the results in a similar way that the equations for $\lambda_2$ through $\lambda_4$ are displayed on the left side of Figure 22.5.

```{r, message = F, warning = F}
draws %>% 
  pivot_longer(starts_with("b_")) %>% 
  mutate(name = str_remove(name, "b_")) %>% 
  mutate(lambda    = str_extract(name, "[2-4]+") %>% str_c("lambda[", ., "]"),
         parameter = case_when(str_detect(name, "Intercept") ~ "beta[0]",
                               str_detect(name, "X1")        ~ "beta[1]",
                               str_detect(name, "X2")        ~ "beta[2]")) %>% 
  group_by(lambda, parameter) %>% 
  summarise(mean = mean(value) %>% round(digits = 1)) %>% 
  pivot_wider(names_from = parameter,
              values_from = mean)
```

As Kruschke mentioned in the text, "the estimated parameter values should be near the generating values, but not exactly the same because the data are merely a finite random sample" (pp. 662--663). Furthermore,

>  interpreting the parameters is always contextualized relative to the model. For the softmax model, the regression coefficient for outcome $k$ on predictor $x_j$ indicates that rate at which the log odds of that outcome increase relative to the reference outcome for a one unit increase in $x_j$, assuming that a softmax model is a reasonable description of the data. (p. 663)

Unfortunately, this makes the parameters difficult to interpret directly. Kruschke didn't show a plot like this, but it might be helpful to further understand what this model means in terms of probabilities for a given `Y` value. Here we'll use the `fitted()` function to return the conditional probabilities for all four response options for `Y` based on various combinations of `X1` and `X2`.

```{r, fig.width = 6, fig.height = 5, warning = F, message = F}
nd <- crossing(X1 = seq(from = -2, to = 2, length.out = 50),
               X2 = seq(from = -2, to = 2, length.out = 50))

fitted(fit22.1,
       newdata = nd) %>% 
  data.frame() %>% 
  select(contains("Estimate")) %>% 
  set_names(str_c("Y==", 1:4)) %>% 
  bind_cols(nd) %>% 
  pivot_longer(contains("Y"),
               values_to = "phi") %>% 
  
  ggplot(aes(x = X1, y = X2, fill = phi)) +
  geom_raster(interpolate = T) +
  scale_fill_gradientn(expression(phi[italic(k)*"|"*italic(S)]),
                       colours = pnw_palette(name = "Lake", n = 101),
                       limits = c(0, 1)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  facet_wrap(~ name, labeller = label_parsed)
```

Now use that plot while you walk through the final paragraph in this subsection.

> It is easy to transform the estimated parameter values to a different reference category. Recall from Equation 22.3 (p. 651) that arbitrary constants can be added to all the regression coefficients without changing the model prediction. Therefore, to change the parameters estimates so they are relative to outcome $R$, we simply subtract $\beta_{j, R}$ from $\beta_{j, k}$ for all predictors $j$ and all outcomes $k$. We do this at every step in the MCMC chain. For example, in Figure 22.5, consider the regression coefficient on $x_1$ for outcome 2. Relative to reference outcome 1, this coefficient is positive, meaning that the probability of outcome 2 increases relative to outcome 1 when $x_1$ increases. You can see this in the data graph, as the region of 2's falls to right side (positive $x_1$ direction) of the region of 1's. But if the reference outcome is changed to outcome 4, then the coefficient on $x_1$ for outcome 2 changes to a negative value. Algebraically this happens because the coefficient on $x_1$ for outcome 4 is larger than for outcome 2, so when the coefficient for outcome 4 is subtracted, the result is a negative value for the coefficient on outcome 2. Visually, you can see this in the data graph, as the region of 2's falls to the left side (negative $x_1$ direction) of the region of 4's. Thus, *interpreting regression coefficients in a softmax model is rather different than in linear regression. In linear regression, a positive regression coefficient implies that* $y$ *increases when the predictor increases. But not in softmax regression, where a positive regression coefficient is only positive with respect to a particular reference outcome*. (p. 664, *emphasis* added)

##### Bonus: Consider the interceps-only softmax model.

Models like `fit22.1`, above, are great when you want to explore predictors for your nominal variables. However, these models are also really useful when you aren't interested in predictor variables. In these cases, the intercepts-only model will help you compute high-quality Bayesian intervals around the category percentages. Let's walk through an example to see what I mean. Fit an intercepts-only version of the model, above.

```{r fit22.2}
fit22.2 <-
  brm(data = d1, 
      family = categorical(link = logit),
      Y ~ 1,
      prior = c(prior(normal(0, 20), class = Intercept, dpar = mu2),
                prior(normal(0, 20), class = Intercept, dpar = mu3),
                prior(normal(0, 20), class = Intercept, dpar = mu4)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      file = "fits/fit22.02")
```

Review the model summary.

```{r}
print(fit22.2)
```

Even with an intercepts-only model, the parameters in a softmax model are difficult to interpret directly. Though you might be tempted to presume `mu2_Intercept` through `mu4_Intercept` were probabilities on the log-odds scale, they're not. This, recall, is due to their connection to the reference category. If we return to the equation from [Section 22.1][Softmax regression],

$$\phi_k = \operatorname{softmax}_S (\{\lambda_k\}) = \frac{\exp (\lambda_k)}{\sum_{c \in S} \exp  (\lambda_c)},$$

we can get a sense of how to convert these parameters to relative probabilities. Here's how to do so by hand with the posterior draws.

```{r, warning = F}
as_draws_df(fit22.2) %>% 
  mutate(`lambda[1]` = 0,  # recall this is the default
         `lambda[2]` = b_mu2_Intercept,
         `lambda[3]` = b_mu3_Intercept,
         `lambda[4]` = b_mu4_Intercept) %>% 
  pivot_longer(contains("lambda")) %>% 
  # the next two rows are where the magic happens
  group_by(.draw) %>% 
  mutate(phi = exp(value) / sum(exp(value)),
         Y   = str_extract(name, "\\d")) %>% 
  group_by(Y) %>% 
  mean_qi(phi) %>% 
  mutate_if(is.double, round, digits = 2)
```

We can compute these relative probability values ($\phi_k$) much easier with `fitted()`.

```{r}
f <- fitted(fit22.2)

f[1, , ] %>% 
  t() %>% 
  round(digits = 2)
```

Anyway, the reason you might want to go through the trouble of fitting an intercepts-only softmax model is to improve on the kinds of bar plots people often report in their manuscripts. Consider these two:

```{r, fig.width = 8, fig.height = 4}
# descriptive statistics
p1 <-
  d1 %>% 
  count(Y) %>%  
  mutate(p = n / sum(n)) %>% 
  
  ggplot(aes(x = Y, y = p)) +
  geom_col(fill = pl[6]) +
  scale_y_continuous(NULL, labels = scales::percent, breaks = 0:4 / 10,
                     expand = expansion(mult = c(0, 0.05)), limits = c(0, .4)) +
  labs(subtitle = "sample statistics")

# population percentages
p2 <-
  f[1, , ] %>% 
  t() %>% 
  data.frame() %>% 
  rownames_to_column("level") %>% 
  mutate(Y = str_extract(level, "\\d")) %>% 
  
  ggplot(aes(x = Y, y = Estimate)) +
  geom_col(fill = pl[6]) +
  geom_linerange(aes(ymin = Q2.5, ymax = Q97.5),
                 color = pl[2], size = 1) +
  scale_y_continuous(NULL, labels = scales::percent, breaks = 0:4 / 10,
                     expand = expansion(mult = c(0, 0.05)), limits = c(0, .4)) +
  labs(subtitle = "model-based population percentages\n(with 95% interval bars)")

p1 + p2 + plot_annotation(title = "The softmax model adds information to the conventional sample-based\nbar plot.")
```

The plot on the left is the kind of sample data summary you'll see in countless articles and data presentations. Though it's a great way to quickly summarize the relative percentages of each category, it does nothing to express how (un)certain we are those sample statistics will describe the population. The intercepts-only softmax model returns the posterior distributions for the population probabilities. In the plot on the right, the bars mark off the posterior means and the vertical lines mark off the 95% intervals. This is why we want the intercepts-only softmax model.

Okay, now just for kicks and giggles, I'd like to go on a plotting tangent. If data analysts broadly replaced the typical sample-based plots (on the left) for the model-based plots (on the right), it would be a great improvement. At a personal level, though, I think simple bar plots are over used. Let's explore four alternatives. In this block, we'll make and save the first three.

```{r}
# rotated bar plot
p1 <-
  f[1, , ] %>% 
  t() %>% 
  data.frame() %>% 
  rownames_to_column("level") %>% 
  mutate(Y = str_extract(level, "\\d")) %>% 
  
  ggplot(aes(x = Estimate, y = Y)) +
  geom_col(fill = pl[6]) +
  geom_linerange(aes(xmin = Q2.5, xmax = Q97.5),
                 color = pl[2], size = 1) +
  scale_x_continuous(NULL, labels = scales::percent,
                     expand = expansion(mult = c(0, 0.05))) +
  labs(subtitle = "rotated bar plot")

# coefficient plot
p2 <-
  f[1, , ] %>% 
  t() %>% 
  data.frame() %>% 
  rownames_to_column("level") %>% 
  mutate(Y = str_extract(level, "\\d")) %>% 
  
  ggplot(aes(x = Y, y = Estimate)) +
  geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5),
                 color = pl[2], size = 1, fatten = 2) +
  scale_y_continuous(NULL, labels = scales::percent,
                     expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +
  labs(subtitle = "coefficient plot")

# CCDF bar plots
p3 <-
  fitted(fit22.2, summary = F)[, 1, ] %>% 
  data.frame() %>% 
  set_names(1:4) %>% 
  pivot_longer(everything(), values_to = "p") %>% 
  mutate(Y = factor(name)) %>% 
  
  ggplot(aes(x = Y, y = p)) +
  stat_ccdfinterval(.width = .95, fill = pl[6], color = pl[2],
                    size = 1.5, point_size = 2) +
  scale_y_continuous(NULL, labels = scales::percent, breaks = 0:4 / 10,
                     expand = expansion(mult = c(0, 0.05)), limits = c(0, .4)) +
  labs(subtitle = "CCDF bar plot")
```

The fourth alternative is a little weird AND it's going to take a bit more work than the first three.

```{r}
# for annotation
text <-
 fitted(fit22.2)[1, , ] %>% 
  t() %>% 
  data.frame() %>% 
  rownames_to_column("level") %>% 
  mutate(y       = (Estimate / 2) + lag(cumsum(Estimate), default = 0),
         summary = str_c(round(100 * Estimate, 1), "*'% ± '*", round(200 * Est.Error, 1)),
         label_y = str_c("Y = ", 1:4))

p4 <-
  # wrangle
  fitted(fit22.2, summary = F)[, 1, ] %>% 
  data.frame() %>% 
  set_names(1:4) %>% 
  mutate(row = 1:n()) %>% 
  pivot_longer(-row) %>% 
  mutate(Y = fct_rev(name)) %>% 

  # plot
  ggplot(aes(x = row, y = value)) +
  geom_col(aes(fill = Y),
           position = "stack", size = 0, width = 1) +
  geom_text(data = text,
            aes(x = -2000, y = y, label = label_y),
            color = pl[1]) +
  geom_text(data = text,
            aes(x = 6000, y = y, label = summary),
            color = pl[1], parse = T) +
  scale_fill_manual(values = pl[7:4], breaks = NULL) +
  scale_x_continuous(NULL, breaks = NULL, limits = c(-4000, 8000)) +
  scale_y_continuous(NULL, labels = scales::percent, expand = c(0, 0)) +
  labs(subtitle = "stacked bar plot with uncertain boundaries")
```

Okay, now combine the four ggplots and behold!

```{r, fig.width = 8, fig.height = 7.5}
# combine
p1 + p2 + p3 + p4 + 
  plot_annotation(title = "Alternatives to the conventional bar plot",
                  tag_levels = "a", tag_prefix = "(", tag_suffix = ")")
```

The rotated bar plot (a) is most useful when the names of the levels are longer character strings. For example, imagine that instead of `1` through `4`, the four levels were countries or pharmaceutical drugs. You wouldn't want to mess with formatting those on an $x$-axis. Use a rotated bar plot, instead. For my taste, the simple coefficient plot (b) gets the job done nicely without the unnecessary clutter of the bars. Matthew Kay's CCDF bar plots (c) provide a fuller expression of the shape of the posterior uncertainty in each percentage. If desired, you could even omit the dot intervals from those and the visual expression of uncertainty would still remain. The final stacked bar plot with the fuzzy boundaries (d) is the result of a little back-and-forth on twitter ([original tweet](https://twitter.com/SolomonKurz/status/1372632774285348864)). It's the oddball of the group, but what it does uniquely well is show how the percentages of the groups all depend on one another and must, by definition, sum to 100.

#### Conditional logistic model.

Since we will be fitting the conditional logistic model with two different strategies, I'm going to deviate from how Kruschke organized this part of the text and break this section up into two subsections:

1. First we'll walk through the custom family approach.
2. Second we'll explore the sequential ordinal approach.

##### Conditional logistic models with custom likelihoods.

As we briefly learned in [Section 8.6.1][Defining new likelihood functions.], **brms** users can define their own custom likelihood functions, which Bürkner outlined in his [-@Bürkner2022Define] vignette, [*Define custom response distributions with brms*](https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html). As part of the [*Nominal data and Kruschke's "conditional logistic" approach*](https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433) thread on the Stan forums, Henrik Singmann showed how you can use this functionality to fit conditional logistic models with **brms**. We will practice how to do this for the models of both the `d3` and `d4` data sets, which were showcased in the left and right panels of Figure 22.3 in [Section 22.2][Conditional logistic regression]. Going in order, we'll focus first on how to model the data in `d3`.

For the first step, we use the `custom_family()` function to 

* name the new family with the `name` argument,
* name the family's parameters with the `dpars` argument,
* name the link function(s) with the `links` argument,
* define whether the distribution is discrete or continuous with the `type` argument,
* provide the names of any variables that are part of the internal workings of the family but are not among the distributional parameters with the `vars` argument, and
* provide supporting information with the `specials` argument.

```{r}
cond_log_1 <- custom_family(
  name     = "cond_log_1", 
  dpars    = c("mu", "mub", "muc"), 
  links    = "identity", 
  type     = "int",
  vars     = c("n_cat"),
  specials = "categorical"
)
```

In the second step, we use the `stanvar()` function to define our custom probability mass function and the corresponding function that will allow us to return predictions.

```{r}
stan_lpmf_1 <- stanvar(block = "functions", 
                       scode = "
real cond_log_1_lpmf(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mu;
  prob[2] = p_mub * (1 - p_mu);
  prob[3] = p_muc * (1 - p_mub) * (1 - p_mu);
  prob[4] = (1 - p_mu) * (1 - p_mub) * (1 - p_muc);
  return(categorical_lpmf(y | prob));
}

vector cond_log_1_pred(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mu;
  prob[2] = p_mub * (1 - p_mu);
  prob[3] = p_muc * (1 - p_mub) * (1 - p_mu);
  prob[4] = (1 - p_mu) * (1 - p_mub) * (1 - p_muc);
  return(prob);
}
") 
```

Note how we have defined the four `prob[i]` values based on the four equations from above:

$$
\begin{align*}
\phi_1 & = \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \\
\phi_2 & = \phi_{\{ 2 \} | \{ 2,3,4 \}} \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_3 & = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 & = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right).
\end{align*}
$$

Third, we save another `stanvar()` object with additional information.

```{r}
stanvars <- stanvar(x = 4, name = "n_cat", scode = "  int n_cat;")
```

Now we're ready to fit the model with `brm()`. Notice how our use of the `family` and `stanvars` functions.

```{r fit22.3, warning = F}
fit22.3 <-
  brm(data = d3, 
      family = cond_log_1,
      Y ~ 1 + X1 + X2,
      prior = c(prior(normal(0, 20), class = Intercept),
                prior(normal(0, 20), class = Intercept, dpar = mub),
                prior(normal(0, 20), class = Intercept, dpar = muc),
                prior(normal(0, 20), class = b),
                prior(normal(0, 20), class = b, dpar = mub),
                prior(normal(0, 20), class = b, dpar = muc)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      stanvars = stan_lpmf_1 + stanvars,
      file = "fits/fit22.03")
```

Check the model summary.

```{r}
print(fit22.3)
```

As they aren't the most intuitive, here's how to understand our two prefixes:

* the lines with no prefix have to do with $\lambda_{\{ 1 \} | \{ 1,2,3,4 \}}$,
* `mub_` has to do with $\lambda_{\{ 2 \} | \{ 2,3,4 \}}$, and
* `muc_` has to do with $\lambda_{\{ 3 \} | \{ 3,4 \}}$.

If you compare those posterior means of each of those parameters from the data-generating equations at the top of Figure 22.3, you'll see they are spot on (within simulation variance). Here's how we might visualize those posteriors in our version of the histograms in the top right panel(s) of Figure 22.6.

```{r, fig.width = 8, fig.height = 4.5, warning = F, message = F}
# extract the posterior draws
draws <- as_draws_df(fit22.3) %>% 
  # our lives will be easier if we adjust the column names
  rename(a_Intercept = b_Intercept,
         b_Intercept = b_mub_Intercept,
         c_Intercept = b_muc_Intercept,
         a_X1 = b_X1,
         a_X2 = b_X2,
         b_X1 = b_mub_X1,
         b_X2 = b_mub_X2,
         c_X1 = b_muc_X1,
         c_X2 = b_muc_X2)

# wrangle
p1 <-
  draws %>% 
  pivot_longer(a_Intercept:c_X2) %>% 
  mutate(lambda    = case_when(str_detect(name, "a_") ~ "lambda['{1}|{1,2,3,4}']",
                               str_detect(name, "b_") ~ "lambda['{2}|{2,3,4}']",
                               str_detect(name, "c_") ~ "lambda['{3}|{3,4}']"),
         parameter = case_when(str_detect(name, "Intercept") ~ "beta[0]",
                               str_detect(name, "X1")        ~ "beta[1]",
                               str_detect(name, "X2")        ~ "beta[2]")) %>% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = pl[4], slab_color = pl[3], color = pl[1], point_color = pl[2],
                    normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("marginal posterior") +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = "free_x") +
  theme(axis.text = element_text(size = 8))
```

If we use the threshold formula from above,

$$x_2 = (-\beta_0 / \beta_2) + (-\beta_1 / \beta_2)x_1,$$

to the posterior draws, we can make our version of the upper left panel of Figure 22.6.

```{r, warning = F}
set.seed(22)

p2 <-
  draws %>% 
  slice_sample(n = 30) %>% 
  pivot_longer(a_Intercept:c_X2) %>% 
  separate(name, into = c("mu", "parameter"), sep = "_") %>% 
  pivot_wider(names_from = parameter, values_from = value) %>% 
  mutate(intercept = -Intercept / X2,
         slope     = -X1 / X2) %>% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(.draw, mu),
                  linetype = mu),
              size = 1/4, alpha = 1/2, color = pl[1]) +
  scale_color_manual(values = pl[2:5]) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   "lambda['{1}|{1,2,3,4}']", 
                   "lambda['{2}|{2,3,4}']", 
                   "lambda['{3}|{3,4}']")),
                 guide = guide_legend(
                   direction = "vertical",
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = "top")
```

Now combine the two ggplots, add a little formatting, and show the full upper half of Figure 22.6, based on the `custom_family()` approach.

```{r, fig.width = 9, fig.height = 5.5}
(p2 + p1) & 
  plot_layout(widths = c(1, 2)) &
  plot_annotation(title = "Figure 22.6, upper half",
                  subtitle = "Results from the conditional logistic model fit to the d3 data via the custom-family approach")
```

Though it isn't necessary to reproduce any of the plots in this section of Kruschke's text, we'll want to use the `expose_functions()` function if we wanted to use any of the **brms** post-processing functions for our model fit with the custom likelihood.

```{r, warning = F, message = F, results = 'hide'}
expose_functions(fit22.3, vectorize = TRUE)
```

Here's what we'd need to do before computing information criteria estimates, such as with the WAIC.

```{r}
log_lik_cond_log_1 <- function(i, prep) {
  mu  <- brms::get_dpar(prep, "mu", i = i)
  mub <- brms::get_dpar(prep, "mub", i = i)
  muc <- brms::get_dpar(prep, "muc", i = i)
  n_cat <- prep$data$n_cat
  y <- prep$data$Y[i]
  cond_log_1_lpmf(y, mu, mub, muc, n_cat)
}

fit22.3 <- add_criterion(fit22.3, criterion = "waic")

waic(fit22.3)
```

If we wanted to use one of the functions that relies on conditional expectations, such as `conditional_effects()`, we'd execute something like this.

```{r, fig.width = 4, fig.height = 2.75}
posterior_epred_cond_log_1 <- function(prep) {
  mu   <- brms::get_dpar(prep, "mu")
  mu_b <- brms::get_dpar(prep, "mub")
  mu_c <- brms::get_dpar(prep, "muc")
  n_cat <- prep$data$n_cat
  y <- prep$data$Y
  prob <- cond_log_1_pred(y = y, mu = mu, mu_b = mu_b, mu_c = mu_c, n_cat = n_cat)
  dim(prob) <- c(dim(prob)[1], dim(mu))
  prob <- aperm(prob, c(2,3,1))
  dimnames(prob) <- list(
    as.character(seq_len(dim(prob)[1])), 
    NULL, 
    as.character(seq_len(dim(prob)[3]))
  )
  prob
}

ce <- conditional_effects(
  fit22.3, 
  categorical = T,
  effects = "X1")

plot(ce, plot = FALSE)[[1]] + 
  scale_fill_manual(values = pl[2:5]) +
  scale_color_manual(values = pl[2:5])
```

If we wanted to do a posterior predictive check with the `pp_check()` function, we'd need to do something like this.

```{r, fig.width = 4, fig.height = 2.75}
posterior_predict_cond_log_1 <- function(i, prep, ...) {
  mu   <- brms::get_dpar(prep, "mu", i = i)
  mu_b <- brms::get_dpar(prep, "mub", i = i)
  mu_c <- brms::get_dpar(prep, "muc", i = i)
  n_cat <- prep$data$n_cat
  y <- prep$data$Y[i]
  prob <- cond_log_1_pred(y, mu, mu_b, mu_c, n_cat)
  # make sure you have the extraDistr package
  extraDistr::rcat(length(mu), t(prob))
}

bayesplot::color_scheme_set(pl[7:2])

pp_check(fit22.3, 
         type = "bars", 
         ndraws = 100, 
         size = 1/2, 
         fatten = 2)
```

So far all of this has been with the conditional logistic model based on the first hierarchy of two-set divisions, which Kruschke used to simulate the `d3` data. Now we'll switch to consider the second hierarchy of two-set divisions, with which Kruschke simulated the `d4` data. That second hierarchy, recall, resulted in the following definition for the conditional probabilities for the four levels of `Y`:

$$
\begin{align*}
\phi_1 & = \phi_{\{ 1 \} | \{ 1,2 \}} \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_2 & = \left ( 1 - \phi_{\{ 1 \} | \{ 1,2 \}} \right) \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_3 & = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 & = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right).
\end{align*}
$$

This will require us to define a new custom family, which we'll call `cond_log_2`.

```{r}
cond_log_2 <- custom_family(
  name     = "cond_log_2", 
  dpars    = c("mu", "mub", "muc"), 
  links    = "identity", 
  type     = "int",
  vars     = c("n_cat"),
  specials = "categorical"
)
```

Next, we use the `stanvar()` function to define our custom probability mass function and the corresponding function that will allow us to return predictions, which we'll just save as `stan_lpmf_2`. Other than the names, notice that the major change is how we have defined the `prob[i]` parameters.

```{r}
stan_lpmf_2 <- stanvar(block = "functions", 
                       scode = "
real cond_log_2_lpmf(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mub * p_mu;
  prob[2] = (1 - p_mub) * p_mu;
  prob[3] = p_muc * (1 - p_mu);
  prob[4] = (1 - p_muc) * (1 - p_mu);
  return(categorical_lpmf(y | prob));
}

vector cond_log_2_pred(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mub * p_mu;
  prob[2] = (1 - p_mub) * p_mu;
  prob[3] = p_muc * (1 - p_mu);
  prob[4] = (1 - p_muc) * (1 - p_mu);
  return(prob);
}
") 
```

Now we're ready to fit the model with `brm()`. Again, notice how our use of the `family` and `stanvars` functions.

```{r fit22.4}
fit22.4 <-
  brm(data = d4, 
      family = cond_log_2,
      Y ~ 1 + X1 + X2,
      prior = c(prior(normal(0, 20), class = Intercept),
                prior(normal(0, 20), class = Intercept, dpar = mub),
                prior(normal(0, 20), class = Intercept, dpar = muc),
                prior(normal(0, 20), class = b),
                prior(normal(0, 20), class = b, dpar = mub),
                prior(normal(0, 20), class = b, dpar = muc)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      stanvars = stan_lpmf_2 + stanvars,
      file = "fits/fit22.04")
```

Check the model summary.

```{r}
print(fit22.4)
```

We can use the same basic workflow as before to make our version of the upper half of Figure 22.7.

```{r, fig.width = 9, fig.height = 5.5, warning = F, message = F}
# extract the posterior draws
draws <- as_draws_df(fit22.4) %>% 
  # like before, let's adjust the column names
  rename(a_Intercept = b_Intercept,
         b_Intercept = b_mub_Intercept,
         c_Intercept = b_muc_Intercept,
         a_X1 = b_X1,
         a_X2 = b_X2,
         b_X1 = b_mub_X1,
         b_X2 = b_mub_X2,
         c_X1 = b_muc_X1,
         c_X2 = b_muc_X2)

# 2D thresholds on the left
set.seed(22)

p1 <-
  draws %>% 
  slice_sample(n = 30) %>% 
  pivot_longer(a_Intercept:c_X2) %>% 
  separate(name, into = c("mu", "parameter"), sep = "_") %>% 
  pivot_wider(names_from = parameter, values_from = value) %>% 
  mutate(intercept = -Intercept / X2,
         slope     = -X1 / X2) %>% 

  ggplot() +
  geom_text(data = d4,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(.draw, mu),
                  linetype = mu),
              size = 1/4, alpha = 1/2, color = pl[1]) +
  scale_color_manual(values = pl[2:5]) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   "lambda['{1,2}|{1,2,3,4}']", 
                   "lambda['{1}|{1,2}']", 
                   "lambda['{3}|{3,4}']")),
                 guide = guide_legend(
                   direction = "vertical",
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = "top")

# marginal posteriors on the right
p2 <-
  draws %>% 
  pivot_longer(a_Intercept:c_X2) %>% 
  mutate(lambda    = case_when(str_detect(name, "a_") ~ "lambda['{1,2}|{1,2,3,4}']",
                               str_detect(name, "b_") ~ "lambda['{1}|{1,2}']",
                               str_detect(name, "c_") ~ "lambda['{3}|{3,4}']"),
         parameter = case_when(str_detect(name, "Intercept") ~ "beta[0]",
                               str_detect(name, "X1")        ~ "beta[1]",
                               str_detect(name, "X2")        ~ "beta[2]")) %>% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = pl[4], slab_color = pl[3], color = pl[1], point_color = pl[2],
                    normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("marginal posterior") +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = "free_x")

# combine, entitle, and display the results
(p1 + p2) & 
  plot_layout(widths = c(1, 2)) &
  plot_annotation(title = "Figure 22.7, upper half",
                  subtitle = "Results from the conditional logistic model fit to the d4 data via the custom-family approach")
```

As Kruschke pointed out in the text, 

> notice that the estimates for $\lambda_2$ are more uncertain, with wider HDI's, than the other coefficients. This uncertainty is also shown in the threshold lines on the data: The lines separating the 1's from the 2's have a much wider spread than the other boundaries. Inspection of the scatter plot explains why: There is only a small zone of data that informs the separation of 1's from 2's, and therefore the estimate must be relatively ambiguous. (p. 665)

I'm not going to go through a full demonstration like before, but if you want to use more **brms** post processing functions for `fit22.4` or any other model fit with our custom `cond_log_2` function, you'd need to execute this block of code first. Then post process to your heart's desire.

```{r, eval = F}
expose_functions(fit22.4, vectorize = TRUE)

# for information criteria
log_lik_cond_log_2 <- function(i, prep) {
  mu  <- brms::get_dpar(prep, "mu", i = i)
  mub <- brms::get_dpar(prep, "mub", i = i)
  muc <- brms::get_dpar(prep, "muc", i = i)
  n_cat <- prep$data$n_cat
  y <- prep$data$Y[i]
  cond_log_2_lpmf(y, mu, mub, muc, n_cat)
}

# for conditional expectations
posterior_epred_cond_log_2 <- function(prep) {
  mu   <- brms::get_dpar(prep, "mu")
  mu_b <- brms::get_dpar(prep, "mub")
  mu_c <- brms::get_dpar(prep, "muc")
  n_cat <- prep$data$n_cat
  y <- prep$data$Y
  prob <- cond_log_2_pred(y = y, mu = mu, mu_b = mu_b, mu_c = mu_c, n_cat = n_cat)
  dim(prob) <- c(dim(prob)[1], dim(mu))
  prob <- aperm(prob, c(2,3,1))
  dimnames(prob) <- list(
    as.character(seq_len(dim(prob)[1])), 
    NULL, 
    as.character(seq_len(dim(prob)[3]))
  )
  prob
}

# for posterior predictions
posterior_predict_cond_log_2 <- function(i, prep, ...) {
  mu   <- brms::get_dpar(prep, "mu", i = i)
  mu_b <- brms::get_dpar(prep, "mub", i = i)
  mu_c <- brms::get_dpar(prep, "muc", i = i)
  n_cat <- prep$data$n_cat
  y <- prep$data$Y[i]
  prob <- cond_log_2_pred(y, mu, mu_b, mu_c, n_cat)
  # make sure you have the extraDistr package
  extraDistr::rcat(length(mu), t(prob))
}
```

In this section of the text, Kruschke also showed the results of when he analyzed the two data sets with the non-data-generating likelihoods. In the lower half of Figure 22.6, he showed the results of his second version of the conditional logistic model applied to the `d3` data. In the lower half of Figure 22.7, he showed the results of his first version of the conditional logistic model applied to the `d4` data. Since this section is already complicated enough, we're not going to do that. But if you'd like to see what happens, consider it a personal homework assignment.

> In principle, the different conditional logistic models could be put into an overarching hierarchical model comparison. If you have only a few specific candidate models to compare, this could be a feasible approach. But it is not an easily pursued approach to selecting a partition of outcomes from all possible partitions of outcomes when there are many outcomes... Therefore, it is typical to consider a single model, or small set of models, that are motivated by being meaningful in the context of the application, and interpreting the parameter estimates in that meaningful context. (p. 667)

Kruschke finished this section with:

> Finally, when you run the models in JAGS, you may find that there is high autocorrelation in the MCMC chains (even with standardized data), which requires a very long chain for adequate ESS. This suggests that Stan might be a more efficient approach.

Since we fit our models with Stan via **brms**, high autocorrelations and low effective sample sizes weren't a problem. For example, here are the bulk and tail effective sample sizes for both of our two models.

```{r, fig.width = 5.5, fig.height = 4, warning = F, message = F}
library(posterior)

bind_rows(
  as_draws_df(fit22.3) %>% summarise_draws(),
  as_draws_df(fit22.4) %>% summarise_draws()
  ) %>% 
  mutate(fit = rep(c("fit22.3", "fit22.4"), each = n() / 2)) %>% 
  pivot_longer(starts_with("ess")) %>% 
  
  ggplot(aes(x = value)) +
  geom_dotplot(binwidth = 325, color = pl[8], fill = pl[5], stroke = 1/2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlim(0, NA) +
  facet_grid(fit ~ name) +
  theme(axis.text = element_text(size = 9))
```

The values look pretty good. We may as well look at the autocorrelations. To keep things simple, this time we'll restrict our analysis to `fit22.4`. [The results are largely the same for `fit22.3`.]

```{r, warning = F, message = F}
library(bayesplot)

ac <-
  as_draws_df(fit22.4) %>% 
  mutate(chain = .chain) %>% 
  select(b_Intercept:b_muc_X2, chain) %>% 
  mcmc_acf(lags = 5)

ac$data %>% 
  filter(Lag > 0) %>% 
  
  ggplot(aes(x = AC)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_dotplot(binwidth = 1/14, color = pl[8], fill = pl[5], stroke = 1/2) +
  scale_x_continuous("autocorrelation", limits = c(-1, 1),
                     labels = c("-1", "-.5", "0", ".5", "1")) +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_grid(Lag ~ Chain, labeller = label_both)
```

On the whole, the autocorrelations are reasonably low across all parameters, chains, and lags.

##### Conditional logistic models by sequential ordinal regression.

In their [-@burknerOrdinalRegressionModels2019] paper, [*Ordinal regression models in psychology: A tutorial*](https://psyarxiv.com/x8swp/), Bürkner and Vourre outlined a framework for fitting a variety of orginal models with **brms**. We'll learn more about ordinal models in [Chapter 23][Ordinal Predicted Variable]. In this section, we'll use Mattan Ben-Shachar's strategy and purpose one of the ordinal models to fit a conditional logistic model to our nominal data.

As outlined in @burknerOrdinalRegressionModels2019, and as we will learn in greater detain in the next chapter, many ordinal regression models presume an underlying continuous process. However, you can use a sequential model in cases where one level of the criterion is only possible after the lower levels of the criterion have been achieved. Although this is not technically correct for the nominal variable `Y` in the `d3` data set, the simple hierarchical sequence Kruschke used to model those data does follow that same pattern. Ben-Shachar's insight was that if we treat our nominal variable `Y` as ordinal, the sequential model will mimic the sequential-ness of Kruschke's binary-choices hierarchy. To get this to work, we first have to save an ordinal version of `Y`, which we'll call `Y_ord`.

```{r}
d3 <-
  d3 %>% 
  mutate(Y_ord = ordered(Y))

# what are the new attributes?
attributes(d3$Y_ord)
```

Within `brm()` we fit sequential models using `family = sratio`, which defaults to the logit link. If you want to use predictors in a model of this kind and you would like those coefficients to vary across the different levels of the criterion, you need to insert the predictor terms within the `cs()` function. Here's how to fit the model with `brm()`.

```{r fit22.5}
fit22.5 <-
  brm(data = d3, 
      family = sratio,
      Y_ord ~ 1 + cs(X1) + cs(X2),
      prior = c(prior(normal(0, 20), class = Intercept),
                prior(normal(0, 20), class = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      file = "fits/fit22.05")
```

Check the model summary.

```{r}
print(fit22.5)
```

One thing that might not be apparent at first glance is that although this model is essentially equivalent to the `family = cond_log_1` version of the model we fit with `fit22.3`, above, the parameters are a little different. The intercepts are largely the same. However, the coefficients for the `X1` and `X2` predictors have switched signs. This will be easier to see with a coefficient plot comparing `fit22.3` and `fit22.5`.

```{r, fig.width = 6, fig.height = 2.5}
rbind(fixef(fit22.3)[c(1:4, 6, 8, 5, 7, 9), ], fixef(fit22.5)) %>% 
  data.frame() %>% 
  mutate(beta = rep(str_c("beta[", c(0:2, 0:2), "]"), each = 3),
         lambda = rep(str_c("lambda==", 1:3), times = 3 * 2),
         family = rep(c("cond_log_1", "sratio"), each = 9)) %>% 
  
  # plot!
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = family)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange(size = 1/2, fatten = 5/4) +
  # stat_pointinterval(.width = .95, point_size = 1.5, size = 1) +
  labs(x = "marginal posterior",
       y = NULL) +
  facet_grid(lambda ~ beta, labeller = label_parsed, scales = "free_x")
```

Even though the $\beta_1$ and $\beta_2$ parameters switched signs, their magnitudes are about the same. Thus, if we want to use our `fit22.5` to plot the thresholds as in Figure 22.6, we'll have to update our threshold formula to 

```{r, eval = F, echo = F}
pnw_palette(name = "Moth")[2]
```


$$x_2 = (\color{#984136}{+\beta_0} / \beta_2) + (\beta_1 / \beta_2)x_1.$$

With that adjustment in line, here's our updated version of the left panel of Figure 22.6.

```{r, fig.width = 2.75, warning = F}
# no need to rename the columns, this time
as_draws_df(fit22.5)  %>% 
  slice_sample(n = 30) %>% 
  pivot_longer(starts_with("b")) %>% 
  mutate(name = str_remove(name, "b_") %>% str_remove(., "bcs_")) %>% 
  separate(name, into = c("parameter", "lambda")) %>% 
  pivot_wider(names_from = parameter, values_from = value) %>% 
  mutate(intercept = Intercept / X2,
         slope     = -X1 / X2) %>% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(.draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2, color = pl[1]) +
  scale_color_manual(values = pl[2:5]) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   "lambda['{1}|{1,2,3,4}']", 
                   "lambda['{2}|{2,3,4}']", 
                   "lambda['{3}|{3,4}']")),
                 guide = guide_legend(
                   direction = "vertical",
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = "top")
```

Though we used a different likelihood and a different formula for the thresholds, we got same basic model results. They're just parameterized in a slightly different way. The nice thing with the `family = sratio` approach is all of the typical **brms** post processing functions will work out of the box. For example, here's the posterior predictive check via `pp_check()`.

```{r, fig.width = 4, fig.height = 2.75}
pp_check(fit22.5, 
         type = "bars", 
         ndraws = 100, 
         size = 1/2, 
         fatten = 2)
```

Also note how the information criteria estimates for the two approaches are essentially the same.

```{r, warning = F}
fit22.5 <- add_criterion(fit22.5, criterion = "waic")

loo_compare(fit22.3, fit22.5, criterion = "waic") %>% 
  print(simplify = F)
```

A limitation of the `family = sratio` method for conditional logistic models is it requires a simple binary-divisions hierarchy that resembles the one we just used, the one in the left panel of Figure 22.2. It is not well suited for the more complicated hierarchy displayed in the right panel of Figure 22.2, nor will it help you make sense of data generated by that kind of mechanism. For example, consider what happens when we try to use `family = sratio` with the `d4` data.

```{r fit22.6}
# make an ordinal version of Y
d4 <-
  d4 %>% 
  mutate(Y_ord = ordered(Y))

# fit the model
fit22.6 <-
  brm(data = d4, 
      family = sratio,
      Y_ord ~ 1 + cs(X1) + cs(X2),
      prior = c(prior(normal(0, 20), class = Intercept),
                prior(normal(0, 20), class = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      file = "fits/fit22.06")
```

```{r}
print(fit22.6)
```

If you look at the parameter summary, nothing obviously bad happened. The computer didn't crash or anything. To get a better sense of the damage, we plot.

```{r, fig.width = 9, fig.height = 5.5, warning = F}
# extract the posterior draws for fit22.6
draws <- as_draws_df(fit22.6)

# 2D thresholds on the left
set.seed(22)

p1 <- 
  draws %>% 
  slice_sample(n = 30) %>% 
  pivot_longer(starts_with("b")) %>% 
  mutate(name = str_remove(name, "b_") %>% str_remove(., "bcs_")) %>% 
  separate(name, into = c("parameter", "lambda")) %>% 
  pivot_wider(names_from = parameter, values_from = value) %>% 
  # still using the adjusted formula for the thresholds
  mutate(intercept = Intercept / X2,
         slope     = -X1 / X2) %>% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(.draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2, color = pl[1]) +
  scale_color_manual(values = pl[2:5]) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   "lambda['{1}|{1,2,3,4}']", 
                   "lambda['{2}|{2,3,4}']", 
                   "lambda['{3}|{3,4}']")),
                 guide = guide_legend(
                   direction = "vertical",
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = "top")

# marginal posteriors on the right
p2 <-
draws %>% 
  pivot_longer(starts_with("b")) %>% 
  mutate(name = str_remove(name, "b_") %>% str_remove(., "bcs_")) %>% 
  separate(name, into = c("parameter", "lambda")) %>% 
  mutate(lambda    = case_when(lambda == "1" ~ "lambda['{1}|{1,2,3,4}']",
                               lambda == "2" ~ "lambda['{2}|{2,3,4}']",
                               lambda == "3" ~ "lambda['{3}|{3,4}']"),
         parameter = case_when(parameter == "Intercept" ~ "beta[0]",
                               parameter == "X1"        ~ "beta[1]",
                               parameter == "X2"        ~ "beta[2]")) %>% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = pl[4], slab_color = pl[3], color = pl[1], point_color = pl[2],
                    normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("marginal posterior") +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = "free_x") +
  theme(axis.text = element_text(size = 9))

# combine, entitle, and display the results
(p1 + p2) & 
  plot_layout(widths = c(1, 2)) &
  plot_annotation(title = "Figure 22.7, lower half",
                  subtitle = "Results from the conditional logistic model fit to the d4 data via the sequential-ordinal approach")
```

We ended up with our version of the lower half of Figure 22.7. As with the previous model, the sequential-ordinal approach reverses the signs for the $\beta_1$ and $\beta_2$ parameters, which isn't a big deal as long as you keep that in mind. The larger issue is that the thresholds displayed in the left panel do a poor job differentiating among the various `Y` categories. The model underlying those thresholds is a bad match for the data.

##### Conditional logistic wrap-up.

To wrap this section up, we walked through approaches for fitting conditional logistic models with **brms**. First we considered Singmann's method for using the **brms** custom family functionality to define bespoke likelihood functions. Though it requires a lot of custom coding and an above-average knowledge of the inner workings of **brms** and Stan, the custom-family approach is very general and will possibly work for all your conditional-logistic needs. Then we considered Ben-Shachar sequential-ordinal approach. Ben-Shachar's insight was that if we are willing to augment the nominal data with the `ordered()` function, modeling them with a sequential-ordinal model via `family = sratio` will return near equivalent results to the conditional-logistic method. Though this method is attractive in that it uses a built-in likelihood and thus avoids a lot of custom coding, it is limited in that it will only handle nominal data which are well described by the simple  binary-divisions hierarchy displayed in the left panel of Figure 22.2.

In closing, I would like to thank Singmann and Ben-Shachar for their time and insights. `emo::ji("beers")` I could not have finished this section without them. If you would like more examples of both of their methods applied to different data sets, check out the Stan forum thread called [Nominal data and Kruschke's "conditional logistic" approach](https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433).

## Generalizations and variations of the models

These models can be generalized to include different kinds of predictors, variants robust to outliers, and model comparison via information criteria and so forth. You can find a few more examples with softmax regression in Chapter 10 of the first edition of McElreath's [-@mcelreathStatisticalRethinkingBayesian2015] [*Statistical rethinking*](https://xcelab.net/rm/statistical-rethinking/) and Chapter 11 of his second edition [@mcelreathStatisticalRethinkingBayesian2020]. See also Kurz [-@kurzStatisticalRethinkingSecondEd2021, [Section 11.3](https://bookdown.org/content/4857/god-spiked-the-integers.html#multinomial-and-categorical-models)] and Kurz [-@kurzStatisticalRethinkingBrms2020, [Section 10.3.1](https://bookdown.org/content/3890/counting-and-classification.html#multinomial)] for walk-throughs with **brms**.

## Session info {-}

```{r}
sessionInfo()
```

```{r, message = F, echo = F}
# remove the objects
rm(d1, d2, pl, d3, d4, numbers, greek, b01, b11, b21, b02, b12, b22, b03, b13, b23, lines, p1, p2, my_arrow, p3, p4, p5, p6, p7, p8, p9, layout, a, c, b, fit22.1, draws, nd, fit22.2, f, text, cond_log_1, stan_lpmf_1, cond_log_1_lpmf, cond_log_1_pred, stanvars, fit22.3, log_lik_cond_log_1, posterior_epred_cond_log_1, ce, posterior_predict_cond_log_1, cond_log_2, stan_lpmf_2, fit22.4, cond_log_2_lpmf, cond_log_2_pred, log_lik_cond_log_2, posterior_epred_cond_log_2, posterior_predict_cond_log_2, ac, fit22.5, fit22.6)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:22.Rmd-->


```{r, echo = FALSE, cachse = FALSE}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 110)
```

# Ordinal Predicted Variable

> This chapter considers data that have an ordinal predicted variable. For example, we might want to predict people's happiness ratings on a 1-to-7 scale as a function of their total financial assets. Or we might want to predict ratings of movies as a function of the year they were made.
>
> One traditional treatment of this sort of data structure is called *ordinal or ordered probit regression*. We will consider a Bayesian approach to this model. As usual, in Bayesian software, it is easy to generalize the traditional model so it is robust to outliers, allows different variances within levels of a nominal predictor, or has hierarchical structure to share information across levels or factors as appropriate.
>
In the context of the generalized linear model (GLM) introduced in Chapter 15, this chapter's situation involves an inverse-link function that is a thresholded cumulative normal with a categorical distribution for describing noise in the data, as indicated in the fourth row of Table 15.2 (p. 443). For a reminder of how this chapter's combination of predicted and predictor variables relates to other combinations, see Table 15.3 (p. 444). [@kruschkeDoingBayesianData2015, p. 671, *emphasis* in the original]

We might follow Kruschke's advice and rewind a little before tackling this chapter. The cumulative normal function is denoted $\Phi(x; \mu, \sigma)$, where $x$ is some real number and $\mu$ and $\sigma$ have their typical meaning. The function is cumulative in that it produces values ranging from zero to 1. Figure 15.8 showed an example of the normal distribution atop of the cumulative normal. Here it is, again.

```{r, warning = F, message = F}
library(tidyverse)

d <-
  tibble(z = seq(from = -3, to = 3, by = .1)) %>% 
  # add the density values
  mutate(`p(z)`   = dnorm(z, mean = 0, sd = 1),
         # add the CDF values
         `Phi(z)` = pnorm(z, mean = 0, sd = 1))

head(d)
```

It's time to talk color and theme. For this chapter, we'll take our color palette from the [**scico** package](https://CRAN.R-project.org/package=scico) [@R-scico], which provides 17 perceptually-uniform and colorblind-safe palettes based on the work of [Fabio Crameri](https://twitter.com/fcrameri). Our palette of interest will be `"lajolla"`.

```{r, warning = F, message = F, fig.height = 3}
library(scico)

sl <- scico(palette = "lajolla", n = 9)

scales::show_col(sl)
```

Our overall plot theme will be based on `ggplot2::theme_linedraw()` with just a few adjustments.

```{r, warning = F, message = F}
theme_set(
  theme_linedraw() +
    theme(panel.grid = element_blank(),
          strip.background = element_rect(color = sl[9], fill = sl[9]),
          strip.text = element_text(color = sl[1]))
)
```

Now plot!

```{r, fig.width = 3.5, fig.height = 5.5}
p1 <-
  d %>% 
  ggplot(aes(x = z, y = `p(z)`)) +
  geom_area(aes(fill = z <= 1),
            show.legend = F) +
  geom_line(size = 1, color = sl[3]) +
  scale_fill_manual(values = c("transparent", sl[2])) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + 
  labs(title = "Normal Density",
       y = expression(p(italic(z))))

p2 <-
  d %>% 
  ggplot(aes(x = z, y = `Phi(z)`)) +
  geom_area(aes(fill = z <= 1),
            show.legend = F) +
  geom_line(size = 1, color = sl[3]) +
  scale_fill_manual(values = c("transparent", sl[2])) +
  scale_y_continuous(expand = expansion(mult = c(0, 0))) + 
  labs(title = "Cumulative Normal",
       y = expression(Phi(italic(z))))

# combine and adjust with patchwork
library(patchwork)

p1 / p2 & 
  scale_x_continuous(breaks = -2:2) & 
  coord_cartesian(xlim = c(-2.5, 2.5))
```

For both plots, `z` is in a standardized metric (i.e., $z$-score). With the cumulative normal function, the cumulative probability $\Phi(z)$ increases nonlinearly with the $z$-scores such that, much like with the logistic curve, the greatest change occurs around $z = 0$ and tapers off in the tails.

The inverse of $\Phi(x)$ is the *probit* function. As indicated in the above block quote, we'll be making extensive use of the probit function in this chapter for our Bayesian models.

## Modeling ordinal data with an underlying metric variable

> You can imagine that the distribution of ordinal values might not resemble a normal distribution, even though the underlying metric values are normally distributed. Figure 23.1 shows some examples of ordinal outcome probabilities generated from an underlying normal distribution. The horizontal axis is the underlying continuous metric value. Thresholds are plotted as vertical dashed lines, labeled $\theta$. In all examples, the ordinal scale has 7 levels, and hence, there are 6 thresholds. The lowest threshold is set at $\theta_1 = 1.5$ (to separate outcomes 1 and 2), and the highest threshold is set at $\theta_1 = 6.5$ (to separate outcomes 6 and 7). The normal curve in each panel shows the distribution of underlying continuous values. What differs across panels are the settings of means, standard deviations, and remaining thresholds. (p. 672)

The various Figure 23.1 subplots require a lot of ins and outs. We'll start with the top panel and build from there. Here is how we might make the values necessary for the density curve.

```{r}
den <-
  # define the parameters for the underlying normal distribution
  tibble(mu    = 4,
         sigma = 1.5) %>% 
  mutate(strip = str_c("mu==", mu, "~~sigma==", sigma)) %>% 
  # this will allow us to rescale the density in terms of the bar plot
  mutate(multiplier = 26 / dnorm(mu, mu, sigma)) %>% 
  # we need values for the x-axis
  expand(nesting(mu, sigma, strip, multiplier),
         y = seq(from = -1, to = 9, by = .1)) %>% 
  # compute the density values
  mutate(density = dnorm(y, mu, sigma)) %>% 
  # use that multiplier value from above to rescale the density values
  mutate(percent = density * multiplier)

head(den)
```

Before making the data for the bar portion of the plot, we'll need to define the $\theta$-values they'll be placed between. We also need to define the exact points on the $x$-axis from which we'd like those bars to originate. Those points, which we'll call `label_1`, will double as names for the individual bars.

```{r}
(theta_1 <- seq(from = 1.5, to = 6.5, by = 1))
(label_1 <- 1:7)
```

Now we can define the data for the bars.

```{r}
bar <-
  # define the parameters for the underlying normal distribution
  tibble(mu    = 4,
         sigma = 1.5) %>% 
  mutate(strip = str_c("mu==", mu, "~~sigma==", sigma)) %>% 
  # take random draws from the underlying normal distribution
  mutate(draw = map2(mu, sigma, ~rnorm(1e4, mean = .x, sd = .y))) %>% 
  unnest(draw) %>% 
  # bin those draws into ordinal categories defined by `theta_1`
  # and named by `label_1`
  mutate(y = case_when(
    draw  < theta_1[1] ~ label_1[1],
    draw  < theta_1[2] ~ label_1[2],
    draw  < theta_1[3] ~ label_1[3],
    draw  < theta_1[4] ~ label_1[4],
    draw  < theta_1[5] ~ label_1[5],
    draw  < theta_1[6] ~ label_1[6],
    draw >= theta_1[6] ~ label_1[7]
  )) %>% 
  # summarize
  count(y) %>% 
  mutate(percent = (100 * n / sum(n)) %>% round(0))   %>% 
  mutate(percent_label = str_c(percent, "%"),
         percent_max   = max(percent))

head(bar)
```

Make the top subplot.

```{r, fig.width = 5, fig.height = 2.25}
bar %>% 
  ggplot(aes(x = y, y = percent)) +
  geom_area(data = den,
            fill = sl[3]) +
  geom_vline(xintercept = theta_1, color = "white", linetype = 3) +
  geom_col(width = .5, alpha = .85, fill = sl[7]) +
  geom_text(aes(y = percent + 2, label = percent_label),
            size = 3.5) +
  annotate(geom = "text", 
           x = theta_1, y = -6.5, label = theta_1, 
           size = 3) +
  scale_x_continuous(NULL, expand = c(0, 0),
                     breaks = theta_1,
                     labels = parse(text = str_c("theta[", 1:6, "]"))) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(ylim = c(0, 28.5), clip = F) +
  theme(plot.margin = margin(5.5, 5.5, 11, 5.5)) +
  facet_wrap(~ strip, labeller = label_parsed)
```

This method works okay for plotting one or two panels. The sheer number of code lines and moving parts seem unwieldy for plotting four. It'd be convenient if we could save the density information for all four panels in one data object. Here's one way how.

```{r}
den <-
  tibble(panel = 1:4,
         mu    = c(4, 1, 4, 4),
         sigma = c(1.5, 2.5, 1, 3)) %>% 
  mutate(strip = factor(panel,
                        labels  = str_c("mu==", mu, "~~sigma==", sigma),
                        ordered = T)) %>% 
  mutate(multiplier = c(26, 58, 24, 26) / dnorm(mu, mu, sigma)) %>% 
  expand(nesting(panel, mu, sigma, strip, multiplier),
         y = seq(from = -1, to = 9, by = .1)) %>% 
  mutate(density = dnorm(y, mu, sigma)) %>% 
  mutate(percent = density * multiplier)

head(den)
```

Notice we added a `panel` column for indexing the subplots. Next we'll need to define `theta_[i]` and `label_[i]` values for the remaining plots.

```{r}
theta_3 <- c(1.5, 3.1, 3.7, 4.3, 4.9, 6.5)
theta_4 <- c(1.5, 2.25, 3, 5, 5.75, 6.5)

label_3 <- c(1, 2.2, 3.4, 4, 4.6, 5.7, 7)
label_4 <- c(1, 1.875, 2.625, 4, 5.375, 6.125, 7)
```

Since the values are the same for the top two panels, we didn't bother defining a `theta_2` or `label_2`. Now we have all the `theta_[i]` and `label_[i]` values, we'll want to make a function that can use them within `case_when()` for any of the four panels. Here's one way to make such a function, which we'll call `make_ordinal()`.

```{r}
make_ordinal <- function(x, panel) {
  
  if (panel < 3) {
    
    case_when(
      x  < theta_1[1] ~ label_1[1],
      x  < theta_1[2] ~ label_1[2],
      x  < theta_1[3] ~ label_1[3],
      x  < theta_1[4] ~ label_1[4],
      x  < theta_1[5] ~ label_1[5],
      x  < theta_1[6] ~ label_1[6],
      x >= theta_1[6] ~ label_1[7]
    )
    
  } else if (panel == 3) {
    
    case_when(
      x  < theta_3[1] ~ label_3[1],
      x  < theta_3[2] ~ label_3[2],
      x  < theta_3[3] ~ label_3[3],
      x  < theta_3[4] ~ label_3[4],
      x  < theta_3[5] ~ label_3[5],
      x  < theta_3[6] ~ label_3[6],
      x >= theta_3[6] ~ label_3[7]
    )
    
  } else {
    
    case_when(
      x  < theta_4[1] ~ label_4[1],
      x  < theta_4[2] ~ label_4[2],
      x  < theta_4[3] ~ label_4[3],
      x  < theta_4[4] ~ label_4[4],
      x  < theta_4[5] ~ label_4[5],
      x  < theta_4[6] ~ label_4[6],
      x >= theta_4[6] ~ label_4[7]
    )
   }
}
```

Now put those values and our `make_ordinal()` function to work to make the data for the bar plots.

```{r, fig.width = 5, fig.height = 8}
set.seed(23)

bar <-
  tibble(panel = 1:4,
         mu    = c(4, 1, 4, 4),
         sigma = c(1.5, 2.5, 1, 3)) %>% 
  mutate(strip = factor(panel,
                        labels  = str_c("mu==", mu, "~~sigma==", sigma),
                        ordered = T)) %>% 
  mutate(draw = map2(mu, sigma, ~rnorm(1e5, mean = .x, sd = .y))) %>% 
  unnest(draw) %>% 
  mutate(y = map2_dbl(draw, panel, make_ordinal)) %>% 
  group_by(panel, strip) %>% 
  count(y) %>% 
  mutate(percent = (100 * n / sum(n)) %>% round(0)) %>% 
  mutate(percent_label = str_c(percent, "%"),
         percent_max   = max(percent))

head(bar)
```

Like before, we added a `panel` index. As our final preparatory step, we will make something of a super function with which we'll plug the desired information into **ggplot2**, which will then make each subplot. Much of the plotting and data wrangling code will be the same across subplots. As far as I can tell, we only need to vary four parameters. First, we'll want to be able to subset the data by `panel` index. We'll do that with the `panel_n` argument. Second, we'll want to select which of the `theta_[i]` values we'd like to use in `geom_vline()`, `annotate()`, and `scale_x_continuous()`. We'll do that with the `theta` argument. We'll make a `y_second_x` to pin down exactly where below the $x$-axis we'd like to put those secondary axis values defined by the `theta_[i]` values. Finally, we'll want an `ylim_ub` parameter to set the upper limit of the $y$-axis with. The name of our four-parameter super function will be `plot_bar_den()`.

```{r}
plot_bar_den <- function(panel_n, theta, y_second_x, ylim_ub) {
  
  bar %>% 
    filter(panel == panel_n) %>% 
    
    ggplot(aes(x = y)) +
    geom_area(data = den %>% filter(panel == panel_n),
              aes(y = percent),
              fill = sl[3]) +
    geom_vline(xintercept = theta, color = "white", linetype = 3) +
    geom_linerange(aes(ymin = 0, ymax = percent),
                   color = sl[7], alpha = .85, size = 8) +
    geom_text(aes(y = percent + (percent_max / 15), label = percent_label),
              size = 3.5) +
    annotate(geom = "text", x = theta, y = y_second_x,
             label = theta, size = 3) +
    scale_x_continuous(NULL, 
                       breaks = theta,
                       labels = parse(text = str_c("theta[", 1:6, "]")),
                       expand = c(0, 0)) +
    scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
    coord_cartesian(ylim = c(0, ylim_ub), 
                    clip = F) +
    theme(plot.margin = margin(5.5, 5.5, 11, 5.5)) +
    facet_wrap(~ strip, labeller = label_parsed)
  
}
```

Finally, make all four subplots and combine them with **patchwork** syntax!

```{r, eval = F, echo = F}
# approximate values for the bookdown 0.4.0 approach
bar %>% 
  group_by(panel) %>% 
  filter(percent == max(percent)) %>% 
  mutate(y_second_x = (percent + (percent_max / 15)) / -4.1)

# suggested updates
bar %>% 
  group_by(panel) %>% 
  filter(percent == max(percent)) %>% 
  mutate(y_second_x = (percent + (percent_max / 15)) / -5)
```


```{r, fig.width = 5, fig.height = 9}
p1 <- plot_bar_den(panel_n    = 1, 
                   theta      = theta_1, 
                   # y_second_x = -6.75, 
                   y_second_x = -5.55, 
                   ylim_ub    = 28)

p2 <- plot_bar_den(panel_n    = 2, 
                   theta      = theta_1, 
                   # y_second_x = -15.5 * 3/4, 
                   y_second_x = -12.37, 
                   ylim_ub    = 63)

p3 <- plot_bar_den(panel_n    = 3, 
                   theta      = theta_3, 
                   # y_second_x = -6.25 * 3/4, 
                   y_second_x = -5.12, 
                   ylim_ub    = 25.75)

p4 <- plot_bar_den(panel_n    = 4, 
                   theta      = theta_4, 
                   # y_second_x = -6.75 * 3/4, 
                   y_second_x = -5.55, 
                   ylim_ub    = 28)

p1 / p2 / p3 / p4
```

Oh mamma. "*The crucial concept in Figure 23.1 is that the probability of a particular ordinal outcome is the area under the normal curve between the thresholds of that outcome*" (p. 672, *emphasis* in the original). In each of the subplots, we used six thresholds to discretize the continuous data into seven categories. More generally, we need $K$ thresholds to make $K + 1$ ordinal categories. To make this work,

> the idea is that we consider the cumulative area under the normal up the high-side threshold, and subtract away the cumulative area under the normal up to the low-side threshold. Recall that the cumulative area under the standardized normal is denoted $\Phi(z)$, as was illustrated in Figure 15.8 [which we remade at the top of this chapter]. Thus, the area under the normal to the left of $\theta_k$ is $\Phi((\theta_k - \mu) / \sigma)$, and the area under the normal to the left of $\theta_{k - 1}$ is $\Phi((\theta_{k - 1} - \mu) / \sigma)$. Therefore, the area under the normal curve between the two thresholds, which is the probability of outcome $k$, is
>
> $$p(y = k | \mu, \sigma, \{ \theta_j \}) = \Phi((\theta_k - \mu) / \sigma) - \Phi((\theta_{k - 1} - \mu) / \sigma)$$
>
> [This equation] applies even to the least and greatest ordinal values if we append two "virtual" thresholds at $- \infty$ and $+ \infty$...
>
> Thus, a normally distributed underlying metric value can yield a clearly non-normal distribution of discrete ordinal values. This result does not imply that the ordinal values can be treated as if they were themselves metric and normally distributed; in fact it implies the opposite: We might be able to model a distribution of ordinal values as consecutive intervals of a normal distribution on an underlying metric scale with appropriately positioned thresholds. (pp. 674--675)

## The case of a single group

Given a model with no predictors, "if there are $K$ ordinal values, the model has $K + 1$ parameters: $\theta_1, \dots,\theta_{K - 1}, \mu$, and $\sigma$. If you think about it a moment, you’ll realize that the parameter values trade-off and are undetermined" (p. 675). The solution Kruschke took throughout this chapter was to fix the two thresholds at the ends, $\theta_1$  and $\theta_{K - 1}$, to the constants

\begin{align*}
\theta_1 \equiv 1 + 0.5 && \text{and}  && \theta_{K - 1} \equiv K - 0.5.
\end{align*}

For example, all four subplots from Figure 23.1 had $K = 7$ categories, ranging from 1 to 7. Following Kruschke's convention would mean setting the endmost thresholds to

\begin{align*}
\theta_1 \equiv 1.5 && \text{and}  && \theta_6 \equiv 6.5.
\end{align*}

As we'll see, there are other ways to parameterize these models.

### Implementation in ~~JAGS~~ **brms**.

The syntax to fit a basic ordered probit model with `brms::brm()` is pretty simple.

```{r, eval = F}
fit <-
  brm(data = my_data,
      family = cumulative(probit),
      y ~ 1,
      prior(normal(0, 4), class = Intercept))
```

The `family = cumulative(probit)` tells **brms** you'd like to use the probit link for the ordered-categorical data. It's important to specify `probit` because the **brms** default is to use the `logit` link, instead. We'll talk more about that approach at the end of this chapter.

Remember how, at the end of the last section, we said there are other ways to parameterize the ordered probit model? As it turns out, **brms** does not follow Kruschke's approach for fixing the thresholds on the ends. Rather, **brms** freely estimates all thresholds, $\theta_1,...,\theta_{K - 1}$, by fixing $\mu = 0$ and $\sigma = 1$. That is, instead of estimating $\mu$ and $\sigma$ from the normal cumulative density function $\Phi(x)$, `brms::brm()` uses the standard normal cumulative density function $\Phi(z)$.

This all probably seems abstract. We'll get a lot of practice comparing the two approaches as we go along. Each has its strengths and weaknesses. At this point, the thing to get is that when fitting a single-group ordered-probit model with the `brm()` function, there will be no priors for $\mu$ and $\sigma$. We only have to worry about setting the priors for all $K - 1$ thresholds. And because those thresholds are conditional on $\Phi(z)$, we should think about their priors with respect to the scale of standard normal distribution. Thus, to continue on with Kruschke's minimally-informative prior approach, something like `prior(normal(0, 4), class = Intercept)` might be a good starting place. Do feel free to experiment with different settings.

### Examples: Bayesian estimation recovers true parameter values.

The data for Kruschke's first example come from his `OrdinalProbitData-1grp-1.csv` file. Load the data.

```{r, warning = F, message = F}
my_data_1 <- read_csv("data.R/OrdinalProbitData-1grp-1.csv")

glimpse(my_data_1)
```

Plot the distribution for `Y`.

```{r, fig.width = 4, fig.height = 2.5}
my_data_1 %>% 
  mutate(Y = factor(Y)) %>% 
  
  ggplot(aes(x = Y)) +
  geom_bar(fill = sl[5]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)))
```

It looks a lot like the distribution of the data from one of the panels from Figure 23.1. Load **brms**.

```{r, warning = F, message = F}
library(brms)
```

Fit the first cumulative-probit model.

```{r fit23.1}
fit23.1 <-
  brm(data = my_data_1,
      family = cumulative(probit),
      Y ~ 1,
      prior(normal(0, 4), class = Intercept),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 23,
      file = "fits/fit23.01")
```

Examine the model summary.

```{r}
print(fit23.1)
```

The **brms** output for these kinds of models names the thresholds $\theta_{[i]}$ as `Intercept[i]`. Again, whereas Kruschke identified his model by fixing $\theta_1 = 1.5$ (i.e., $1 + 0.5$) and $\theta_6 = 5.5$ (i.e., $6 - 0.5$), we freely estimated all six thresholds by using the cumulative density function for the standard normal. As a result, our thresholds are in a different metric from Kruschke's.

Let's extract the posterior draws.

```{r}
draws <- as_draws_df(fit23.1)

glimpse(draws)
```

Wrangle the `draws` a bit.

```{r, warning = F}
draws <-
  draws %>%
  select(.draw, `b_Intercept[1]`:`b_Intercept[6]`)
```

Here's our **brms** version of the bottom plot of Figure 23.2.

```{r, fig.width = 5, fig.height = 2}
# compute the posterior means for each threshold
means <-
  draws %>% 
  summarise_at(vars(`b_Intercept[1]`:`b_Intercept[6]`), mean) %>% 
  pivot_longer(everything(),
               values_to = "mean")

# wrangle
draws %>% 
  pivot_longer(-.draw, values_to = "threshold") %>% 
  group_by(.draw) %>% 
  mutate(theta_bar = mean(threshold)) %>% 
  
  # finally we plot
  ggplot(aes(x = threshold, y = theta_bar, color = name)) +
  geom_vline(data = means,
             aes(xintercept = mean, color = name),
             linetype = 2) +
  geom_point(alpha = 1/10) +
  scale_color_scico_d(palette = "lajolla", begin = .25) +
  ylab("mean threshold") +
  theme(legend.position = "none")
```

The initial `means` data at the top contains the $\theta_i$-specific means, which we used to make the dashed vertical lines with `geom_vline()`. Did you see what we did there with those `group_by()` and `mutate()` lines? That's how we computed the mean threshold within each step of the HMC chain, what Kruschke (p. 680) denoted as $\bar \theta (s) = \sum_k^{K-1} \theta_k (s) / (K - 1)$, where $s$ refers to particular steps in the HMC chain.

Perhaps of greater interest, you might have noticed how different our plot is from the one in the text. We might should compare the results of our **brms** parameterization of $\theta_{[i]}$ with one based on the parameterization in the text in an expanded version of the bottom plot of Figure 23.2. To convert our **brms** output to match Kruschke's, we'll rescale our $\theta_{[i]}$ draws with help from the `scales::rescale()` function, about which you might learn more [here](https://www.rdocumentation.org/packages/scales/versions/0.4.1/topics/rescale).

```{r, fig.width = 5, fig.height = 4, message = F}
# primary data wrangling
p <-
  bind_rows(
    # brms parameterization
    draws %>% 
      pivot_longer(-.draw, values_to = "threshold") %>% 
  group_by(.draw) %>% 
      mutate(theta_bar = mean(threshold)),
    # Kruschke's parameterization
    draws %>% 
      pivot_longer(-.draw, values_to = "threshold") %>% 
  group_by(.draw) %>% 
      mutate(threshold = scales::rescale(threshold, to = c(1.5, 6.5))) %>% 
      mutate(theta_bar = mean(threshold))
  ) %>% 
  # add an index
  mutate(model = rep(c("brms parameterization", "Kruschke's parameterization"), each = n() / 2))

# compute the means by model and threshold for the vertical lines
means <-
  p %>% 
  ungroup() %>% 
  group_by(model, name) %>% 
  summarise(mean = mean(threshold))

# plot!
p %>% 
  ggplot(aes(x = threshold, y = theta_bar)) +
  geom_vline(data = means,
             aes(xintercept = mean, color = name),
             linetype = 2) +
  geom_point(aes(color = name),
             alpha = 1/10, size = 1/2) +
  scale_color_scico_d(palette = "lajolla", begin = .25) +
  ylab("mean threshold") +
  theme(legend.position = "none") +
  facet_wrap(~ model, ncol = 1, scales = "free")
```

We can take our rescaling approach further to convert the posterior distributions for $\mu$ and $\sigma$ from the **brms** $\operatorname{Normal}(0, 1)$ constants to the metric from Kruschke's approach. Say $y_1$ and $y_2$ are two draws from some Gaussian and $z_1$ and $z_2$ are their corresponding $z$-scores. Here's how to solve for $\sigma$.

\begin{align*}
z_1 - z_2 & = \frac{(y_1 - \mu)}{\sigma} - \frac{(y_2 - \mu)}{\sigma} \\
          & = \frac{(y_1 - \mu) - (y_2 - \mu)}{\sigma} \\
          & = \frac{y_1 - \mu - y_2 + \mu}{\sigma} \\
          & = \frac{y_1 - y_2}{\sigma}, \;\; \text{therefore} \\
\sigma    & = \frac{y_1 - y_2}{z_1 – z_2}.
\end{align*}

If you'd like to compute $\mu$, it's even simpler.

\begin{align*}
z_1 & = \frac{y_1 - \mu}{\sigma} \\
z_1 \sigma & = y_1 - \mu \\
z_1 \sigma + \mu & = y_1, \;\; \text{therefore}  \\
\mu & = y_1 - z_1 \sigma
\end{align*}

Big shout out to my math-stats savvy friends academic twitter for the formulas, especially [Ph.Demetri](https://twitter.com/PhDemetri), [Lukas Neugebauer](https://twitter.com/lukasneugebau), and [Brenton Wiernik](https://twitter.com/bmwiernik) for walking the formulas out (see [this twitter thread](https://twitter.com/SolomonKurz/status/1219649489016958976)). For our application, `Intercept[1]` and `Intercept[6]` will be our two $z$-scores and Kruschke's 1.5 and 6.5 will be their corresponding $y$-values.

```{r, fig.width = 8, fig.height = 2.25, message = F, warning = F}
library(tidybayes)

draws %>% 
  select(.draw, `b_Intercept[1]`, `b_Intercept[6]`) %>% 
  mutate(`y[1]` = 1.5,
         `y[6]` = 6.5) %>% 
  mutate(mu    = `y[1]` - `b_Intercept[1]` * 1,
         sigma = (`y[1]` - `y[6]`) / (`b_Intercept[1]` - `b_Intercept[6]`)) %>% 
  mutate(`(mu-2)/sigma` = (mu - 2) / sigma) %>% 
  pivot_longer(mu:`(mu-2)/sigma`) %>% 
  mutate(name = factor(name, levels = c("mu", "sigma", "(mu-2)/sigma"))) %>%
  
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8],
               normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)
```

Our results are similar to Kruschke's. Given we used a different algorithm, a different parameterization, and different priors, I'm not terribly surprised they're a little different. If you have more insight on the matter or have spotted a flaw in this method, [please share with the rest of us](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues).

It's unclear, to me, how we'd interpret the effect size. The difficulty isn't that Kruschke's comparison of $C = 2.0$ is arbitrary, but that we can only interpret the comparison given the model assumption of $\theta_1 = 1.5$ and $\theta_6 = 6.5$. If your theory doesn't allow you to understand the meaning of those constants and why you'd prefer them to slightly different ones, you'd be fooling yourself if you attempted to interpret any effect sizes conditional on those values. Proceed with caution, friends.

In the large paragraph on the lower part of page 679, Kruschke discussed why the thresholds tend to have nontrivial covariances. This is what he was trying to convey with the bottom subplot in Figure 23.2. Just for practice, we might also explore the correlation matrix among the thresholds with a customized `GGally::ggpairs()` plot.

```{r, fig.width = 6, fig.height = 6, warning = F, message = F}
# load
library(GGally)

# customize
my_upper <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) + 
    geom_point(size = 1/5, alpha = 1/5, color = sl[7])
}

my_diag <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) + 
    geom_density(size = 0, fill = sl[3]) +
    scale_x_continuous(NULL, breaks = NULL) +
    scale_y_continuous(NULL, breaks = NULL)
}

my_lower <- function(data, mapping, ...) {
  
  # get the x and y data to use the other code
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  
  # compute the correlations
  corr <- cor(x, y, method = "p", use = "pairwise")
  
  # plot the cor value
  ggally_text(
    label = formatC(corr, digits = 2, format = "f") %>% str_replace(., "0\\.", "."),
    mapping = aes(),
    color = "black",
    size = 4) +
    scale_x_continuous(NULL, breaks = NULL) +
    scale_y_continuous(NULL, breaks = NULL)
}

# wrangle
as_draws_df(fit23.1) %>% 
  select(contains("Intercept")) %>% 
  set_names(str_c("theta[", 1:6, "]")) %>% 
  
  # plot!
  ggpairs(upper = list(continuous = my_upper),
          diag = list(continuous = my_diag),
          lower = list(continuous = my_lower),
          labeller = label_parsed) +
  ggtitle("The thresholds are highly correlated")
```

```{r, echo = F, eval = F}
# Valid distributional parameters are: 'mu', 'disc'
# fitted(fit23.1)
```

Kruschke didn't do this in the text, but it might be informative to plot the probability distributions for the seven categories from `Y` (i.e., $p(y = k | \mu = 0, \sigma = 1, \{ \theta_i \})$).

```{r, fig.width = 6, fig.height = 2.25, warning = F, message = F}
library(tidybayes)

draws %>% 
  select(-.draw) %>% 
  mutate_all(.funs = ~pnorm(. ,0, 1)) %>% 
  transmute(`p[Y==1]` = `b_Intercept[1]`,
            `p[Y==2]` = `b_Intercept[2]` - `b_Intercept[1]`,
            `p[Y==3]` = `b_Intercept[3]` - `b_Intercept[2]`,
            `p[Y==4]` = `b_Intercept[4]` - `b_Intercept[3]`,
            `p[Y==5]` = `b_Intercept[5]` - `b_Intercept[4]`,
            `p[Y==6]` = `b_Intercept[6]` - `b_Intercept[5]`,
            `p[Y==7]` = 1 - `b_Intercept[6]`) %>% 
  set_names(1:7) %>% 
  pivot_longer(everything(), names_to = "Y") %>% 
  
  ggplot(aes(x = value, y = Y)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95, 
               fill = sl[4], color = sl[8], size = 1/2, height = 2.5) +
  scale_x_continuous(expression(italic(p)*"["*Y==italic(i)*"]"),
                     breaks = 0:5 / 5,
                     expand = c(0, 0), limits = c(0, 1))
```

Happily, the model produces data that look a lot like those from which it was generated.

```{r, fig.width = 4, fig.height = 2.5}
set.seed(23)

draws %>% 
  mutate(z = rnorm(n(), mean = 0, sd = 1)) %>% 
  mutate(Y = case_when(
    z < `b_Intercept[1]` ~ 1,
    z < `b_Intercept[2]` ~ 2,
    z < `b_Intercept[3]` ~ 3,
    z < `b_Intercept[4]` ~ 4,
    z < `b_Intercept[5]` ~ 5,
    z < `b_Intercept[6]` ~ 6,
    z >= `b_Intercept[6]` ~ 7
  ) %>% as.factor(.)) %>% 

  ggplot(aes(x = Y)) +
  geom_bar(fill = sl[5]) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)))
```

Along similar lines, we can use the `pp_check()` function to make a version of the upper right panel of Figure 23.2. The `type = "bars"` argument will allow us to summarize the posterior predictions as a dot (mean) and standard error bars superimposed on a bar plot of the original data. Note how this differs a little from Kruschke's use of the posterior median and 95% HDIs. The `ndraws = 1000` argument controls how many posterior predictions we wanted to summarize over. The rest is just formatting.

```{r, fig.width = 4, fig.height = 2.5, message = F, warning = F}
bayesplot::color_scheme_set(sl[2:7])

set.seed(23)

pp_check(fit23.1, type = "bars", ndraws = 1000, fatten = 2) +
  scale_x_continuous("y", breaks = 1:7) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Data with posterior predictions",
          subtitle = "N = 100") +
  theme(legend.background = element_blank(),
        legend.position = c(.9, .8))
```

Load the data for the next model.

```{r, warning = F, message = F}
my_data_2 <- read_csv("data.R/OrdinalProbitData-1grp-2.csv")
```

Since we're reusing all the specifications from the last model for this one, we can just use `update()`.

```{r fit23.2}
fit23.2 <-
  update(fit23.1, 
         newdata = my_data_2,
         iter = 3000, warmup = 1000, chains = 4, cores = 4,
         seed = 23,
         file = "fits/fit23.02")
```

```{r}
print(fit23.2)
```

Extract and wrangle the posterior draws.

```{r, warning = F}
draws <- 
  as_draws_df(fit23.2) %>%
  select(.draw, `b_Intercept[1]`:`b_Intercept[6]`)
```

Now we might compare the **brms** parameterization of $\theta_{[i]}$ with Kruschke's parameterization in an expanded version of the bottom plot of Figure 23.3. As we'll be making a lot of these plots throughout this chapter, it might be worthwhile to just make a custom function. We'll call it `compare_thresholds()`.

```{r}
compare_thresholds <- function(data, lb = 1.5, ub = 6.5) {
  
  # we have two parameters:
  # lb = lower bound
  # ub = upper bound
  
  # primary data wrangling
  p <-
    bind_rows(
      data %>% 
        pivot_longer(-.draw, values_to = "threshold") %>% 
        group_by(.draw) %>% 
        mutate(theta_bar = mean(threshold)),
      data %>% 
        pivot_longer(-.draw, values_to = "threshold") %>% 
        group_by(.draw) %>% 
        mutate(threshold = scales::rescale(threshold, to = c(lb, ub))) %>% 
        mutate(theta_bar = mean(threshold))
    ) %>% 
    mutate(model = rep(c("brms parameterization", "Kruschke's parameterization"), each = n() / 2)) 
  
  # compute the means by model and threshold for the vertical lines
  means <-
    p %>% 
    ungroup() %>% 
    group_by(model, name) %>% 
    summarise(mean = mean(threshold))
  
  # plot!
  p %>% 
    ggplot(aes(x = threshold, y = theta_bar)) +
    geom_vline(data = means,
               aes(xintercept = mean, color = name),
               linetype = 2) +
    geom_point(aes(color = name),
               alpha = 1/10, size = 1/2) +
    scale_color_scico_d(palette = "lajolla", begin = .25) +
    ylab("mean threshold") +
    theme(legend.position = "none") +
    facet_wrap(~ model, ncol = 1, scales = "free")
  
}
```

Take that puppy for a spin.

```{r, fig.width = 5, fig.height = 4, message = F}
draws %>% 
  compare_thresholds(lb = 1.5, ub = 6.5)
```

Oh man, that works sweet. Now let's use the same parameter-transformation approach from before to get our un-standardized posteriors for $\mu$, $\sigma$, and the effect size.

```{r, fig.width = 8, fig.height = 2.25}
draws %>% 
  select(.draw, `b_Intercept[1]`, `b_Intercept[6]`) %>% 
  mutate(`y[1]` = 1.5,
         `y[6]` = 6.5) %>% 
  mutate(mu    = `y[1]` - `b_Intercept[1]` * 1,
         sigma = (`y[1]` - `y[6]`) / (`b_Intercept[1]` - `b_Intercept[6]`)) %>% 
  mutate(`(mu-4)/sigma` = (mu - 4) / sigma) %>% 
  pivot_longer(mu:`(mu-4)/sigma`) %>% 
  mutate(name = factor(name, levels = c("mu", "sigma", "(mu-4)/sigma"))) %>%
  
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8],
               normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)
```

Use `pp_check()` to make our version of the upper-right panel of Figure 23.3.

```{r, fig.width = 4, fig.height = 2.5, message = F, warning = F}
set.seed(23)

pp_check(fit23.2, type = "bars", ndraws = 1000, fatten = 2) +
  scale_x_continuous("y", breaks = 1:7) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Data with posterior predictions",
          subtitle = "N = 70") +
  theme(legend.background = element_blank(),
        legend.position = c(.9, .8))
```

Just as in the text, "the posterior predictive distribution in the top-right subpanel accurately describes the bimodal distribution of the outcomes" (p. 680).

Here are the probability distributions for each of the 7 categories of `Y`.

```{r, fig.width = 6, fig.height = 2.25}
draws %>% 
  select(-.draw) %>% 
  mutate_all(.funs = ~pnorm(. ,0, 1)) %>% 
  transmute(`p[Y==1]` = `b_Intercept[1]`,
            `p[Y==2]` = `b_Intercept[2]` - `b_Intercept[1]`,
            `p[Y==3]` = `b_Intercept[3]` - `b_Intercept[2]`,
            `p[Y==4]` = `b_Intercept[4]` - `b_Intercept[3]`,
            `p[Y==5]` = `b_Intercept[5]` - `b_Intercept[4]`,
            `p[Y==6]` = `b_Intercept[6]` - `b_Intercept[5]`,
            `p[Y==7]` = 1 - `b_Intercept[6]`) %>% 
  set_names(1:7) %>% 
  pivot_longer(everything(), names_to = "Y") %>% 
  
  ggplot(aes(x = value, y = Y)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8], size = 1/2) +
  scale_x_continuous(expression(italic(p)*"["*Y==italic(i)*"]"), 
                     breaks = 0:5 / 5,
                     expand = c(0, 0), limits = c(0, 1))
```

Before we move on, it might be helpful to nail down what the thresholds mean within the context of our **brms** parameterization. To keep things simple, we'll focus on their posterior means.

```{r, fig.width = 5, fig.height = 3}
tibble(x = seq(from = -3.5, to = 3.5, by = .01)) %>%
  mutate(d = dnorm(x)) %>% 
  
  ggplot(aes(x = x, ymin = 0, ymax = d)) +
  geom_ribbon(fill = sl[3]) +
  geom_vline(xintercept = fixef(fit23.2)[, 1], color = "white", linetype = 3) +
  scale_x_continuous(NULL, breaks = fixef(fit23.2)[, 1],
                     labels = parse(text = str_c("theta[", 1:6, "]"))) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Standard normal distribution underlying the ordinal Y data:",
          subtitle = "The dashed vertical lines mark the posterior means for the thresholds.") +
  coord_cartesian(xlim = c(-3, 3))
```

Compare that to Figure 23.1.

#### Not the same results as pretending the data are metric.

"In some conventional approaches to ordinal data, the data are treated as if they were metric and normally distributed" (p. 681). Here's what that `brms::brm()` model might look like using methods from back in Chapter 16. First, we'll define our `stanvars`.

```{r}
mean_y <- mean(my_data_1$Y)
sd_y   <- sd(my_data_1$Y)

stanvars <- 
  stanvar(mean_y, name = "mean_y") + 
  stanvar(sd_y,   name = "sd_y")
```

Fit the model.

```{r fit23.3}
fit23.3 <-
  brm(data = my_data_1,
      family = gaussian,
      Y ~ 1,
      prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept),
                prior(normal(0, sd_y), class = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      seed = 23,
      file = "fits/fit23.03")
```

Check the results.

```{r}
print(fit23.3)
```

As Kruschke indicated in the text, it yielded a distributional mean of about 1.95 and a standard deviation of about 1.41. Here we'll use a posterior predictive check to compare histograms of data generated from this model to that of the original data.

```{r, fig.width = 6, fig.height = 3.5}
pp_check(fit23.3, type = "hist", ndraws = 10, binwidth = 1) +
  scale_x_continuous(breaks = seq(from = -3, to = 7, by = 2)) +
  theme(legend.position = c(.9, .15))
```

Yeah, that's not a good fit. We won't be conducting a $t$-test like Kruschke did on page 681. But we might compromise and take a look at the marginal distribution of the intercept (i.e., for $\mu$) and its difference from 2, the reference value.

```{r, fig.width = 6, fig.height = 2.25, warning = F}
as_draws_df(fit23.3) %>% 
  mutate(`2 - b_Intercept` = 2 - b_Intercept,
         `effect size`     = (2 - b_Intercept) / sigma) %>% 
  pivot_longer(`2 - b_Intercept`:`effect size`) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8], normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~ name, scales = "free")
```

Yes indeed, 2 is a credible value for the intercept. And as reported in the text, we got a very small $d$ effect size. Now we repeat the process for the second data set.

```{r fit23.4}
mean_y <- mean(my_data_2$Y)
sd_y   <- sd(my_data_2$Y)

stanvars <- 
  stanvar(mean_y, name = "mean_y") + 
  stanvar(sd_y,   name = "sd_y")

fit23.4 <-
  update(fit23.3,
         newdata = my_data_2,
         chains = 4, cores = 4,
         stanvars = stanvars, 
         seed = 23,
         file = "fits/fit23.04")
```

Let's just jump to the plot. This time we're comparing the `b_Intercept` to the value of 4.0.

```{r, fig.width = 6, fig.height = 2.25, warning = F}
as_draws_df(fit23.4) %>% 
  mutate(`2 - b_Intercept` = 4 - b_Intercept,
         `effect size`     = (4 - b_Intercept) / sigma) %>% 
  pivot_longer(`2 - b_Intercept`:`effect size`) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8], normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~ name, scales = "free")
```

As in the text, our $d$ is centered around 0.3. Let's use a posterior predictive check to see how well `fit23.4` summarized these data.

```{r, fig.width = 6, fig.height = 3.5}
pp_check(fit23.4, type = "hist", ndraws = 10, binwidth = 1) +
  scale_x_continuous(breaks = seq(from = -3, to = 7, by = 2)) +
  theme(legend.position = c(.9, .15))
```

The histograms aren't as awful as the ones from the previous model. But they're still not great. We might further inspect the model misspecification with a cumulative distribution function overlay, this time comparing `fit23.2` directly to `fit23.4`.

```{r, fig.width = 6.25, fig.height = 2.75, warning = F, message = F}
p1 <-
  pp_check(fit23.2, type = "ecdf_overlay", ndraws = 50) +
  ggtitle("Cumulative-normal (fit23.2)")

p2 <-
  pp_check(fit23.4, type = "ecdf_overlay", ndraws = 50) +
  ggtitle("Conventional-normal (fit23.4)")
  
(p1 + p2 & 
  scale_x_continuous(breaks = 0:7, limits = c(0, 7)) &
  scale_y_continuous(expand = c(0, 0)) &
  theme(title = element_text(size = 10.5))) + 
  plot_layout(guides = 'collect')
```

"Which of the analyses yields the more trustworthy conclusion? The one that describes the data better. In these cases, there is no doubt that the cumulative-normal model is the better description of the data" than the conventional Gaussian model (p. 682).

#### Ordinal outcomes versus Likert scales.

Just for fun,

> rate how much you agree with the statement, "Bayesian estimation is more informative than null-hypothesis significance testing," by selecting one option from the following: 1 = strongly disagree; 2 = disagree; 3 = undecided; 4 = agree; 5 = strongly agree. This sort of ordinal response interface is often called a Likert-type response [@likertTechniqueMeasurementAttitudes1932], pronounced LICK-ert not LIKE-ert). Sometimes, it is called a Likert "scale" but the term "scale" in this context is more properly reserved for referring to an underlying metric variable that is indicated by the arithmetic mean of several meaningfully related Likert-type responses [e.g., @carifio2007ten; @carifioResolving50yearDebate2008; @normanLikertScalesLevels2010]. (p. 681)

Kruschke then briefly introduced how one might combine several such meaningfully-related Likert-type responses with latent variable methods. He then clarified this text will not explore that approach, further. The current version of **brms** (i.e., [2.12.0](https://CRAN.R-project.org/package=brms/news/news.html)) has very limited latent variable capacities. However, they are in the works. Interested modelers can follow Bürkner's progress in [GitHub issue #304](https://github.com/paul-buerkner/brms/issues/304). He also has a [-@burknerBayesianItemResponse2020] [paper](https://arxiv.org/pdf/1905.09501.pdf) on how one might use **brms** to fit item response theory models, which can be viewed as a special family of latent variable models. One can also fit Bayesian latent variable models with the [**blavaan** package](https://faculty.missouri.edu/~merklee/blavaan/).

## The case of two groups

> In both examples in the preceding text, the two groups of outcomes were on the same ordinal scale. In the first example, both questionnaire statements were answered on the same disagree–agree scale. In the second example, both groups responded on the same very unhappy–very happy scale. Therefore, we assume that both groups have the same underlying metric variable with the same thresholds. (p. 682)

### Implementation in ~~JAGS~~ **brms**.

The `brm()` syntax for adding a single categorical predictor to an ordered-probit model is much like that for any other likelihood. We just add the variable name to the right side of the `~` in the `formula` argument. If you're like me and like to use the verbose `1` syntax for your model intercepts--thresholds in these models--just use the `+` operator between them. For example, this is what it'd look like for an ordered-categorical criterion `y` and a single categorical predictor `x`.

```{r, eval = F}
fit <-
  brm(data = my_data,
      family = cumulative(probit),
      y ~ 1 + x,
      prior = c(prior(normal(0, 4), class = Intercept),
                prior(normal(0, 4), class = b)))
```

Also of note, we've expanded the `prior` section to include a line for `class = b`. As with the thresholds, interpret this prior through the context of the underlying standard normal cumulative distribution, $\Phi(z)$. Note the interpretation, though. By **brms** defaults, the underlying Gaussian for the reference category of `x` will be $\operatorname{Normal}(0, 1)$. Thus whatever parameter value you get for the other categories in `x`, those will be standardized mean differences, making them a kind of effect size.

Note, the above all presumes you're only interested in comparing means between groups. Things get more complicated if you want groups to vary by $\sigma$, too. Hold on tight!

First, look back at the output from `print(fit1)` or `print(fit2)`. The second line for both reads: `Links: mu = probit; disc = identity`. Hopefully the `mu = probit` part is no surprise. Probit regression is the primary focus of this chapter. But check out the `disc = identity` part and notice that nowhere in there is there any mention of `sigma = identity` like we get when treating the criterion as metric as in conventional Gaussian models (i.e., execute `print(fit3)` or `print(fit4)`).

Yes, there is a relationship between `disc` and `sigma`. `disc` is shorthand for *discrimination*. The term comes from the item response theory (IRT) literature and discrimination is the inverse of $\sigma$ (see Bürkner's [*Bayesian item response modelling in R with brms and Stan*](https://arxiv.org/pdf/1905.09501.pdf)). In IRT, discrimination is often denoted $a$ or $\alpha$. Here I'll adopt the latter, making $\sigma = 1 / \alpha$. But focusing back on **brms** summary output, notice how both `disc` and `sigma` are modeled using the identity link. If you recall from earlier chapters, we switched to the log link to constrain the values to zero and above when we allowed $\sigma$ to vary across groups. It's the same thing for our discrimination parameter, $\alpha$. Because $\alpha$ should always be zero or above, **brms** defaults to the log link when modeling it with predictors.

As with $\sigma$ in conventional Gaussian models, we'll be using some version of the `bf()` syntax when modeling the discrimination parameter in **brms**. For a general introduction to what Bürkner calls distributional modeling, see his [-@Bürkner2022Distributional] vignette, [*Estimating distributional models with brms*](https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html). In the case of the discrimination parameter for the cumulative model, we'll want more focused instructions. Happily, @burknerOrdinalRegressionModels2019 have our backs. We read:

> Conceptually, unequal variances are incorporated in the model by specifying an additional regression formula for the variance component of the latent variable $\tilde Y$. In brms, the parameter related to latent variances is called *disc* (short for "discrimination"), following conventions in item response theory. Note that disc is not the variance itself, but the inverse of the standard deviation, $s.$ That is, $s = 1/ \text{disc}$. Further, because disc must be strictly positive, it is by default modeled on the log scale.
>
> Predicting auxiliary parameters (parameters of the distribution other than the mean/location) in brms is accomplished by passing multiple regression formulas to the `brm()` function. Each formula must first be wrapped in another function, `bf()` or `lf()` (for "linear formula")--depending on whether it is a main or an auxiliary formula, respectively. The formulas are then combined and passed to the `formula` argument of `brm()`. Because the standard deviation of the latent variable is fixed to 1 for the baseline [group, disc cannot be estimated for the baseline group]. We must therefore ensure that disc is estimated only for [non-baseline groups]. To do so, we omit the intercept from the model of disc by writing `0 + ...` on the right-hand side of the regression formula. By default, R applies cell-mean coding to factors in formulas without an intercept. That would lead to disc being estimated for [all groups], so we must deactivate it via the `cmc` argument of `lf()`. (pp. 11--12)

Here's what that might look like.

```{r, eval = F}
fit <-
  brm(data = my_data,
      family = cumulative(probit),
      bf(y ~ 1 + x) +
        lf(disc ~ 0 + x, cmc = F),
      prior = c(prior(normal(0, 4), class = Intercept),
                prior(normal(0, 4), class = b),
                prior(normal(0, 4), class = b, dpar = disc)))
```

Note how when using the `disc ~ 0 + ...` syntax, the `disc` parameters are of `class = b` within the `prior()` function. If you'd like to assign them priors differing from the other `b` parameters, you'll need to specify `dpar = disc`. Again, though the mean structure for this model is on the probit scale, the discrimination structure is on the log scale. Recalling that $\sigma = 1/\alpha$, which means $\alpha = 1/\sigma$, and also that we're modeling $\log (\alpha)$, the priors for the standard deviations of the non-reference category groups are on the scale of $\log (1 / \sigma)$.

To get a better sense of how one might set a prior on such a scale, we might compare $\sigma$, $\alpha$, and $\log (\alpha)$. Here are the density and cumulative density functions for $\operatorname{Normal}(0, 0.5)$, $\operatorname{Normal}(0, 1)$, and $\operatorname{Normal}(0, 2)$.

```{r, fig.width = 6.5, fig.height = 4}
tibble(mu    = 0,
       sigma = c(0.5, 1, 2)) %>%
  expand(nesting(mu, sigma),
         y = seq(from = -5, to = 5, by = 0.1)) %>% 
  mutate(`p(y)`   = dnorm(y, mu, sigma),
         `Phi(y)` = pnorm(y, mu, sigma)) %>% 
  mutate(alpha = 1 / sigma,
         loga  = log(1 / sigma)) %>% 
  mutate(label = str_c("list(sigma==", sigma, ",alpha==", alpha, ",log(alpha)==", round(loga, 2), ")")) %>% 
  pivot_longer(`p(y)`:`Phi(y)`) %>% 
  
  ggplot(aes(x = y, y = value)) +
  geom_line(size = 1.5, color = sl[7]) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  coord_cartesian(xlim = c(-4, 4)) +
  facet_grid(name ~ label, labeller = label_parsed, switch = "y")
```

Put another way, here's how $\alpha$ and $\log (\alpha)$ scale on values of $\sigma$ ranging from 0.0001 to 10.

```{r, fig.width = 6, fig.height = 2.5}
tibble(sigma = seq(from = 0.0001, to = 10, by = 0.01)) %>%
  mutate(alpha        = 1 / sigma,
         `log(alpha)` = log(1 / sigma)) %>% 
  pivot_longer(-sigma, names_to = "labels") %>% 
  
  ggplot(aes(x = sigma, y = value)) +
  geom_hline(yintercept = 0, color = sl[3], linetype = 2) +
  geom_vline(xintercept = 0, color = sl[3], linetype = 2) +
  geom_line(size = 1.5, color = sl[7]) +
  coord_cartesian(ylim = c(-2, 10)) +
  facet_grid(~ labels, labeller = label_parsed)
```

When $\sigma$ goes below 1, both explode upward. As $\sigma$ increases, $\alpha$ asymptotes at zero and $\log (\alpha)$ slowly descends below zero. Put another way, here is how $\sigma$ scales as a function of $\log (\alpha)$.

```{r, fig.width = 6, fig.height = 2.5}
tibble(`log(alpha)` = seq(from = -3, to = 3, by = 0.01)) %>%
  mutate(sigma = 1 / exp(`log(alpha)`)) %>%
  
  ggplot(aes(x = `log(alpha)`, y = sigma)) +
  geom_hline(yintercept = 0, color = sl[3], linetype = 2) +
  geom_line(size = 1.5, color = sl[7]) +
  labs(x = expression(log(alpha)),
       y = expression(sigma)) +
  coord_cartesian(xlim = c(-2.5, 2.5),
                  ylim = c(0, 10))
```

In the context where the underlying distribution for the reference category will be the standard normal, it seems like a $\operatorname{Normal}(0, 1)$ prior would be fairly permissive for $\log (\alpha)$. This is what I will use going forward. Choose your priors with care.

### Examples: Not funny.

Load the data for the next model.

```{r, message = F}
my_data <- read_csv("data.R/OrdinalProbitData1.csv")

glimpse(my_data)
```

Fit the first ordinal probit model with group-specific $\mu$ and $\sigma$ values for the underlying normal distributions for the ordinal variable `Y`.

```{r fit23.5}
fit23.5 <-
  brm(data = my_data,
      family = cumulative(probit),
      bf(Y ~ 1 + X) +
        lf(disc ~ 0 + X, cmc = FALSE),
      prior = c(prior(normal(0, 4), class = Intercept),
                prior(normal(0, 4), class = b),
                prior(normal(0, 1), class = b, dpar = disc)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 23,
      file = "fits/fit23.05")
```

Look over the summary.

```{r}
print(fit23.5)
```

Because our fancy new parameter `disc_XB` is on the $\log (\alpha)$ scale, we can convert it to the $\sigma$ scale with $\frac{1}{\exp (\log \alpha)}$. For a quick and dirty example, here it is with the posterior mean.

```{r}
1 / (exp(fixef(fit23.5)["disc_XB", 1]))
```

Before we follow along with Kruschke, let's hammer the meaning of these model parameters home. Here is a density plot of the two underlying latent distributions for `Y`, given `X`. We'll throw in the thresholds for good measure. To keep things simple, we'll just express the distributions in terms of the posterior means of each parameter.

```{r, fig.width = 4.25, fig.height = 3}
tibble(X     = LETTERS[1:2],
       mu    = c(0, fixef(fit23.5)["XB", 1]),
       sigma = c(1, 1 / (exp(fixef(fit23.5)["disc_XB", 1])))) %>% 
  expand(nesting(X, mu, sigma),
         y = seq(from = -5, to = 5, by = 0.1)) %>% 
  mutate(d = dnorm(y, mu, sigma)) %>% 
  
  ggplot(aes(x = y, y = d, fill = X)) +
  geom_area(alpha = 2/3) +
  geom_vline(xintercept = fixef(fit23.5)[1:4, 1], linetype = 3, color = sl[9]) +
  scale_fill_scico_d(palette = "lajolla", begin = .33, end = .67) +
  scale_x_continuous(sec.axis = dup_axis(breaks = fixef(fit23.5)[1:4, 1] %>% as.double(),
                                         labels = parse(text = str_c("theta[", 1:4, "]")))) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Underlying latent scale for Y, given X",
       x = NULL) +
  theme(axis.ticks.x.top = element_blank())
```

Returning to our previous workflow, extract the posterior draws and wrangle.

```{r}
draws <- as_draws_df(fit23.5)

glimpse(draws)
```

Now, let's use our handy `compare_thresholds()` function to make an expanded version of the lower-left plot of Figure 23.4.

```{r, fig.width = 5, fig.height = 4, message = F, warning = F}
draws %>% 
  select(`b_Intercept[1]`:`b_Intercept[4]`, .draw) %>% 
  compare_thresholds(lb = 1.5, ub = 4.5)
```

It will no longer be straightforward to use the formulas from 23.2.2 to convert the output from our **brms** parameterization to match the way Kruschke parameterized his conditional means and standard deviations. I will leave the conversion up to the interested reader. Going forward, we will focus on the output from our **brms** parameterization.

```{r, fig.width = 8, fig.height = 6, eval = F, echo = F, warning = F}
# this does not work. I believe it's because the formulas do not apply to the non-reference group

draws %>% 
  select(.draw, b_disc_XB, `b_Intercept[1]`, `b_Intercept[4]`) %>% 
  mutate(`y[1]` = 1.5,
         `y[4]` = 4.5) %>% 
  mutate(`sigma[A]` = (`y[1]` - `y[4]`) / (`b_Intercept[1]` - `b_Intercept[4]`)) %>% 
  mutate(`mu[A]` = `y[1]` - (`b_Intercept[1]` * `sigma[A]`)) %>% 
  mutate(`sigma[B]` = `sigma[A]` * (1 / exp(b_disc_XB))) %>% 
  mutate(`mu[B]` = `y[1]` - `b_Intercept[1]` * `sigma[B]`) %>% 
  
  mutate(`mu[B]-mu[A]`       = `mu[B]` - `mu[A]`,
         `sigma[B]-sigma[A]` = `sigma[B]` - `sigma[A]`) %>% 
  
  mutate(`(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)` = `mu[B]-mu[A]` / sqrt((`sigma[A]`^2 + `sigma[B]`^2) / 2)) %>% 

  pivot_longer(`sigma[A]`:`(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)`) %>% 
  mutate(name = factor(name,
                       levels = c("mu[A]",    "mu[B]",    "mu[B]-mu[A]",
                                  "sigma[A]", "sigma[B]", "sigma[B]-sigma[A]",
                                  "(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)"))) %>%
  
  # plot 
  ggplot(aes(x = value)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8], normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)
```

```{r, fig.width = 8, fig.height = 6, warning = F}
draws %>% 
  # simple parameters
  mutate(`mu[A]`    = 0,
         `mu[B]`    = b_XB,
         `sigma[A]` = 1,
         `sigma[B]` = 1 / exp(b_disc_XB)) %>% 
  # simple differences
  mutate(`mu[B]-mu[A]`       = `mu[B]` - `mu[A]`,
         `sigma[B]-sigma[A]` = `sigma[B]` - `sigma[A]`) %>% 
  # effect size
  mutate(`(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)` = (`mu[B]-mu[A]`) / sqrt((`sigma[A]`^2 + `sigma[B]`^2) / 2)) %>% 
  # wrangle
  pivot_longer(`mu[A]`:`(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)`) %>% 
  mutate(name = factor(name,
                       levels = c("mu[A]", "mu[B]", "mu[B]-mu[A]",
                                  "sigma[A]", "sigma[B]", "sigma[B]-sigma[A]",
                                  "(mu[B]-mu[A])/sqrt((sigma[A]^2+sigma[B]^2)/2)"))) %>%
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8], normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)
```

$\mu_A$ and $\sigma_A$ are both constants, which doesn't show up well with our `stat_halfeye()` approach with the scales freed across facets. If these plots really mattered for a scientific presentation or something for industry, you could experiment using either a common scale across all facets, or making the plots individually and then combining them with **patchwork** syntax. Returning to interpretation, because $\mu_A = 0$, it turns out that $\mu_B - \mu_A = \mu_B$, which is on display on the top row. Because $\sigma_A = 1$, it turns out that $\sigma_B - \sigma_A$ is just $\sigma_B$ moved over one unit to the left, which is hopefully clear in the panels of the second row. Very happily, the effect size formula worked with our **brms** parameters the same way it did for Kruschke's. Both yield an effect size of about 0.5, with 95% intervals extending about $\mp 0.5$.

Here we make good use of the `type = "bars_grouped"` and `group = "X"` arguments to make the posterior predictive plots at the top right of Figure 23.4 with the `brms::pp_check()` function.

```{r, fig.width = 8, fig.height = 2.5, message = F, warning = F}
set.seed(23)

pp_check(fit23.5, type = "bars_grouped", ndraws = 100, group = "X", fatten = 2) +
  scale_x_continuous("y", breaks = 1:7) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Data with posterior predictions",
          subtitle = expression(list(italic(N[A])==44, italic(N[B])==44))) +
  theme(legend.background = element_blank(),
        legend.position = c(.9, .75))
```

Using more tricks from back in [Chapter 16][Metric-Predicted Variable on One or Two Groups], here's the corresponding conventional Gaussian model for metric data.

```{r fit23.6}
mean_y <- mean(my_data$Y)
sd_y   <- sd(my_data$Y)

stanvars <- 
  stanvar(mean_y, name = "mean_y") + 
  stanvar(sd_y,   name = "sd_y")

fit23.6 <-
  brm(data = my_data,
      family = gaussian,
      bf(Y ~ 0 + X, sigma ~ 0 + X),
      prior = c(prior(normal(mean_y, sd_y * 100), class = b),
                prior(normal(0, 1), class = b, dpar = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      seed = 23,
      file = "fits/fit23.06")
```

Check the summary.

```{r}
print(fit23.6)
```

Here are the marginal posteriors, including the effect size (i.e., a standardized mean difference using the pooled standard deviation formula presuming equal sample sizes, $(\mu_2 - \mu_1) / \sqrt{(\sigma_1^2 + \sigma_2^2) / 2}$).

```{r, fig.width = 6, fig.height = 7, warning = F}
as_draws_df(fit23.6) %>% 
  mutate(`A mean`      = b_XA,
         `B mean`      = b_XB,
         `A Std. Dev.` = exp(b_sigma_XA),
         `B Std. Dev.` = exp(b_sigma_XB)) %>% 
  mutate(`Difference of Means`     = `B mean` - `A mean`,
         `Difference of Std. Devs` = `B Std. Dev.` - `A Std. Dev.`) %>% 
  mutate(`Effect Size` = `Difference of Means` / sqrt((`A Std. Dev.`^2 + `B Std. Dev.`^2) / 2)) %>% 
  pivot_longer(`A mean`:`Effect Size`) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8], normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "These are based on the conventional Gaussian model, NOT the cumulative-normal\nmodel Kruschke displayed in Figure 23.4",
       x = "Marginal posterior") +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

Compare those results to those Kruschke reported from an NHST analysis in the note below Figure 23.4:

> $M_1 = 1.43, M_2 = 1.86, t = 2.18, p = 0.032$, with effect size $d = 0.466$ with 95% CI of $0.036-0.895$. An $F$ test of the variances concludes that the standard deviations are significantly different: $S_1 = 0.76, S_2 = 1.07, p = 0.027$. Notice in this case that treating the values as metric greatly underestimates their variances, as well as erroneously concludes the variances are different. (p. 684)

As to the data in the analyses Kruschke reported in Figure 23.5 and the prose in the second paragraph on page 685, I'm not aware that Kruschke provided them. From his footnote #2, we read: "Data in Figure 23.5 are from an as-yet unpublished study I conducted with the collaboration of Allison Vollmer as part of her undergraduate honors project." In place of the real data, I eyeballed the values based on the upper-right panels in Figure 23.5. Here they are.

```{r}
d <-
  tibble(x = rep(str_c("joke ", c(1, 6)), each = 177),
         y = c(rep(1:7, times = c(95, 19, 18, 10, 17, 10, 8)),
               rep(1:7, times = c(53, 33, 31, 22, 23, 14, 1))))

glimpse(d)
```

My approximation to Kruschke's data looks like this.

```{r, fig.width = 3.5, fig.height = 2.75}
d %>% 
  ggplot(aes(x = y)) +
  geom_bar(fill = sl[5]) +
  scale_x_continuous(breaks = 1:7) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  facet_wrap(~ x, ncol = 1)
```

Here we fit the cumulative-normal model based on our version of the data, continuing to allow both $\mu$ and $\sigma$ (i.e., $1 / \exp(\log \alpha)$) to differ across groups.

```{r fit23.7}
fit23.7 <-
  brm(data = d,
      family = cumulative(probit),
      bf(y ~ 1 + x) +
        lf(disc ~ 0 + x, cmc = FALSE),
      prior = c(prior(normal(0, 4), class = Intercept),
                prior(normal(0, 4), class = b),
                prior(normal(0, 1), class = b, dpar = disc)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 23,
      file = "fits/fit23.07")
```

Check the model summary.

```{r}
print(fit23.7)
```

Save and wrangle the posterior draws, then use our `compare_thresholds()` function to compare the **brms** parameterization of $\theta_{[i]}$ with the parameterization in the text in an expanded version of the lower-left plot of Figure 23.5.

```{r, fig.width = 5, fig.height = 4, message = F, warning = F}
draws <- as_draws_df(fit23.7)

draws %>% 
  select(`b_Intercept[1]`:`b_Intercept[6]`, .draw) %>% 
  compare_thresholds(lb = 1.5, ub = 6.5)
```

Given our data are only approximations of Kruschke's, I think we did pretty good. Here are the histograms for our **brms** versions of the $\mu$- and $\sigma$-related parameters.

```{r, fig.width = 8, fig.height = 6, warning = F}
draws %>% 
  transmute(`mu[Joke~1]`    = 0,
            `mu[Joke~6]`    = b_xjoke6,
            `sigma[Joke~1]` = 1,
            `sigma[Joke~6]` = 1 / exp(b_disc_xjoke6)) %>% 
  mutate(`mu[Joke~6]-mu[Joke~1]`       = `mu[Joke~6]` - `mu[Joke~1]`,
         `sigma[Joke~6]-sigma[Joke~1]` = `sigma[Joke~6]` - `sigma[Joke~1]`) %>% 
  mutate(`(mu[Joke~6]-mu[Joke~1])/sqrt((sigma[Joke~1]^2+sigma[Joke~6]^2)/2)` = (`mu[Joke~6]-mu[Joke~1]`) / sqrt((`sigma[Joke~1]`^2 + `sigma[Joke~6]`^2) / 2)) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name,
                       levels = c("mu[Joke~1]", "mu[Joke~6]", "mu[Joke~6]-mu[Joke~1]",
                                  "sigma[Joke~1]", "sigma[Joke~6]", "sigma[Joke~6]-sigma[Joke~1]",
                                  "(mu[Joke~6]-mu[Joke~1])/sqrt((sigma[Joke~1]^2+sigma[Joke~6]^2)/2)"))) %>%
  
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8],
               normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)
```

Here are our versions of the two panels in the upper right of Figure 23.5.

```{r, fig.width = 8, fig.height = 2.5, message = F, warning = F}
set.seed(23)

pp_check(fit23.7, type = "bars_grouped", ndraws = 100, group = "x", fatten = 2) +
  scale_x_continuous("y", breaks = 1:7) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  ggtitle("Data with posterior predictions",
          subtitle = expression(list(italic(N["joke "*1])==177, italic(N["joke "*6])==177))) +
  theme(legend.position = "none")
```

Now here's the corresponding model where we treat the `y` data as metric.

```{r fit23.8}
mean_y <- mean(d$y)
sd_y   <- sd(d$y)

stanvars <- 
  stanvar(mean_y, name = "mean_y") + 
  stanvar(sd_y,   name = "sd_y")

fit23.8 <-
  brm(data = d,
      family = gaussian,
      bf(y ~ 0 + x, sigma ~ 0 + x),
      prior = c(prior(normal(mean_y, sd_y * 100), class = b),
                prior(normal(0, exp(sd_y)), class = b, dpar = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      seed = 23,
      file = "fits/fit23.08")
```

Check the summary.

```{r}
print(fit23.8)
```

Make the marginal posteriors, including the effect size.

```{r, fig.width = 6, fig.height = 7, warning = F}
as_draws_df(fit23.8) %>% 
  mutate(`Joke 1 Mean`      = b_xjoke1,
         `Joke 6 Mean`      = b_xjoke6,
         `Joke 1 Std. Dev.` = exp(b_sigma_xjoke1),
         `Joke 6 Std. Dev.` = exp(b_sigma_xjoke6)) %>% 
  mutate(`Difference of Means`     = `Joke 6 Mean` - `Joke 1 Mean`,
         `Difference of Std. Devs` = `Joke 6 Std. Dev.` - `Joke 1 Std. Dev.`) %>% 
  mutate(`Effect Size` = `Difference of Means` / sqrt((`Joke 1 Std. Dev.`^2 + `Joke 6 Std. Dev.`^2) / 2)) %>% 
  pivot_longer(`Joke 1 Mean`:`Effect Size`) %>% 
  mutate(name = factor(name, 
                       levels = c("Joke 1 Mean", "Joke 1 Std. Dev.", 
                                  "Joke 6 Mean", "Joke 6 Std. Dev.", 
                                  "Difference of Means", "Difference of Std. Devs", 
                                  "Effect Size"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8], normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "These are based on the conventional Gaussian model, NOT the cumulative-normal\nmodel Kruschke displayed in Figure 23.5",
       x = "Marginal posterior") +
  facet_wrap(~ name, scales = "free", ncol = 2)
```

If you think you have a better approximation of Kruschke's data, [please share](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues).

## The Case of metric predictors

"This type of model is often referred to as *ordinal probit regression* or *ordered probit regression* because the probit function is the link function corresponding to the cumulative-normal inverse-link function" (p. 688, *emphasis* in the original).

### Implementation in ~~JAGS~~ brms.

This model is easy to specify in **brms**. Just make sure to think clearly about your priors.

### Example: Happiness and money.

Load the data for the next model.

```{r, message = F}
my_data <- read_csv("data.R/OrdinalProbitData-LinReg-2.csv")

glimpse(my_data)
```

Take a quick look at the data.

```{r, fig.width = 4, fig.height = 3}
my_data %>% 
  ggplot(aes(x = X, y = Y)) +
  geom_point(alpha = 1/3, color = sl[9]) +
  scale_y_continuous(breaks = 1:7)
```

Kruschke standardized his predictor within his model code. Here we'll standardize `X` before fitting the model.

```{r}
my_data <-
  my_data %>% 
  mutate(X_s = (X - mean(X)) / sd(X))
```

Fit the model.

```{r fit23.9}
fit23.9 <-
  brm(data = my_data,
      family = cumulative(probit),
      Y ~ 1 + X_s,
      prior = c(prior(normal(0, 4), class = Intercept),
                prior(normal(0, 4), class = b)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 23,
      file = "fits/fit23.09")
```

Check the summary.

```{r}
print(fit23.9)
```

Extract the posterior draws and compare the **brms** parameterization of $\theta_{[i]}$ with the parameterization in the text in an expanded version of the bottom panel of Figure 23.7.

```{r, fig.width = 5, fig.height = 4, message = F, warning = F}
draws <- as_draws_df(fit23.9)

draws %>% 
  select(`b_Intercept[1]`:`b_Intercept[6]`, .draw) %>% 
  compare_thresholds(lb = 1.5, ub = 6.5)
```

Here's the marginal distribution of `b_X_s`, our effect size for the number of jokes.

```{r, fig.width = 3.5, fig.height = 2.25}
draws %>% 
  ggplot(aes(x = b_X_s, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8]) +
  scale_y_continuous(NULL, breaks = NULL)
```

This differs from Kruschks's $\beta_1$, which is in an unstandardized metric based on the parameters in his version of the model. But unlike the effect sizes from previous models, this one is not in a Cohen's-$d$ metric. Rather, this is a fully-standardized regression coefficient. As to the large subplot at the top of Figure 23.7, we can make something like it by nesting `conditional_effects()` within `plot()`.

```{r, fig.width = 4, fig.height = 2.75, warning = F, message = F}
conditional_effects(fit23.9) %>% 
  plot(line_args = list(color = sl[7], fill = sl[3]))
```

Here's a more elaborated version of the same plot, this time depicting the model with 100 fitted lines randomly drawn from the posterior.

```{r, fig.width = 4, fig.height = 2.75}
set.seed(23)

conditional_effects(fit23.9,
                    spaghetti = TRUE, 
                    ndraws = 100) %>% 
  plot(points = T,
       line_args = list(size = 0),
       point_args = list(alpha = 1/3, color = sl[9]),
       spaghetti_args = list(colour = alpha(sl[6], .2)))
```

Note the warning message. There was a similar one in the first plot, which I suppressed for simplicity sake. The message suggests treating the fitted lines as "continuous variables" might lead to a deceptive plot. Here's what happens if we follow the suggestion.

```{r, fig.width = 4.7, fig.height = 2.75}
set.seed(23)

conditional_effects(fit23.9, categorical = T)
```

Recall that our thresholds, $\theta_1,...,\theta_{K-1}$, in conjunction with the standard normal density, give us the probability of a given `Y` value, given `X_s` (i.e., $p(y = k | \mu, \sigma, \{ \theta_j \})$, where $\mu$ is conditional on $x$). This plot returned the fitted lines of those conditional probabilities, each depicted by the posterior mean and percentile-based 95% intervals. At lower values of `X_s`, lower values of `Y` are more probable. At higher values of `X_s`, higher values of `Y` are more probable.

It might be useful to get more practice in with this model. Here we'll use `fitted()` to make a similar plot, depicting the model with may fitted lines instead of summary statistics.

```{r}
# how many fitted lines do you want?
n_draws <- 50

# define the `X_s` values you want to condition on
# because the lines are nonlinear, you'll need many of them
nd <- tibble(X_s = seq(from = -2, to = 2, by = 0.05))

f <-
  fitted(fit23.9,
         newdata = nd,
         summary = F,
         ndraws = n_draws)

# inspect the output
f %>% 
  str()
```

Our output came in three dimensions. We have 50 rows, corresponding to `n_draws <- 50` (i.e., 50 posterior draws). There are 81 columns, based on how we defined the `X_s` values within our `nd` data (i.e., `seq(from = -2, to = 2, by = 0.05)`). The third dimension has seven levels, one corresponding to each of the seven levels of our criterion variable `Y`. Here's a way to rearrange that output into a useful format for plotting.

```{r, fig.width = 8, fig.height = 2}
# rearrange the output
rbind(
  f[, , 1],
  f[, , 2],
  f[, , 3],
  f[, , 4],
  f[, , 5],
  f[, , 6],
  f[, , 7]
) %>% 
  # wrangle
  data.frame() %>% 
  set_names(nd %>% pull(X_s)) %>% 
  mutate(draw   = rep(1:n_draws, times = 7),
         rating = rep(1:7, each = n_draws)) %>% 
  pivot_longer(-c(draw, rating),
               names_to = "X_s",
               values_to = "probability") %>% 
  mutate(rating = str_c("Y: ", rating),
         X_s    = X_s %>% as.double()) %>% 
  
  # plot
  ggplot(aes(x = X_s, y = probability, 
             group = interaction(draw, rating),
             color = rating)) +
  geom_line(size = 1/4, alpha = 1/2)  +
  scale_color_scico_d(palette = "lajolla", begin = .25) +
  scale_y_continuous(breaks = c(0, .5, 1), limits = c(0, 1), expand = c(0, 0)) +
  theme(legend.position = "none") +
  facet_wrap(~ rating, ncol = 7)
```

So far, we've been plotting our model with the context of the default `scale = "response"` setting within `fitted()`. Within the context of the response variable `Y`, our model returns response probabilities. We can also look at the output within the context of `scale = "linear"`. We'll plot these fitted lines across our `nd` values two different ways. For the first, `p1`, we'll use summary statistics. For the second, `p2`, we'll set `summary = T`.

```{r, fig.width = 8, fig.height = 3.25}
# adjust the nd
nd <- tibble(X_s = seq(from = -2.5, to = 2.5, by = 0.1))

# use summary statistics
p1 <-
  fitted(fit23.9, 
         scale = "linear",
         newdata = nd) %>% 
  data.frame() %>% 
  bind_cols(nd) %>% 
  
  ggplot(aes(x = X_s, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_smooth(stat = "identity",
              alpha = 1/2, color = sl[7], fill = sl[3]) +
  labs(title = "summary statistics",
       y = "underlying standard normal")


# set `summary = F`
set.seed(23)
p2 <-
  fitted(fit23.9, 
         scale = "linear",
         newdata = nd,
         summary = F,
         ndraws = n_draws) %>% 
  data.frame() %>% 
  set_names(nd %>% pull(X_s)) %>% 
  mutate(draw = 1:n_draws) %>% 
  pivot_longer(-draw,
               names_to = "X_s") %>% 
  mutate(X_s = X_s %>% as.double()) %>% 
  
  ggplot(aes(x = X_s, y = value, group = draw)) +
  geom_line(alpha = 1/2, color = sl[7]) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("50 posterior draws")

# combine and plot!
p1 + p2 &
  coord_cartesian(ylim = c(-2, 2))
```

Both methods returned the fitted lines in the metric of the underlying standard normal distribution. The fitted lines are nonlinear in the metric of the raw data `Y`, but they're linear in the metric of the presumed underlying distribution. If it helps, we'll make a marginal plot of the standard normal distribution and tack it onto the right.

```{r, fig.width = 8, fig.height = 3.25}
# make Phi
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .01)) %>%
  mutate(d = dnorm(x)) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = sl[5]) +
  # add the thresholds!
  geom_vline(xintercept = posterior_summary(fit23.9)[1:6, 1], 
             color = sl[9], linetype = 3) +
  # mark the thresholds with the axis breaks
  scale_x_reverse(NULL, breaks = fixef(fit23.9)[1:6, 1], position = "top",
                  labels = parse(text = str_c("theta[", 1:6, "]"))) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle(expression(italic(N)(0,1))) +
  coord_flip(xlim = c(-2, 2))

# combine, format a bit, and plot
(
  ((p1 | p2 ) & 
     geom_hline(yintercept = posterior_summary(fit23.9)[1:6, 1], 
                color = sl[9], linetype = 3) &
     coord_cartesian(ylim = c(-2, 2)) | 
     p3)
) + 
  plot_layout(widths = c(4, 4, 1))
```

The lines intersecting the plots are the posterior means for thresholds, $\theta_1, \dots ,\theta_6$.

*But these still aren't faithful depictions of the top panel of Figure 23.7*, you say. Okay, fine. One of the distinctive elements of that panel is the left-tilted bar-and-error plots. If you look closely at the vertical lines at their bases, you'll see that the leftmost subplot starts at the minimum value of `X` and the rightmost subplot starts at the maximum value of `X`. Since our plots, so far, have been based on `X_s`, we'll use the minimum and maximum values from that. Here are those values.

```{r}
(r <- range(my_data$X_s))
```

To my eye, it appears that the three middle subplots are equally distributed between those at the ends. If we proceed under that assumption, here's how we might use `fitted()` to get us rolling on computing the relevant values.

```{r}
nd <- tibble(X_s = seq(from = r[1], to = r[2], length.out = 5))

f <-
  fitted(fit23.9,
         newdata = nd)

# inspect the output
f %>% 
  str()
```

Here we'll rearrange the output to make it useful for plotting.

```{r}
# rearrange the output
f <-
  rbind(
    f[, , 1],
    f[, , 2],
    f[, , 3],
    f[, , 4],
    f[, , 5],
    f[, , 6],
    f[, , 7]
  ) %>% 
  # wrangle
  data.frame() %>% 
  bind_cols(expand(nd, Y = 1:7, X_s))

head(f)
```

Now we can make those bar-and-error plots.

```{r, fig.width = 7, fig.height = 3}
f %>% 
  mutate(X_s = round(X_s, digits = 3)) %>% 
    
  ggplot(aes(x = Y, y = Estimate,
             ymin = Q2.5, ymax = Q97.5)) +
  geom_col(fill = sl[4]) +
  geom_pointrange(fatten = 1.5, size = 1, color = sl[7])  +
  scale_x_continuous(breaks = 1:7) +
  scale_y_reverse(NULL, position = "right", limits = c(1, 0), expand = c(0, 0),
                  breaks = c(1, .5, 0), labels = c("1", ".5", "0")) +
  coord_flip() +
  facet_wrap(~ X_s, ncol = 7, strip.position = "bottom")
```

The `X_s` values are depicted in the panel strips on the bottom. The response probabilities are scaled based on the axis on the top. The points and leftmost sides of the bars are the posterior means. The thin, dark horizontal lines are the percentile-based 95% intervals. Here we reformat `f` a little more to put those bar-and-error plots in a format more similar to that of Figure 23.7.

```{r, fig.width = 6, fig.height = 3.5}
f %>% 
  select(-Est.Error) %>% 
  # rescale the probability summaries
  mutate_at(vars(Estimate:Q97.5), ~. / 2) %>% 
  
  # plot!
  ggplot() +
  geom_vline(xintercept = seq(from = r[1], to = r[2], length.out = 5),
             color = sl[2]) +
  # bar marking the Estimate
  geom_segment(aes(x = X_s, xend = X_s - Estimate, 
                   y = Y + 0.1, yend = Y + 0.1),
               size = 8, color = sl[4]) +
  # bar marking the 95% interval
  geom_segment(aes(x = X_s - Q2.5, xend = X_s - Q97.5,
                   y = Y + 0.2, yend = Y + 0.2),
               size = 1, color = sl[7]) +
  # data
  geom_point(data = my_data,
             aes(x = X_s, y = Y),
             shape = 1, size = 2, color = sl[9]) +
  scale_y_continuous("Y", breaks = 1:7, limits = c(0.5, 7.5)) +
  coord_cartesian(xlim = c(-2.4, 2.4))
```

I'm not going to attempt superimposing fitted lines on this plot the way Kruschke did. Given that our ordered-probit model is nonlinear on the scale of the criterion, it seems misleading to present linear fitted lines atop the raw data. If you'd like to do so, you're on your own.

Now here's the corresponding model is we treat the `y` data as metric with tricks from [Chapter 17][Metric Predicted Variable with One Metric Predictor].

```{r fit23.10}
sd_x <- sd(my_data$X)
sd_y <- sd(my_data$Y)
m_x  <- mean(my_data$X)
m_y  <- mean(my_data$Y)

beta_0_sigma <- 10 * abs(m_x * sd_y / sd_x)
beta_1_sigma <- 10 * abs(sd_y / sd_x) 

stanvars <- 
  stanvar(beta_0_sigma, name = "beta_0_sigma") + 
  stanvar(beta_1_sigma, name = "beta_1_sigma") +
  stanvar(sd_y, name = "sd_y")

fit23.10 <-
  brm(data = my_data,
      family = gaussian,
      Y ~ 1 + X,
      prior = c(prior(normal(0, beta_0_sigma), class = Intercept),
                prior(normal(0, beta_1_sigma), class = b),
                prior(normal(0, sd_y), class = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      seed = 23,
      file = "fits/fit23.10")
```

It may not have been readily apparent from Kruschke's prose in the note for Figure 23.7, but his OLS model was based on the fully unstandardized data (i.e., using `X` as the predictor), not the partially standardized data he used in his JAGS code from 23.4.1. We followed the same sensibilities for `fit23.10`. Speaking of which, here are the summaries for the marginal posteriors.

```{r}
posterior_summary(fit23.10)[1:3, ] %>% round(digits = 3)
```

These values are very close to those he reported at the bottom of page 690. Here are what the fitted lines from that model look like when superimposed on the data, when presuming both variables are metric.

```{r, fig.width = 4, fig.height = 2.75}
set.seed(23)

conditional_effects(fit23.10,
                    spaghetti = TRUE, 
                    ndraws = 100) %>% 
  plot(points = T,
       line_args = list(size = 0),
       point_args = list(alpha = 1/3, color = sl[9]),
       spaghetti_args = list(colour = alpha(sl[6], .2)))
```

For the next example, we'll load the `HappinessAssetsDebt.csv` data from Shi ([2009](https://www.icpsr.umich.edu/icpsrweb/DSDR/studies/21741)).

```{r, message = F}
my_data <- read_csv("data.R/HappinessAssetsDebt.csv")

glimpse(my_data)
```

Here's a quick scatter plot of the data. To help with the overplotting, the points have been horizontally jittered a bit. But as indicated in the text, `Happiness` is a discrete variable.

```{r, fig.width = 4.5, fig.height = 3}
my_data %>% 
  ggplot(aes(x = Assets, y = Happiness)) +
  geom_jitter(alpha = 1/4, height = .25, color = sl[9]) +
  scale_x_continuous(expand = expansion(0, 0.05))
```

Standardize our predictor.

```{r}
my_data <-
  my_data %>% 
  mutate(Assets_s = (Assets - mean(Assets)) / sd(Assets))
```

Fit the model like before.

```{r fit23.11}
fit23.11 <-
  brm(data = my_data,
      family = cumulative(probit),
      Happiness ~ 1 + Assets_s,
      prior = c(prior(normal(0, 4), class = Intercept),
                prior(normal(0, 4), class = b)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 23,
      file = "fits/fit23.11")
```

Check the summary.

```{r}
print(fit23.11)
```

Extract the posterior draws and compare the **brms** parameterization of $\theta_{[i]}$ with the parameterization in the text in an expanded version of the bottom panel of Figure 23.8.

```{r, fig.width = 5, fig.height = 4, message = F, warning = F}
draws <- as_draws_df(fit23.11)

draws %>% 
  select(`b_Intercept[1]`:`b_Intercept[4]`, .draw) %>% 
  compare_thresholds(lb = 1.5, ub = 4.5)
```

Behold the marginal distribution of `b_Assets_s`, our effect size for `Assets`.

```{r, fig.width = 3.5, fig.height = 2.25}
draws %>% 
  ggplot(aes(x = b_Assets_s, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8]) +
  scale_y_continuous(NULL, breaks = NULL)
```

Here's the `fitted()`-oriented preparatory work for our version of the top panel of Figure 23.8.

```{r}
# define the range for the predictor
r <- range(my_data$Assets_s)

# re-define the new data
nd <- tibble(Assets_s = seq(from = r[1], to = r[2], length.out = 5))

# compute the fitted summaries
f <-
  fitted(fit23.11,
         newdata = nd)

# rearrange the output
f <-
  rbind(
    f[, , 1],
    f[, , 2],
    f[, , 3],
    f[, , 4],
    f[, , 5]
  ) %>% 
  # wrangle
  data.frame() %>% 
  bind_cols(expand(nd, Happiness = 1:5, Assets_s))

# examine
head(f)
```

Like with the same variant from Figure 23.7, we will not be superimposing linear fitted lines. The model is nonlinear on the scale of the data and I don't want to confuse readers by pretending otherwise.

```{r, fig.width = 6, fig.height = 3.5}
f %>% 
  select(-Est.Error) %>% 
  # rescale the probability summaries
  mutate_at(vars(Estimate:Q97.5), ~. * 2.5) %>% 
  
  # plot!
  ggplot() +
  geom_vline(xintercept = seq(from = r[1], to = r[2], length.out = 5),
             color = sl[2]) +
  # bar marking the Estimate
  geom_segment(aes(x = Assets_s, xend = Assets_s - Estimate, 
                   y = Happiness + 0.1, yend = Happiness + 0.1),
               size = 8, color = sl[4]) +
  # bar marking the 95% interval
  geom_segment(aes(x = Assets_s - Q2.5, xend = Assets_s - Q97.5,
                   y = Happiness + 0.2, yend = Happiness + 0.2),
               size = 1, color = sl[7]) +
  # data
  geom_point(data = my_data,
             aes(x = Assets_s, y = Happiness),
             shape = 1, size = 2, color = sl[9]) +
  scale_y_continuous("Happiness", breaks = 1:5, limits = c(0.5, 5.5)) +
  coord_cartesian(xlim = c(-4, 24))
```

Now here's the corresponding model is we treat `Happiness` as metric. Unlike our method for the corresponding model from Figure 23.7, `fit10`, we will use the standardized version of the predictor, `Assets_s`. The unstandardized values for `Happiness` and `Assets` are on vastly different scales, which can be difficulty for HMC with broad priors of the type Kruschke often uses. Standardizing the predictor helps.

```{r fit23.12}
sd_y <- sd(my_data$Happiness)
stanvars <- 
  stanvar(sd_y, name = "sd_y")

fit23.12 <-
  brm(data = my_data,
      family = gaussian,
      Happiness ~ 1 + Assets_s,
      prior = c(prior(normal(3.5, 5), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(normal(0, sd_y), class = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      seed = 23,
      file = "fits/fit23.12")
```

Here are the summaries for the marginal posteriors.

```{r}
posterior_summary(fit23.12)[1:3, ] %>% round(digits = 6)
```

They're just a bit different from those produced by OLS. Here are what the fitted lines from that model look like when superimposed on the data, when presuming both variables are metric.

```{r, fig.width = 4, fig.height = 2.75}
set.seed(23)

conditional_effects(fit23.12,
                    spaghetti = TRUE, 
                    ndraws = 100) %>% 
  plot(points = T,
       line_args = list(size = 0),
       point_args = list(alpha = 1/3, color = sl[9]),
       spaghetti_args = list(colour = alpha(sl[6], .2)))
```

### Example: Movies--They don't make 'em like they used to.

For this section, we'll load the [Moore (2006)](https://amstat.tandfonline.com/doi/pdf/10.1080/10691898.2006.11910579?needAccess=true) `Movies.csv` data.

```{r, message = F}
my_data <- read_csv("data.R/Movies.csv")

glimpse(my_data)
```

In footnote #5 at the bottom of page 693, Kruschke explained that whereas the original `Ratings` data ranged from `1.0` to `4.0` in half-unit increments, he recoded them to range from `1` to `7`. Here we recode `Ratings` in the same way using `dplyr::recode()`. While we're at it, we'll make standardized versions of the predictors, too.

```{r}
my_data <-
  my_data %>% 
  mutate(Rating   = recode(Rating,
                           `1.0` = 1,
                           `1.5` = 2,
                           `2.0` = 3,
                           `2.5` = 4,
                           `3.0` = 5,
                           `3.5` = 6,
                           `4.0` = 7),
         Year_s   = (Year   - mean(Year))   / sd(Year),
         Length_s = (Length - mean(Length)) / sd(Length))
```

Here's a scatter plot of the data, with points colored by `Rating`.

```{r, fig.width = 5, fig.height = 3.25}
my_data %>% 
  mutate(Rating = factor(Rating)) %>% 
  
  ggplot(aes(x = Year, y = Length, label = Rating)) +
  geom_point(aes(color = Rating),
             size = 3) +
  geom_text(size = 3) +
  scale_color_scico_d(palette = "lajolla", begin = .15, end = .7)
```

Fitting the multivariable ordered-probit model with **brms** is about as simple as fitting any other multivariable model. Just tack on predictors with the `+` operator.

```{r fit23.13}
fit23.13 <-
  brm(data = my_data,
      family = cumulative(probit),
      Rating ~ 1 + Year_s + Length_s,
      prior = c(prior(normal(0, 4), class = Intercept),
                prior(normal(0, 4), class = b)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 23,
      file = "fits/fit23.13")
```

Check the model summary.

```{r}
print(fit23.13)
```

Extract the posterior draws and compare the **brms** parameterization of $\theta_{[i]}$ with the parameterization in the text in an expanded version of the bottom panel of Figure 23.9.

```{r, fig.width = 5, fig.height = 4, message = F, warning = F}
draws <- as_draws_df(fit23.13)

draws %>% 
  select(`b_Intercept[1]`:`b_Intercept[6]`, .draw) %>% 
  compare_thresholds(lb = 1.5, ub = 6.5)
```

To sate any curiosity, here are the correlations among the parameters.

```{r, fig.width = 6, fig.height = 6, warning = F, message = F}
# wrangle
as_draws_df(fit23.13) %>% 
  select(contains("Intercept"), b_Year_s, b_Length_s) %>% 
  set_names(str_c("theta[", 1:6, "]"), str_c("beta[", 1:2, "]")) %>% 
  
  # plot!
  ggpairs(upper = list(continuous = my_upper),
          diag = list(continuous = my_diag),
          lower = list(continuous = my_lower),
          labeller = label_parsed) +
  ggtitle("These parameters have milder correlations")
```

Now focus on the marginal distribution of our two effect-size parameters.

```{r, fig.width = 6, fig.height = 2.25, warning = F}
draws %>% 
  pivot_longer(ends_with("_s")) %>% 
  mutate(name = factor(name,
                       levels = c("b_Year_s", "b_Length_s"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8], normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("effect size") +
  facet_wrap(~ name, scales = "free")
```

Before we make the top panel from Figure 23.9, I'd like to wander a bit and look at something related. We'll use `fitted()`.

```{r}
# define the new data
nd <- crossing(Year_s   = seq(from = -3, to = 3, by = 0.25), 
               Length_s = seq(from = -3, to = 3, by = 0.25))

# compute the `Response` probabilities
f <-
  fitted(fit23.13,
         newdata = nd)

# rearrange the output
f <-
  rbind(
    f[, , 1],
    f[, , 2],
    f[, , 3],
    f[, , 4],
    f[, , 5],
    f[, , 6],
    f[, , 7]
  ) %>% 
  # wrangle
  data.frame() %>% 
  bind_cols(
    nd %>% 
      expand(Rating = 1:7,
             nesting(Year_s, Length_s))
  )

# what did we do?
head(f)
```

We just computed the response probabilities across the two-dimensional grid of the predictor values. Now plot using the posterior means.

```{r, fig.width = 8, fig.height = 2.7}
f %>% 
  mutate(strip = str_c("Rating: ", Rating)) %>% 
  
  ggplot(aes(x = Year_s, y = Length_s)) +
  geom_raster(aes(fill = Estimate),
              interpolate = T) +
  geom_text(data = my_data %>% mutate(strip = str_c("Rating: ", Rating)),
            aes(label = Rating),
            color = "white", size = 2.5) +
  scale_fill_scico(palette = 'lajolla', direction = -1, limits = c(0, 1),
                   breaks = c(0, .5, 1), labels = c("0", ".5", "1")) +
  scale_x_continuous(breaks = seq(from = -2, to = 2, by = 2),
                     expand = c(0, 0)) +
  scale_y_continuous(breaks = seq(from = -2, to = 2, by = 2),
                     expand = c(0, 0)) +
  theme(legend.position = "bottom") +
  facet_wrap(~ strip, nrow = 1)
```

This model didn't do a great job capturing the `Response` probabilities. If you're curious, you'll find you can do a little bit better if you allow the two predictors to interact (i.e., add `+ Year_s:Length_s` to the `formula` line). Even then, the model isn't great. I leave that as an exercise for the interested reader.

```{r, fig.width = 8, fig.height = 2.7, eval = F, echo = F}
fit23.14 <-
  brm(data = my_data,
      family = cumulative(probit),
      Rating ~ 1 + Year_s + Length_s + Year_s:Length_s,
      prior = c(prior(normal(0, 4), class = Intercept),
                prior(normal(0, 4), class = b)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 23)

print(fit23.14)

# define the new data
nd <- crossing(Year_s   = seq(from = -3, to = 3, by = 0.25), 
               Length_s = seq(from = -3, to = 3, by = 0.25))

# compute the `Response` probabilities
f <-
  fitted(fit23.14,
         newdata = nd)

# rearrange the output
f <-
  rbind(
    f[, , 1],
    f[, , 2],
    f[, , 3],
    f[, , 4],
    f[, , 5],
    f[, , 6],
    f[, , 7]
  ) %>% 
  # wrangle
  data.frame() %>% 
  bind_cols(
    nd %>% 
      expand(Rating = 1:7,
             nesting(Year_s, Length_s))
  )

f %>% 
  mutate(strip = str_c("Rating: ", Rating)) %>% 
  
  ggplot(aes(x = Year_s, y = Length_s)) +
  geom_raster(aes(fill = Estimate),
              interpolate = T) +
  geom_text(data = my_data %>% mutate(strip = str_c("Rating: ", Rating)),
            aes(label = Rating),
            color = "white", size = 2.5) +
  scale_fill_scico(palette = 'lajolla', direction = -1, limits = c(0, 1),
                   breaks = c(0, .5, 1), labels = c("0", ".5", "1")) +
  scale_x_continuous(breaks = seq(from = -2, to = 2, by = 2),
                     expand = c(0, 0)) +
  scale_y_continuous(breaks = seq(from = -2, to = 2, by = 2),
                     expand = c(0, 0)) +
  theme(legend.position = "bottom") +
  facet_wrap(~ strip, nrow = 1)
```

For this model, however, we will follow Kruschke and make a more faithful version of the top panel of Figure 23.9. We'll need to wrangle our `draws` data a bit to get things ready. Here's the work.

```{r}
draws <-
  draws %>% 
  # we just need the data from three steps in the HMC chain
  slice(1:3) %>% 
  mutate(.draw = .draw %>% as.factor(),
         b1   = b_Year_s,
         b2   = b_Length_s) %>% 
  expand(nesting(.draw, b1, b2, `b_Intercept[1]`, `b_Intercept[2]`, `b_Intercept[3]`, `b_Intercept[4]`, `b_Intercept[5]`, `b_Intercept[6]`),
         # because these are straight lines, two extreme x1-values are all we need
         x1 = c(-10, 10)) %>% 
  pivot_longer(contains("["),
               names_to = "theta") %>% 
  # use Kruschke's Formula 23.5
  mutate(x2       = (value / b2) + (-b1 / b2) * x1,
         # this just renames our x variables for easy plotting
         Year_s   = x1,
         Length_s = x2) 

glimpse(draws)
```

Now just plot.

```{r, fig.width = 7.25, fig.height = 6}
draws %>% 
  ggplot(aes(x = Year_s, y = Length_s)) +
  geom_line(aes(group = interaction(.draw, theta), color = theta, linetype = .draw)) +
  geom_text(data = my_data,
            aes(label = Rating)) +
  scale_color_scico_d(expression(theta), palette = "lajolla", begin = .25, labels = 1:6) +
  coord_cartesian(xlim = range(my_data$Year_s),
                  ylim = range(my_data$Length_s))
```

It might be easier to see Kruschke's main point if we facet by `.draw`.

```{r, fig.width = 7.25, fig.height = 6}
draws %>% 
  ggplot(aes(x = Year_s, y = Length_s)) +
  geom_line(aes(group = interaction(.draw, theta), color = theta, linetype = .draw)) +
  geom_text(data = my_data,
            aes(label = Rating)) +
  scale_color_scico_d(expression(theta), palette = "lajolla", begin = .25, labels = 1:6) +
  coord_cartesian(xlim = range(my_data$Year_s),
                  ylim = range(my_data$Length_s)) +
  theme(legend.direction = "horizontal",
        legend.position = c(.75, .25)) +
  facet_wrap(~ .draw, ncol = 2)
```

Both our versions of the plot show what Kruschke pointed out in the text:

> Threshold lines from the same step in the chain must be parallel because the regression coefficients are constant at that step, but are different at another step. The threshold lines in Figure 23.9 are level contours on the underlying metric planar surface, and the lines reveal that the ratings increase toward the top left, that is, as $x_1$ decreases and $x_2$ increases. (p. 693)

Before we move on to the next section, what these diagonal 2-dimensional threshold lines also hint at is that when we use two predictors to describe ordinal data as having been produces by an underlying unit Gaussian distribution, that underlying distribution is actually bivariate Gaussian. Here we'll use `fitted()` one more time to depict that bivariate-Gaussian distribution with a little `geom_raster()`.

```{r, fig.width = 6.5, fig.height = 5}
# define the new data
nd <- crossing(Year_s   = seq(from = -5, to = 5, by = 0.1), 
               Length_s = seq(from = -5, to = 5, by = 0.1))

fitted(fit23.13,
       newdata = nd,
       # this will yield z-scores
       scale = "linear") %>% 
  data.frame() %>%
  bind_cols(nd) %>% 
  # convert the z-scores to density values
  mutate(density = dnorm(Estimate, 0, 1)) %>% 

  ggplot(aes(x = Year_s, y = Length_s)) +
  geom_raster(aes(fill = density),
              interpolate = T) +
  geom_text(data = my_data,
            aes(label = Rating),
            size = 2.5) +
  scale_fill_scico("density", palette = 'lajolla', direction = -1, 
                   limits = c(0, 0.4)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) 
```

As with many of our previous approaches with `geom_raster()`, this plot is based on the posterior means in each cell and, therefore, does a poor job depicting the uncertainty in the posterior distribution.

### Why are some thresholds outside the data?

Now load Kruschke's simulated data.

```{r, message = F}
my_data <- read_csv("data.R/OrdinalProbitData-Movies.csv")

glimpse(my_data)
```

> These data imitate the movie ratings, but with two key differences. First and foremost, the artificial data have much smaller noise, with $\sigma = 0.20$ as opposed to $\sigma \approx 1.25$ in the real data. Second, the artificial data have points that span the entire range of both predictors, unlike the real data which had points mostly in the central region. (p. 695)

Like with the real movie data, we'll inspect these data with a colored scatter plot.

```{r, fig.width = 5, fig.height = 3.25}
my_data %>% 
  mutate(Rating = factor(Rating)) %>% 
  
  ggplot(aes(x = Year, y = Length, label = Rating)) +
  geom_point(aes(color = Rating),
             size = 3) +
  geom_text(size = 3) +
  scale_color_scico_d(palette = "lajolla", begin = .15, end = .7)
```

Unlike in last section, there appears to be a clear trend in Kruschke's simulated data. Kruschke's simulated critic liked his movies old and long. Time to standardize the predictors.

```{r}
my_data <-
  my_data %>% 
  mutate(Year_s   = (Year   - mean(Year))   / sd(Year),
         Length_s = (Length - mean(Length)) / sd(Length))
```

Fitting the multivariable ordered-probit model with **brms** is about as simple as fitting any other multivariable model. Just tack on predictors with the `+` operator.

```{r fit23.14}
fit23.14 <-
  update(fit23.13,
         newdata = my_data,
         iter = 3000, warmup = 1000, chains = 4, cores = 4,
         seed = 23,
         file = "fits/fit23.14")
```

Check the model summary.

```{r}
print(fit23.14)
```

Extract the posterior draws and use `compare_thresholds()` to make our expanded version of the bottom panel of Figure 23.10.

```{r, fig.width = 5, fig.height = 4, message = F, warning = F}
draws <- as_draws_df(fit23.14)

draws %>% 
  select(`b_Intercept[1]`:`b_Intercept[6]`, .draw) %>% 
  compare_thresholds(lb = 1.5, ub = 6.5)
```

Make the marginal distribution of our two effect-size parameters.

```{r, fig.width = 6, fig.height = 2.25, warning = F}
draws %>% 
  pivot_longer(ends_with("_s")) %>% 
  mutate(name = factor(name, levels = c("b_Year_s", "b_Length_s"))) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = sl[4], color = sl[8], normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("effect size") +
  facet_wrap(~ name, scales = "free")
```

Make the top panel for Figure 23.10 just like we did for its analogue in Figure 23.9.

```{r, fig.width = 7.25, fig.height = 6}
# extract the posterior draws and wrangle
as_draws_df(fit23.14) %>% 
  slice(1:3) %>% 
  mutate(.draw = .draw %>% as.factor(),
         b1   = b_Year_s,
         b2   = b_Length_s) %>% 
  expand(nesting(.draw, b1, b2, `b_Intercept[1]`, `b_Intercept[2]`, 
                 `b_Intercept[3]`, `b_Intercept[4]`, `b_Intercept[5]`, `b_Intercept[6]`),
         x1 = c(-10, 10)) %>% 
  pivot_longer(contains("["),
               names_to = "theta") %>% 
  mutate(x2       = (value / b2) + (-b1 / b2) * x1,
         Year_s   = x1,
         Length_s = x2) %>%
  
  # plot!
  ggplot(aes(x = Year_s, y = Length_s)) +
  geom_line(aes(group = interaction(.draw, theta), color = theta, linetype = .draw)) +
  geom_text(data = my_data,
            aes(label = Rating)) +
  scale_color_scico_d(expression(theta), palette = "lajolla", begin = .25, labels = 1:6) +
  coord_cartesian(xlim = range(my_data$Year_s),
                  ylim = range(my_data$Length_s))
```

Those are some tight thresholds. They "very clearly cleave parallel regions of distinct ordinal values. The example demonstrates that the threshold lines *do* make intuitive sense when there is low noise and a broad range of data" (p. 695, *emphasis* in the original).

With our various bonus plots, we've been anticipating Figure 23.11 for some time, now. The thresholds from `fit23.14` result in beautifully nonlinear probability curves for the `Rating` levels. Take a quick look with `conditional_effects()`.

```{r, fig.width = 8, fig.height = 2.5}
ce <- conditional_effects(fit23.14, categorical = T)

plot(ce, plot = FALSE)[[1]] +
  scale_fill_scico_d(palette = "lajolla", begin = .25) +
  scale_color_scico_d(palette = "lajolla", begin = .25) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1))

plot(ce, plot = FALSE)[[2]] +
  scale_fill_scico_d(palette = "lajolla", begin = .25) +
  scale_color_scico_d(palette = "lajolla", begin = .25) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1))
```

Because the model had two predictors, we got two plots. What `brms::conditional_effects()` called `Probability` on the $y$-axis is the same as what Kruschke called $p(y)$ on his. Rather than generic predictors $x$ on the $x$-axis, our plots had either `Year_s` or `Length_s`. Whereas Kruschke marked off his different outcomes by line styles, ours were marked by color. Since we don't have the data Kruschke used to make Figure 23.11, we won't be able to reproduce it exactly. However, you'll note that our plot for `Length_s` corresponded nicely with his subplot on the top (i.e., the one for which $\sigma = 0.1$). If we set `effects = "Length_s"`, we can use `conditional_effects()` to make a similar plot to Kruschke's subplot for which $\sigma = 1$.

```{r, fig.width = 8, fig.height = 2.5}
ce <- conditional_effects(fit23.13, 
                          categorical = T, 
                          effects = "Length_s")

plot(ce, plot = FALSE)[[1]] +
  scale_fill_scico_d(palette = "lajolla", begin = .25) +
  scale_color_scico_d(palette = "lajolla", begin = .25) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA))
```

"You can see that each outcome has maximum probability within its corresponding interval, but there is considerable smearing of outcomes into adjacent intervals" (p. 695).

Finishing off, Kruschke's discussion in the text

> referred to $\sigma$ as "noise" merely for linguistic ease. Calling the outcomes "noisy" does not mean the underlying generator of the outcomes is inherently wildly random. *The "noise" is merely variation in the outcome that cannot be accounted for by the particular model we have chosen with the particular predictors we have chosen*. A different model and/or different predictors might account for the outcomes well with little residual noise. In this sense, the noise is in the model, not in the data. (p. 698, *emphasis* in the original)

Through this lens, noisy-looking data are a symptom of weak theory and/or poor data-collection methods.

## Posterior prediction

The cumulative-normal model makes posterior predictions for the probabilities of the $K$ categories in the criterion variable by computing $p (y | \mu (x), \sigma, \{ \theta_k \} )$ in each step in the HMC chain. In this equation, $\mu (x) = \beta_0 + \sum_j \beta_j x_j$. Though recall that with our **brms** parameterization, we have $\beta_0$ fixed at 0. Kruschke framed part of his discussion in this chapter in terms of a single-predictor model, such as was entertained in Figure 23.8. Recall that corresponds to our `fit11`. Here's that `formula`.

```{r}
fit23.11$formula
```

With **brms**, you can get this information with `fitted()`. Let's say we wanted to focus on response probabilities for `Assets_s = -1)`. Here's what we get.

```{r}
fitted(fit23.11, 
       newdata = tibble(Assets_s = -1))
```

As is typical of **brms**, those probability summaries were in terms of the posterior mean and percentile-based 95% intervals. If you're like Kruschke and prefer posterior modes and HDIs, you'll need to set `summary = F` and wrangle a bit.

```{r, warning = F}
f <-
  fitted(fit23.11, 
         newdata = tibble(Assets_s = -1),
         summary = F)

cbind(
  f[, , 1],
  f[, , 2],
  f[, , 3],
  f[, , 4],
  f[, , 5]
) %>% 
  data.frame() %>% 
  set_names(str_c("p(Happiness = ", 1:5, " | Assets_s = -1)")) %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  mode_hdi(value) %>% 
  mutate_if(is.double, round, digits = 4)
```

## Generalizations and extensions

In this section, Kruschke mentioned extensions of this class of models might include using the cumulative $t$ function to handle outliers or adding a guessing parameter. Full disclosure: I have not fit models like these. Based on my knowledge of **brms**, I suspect they're possible. For insights how, you might review Bürkner's [-@Bürkner2022Define] [*Define custom response distributions with brms*](https://cran.r-project.org/web/packages/brms/vignettes/brms_customfamilies.html) and his [-@Bürkner2022Non_linear] [*Estimating non-linear models with brms*](https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html) vignettes.

In addition, there are other likelihoods one might use to model ordinal data using **brms**. Your first stop should be Bürkner and Vourre's [-@burknerOrdinalRegressionModels2019] [*Ordinal regression models in psychology: A Tutorial*](https://psyarxiv.com/x8swp/), where, in addition to the cumulative normal model, they cover the sequential and adjacent category models. You might also check out [Chapter 11](https://bookdown.org/content/3890/monsters-and-mixtures.html) of my [-@kurzStatisticalRethinkingBrms2020] ebook recoding McElreath's [*Statistical rethinking*](https://xcelab.net/rm/statistical-rethinking/), wherein I show how one might use the logit link (i.e., `family = cumulative(logit)`) to fit ordered-categorical models with **brms**.

## Session info {-}

```{r}
sessionInfo()
```

```{r, message = F, echo = F}
# remove the objects
rm(d, sl, p1, p2, den, theta_1, label_1, bar, theta_3, theta_4, label_3, label_4, make_ordinal, plot_bar_den, p3, p4, my_data_1, fit23.1, draws, means, p, my_upper, my_diag, my_lower, my_data_2, fit23.2, compare_thresholds, mean_y, sd_y, stanvars, fit23.3, fit23.4, my_data, fit23.5, fit23.6, fit23.7, fit23.8, fit23.9, n_draws, nd, f, r, sd_x, m_x, m_y, beta_0_sigma, beta_1_sigma, fit23.10, fit23.11, fit23.12, fit23.13, fit23.14, ce)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:23.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

# Count Predicted Variable

> Consider a situation in which we observe two nominal values for every item measured.... Across the whole sample, the result is a table of counts for each combination of values of the nominal variables. The counts are what we are trying to predict and the nominal variables are the predictors. This is the type of situation addressed in this chapter....
>
> In the context of the generalized linear model (GLM) introduced in Chapter 15, this chapter's situation involves a predicted value that is a count, for which we will use an inverse-link function that is exponential along with a Poisson distribution for describing noise in the data [@kruschkeDoingBayesianData2015, pp. 703--704]

## Poisson exponential model

Following Kruschke, we will "refer to the model that will be explained in this section as Poisson exponential because, as we will see, the noise distribution is a Poisson distribution and the inverse-link function is exponential" (p. 704).

### Data structure.

Kruschke has the @sneeGraphicalDisplayTwoway1974 data for Table 24.1 saved as the `HairEyeColor.csv` file.

```{r, warning = F, message = F}
library(tidyverse)
library(janitor)

my_data <- read_csv("data.R/HairEyeColor.csv")

glimpse(my_data)
```

In order to retain some of the ordering in Table 24.1, we'll want to make `Hair` a factor and recode `Brown` as `Brunette`.

```{r}
my_data <-
  my_data %>% 
  mutate(Hair = if_else(Hair == "Brown", "Brunette", Hair) %>% 
           factor(., levels = c("Black", "Brunette", "Red", "Blond")))
```

Here's a quick way to use `pivot_wider()` to make most of the table.

```{r}
my_data %>% 
  pivot_wider(names_from = Hair,
              values_from = Count)
```

However, that didn't get us the marginal totals. For those, we'll `uncount()` the cells in the data and then make the full table with `janitor::tabyl()` and `janitor::adorn_totals()`.

```{r}
my_data %>%
  uncount(weights = Count, .remove = F) %>% 
  tabyl(Eye, Hair) %>%
  adorn_totals(c("row", "col")) %>% 
  knitr::kable()
```

That last `knitr::kable()` line just formatted the output a bit.

### Exponential link function.

To analyze data like those above,

> a natural candidate for the needed likelihood distribution is the Poisson (described later), which takes a non-negative value $\lambda$ and gives a probability for each integer from zero to infinity.  But this motivation may seem a bit arbitrary, even if there’s nothing wrong with it in principle.
>
> A different motivation starts by treating the cell counts as representative of underlying cell probabilities, and then asking whether the two nominal variables contribute independent influences to the cell probabilities. (p. 705).

The additive model of cell counts of a table of rows $r$ and columns $c$ follows the form

$$\lambda_{r, c} = \exp (\beta_0 + \beta_r + \beta_c),$$

where $\lambda_{r, c}$ is the tendency of counts within row $r$ and column $c$. In the case of an interaction model, the equation expands to

$$\lambda_{r, c} = \exp (\beta_0 + \beta_r + \beta_c + \beta_{r, c}),$$

with the following constraints:

\begin{align*}
\sum_r \beta_r = 0, && \sum_c \beta_c = 0, && \sum_r \beta_{r, c} = 0 \text{ for all } c, && \text{and} && \sum_c \beta_{r, c} = 0 \text{ for all } r.
\end{align*}

### Poisson noise distribution.

[Simon-Denis Poisson](https://upload.wikimedia.org/wikipedia/commons/e/e8/E._Marcellot_Siméon-Denis_Poisson_1804.jpg)'s distribution follows the form

$$p(y | \lambda) = \frac{\lambda^y \exp (-\lambda)}{y!},$$

> where $y$ is a non-negative integer and $\lambda$ is a non-negative real number. The mean of the Poisson distribution is $\lambda$. Importantly, the variance of the Poisson distribution is also $\lambda$ (i.e., the standard deviation is $\sqrt \lambda$). Thus, in the Poisson distribution, the variance is completely yoked to the mean. (p. 707)

We can work with that expression directly in base **R**. Here we use $\lambda = 5.5$ and $y = 2$.

```{r}
lambda <- 5.5
y      <- 2

lambda^y * exp(-lambda) / factorial(y)
```

If we'd like to simulate from the Poisson distribution, we'd use the `rpois()` function. It takes two arguments, `n` and `lambda`. Let's generate 1,000 draws based on $\lambda = 5$.

```{r, message = F, warning = F}
set.seed(24)

d <- tibble(y = rpois(n = 1000, lambda = 5))
```

Here are the mean and variance.

```{r}
d %>% 
  summarise(mean     = mean(y),
            variance = var(y))
```

They're not exactly the same because of simulation variance, but they get that way real quick with a larger sample.

```{r}
set.seed(24)

tibble(y = rpois(n = 100000, lambda = 5)) %>% 
  summarise(mean     = mean(y),
            variance = var(y))
```

Let's put `rpois()` to work and make Figure 24.1.

Before we go any further, let's talk color and theme. For this chapter, we'll take our color palette from the [**ochRe** package](https://github.com/ropenscilabs/ochRe) [@R-ochRe], which provides several color palettes inspired by the art, landscapes, and wildlife of Australia. For the plots to follow, we will use the `"healthy_reef"` palette.

```{r, warning = F, message = F, fig.height = 3}
library(ochRe)

scales::show_col(ochre_palettes[["healthy_reef"]])

hr <- ochre_palettes[["healthy_reef"]]
```

Our overall plot theme will be based on `cowplot::theme_minimal_grid()` with a few adjustments.

```{r, warning = F, message = F}
library(cowplot)

theme_set(
  theme_minimal_grid() +
    theme(legend.key = element_rect(fill = hr[2], color = hr[2]),
          panel.background = element_rect(fill = hr[2], color = hr[2]),
          panel.border = element_rect(color = hr[2]),
          panel.grid = element_blank())
)
```

Now we have our palette and theme, we're ready to make our version of the Poisson histograms in Figure 24.1.

```{r, fig.width = 5.5, fig.height = 5}
set.seed(24)

tibble(lambda = c(1.8, 8.3, 32.1)) %>% 
  mutate(y = map(lambda, rpois, n = 1e5)) %>% 
  unnest(y) %>%
  
  ggplot(aes(x = y)) +
  geom_histogram(aes(y = stat(density)),
                 binwidth = 1, fill = hr[3]) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous("p(y)", expand = expansion(mult = c(0, 0.05))) +
  facet_wrap(~ lambda, ncol = 1,
             labeller = label_bquote(dpois(y*"|"*lambda == .(lambda))))
```

For more on our `labeller = label_bquote` syntax, check out [this](https://ggplot2.tidyverse.org/reference/label_bquote.html).

But anyway, given $\lambda$, the Poisson distribution gives the probabilities of specific non-negative integers. And instead of using our hand-coded function from above, we can also use `dpois()` to get precise density values.

```{r}
dpois(2, lambda = 5.5)
```

### The complete model and implementation in ~~JAGS~~ brms.

To get a sense of Kruschke's hierarchical Bayesian alternative to the conventional ANOVA approach, let's make our version of the model diagram in Figure 24.2.

```{r, fig.width = 8, fig.height = 5, message = F}
library(patchwork)

# bracket
p1 <-
  tibble(x = .95,
         y = .5,
         label = "{_}") %>% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 10, hjust = 1, color = hr[4], family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

##  plain arrow
# save our custom arrow settings
my_arrow <- arrow(angle = 20, length = unit(0.35, "cm"), type = "closed")
p2 <-
  tibble(x    = .68,
         y    = 1,
         xend = .68,
         yend = .25) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = hr[8]) +
  xlim(0, 1) +
  theme_void()

# normal density
p3 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = hr[2]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = hr[8]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("italic(M)[0]", "italic(S)[0]"), 
           size = 7, color = hr[8], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = hr[8]))

# second normal density
p4 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = hr[2]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = hr[8]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("0", "sigma[beta][1]"), 
           size = 7, color = hr[8], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = hr[8]))

# third normal density
p5 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = hr[2]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = hr[8]) +
  annotate(geom = "text",
           x = c(0, 1.45), y = .6,
           hjust = c(.5, 0),
           label = c("0", "sigma[beta][2]"), 
           size = 7, color = hr[8], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = hr[8]))

# fourth normal density
p6 <-
  tibble(x = seq(from = -3, to = 3, by = .1)) %>% 
  ggplot(aes(x = x, y = (dnorm(x)) / max(dnorm(x)))) +
  geom_area(fill = hr[2]) +
  annotate(geom = "text",
           x = 0, y = .2,
           label = "normal",
           size = 7, color = hr[8]) +
  annotate(geom = "text",
           x = 0, y = .6,
           hjust = .5,
           label = "0", 
           size = 7, color = hr[8], family = "Times", parse = T) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = hr[8]))

# likelihood formula
p7 <-
  tibble(x = .5,
         y = .25,
         label = "exp(beta[0]+sum()[italic(j)]*beta[1]['['*italic(j)*']']*italic(x)[1]['['*italic(j)*']'](italic(i))+sum()[italic(k)]*beta[2]['['*italic(k)*']']*italic(x)[2]['['*italic(k)*']'](italic(i))+sum()[italic(jk)]*beta[1%*%2]['['*italic(jk)*']']*italic(x)[1%*%2]['['*italic(jk)*']'](italic(i)))") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 6.75, color = hr[8], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  ylim(0, 1) +
  theme_void()

# sigma
p8 <-
  tibble(x = .13,
         y = .6,
         label = "sigma[beta][1%*%2]") %>% 
 
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(hjust = 0, size = 7, color = hr[8], parse = T, family = "Times") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) +
  ylim(0, 1) +
  theme_void()

# four annotated arrows
p9 <-
  tibble(x    = c(.05, .34, .64, .945),
         y    = c(1, 1, 1, 1),
         xend = c(.06, .2, .47, .77),
         yend = c(0, 0, 0, 0)) %>%
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = hr[8]) +
  annotate(geom = "text",
           x = c(.025, .24, .30, .53, .60, .83, .91), y = .5,
           label = c("'~'", "'~'", "italic(j)", "'~'", "italic(k)", "'~'", "italic(jk)"),
           size = c(10, 10, 7, 10, 7, 10, 7), 
           color = hr[8], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# Poisson density
p10 <-
  tibble(x = 0:9) %>% 
  ggplot(aes(x = x, y = .87 * (dpois(x, lambda = 3.5)) / max(dpois(x, lambda = 3.5))) ) +
  geom_col(color = hr[2], fill = hr[2], width = .45) +
  annotate(geom = "text",
           x = 4.5, y = .2,
           label = "Poisson",
           size = 7, color = hr[8]) +
  annotate(geom = "text",
           x = 4.5, y = .89,
           label = "lambda[italic(i)]",
           size = 7, color = hr[8], family = "Times", parse = TRUE) +
  theme_void() +
  theme(axis.line.x = element_line(size = 0.5, color = hr[8]))

# one annotated arrow
p11 <-
  tibble(x    = .5,
         y    = 1,
         xend = .5,
         yend = .35) %>%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow, color = hr[8]) +
  annotate(geom = "text",
           x = .4, y = .725,
           label = "'='",
           size = 10, color = hr[8], family = "Times", parse = T) +
  xlim(0, 1) +
  theme_void()

# the final annotated arrow
p12 <-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c("'~'", "italic(i)")) %>%

  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7),
            color = hr[8], parse = T, family = "Times") +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow, color = hr[8]) +
  xlim(0, 1) +
  theme_void()

# some text
p13 <-
  tibble(x     = .5,
         y     = .5,
         label = "italic(y[i])") %>%

  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, color = hr[8], parse = T, family = "Times") +
  xlim(0, 1) +
  theme_void()

# define the layout
layout <- c(
  area(t = 1, b = 1, l = 6, r = 7),
  area(t = 1, b = 1, l = 10, r = 11),
  area(t = 1, b = 1, l = 14, r = 15),
  area(t = 2, b = 3, l = 6, r = 7),
  area(t = 2, b = 3, l = 10, r = 11),
  area(t = 2, b = 3, l = 14, r = 15),
  area(t = 3, b = 4, l = 1, r = 3),
  area(t = 3, b = 4, l = 5, r = 7),
  area(t = 3, b = 4, l = 9, r = 11),
  area(t = 3, b = 4, l = 13, r = 15),
  area(t = 6, b = 7, l = 1, r = 15),
  area(t = 3, b = 4, l = 15, r = 16),
  area(t = 5, b = 6, l = 1, r = 15),
  area(t = 10, b = 11, l = 7, r = 9),
  area(t = 8, b = 10, l = 7, r = 9),
  area(t = 12, b = 12, l = 7, r = 9),
  area(t = 13, b = 13, l = 7, r = 9)
)

# combine and plot!
(p1 + p1 + p1 + p2 + p2 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10 + p11 + p12 + p13) + 
  plot_layout(design = layout) &
  ylim(0, 1) &
  theme(plot.margin = margin(0, 5.5, 0, 5.5))
```

Using Kruschke's method,

> the prior is supposed to be broad on the scale of the data, but we must be careful about what scale is being modeled by the baseline and deflections. *The counts are being directly described by* $\lambda$*, but it is* $\log (\lambda)$ *being described by the baseline and deflections*. Thus, the prior on the baseline and deflections should be broad on the scale of the logarithm of the data. To establish a generic baseline, consider that if the data points were distributed equally among the cells, the mean count would be the total count divided by the number of cells. The biggest possible standard deviation across cells would occur when all the counts were loaded into a single cell and all the other cells were zero. (pp. 709--710, *emphasis* added)

Before we show how to fit the model, we need the old `gamma_a_b_from_omega_sigma()` function.

```{r}
gamma_a_b_from_omega_sigma <- function(mode, sd) {
  
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate <- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape <- 1 + mode * rate
  
  return(list(shape = shape, rate = rate))
  
}
```

Here are a few intermediate values before we set the `stanvars`.

```{r, eval = F}
n_x1_level <- length(unique(my_data$x1))
n_x2_level <- length(unique(my_data$x2))
n_cell     <- nrow(my_data)
```

Now we're ready to define the `stanvars`.

```{r, eval = F}
y_log_mean <-
  log(sum(my_data$y) / (n_x1_level * n_x2_level))

y_log_sd <-
  c(rep(0, n_cell - 1), sum(my_data$y)) %>% 
  sd() %>% 
  log()

s_r <- gamma_a_b_from_omega_sigma(mode = y_log_sd, sd = 2 * y_log_sd)

stanvars <- 
  stanvar(y_log_mean, name = "y_log_mean") + 
  stanvar(y_log_sd,   name = "y_log_sd") + 
  stanvar(s_r$shape,  name = "alpha") +
  stanvar(s_r$rate,   name = "beta")
```

You'd then fit a Poisson model with two nominal predictors using Kruschke's hierarchical-shrinkage method like this.

```{r, eval = F}
fit <-
  brm(data = my_data,
      family = poisson,
      y ~ 1 + (1 | x1) + (1 | x2) + (1 | x1:x2),
      prior = c(prior(normal(y_log_mean, y_log_sd * 2), class = Intercept),
                prior(gamma(alpha, beta), class = sd)),
      stanvars = stanvars)
```

By **brms** default, `family = poisson` uses the log link. Thus `family = poisson(link = "log")` should return the same results. Notice the right-hand side of the model `formula`. We have three hierarchical variance parameters. This hierarchical-shrinkage approach to frequency-table data has its origins in @gelmanAnalysisVarianceWhy2005, [*Analysis of variance--why it is more important than ever*](https://projecteuclid.org/download/pdfview_1/euclid.aos/1112967698).

## Example: Hair eye go again

We'll be using the same data, from above. As an alternative to Table 24.1, it might be handy to take a more colorful approach to wading into the data.

```{r, fig.width = 6, fig.height = 2}
# wrangle
my_data %>%
  uncount(weights = Count, .remove = F) %>% 
  tabyl(Eye, Hair) %>%
  adorn_totals(c("row", "col")) %>% 
  data.frame() %>% 
  pivot_longer(-Eye, names_to = "Hair") %>% 
  mutate(Eye = fct_rev(Eye)) %>% 
  
  # plot
  ggplot(aes(x = Hair, y = Eye, label = value)) +
  geom_raster(aes(fill = value)) +
  geom_text(aes(color = value < 250)) +
  scale_fill_gradient(low = hr[9], high = hr[2]) +
  scale_color_manual(values = hr[c(9, 2)]) +
  scale_x_discrete(expand = c(0, 0), position = "top") +
  scale_y_discrete(expand = c(0, 0)) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks = element_blank(),
        legend.position = "none")
```

Load the **brms** and **tidybayes** packages.

```{r, warning = F, message = F}
library(brms)
library(tidybayes)
```

Now we'll save the preparatory values necessary for the `stanvars`.

```{r}
n_x1_level <- length(unique(my_data$Hair))
n_x2_level <- length(unique(my_data$Eye))
n_cell     <- nrow(my_data)

n_x1_level
n_x2_level
n_cell
```

Here are the values we'll save as `stanvars`.

```{r}
y_log_mean <-
  log(sum(my_data$Count) / (n_x1_level * n_x2_level))

y_log_sd <-
  c(rep(0, n_cell - 1), sum(my_data$Count)) %>% 
  sd() %>% 
  log()

s_r <- gamma_a_b_from_omega_sigma(mode = y_log_sd, sd = 2 * y_log_sd)

y_log_mean
y_log_sd
s_r$shape
s_r$rate
```

As a quick detour, it might be interesting to see what the kind of gamma distribution is entailed by those last two values.

```{r, fig.width = 4, fig.height = 2}
tibble(x = seq(from = 0, to = 70, length.out = 1e3)) %>% 
  mutate(density = dgamma(x, s_r$shape, s_r$rate)) %>% 
  
  ggplot(aes(x = x, y = density)) +
  geom_area(fill = hr[4]) +
  scale_x_continuous(NULL, expand = expansion(mult = c(0, -0.05))) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  ggtitle(expression("Kruschke's wide prior for "*sigma[beta*x]))
```

Save the `stanvars`.

```{r}
stanvars <- 
  stanvar(y_log_mean, name = "y_log_mean") + 
  stanvar(y_log_sd,   name = "y_log_sd") + 
  stanvar(s_r$shape,  name = "alpha") +
  stanvar(s_r$rate,   name = "beta")
```

Fit Kruschke's model with **brms**.

```{r, eval = F}
fit24.1 <-
  brm(data = my_data,
      family = poisson,
      Count ~ 1 + (1 | Hair) + (1 | Eye) + (1 | Hair:Eye),
      prior = c(prior(normal(y_log_mean, y_log_sd * 2), class = Intercept),
                prior(gamma(alpha, beta), class = sd)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      seed = 24,
      stanvars = stanvars,
      file = "fits/fit24.01")
```

As it turns out, if you try to fit Kruschke's model with **brms** as is, you'll run into difficulties with divergent transitions and the like. One approach is to try tuning the `adapt_delta` and `max_treedepth` parameters. I had no luck with that approach. E.g., cranking `adapt_delta` up past `0.9999` still returned a divergent transition or two.

Another approach is to step back and assess the model. We're trying to fit a multilevel model with two grouping variables and their interaction with a total of 16 data points. That's not a lot of data for fitting such a model. If you take a close look at our priors, you'll notice they're really quite weak. If you're willing to tighten them up just a bit, the model can fit more smoothly. That will be our approach.

For the $\sigma$ hyperparameter of the overall intercept's Gaussian prior, Kruschke would have us multiply `y_log_sd` by 2. Here we'll tighten up that $\sigma$ hyperparameter by simply setting it to `y_log_sd`. The gamma priors for the upper-level variance parameters were based on a mode of `y_log_sd` and a standard deviation of the same but multiplied by 2 (i.e., `2 * y_log_sd`). We'll tighten that up a bit by simply defining those gammas by a standard deviation of `y_log_sd`. When you make those adjustments, the model fits with less fuss. In case you're curious, here is what those priors look like.

```{r, fig.width = 6, fig.height = 2}
# redifine our shape and rate
s_r <- gamma_a_b_from_omega_sigma(mode = y_log_sd, sd = y_log_sd)

# wrangle
bind_rows(
  # define beta[0]
  tibble(x = seq(from = y_log_mean - (4 * y_log_sd), to = y_log_mean + (4 * y_log_sd), length.out = 1e3)) %>%
    mutate(density = dnorm(x, y_log_mean, y_log_sd)),
  # define sigma[beta[x]]
  tibble(x = seq(from = 0, to = 40, length.out = 1e3)) %>% 
    mutate(density = dgamma(x, s_r$shape, s_r$rate))
) %>%
  mutate(prior = rep(c("beta[0]", "sigma[beta*x]"), each = n() / 2)) %>% 
  
  # plot
  ggplot(aes(x = x, y = density)) +
  geom_area(fill = hr[6]) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Priors",
       x = NULL) +
  facet_wrap(~ prior, scales = "free", labeller = label_parsed)
```

Update the `stanvars`.

```{r}
stanvars <- 
  stanvar(y_log_mean, name = "y_log_mean") + 
  stanvar(y_log_sd,   name = "y_log_sd") + 
  stanvar(s_r$shape,  name = "alpha") +
  stanvar(s_r$rate,   name = "beta")
```

Now we've updated our `stanvars`, we'll fit the modified model. We should note that even this version required some adjustment to the `adapt_delta` and `max_treedepth` parameters. But it wasn't nearly the exercise in frustration entailed in the version, above.

```{r fit24.1}
fit24.1 <-
  brm(data = my_data,
      family = poisson,
      Count ~ 1 + (1 | Hair) + (1 | Eye) + (1 | Hair:Eye),
      prior = c(prior(normal(y_log_mean, y_log_sd), class = Intercept),
                prior(gamma(alpha, beta), class = sd)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.999,
                     max_treedepth = 12),
      stanvars = stanvars,
      seed = 24,
      file = "fits/fit24.01")
```

Take a look at the parameter summary.

```{r}
print(fit24.1)
```

You'll notice that even though we tightened up the priors, the parameter estimates are still quite small relative to the values they allowed for. So even our tightened priors were quite permissive.

Let's post process in preparation for Figure 24.3.

```{r}
nd <-
  my_data %>% 
  arrange(Eye, Hair) %>% 
  # make the titles for the facet strips
  mutate(strip = str_c("E:", Eye, " H:", Hair, "\nN = ", Count))

f <-
  fitted(fit24.1,
         newdata = nd,
         summary = F) %>% 
  data.frame() %>%
  set_names(pull(nd, strip))

glimpse(f)
```

Notice that when working with a Poisson model, `fitted()` defaults to returning estimates in the $\lambda$ metric. If we want proportions/probabilities, we'll have to compute them by dividing by the total $N$. In this case $N = 592$, which we get with `sum(my_data$Count)`. Here we convert the data to the long format, compute the proportions, and plot to make the top portion of Figure 24.3.

```{r, fig.width = 8, fig.height = 6}
f %>% 
  pivot_longer(everything(),
               values_to = "count") %>% 
  mutate(proportion = count / 592) %>% 
  
  ggplot(aes(x = proportion, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = hr[9], color = hr[5],
                     normalize = "panels") +
  scale_x_continuous(breaks = c(0, .1, .2)) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, .25)) +
  facet_wrap(~ name, scales = "free_y")
```

We'll have to work a bit to get the deflection differences. If this was a simple multilevel model with a single random grouping variable, we could just use the `ranef()` function to return the deflections. Like `fitted()`, it'll return summaries by default. But you can get the posterior draws with the `summary = F` argument. But since we used two grouping variables and their interaction, it'd be a bit of a pain to work that way. Happily, we do have a handy alternative. First, if we use the `scale = "linear"` argument, `fitted()` will return the draws in the $\lambda$ scale rather than the original count metric. With the group-level draws in the $\lambda$ metric, all we need to do is subtract the fixed effect (i.e., the grand mean, the population estimate) from each to convert them to the deflection metric. So below, we'll

a) make a custom `make_deflection()` function to do the conversions,
b) redefine our `nd` data to make our naming conventions a little more streamlined,
c) use `fitted()` and its `scale = "linear"` argument to get the draws in the $\lambda$ metric, 
d) wrangle a touch, and 
e) use our handy `make_deflection()` function to convert the results to the deflection metric.

I know; that's a lot. If you get lost, just go step by step and examine the results along the way.

```{r}
# a. make a custom function
make_deflection <- function(x) {
  x - as_draws_df(fit24.1)$b_Intercept
}

# b. streamline `nd`
nd <-
  my_data %>% 
  arrange(Eye, Hair) %>% 
  mutate(strip = str_c("E:", Eye, " H:", Hair))

# c. use `fitted()`
deflections <-
  fitted(fit24.1,
         newdata = nd,
         summary = F,
         scale = "linear") %>% 
  # d. wrangle
  data.frame() %>%
  set_names(pull(nd, strip)) %>% 
  # e. use the `make_deflection()` function
  mutate_all(.funs = make_deflection)

# what have we done?
glimpse(deflections)
```

Now we're ready to define our difference columns and plot our version of the lower panels in Figure 24.3.

```{r, fig.width = 8, fig.height = 2.5}
deflections %>% 
  transmute(`Blue − Brown @ Black` = `E:Blue H:Black` - `E:Brown H:Black`,
            `Blue − Brown @ Blond` = `E:Blue H:Blond` - `E:Brown H:Blond`) %>% 
  mutate(`Blue.v.Brown\n(x)\nBlack.v.Blond` = `Blue − Brown @ Black` - `Blue − Brown @ Blond`) %>% 
  pivot_longer(everything(), values_to = "difference") %>% 

  ggplot(aes(x = difference, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = hr[9], color = hr[5],
                    normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free")
```

If you're curious, here are the precise summary values.

```{r}
deflections %>% 
  mutate(`Blue − Brown @ Black` = `E:Blue H:Black` - `E:Brown H:Black`,
         `Blue − Brown @ Blond` = `E:Blue H:Blond` - `E:Brown H:Blond`) %>% 
  mutate(`Blue.v.Brown\n(x)\nBlack.v.Blond` = `Blue − Brown @ Black` - `Blue − Brown @ Blond`) %>% 
  pivot_longer(`Blue − Brown @ Black`:`Blue.v.Brown\n(x)\nBlack.v.Blond`) %>% 
  group_by(name) %>% 
  mode_hdi(value)
```

## Example: Interaction contrasts, shrinkage, and omnibus test

"In this section, we consider some contrived data to illustrate aspects of interaction contrasts. Like the eye and hair data, the fictitious data have two attributes with four levels each" (p. 713). Let's make the data.

```{r}
my_data <-
  crossing(a = str_c("a", 1:4),
           b = str_c("b", 1:4)) %>% 
  mutate(count = c(rep(c(22, 11), each = 2) %>% rep(., times = 2),
                   rep(c(11, 22), each = 2) %>% rep(., times = 2)))

head(my_data)
```

In the last section, we covered how Kruschke's broad priors can make fitting these kinds of models difficult when using HMC, particularly with so few cells. Our solution was to reign in the $\sigma$ hyperparameter for the level-one intercept and to compute the gamma prior for the hierarchical deflections based on a standard deviation of the log of the maximum standard deviation for the data rather than two times that value. Let's explore more options.

This data set has 16 cells. With so few cells, one might argue for a more conservative prior on the hierarchical deflections. Why not ditch the gamma altogether for a half normal centered on zero and with a $\sigma$ hyperparameter of 1? Even though this is much tighter than Kruschke's gamma prior approach, it's still permissive on the $\log$ scale. As for our intercept, we'll continue with the same approach from last time.

With that in mind, make the `stanvars`.

```{r}
n_x1_level <- length(unique(my_data$a))
n_x2_level <- length(unique(my_data$b))
n_cell     <- nrow(my_data)

y_log_mean <-
  log(sum(my_data$count) / (n_x1_level * n_x2_level))

y_log_sd <-
  c(rep(0, n_cell - 1), sum(my_data$count)) %>% 
  sd() %>% 
  log()

stanvars <- 
  stanvar(y_log_mean, name = "y_log_mean") + 
  stanvar(y_log_sd,   name = "y_log_sd")
```

Just for kicks, let's take a quick look at our priors.

```{r, fig.width = 6, fig.height = 2}
bind_rows(
  # define beta[0]
  tibble(x = seq(from = y_log_mean - (4 * y_log_sd), to = y_log_mean + (4 * y_log_sd), length.out = 1e3)) %>%
    mutate(density = dnorm(x, y_log_mean, y_log_sd)),
  # define sigma[beta[x]]
  tibble(x = seq(from = 0, to = 5, length.out = 1e3)) %>% 
    mutate(density = dnorm(x, 0, 1))
) %>%
  mutate(prior = rep(c("beta[0]", "sigma[beta*x]"), each = n() / 2)) %>% 
  
  ggplot(aes(x = x, y = density)) +
  geom_area(fill = hr[6]) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Priors",
       x = NULL) +
  facet_wrap(~ prior, scales = "free", labeller = label_parsed)
```

Fit the model.

```{r fit24.2}
fit24.2 <-
  brm(data = my_data,
      family = poisson,
      count ~ 1 + (1 | a) + (1 | b) + (1 | a:b),
      prior = c(prior(normal(y_log_mean, y_log_sd), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.995),
      stanvars = stanvars,
      seed = 24,
      file = "fits/fit24.02")
```

Review the summary.

```{r}
print(fit24.2)
```

We might plot our $\sigma_\text{<group>}$ posteriors against our prior to get a sense of how strong it was.

```{r, fig.width = 7, fig.height = 3, warning = F}
as_draws_df(fit24.2) %>% 
  dplyr::select(starts_with("sd")) %>% 
  # set_names(str_c("expression(sigma", c("*a", "*ab", "*b"), ")")) %>% 
  pivot_longer(everything(), names_to = "sd") %>% 
  
  ggplot(aes(x = value)) +
  # prior
  geom_area(data = tibble(value = seq(from = 0, to = 3.5, by  =.01)),
            aes(y = dnorm(value, 0, 1)),
            fill = hr[8]) +
  # posterior
  geom_density(aes(fill = sd),
               size = 0, alpha = 2/3) +
  scale_fill_manual(values = hr[c(4, 6, 3)],
                    labels = c(expression(sigma[a]),
                                  expression(sigma[ab]),
                                  expression(sigma[b])),
                    guide = guide_legend(label.hjust = 0)) +
  scale_x_continuous(NULL, expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  theme(legend.background = element_rect(fill = "transparent"),
        legend.position = c(.9, .8))
```

Our $\operatorname{Normal}^+ (0, 1)$ prior is that short dark shape in the background. The posteriors are the taller and more colorful mounds in the foreground. Here's the top part of Figure 24.4.

```{r, fig.width = 8, fig.height = 6}
nd <-
  my_data %>% 
  mutate(strip = str_c("a:", a, " b:", b, "\nN = ", count))

fitted(fit24.2,
       newdata = nd,
       summary = F) %>% 
  data.frame() %>%
  set_names(pull(nd, strip)) %>% 
  pivot_longer(everything(), values_to = "count") %>% 
  mutate(proportion = count / sum(my_data$count)) %>% 
  
  # plot!
  ggplot(aes(x = proportion, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = hr[7], color = hr[6], normalize = "panels") +
  scale_x_continuous(breaks = c(.05, .1, .15)) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, .15)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free_y")
```

Like before, we'll have to work a bit to get the deflection differences.

```{r}
# streamline `nd`
nd <-
  my_data %>% 
  mutate(strip = str_c("a:", a, " b:", b))

# use `fitted()`
deflections <-
  fitted(fit24.2,
         newdata = nd,
         summary = F,
         scale = "linear") %>% 
  # wrangle
  data.frame() %>%
  set_names(pull(nd, strip)) %>% 
  # use the `make_deflection()` function
  mutate_all(.funs = make_deflection)

# what have we done?
glimpse(deflections)
```

Now we're ready to define some of the difference columns and plot our version of the leftmost lower panel in Figure 24.4.

```{r, fig.width = 3.25, fig.height = 2.75}
deflections %>% 
  transmute(`a2 - a3 @ b2` = `a:a2 b:b2` - `a:a3 b:b2`,
            `a2 - a3 @ b3` = `a:a2 b:b3` - `a:a3 b:b3`) %>% 
  mutate(`a2.v.a3\n(x)\nb2.v.b3` = `a2 - a3 @ b2` - `a2 - a3 @ b3`) %>% 
  pivot_longer(`a2.v.a3\n(x)\nb2.v.b3`, values_to = "difference") %>% 

  ggplot(aes(x = difference, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = hr[7], color = hr[6]) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-.5, 2.5)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free")
```

For Figure 24.4, bottom right, we average across the four cells in each quadrant and then compute the contrast.

```{r, fig.width = 3.25, fig.height = 2.75}
deflections %>% 
  # in this intermediate step, we compute the quadrant averages
  # `tl` = top left, `br` = bottom right, and so on
  transmute(tl = (`a:a1 b:b1` + `a:a1 b:b2` + `a:a2 b:b1` + `a:a2 b:b2`) / 4,
            tr = (`a:a1 b:b3` + `a:a1 b:b4` + `a:a2 b:b3` + `a:a2 b:b4`) / 4,
            bl = (`a:a3 b:b1` + `a:a3 b:b2` + `a:a4 b:b1` + `a:a4 b:b2`) / 4,
            br = (`a:a3 b:b3` + `a:a3 b:b4` + `a:a4 b:b3` + `a:a4 b:b4`) / 4) %>%
  # compute the contrast of interest
  mutate(`A1.A2.v.A3.A4\n(x)\nB1.B2.v.B3.B4` = (tl - bl) - (tr - br)) %>% 
  pivot_longer(`A1.A2.v.A3.A4\n(x)\nB1.B2.v.B3.B4`, values_to = "difference") %>%
  
  # plot
  ggplot(aes(x = difference, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = hr[7], color = hr[6]) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-.5, 2.5)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free")
```

> The model presented here has no way to conduct an "ominbus" test of interaction. However, like the ANOVA-style models presented in Chapters 19 and 20, it is easy to extend the model so it has an inclusion coefficient on the interaction deflections. The inclusion coefficient can have values of 0 or 1, and is given a Bernoulli prior. (p. 716)

Like we discussed in earlier chapters, this isn't a feasible approach for **brms**. However, we can compare this model with a simpler one that omits the interaction. First, fit the model.

```{r fit24.3}
fit24.3 <-
  brm(data = my_data,
      family = poisson,
      count ~ 1 + (1 | a) + (1 | b),
      prior = c(prior(normal(y_log_mean, y_log_sd), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.9999),
      stanvars = stanvars,
      seed = 24,
      file = "fits/fit24.03")
```

Now we can compare them by their stacking weights.

```{r, message = F, warning = F}
model_weights(fit24.2, fit24.3) %>% 
  round(digits = 3)
```

Virtually all the weight went to the interaction model. Also, if we step back and ask ourselves what the purpose of an omnibus text of an interaction is for in this context, we'd conclude such a test is asking the question *Is* $\sigma_{a \times b}$ *the same as zero?* Let's look again at that posterior from `fit24.2`.

```{r, fig.width = 5, fig.height = 3}
as_draws_df(fit24.2) %>% 
  
  ggplot(aes(x = `sd_a:b__Intercept`, y = 0)) +
  stat_halfeye(.width = .95, fill = hr[7], color = hr[6]) +
  scale_x_continuous(expression(sigma[a%*%b]), 
                     expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Does this look the same as zero, to you?")
```

Sure, there's some uncertainty in that posterior. But that is not zero. We didn't need an omnibus test or even model comparison via stacking weights to figure that one out.

If you wanted to get fancy with it, we might even do a hierarchical variance decomposition. Here the question is what percentage of the hierarchical variance is attributed to `a`, `b` and their interaction? Recall that **brms** returns those variance parameters in the $\sigma$ metric. So before we can compare them in terms of percentages of the total variance, we have to first have to square them.

```{r, warning = F}
draws <-
  as_draws_df(fit24.2) %>% 
  transmute(`sigma[a]^2`  = sd_a__Intercept^2,
            `sigma[b]^2`  = sd_b__Intercept^2,
            `sigma[ab]^2` = `sd_a:b__Intercept`^2) %>% 
  mutate(`sigma[total]^2` = `sigma[a]^2` + `sigma[b]^2` + `sigma[ab]^2`)

head(draws)
```

Now we just need to divide the individual variance parameters by their total and multiply by 100 to get the percent of variance for each. We'll look at the results in a plot.

```{r, fig.width = 4, fig.height = 2.75}
draws %>% 
  pivot_longer(-`sigma[total]^2`) %>% 
  mutate(`% hierarchical variance` = 100 * value / `sigma[total]^2`) %>% 
  
  ggplot(aes(x = `% hierarchical variance`, y = name)) +
  stat_halfeye(.width = .95, fill = hr[7], color = hr[6]) +
  scale_x_continuous(limits = c(0, 100), expand = c(0, 0)) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_cartesian(ylim = c(1.5, 3.5)) +
  theme(plot.margin = margin(5.5, 10, 5.5, 5.5))
```

Just as each of the variance parameters was estimated with uncertainty, all that uncertainty got propagated into their transformations. Even in the midst of all this uncertainty, it's clear that a good healthy portion of the hierarchical variance is from the interaction. Again, whatever you might think about $a \times b$, it's definitely not zero.

## ~~Log-linear models for contingency tables~~ Bonus: Alternative parameterization

The Poisson distribution is widely used for count data. But notice how in our figures, we converted the results to the proportion metric. Once you're talking about proportions, it's not hard to further adjust your approach to thinking in terms of probabilities. So instead of thinking about the $n$ within each cell of our contingency table, we might also think about the probability of a given condition. To approach the data this way, we could use a multilevel aggregated binomial model. McElreath covered this in Chapter 10 of his [*Statistical rethinking*](https://xcelab.net/rm/statistical-rethinking/). See my [-@kurzStatisticalRethinkingBrms2020] [translation of the text into brms code](https://bookdown.org/content/3890/counting-and-classification.html#aggregated-binomial-chimpanzees-again-condensed.), too.

Here's how to fit that model.

```{r fit24.4}
fit24.4 <-
  brm(data = my_data,
      family = binomial,
      count | trials(264) ~ 1 + (1 | a) + (1 | b) + (1 | a:b),
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.999),
      seed = 24,
      file = "fits/fit24.04")
```

A few things about the syntax: The aggregated binomial model uses the logit link, just like with typical logistic regression. So when you specify `family = binomial`, you're requesting the logit link. The left side of the `formula` argument, `count | trials(264)` indicates a few things. First, our criterion is `count`. The bar `|` that follows on its right indicates we'd like add additional information about the criterion. In the case of binomial regression, **brms** requires we specify how many trials the value in each cell of the data is referring to. When we coded `trials(264)`, we indicated each cell was a total count of 264 trials. In case it isn't clear, here is where the value 264 came from.

```{r}
my_data %>% 
  summarise(total_trials = sum(count))
```

Now look over the summary.

```{r}
print(fit24.4)
```

See that `mu = logit` part in the second line of the summary? Yep, that's our link function. Since we used a different likelihood and link function from earlier models, it shouldn't be surprising the parameters look different. But notice how the aggregated binomial model yields virtually the same results for the top portion of Figure 24.4.

```{r, fig.width = 8, fig.height = 6}
nd <-
  my_data %>% 
  mutate(strip = str_c("a:", a, " b:", b, "\nN = ", count))

fitted(fit24.4,
       newdata = nd,
       summary = F) %>% 
  data.frame() %>%
  set_names(pull(nd, strip)) %>% 
  pivot_longer(everything(), values_to = "count") %>% 
  mutate(proportion = count / sum(my_data$count)) %>% 
  
  # plot!
  ggplot(aes(x = proportion, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = hr[5], color = hr[3]) +
  scale_x_continuous(breaks = c(.05, .1, .15)) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, .15)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free_y")
```

To further demonstrate the similarity of this approach to Kruschke's multilevel Poisson approach, let's compare the model-based cell estimates for each of the combinations of `a` and `b`, by both `fit24.2` and `fit24.4`.

```{r, fig.width = 8, fig.height = 3}
# compute the fitted summary statistics
rbind(fitted(fit24.2),
      fitted(fit24.4)) %>% 
  data.frame() %>% 
  # add an augmented version of the data
  bind_cols(expand(my_data, 
                   fit = c("fit2 (Poisson likelihood)", "fit4 (binomial likelihood)"),
                   nesting(a, b, count))) %>% 
  mutate(cell = str_c(a, "\n", b)) %>% 
  
  # plot
  ggplot(aes(x = cell)) +
  geom_hline(yintercept = c(11, 22), color = hr[3], linetype = 2) +
  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, color = fit),
                  fatten = 1.5, position = position_dodge(width = 0.5)) +
  geom_point(aes(y = count),
             size = 2, color = hr[9]) +
  scale_color_manual(NULL, values = hr[c(7, 5)]) +
  scale_y_continuous("count", breaks = c(0, 11, 22, 33), limits = c(0, 33)) +
  theme(legend.position = "top")
```

The black points are the raw data. The colored point-ranges to the left and right of each data point are the posterior means and percentile-based 95% intervals for each of the cells. The results are virtually the same between the two models. Also note how both models partially pooled towards the grand mean. That's one of the distinctive features of using the hierarchical approach.

Wrapping up, this chapter focused on how one might use the Poisson likelihood to model contingency-table data from a multilevel modeling framework. The Poisson likelihood is also handy for count data within a single-level structure, with metric predictors, and with various combinations of metric and nominal predictors. For more practice along those lines, check out [Section 10.2](https://bookdown.org/content/3890/counting-and-classification.html#poisson-regression) in my project recoding McElreath's [*Statistical rethinking*](https://xcelab.net/rm/statistical-rethinking/).

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
# remove our objects
rm(lambda, y, d, hr, p1, my_arrow, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, layout, gamma_a_b_from_omega_sigma, n_x1_level, n_x2_level, n_cell, y_log_mean, y_log_sd, s_r, stanvars, my_data, fit24.1, nd, f, make_deflection, deflections, fit24.2, fit24.3, draws, fit24.4)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:24.Rmd-->


```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
```

# Tools in the Trunk

"This chapter includes some important topics that apply to many different models throughout the book... The sections can be read independently of each other and at any time" [@kruschkeDoingBayesianData2015, p. 721].

## Reporting a Bayesian analysis

> Bayesian data analyses are not yet standard procedure in many fields of research, and no conventional format for reporting them has been established. Therefore, the researcher who reports a Bayesian analysis must be sensitive to the background knowledge of his or her specific audience, and must frame the description accordingly. (p. 721)

At the time of this writing (early 2020), this is still the case. See @aczelDiscussionPointsBayesian2020, [*Discussion points for Bayesian inference*](https://www.researchgate.net/publication/338849264_Discussion_points_for_Bayesian_inference), for a recent discussion from several Bayesian scholars.

### Essential points.

> Recall the basic steps of a Bayesian analysis from [Section 2.3][The steps of Bayesian data analysis] (p. 25): Identify the data, define a descriptive model, specify a prior, compute the posterior distribution, interpret the posterior distribution, and, check that the model is a reasonable description of the data. Those steps are in logical order, with each step building on the previous step. That logical order should be preserved in the report of the analysis. (p. 722)

Kruschke then gave recommendations for motivating Bayesian inference. His [-@kruschkeBayesianNewStatistics2018] paper with Liddell, [*The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective*](https://link.springer.com/content/pdf/10.3758/s13423-016-1221-4.pdf), might be helpful in this regard. Many of the other points Kruschke made in this section (e.g., adequately reporting the data structure, the priors, evidence for convergence) can be handled by adopting open science practices.

If your data and research questions are simple and straightforward, you might find it easy to detail these and other concerns in the primary manuscript. The harsh reality is many journals place tight constraints on word and/or page limits. If your projects are not of the simple and straightforward type, supplemental materials are your friend. Regardless of a journal's policy on hosting supplemental materials on the official journal website, you can detail your data, priors, MCMC diagnostics, and all the other fine-grained details of your analysis in supplemental documents hosted in publicly-accessible repositories like the [Open Science Framework (OSF)](https://osf.io/). If possible, do consider making your data openly available. Regardless of the status of your data, please consider making all your **R** scripts available as supplementary material. To reiterate from Chapter 3, I strongly recommend checking out [R Notebooks](https://bookdown.org/yihui/rmarkdown/notebook.html) for that purpose. They are a type of R Markdown document with augmentations that make them more useful for working scientists. You can learn more about them [here](https://rstudio-pubs-static.s3.amazonaws.com/256225_63ebef4029dd40ef8e3679f6cf200a5a.html) and [here](https://www.r-bloggers.com/why-i-love-r-notebooks-2/). And for a more comprehensive overview, check out Xie, Allaire, and Grolemund's [-@xieMarkdownDefinitiveGuide2022] [*R markdown: The definitive guide*](https://bookdown.org/yihui/rmarkdown/).

### Optional points.

For more thoughts on robustness checks, check out a couple Gelman's blog posts, [*What's the point of a robustness check?*](https://statmodeling.stat.columbia.edu/2017/11/29/whats-point-robustness-check/) and [*Robustness checks are a joke*](https://statmodeling.stat.columbia.edu/2018/11/14/robustness-checks-joke/), along with the action in the comments section.

In addition to posterior predictive checks, which are great [see @kruschkePosteriorPredictiveChecks2013], consider prior predictive checks, too. For a great introduction to the topic, check out Gabry, Simpson, Vehtari, Betancourt, and Gelman's [-@gabry2019visualization] [*Visualization in Bayesian workflow*](https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12378).

### Helpful points.

For more ideas on open data, check out Rouder's [-@rouderWhatWhyHow2016] [*The what, why, and how of born-open data*](https://link.springer.com/article/10.3758/s13428-015-0630-z). You might also check out Klein and colleagues' [-@kleinPracticalGuideTransparency2018] [*A practical guide for transparency in psychological science*](https://lirias.kuleuven.be/1999530?limo=0) and Martone, Garcia-Castro, and VandenBos's [-@martoneDataSharingPsychology2018] [*Data sharing in psychology*](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5920518/pdf/nihms935471.pdf).

As to posting your model fits, this could be done in any number of ways, including as official supplemental materials hosted by the journal, on GitHub, or on the OSF. At a base level, this means saving your fits as external files. We've already been modeling this with our `brm()` code throughout this book. With the `save` argument, we saved the model fits within the [`fits` folder on GitHub](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/tree/master/fits). You might adopt a similar approach. But do be warned: **brms** fit objects contain a copy of the data used to create them. For example, here's how we might reload `fit24.1` from last chapter.

```{r, message = F}
fit24.1 <- readRDS("fits/fit24.01.rds")
```

By indexing the fit object with `$data`, you can see the data.

```{r, warning = F, message = F}
library(tidyverse)
library(brms)

fit24.1$data %>% 
  glimpse()
```

Here's a quick way to remove the data from the fit object.

```{r}
fit24.1$data <- NULL
```

Confirm it worked.

```{r}
fit24.1$data
```

Happily, the rest of the information is still there for you. E.g., here's the summary.

```{r}
summary(fit24.1)
```

## Functions for computing highest density intervals

You can find a copy of Kruschke's scripts, including `DBDA2E-utilities.R`, at [https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/tree/master/data.R](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/tree/master/data.R).

### R code for computing HDI of a grid approximation.

> We can imagine the grid approximation of a distribution as a landscape of poles sticking up from each point on the parameter grid, with the height of each pole indicating the probability mass at that discrete point. We can imagine the highest density region by visualizing a rising tide: We gradually flood the landscape, monitoring the total mass of the poles that protrude above water, stopping the flood when 95% (say) of the mass remains protruding. The waterline at that moment defines the highest density region [e.g., @hyndmanComputingGraphingHighest1996]. (p. 725)

```{r}
HDIofGrid <- function(probMassVec, credMass = 0.95) {
  
  # Arguments:
  #   probMassVec is a vector of probability masses at each grid point.
  #   credMass is the desired mass of the HDI region.
  # Return value:
  #   A list with components:
  #   indices is a vector of indices that are in the HDI
  #   mass is the total mass of the included indices
    #   height is the smallest component probability mass in the HDI
  # Example of use: For determining HDI of a beta(30,12) distribution
  #   approximated on a grid:
  #   > probDensityVec = dbeta( seq(0,1,length=201) , 30 , 12 )
  #   > probMassVec = probDensityVec / sum( probDensityVec )
  #   > HDIinfo = HDIofGrid( probMassVec )
  #   > show( HDIinfo )
  
  sortedProbMass <- sort(probMassVec, decreasing = TRUE)
  HDIheightIdx <- min(which(cumsum(sortedProbMass) >= credMass))
  HDIheight <- sortedProbMass[HDIheightIdx]
  HDImass <- sum(probMassVec[probMassVec >= HDIheight])
  
  return(list(indices = which(probMassVec >= HDIheight),
              mass    = HDImass, height = HDIheight))
  
}
```

I found Kruschke's description of his `HDIofGrid()` a bit opaque. Happily, we can understand this function with a little help from an example posted at [https://rdrr.io/github/kyusque/DBDA2E-utilities/man/HDIofGrid.html](https://rdrr.io/github/kyusque/DBDA2E-utilities/man/HDIofGrid.html).

```{r}
prob_density_vec <- dbeta(seq(0, 1, length = 201), 30, 12)
prob_mass_vec    <- prob_density_vec / sum(prob_density_vec)
HDI_info         <- HDIofGrid(prob_mass_vec)

show(HDI_info)
```

To walk that through a bit, `prob_density_vec` is a vector of density values for $\operatorname{Beta} (30, 12)$ based on 201 evenly-spaced values spanning the parameter space for $\theta$ (i.e., from 0 to 1). In the second line, we converted those density values to the probability metric by dividing each by their sum, which we then saved as `prob_mass_vec`. In the third line we shoved those probability values into Kruschke's `HDIofGrid()` and saved the results as `HDI_info`. The output of the fourth line, `show(HDI_info)`, showed us the results (i.e., the contents of `HDI_info`).

As to those results, the values in saved as `$indices` are the row numbers for all cases in `prob_mass_vec` that were within the HDI. The value in `$mass` showed the actual width of the HDI. Because we're only working with finite samples (i.e., `length = 201`), we won't likely get a perfect 95% HDI. The value in `$height` is the density value for *the waterline that defines the highest density region*. A plot might make that less abstract.

```{r, fig.width = 7, fig.height = 2.75, warning = F, message = F}
library(ggthemes)

theme_set(
  theme_base() +
  theme(plot.background = element_rect(color = "transparent"))
)

# wrangle
tibble(row     = 1:length(prob_density_vec),
       theta   = seq(0, 1, length = length(prob_density_vec)),
       density = prob_mass_vec,
       cred    = if_else(row %in% HDI_info$indices, 1, 0)) %>% 
  
  # plot
  ggplot(aes(x = theta, y = density)) +
  # HDI
  geom_area(aes(fill = cred == 1)) +
  # density line
  geom_line(color = "skyblue", size = 1) +
  # waterline
  geom_hline(yintercept = HDI_info$height, linetype = 3, size = 1/4) +
  # fluff
  annotate(geom = "text", x = .2, y = 0.0046,
           label = '"waterline" that defines all points\ninside the highest density region') +
  annotate(geom = "text", x = .715, y = 0.01,
           label = "95.28% HDI", color = "black", size = 5) +
  scale_fill_manual(values = c("transparent", "grey67")) +
  xlab(expression(theta))
```

You might have noticed our `theme_set()` lines at the top of that code block. For the plots in this chapter, we'll give a nod to the source material and make them with a similar aesthetic to Kruschke's plots.

### HDI of unimodal distribution is shortest interval.

> The algorithms [in the next sections] find the HDI by searching among candidate intervals of mass $M$. The shortest one found is declared to be the HDI. It is an approximation, of course. See @chenMonteCarloEstimation1999 for more details, and Chen, He, Shao, and Xu [-@chenMonteCarloGap2003] for dealing with the unusual situation of multimodal distributions. (p. 727)

### R code for computing HDI of a MCMC sample.

In this section, Kruschke provided the code for his `HDIofMCMC()` function. We recreate it, below, with a few mild formatting changes. The `ggthemes::theme_base()` theme gets us most of the way there.

```{r}
HDIofMCMC <- function(sampleVec, credMass = .95) {
  
  # Computes highest density interval from a sample of representative values,
  #   estimated as shortest credible interval.
  # Arguments:
  #   sampleVec
  #     is a vector of representative values from a probability distribution.
  #   credMass
  #     is a scalar between 0 and 1, indicating the mass within the credible
  #     interval that is to be estimated.
  # Value:
  #   HDIlim is a vector containing the limits of the HDI
  
  sortedPts <- sort(sampleVec)
  ciIdxInc <- ceiling(credMass * length(sortedPts))
  nCIs <- length(sortedPts) - ciIdxInc
  ciWidth <- rep(0, nCIs)
  for (i in 1:nCIs) {
    ciWidth[i] <- sortedPts[i + ciIdxInc] - sortedPts[i]
  }
  HDImin <- sortedPts[which.min(ciWidth)]
  HDImax <- sortedPts[which.min(ciWidth) + ciIdxInc]
  HDIlim <- c(HDImin, HDImax)
  
  return(HDIlim)
  
}
```

Let's continue working with `fit24.1` to see how Kruschke's `HDIofMCMC()` works. First we need to extract the posterior draws.

```{r}
draws <- as_draws_df(fit24.1)
```

Here's how you might use the function to get the HDIs for the intercept parameter.

```{r}
HDIofMCMC(draws$b_Intercept)
```

Kruschke's `HDIofMCMC()` works very much the same as the summary functions from **tidybayes**. For example, here's good old `tidybayes::mode_hdi()`.

```{r, warning = F, message = F}
library(tidybayes)

mode_hdi(draws$b_Intercept)
```

If you'd like to use **tidybayes** to just pull the HDIs without the extra information, just use the `hdi()` function.

```{r}
hdi(draws$b_Intercept)
```

Just in case you're curious, Kruschke's `HDIofMCMC()` function returns the same information as `tidybayes::hdi()`. Let's confirm.

```{r}
HDIofMCMC(draws$b_Intercept) == hdi(draws$b_Intercept)
```

Identical.

### R code for computing HDI of a function.

> The function described in this section finds the HDI of a unimodal probability density function that is specified mathematically in R. For example, the function can find HDI's of normal densities or of beta densities or of gamma densities, because those densities are specified as functions in R. (p. 728)

If you recall, we've been using this function off and on since Chapter 4. Here is it, again, with mildly reformatted code and parameter names.

```{r}
hdi_of_icdf <- function(name, width = .95, tol = 1e-8, ... ) {
  
  # Arguments:
  #   `name` is R's name for the inverse cumulative density function
  #   of the distribution.
  #   `width` is the desired mass of the HDI region.
  #   `tol` is passed to R's optimize function.
  # Return value:
  #   Highest density iterval (HDI) limits in a vector.
  # Example of use: For determining HDI of a beta(30, 12) distribution, type
  #   `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)`
  #   Notice that the parameters of the `name` must be explicitly stated;
  #   e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work.
  # Adapted and corrected from Greg Snow's TeachingDemos package.
  
  incredible_mass <-  1.0 - width
  interval_width <- function(low_tail_prob, name, width, ...) {
    name(width + low_tail_prob, ...) - name(low_tail_prob, ...)
  }
  opt_info <- optimize(interval_width, c(0, incredible_mass), 
                       name = name, width = width, 
                       tol = tol, ...)
  hdi_lower_tail_prob <- opt_info$minimum
  
  return(c(name(hdi_lower_tail_prob, ...),
           name(width + hdi_lower_tail_prob, ...)))
  
}
```

Here's how it works for the standard normal distribution.

```{r}
hdi_of_icdf(qnorm, mean = 0, sd = 1)
```

By default, it returns 95% HDIs. Here's how it'd work if you wanted the 80% intervals for $\operatorname{Beta}(2, 2)$.

```{r}
hdi_of_icdf(qbeta, shape1 = 2, shape2 = 2, width = .8)
```

## Reparameterization

> There are situations in which one parameterization is intuitive to express a distribution, but a different parameterization is required for mathematical convenience. For example, we may think intuitively of the standard deviation of a normal distribution, but have to parameterize the distribution in terms of the precision (i.e., reciprocal of the variance). (p. 729)

The details in the rest of this section are beyond the scope of this project.

## Censored data in ~~JAGS~~ brms

"In many situations some data are censored, which means that their values are known only within a certain range" (p. 732) Happily, **brms** is capable of handling censored variables. The setup is a little different from how Kruschke described for JAGS. From the `brmsformula` section of the [**brms** reference manual](https://CRAN.R-project.org/package=brms/brms.pdf) [@brms2022RM], we read:

> With the exception of categorical, ordinal, and mixture families, left, right, and interval censoring can be modeled through `y | cens(censored) ~ predictors`. The censoring variable (named `censored` in this example) should contain the values `'left'`, `'none'`, `'right'`, and `'interval'` (or equivalently `-1`, `0`, `1`, and `2`) to indicate that the corresponding observation is left censored, not censored, right censored, or interval censored. For interval censored data, a second variable (let's call it `y2`) has to be passed to `cens`. In this case, the formula has the structure `y | cens(censored,y2) ~ predictors`. While the lower bounds are given in `y`, the upper bounds are given in `y2` for interval censored data. Intervals are assumed to be open on the left and closed on the right: `(y,y2]`.

We'll make sense of all this in just a moment. First, let's see how Kruschke described the example in the text.

> To illustrate why it is important to include censored data in the analysis, consider a case in which $N = 500$ values are generated randomly from a normal distribution with $\mu = 100$ and $\sigma = 15$. Suppose that values above 106 are censored, as are values in the interval between 94 and 100. For the censored values, all we know is the interval in which they occurred, but not their exact value. (p. 732)

I'm not aware that we have access to Kruschke's censored data, so we'll just make our own based on his description. We'll start off by simulating the idealized uncensored data, `y`, based on $\operatorname{Normal}(100, 15)$.

```{r}
n <- 500

set.seed(25)

d <- tibble(y = rnorm(n, mean = 100, sd = 15))
```

To repeat, Kruschke described two kinds of censoring:

* "values above 106 are censored,"
* "as are values in the interval between 94 and 100."

`r emo::ji("warning")` Rather than examine both kinds of censoring at once, as in the text, I'm going to slow down and break this section up. First, we'll explore right censoring. Second, we'll explore interval censoring. For our grand finale, we'll combine the two, as in the text.

### Right censoring: When you're uncertain beyond a single threshold.

In the case where the "values above 106 are censored," we need to save a single threshold value. We'll call it `t1`.

```{r}
t1 <- 106
```

Now we can use our `t1` threshold value to make a right censored version of the `y` column called `y1`. We'll also make a character value listing out a row's censoring status in nominal terms.

```{r}
d <-
  d %>% 
  mutate(y1   = if_else(y > t1, t1, y),
         cen1 = if_else(y > t1, "right", "none"))

d
```

When the values in `y1` are not censored, we see `cen1 == "none"`. For all cases where the original `y` value exceeded the `t1` threshold (i.e., 106), `y1 == 106` and `cen1 == "right"`. We are using the terms `"none"` and `"right"` because they are based on the block quote from the **brms** reference manual. For plotting purposes, we'll make a new variable, `y_na`, that only has values for which `cen1 == "none"`.

```{r}
d <-
  d %>% 
  mutate(y_na = ifelse(cen1 == "none", y, NA))

d
```

Now let's get a better sense of the data with a few histograms.

```{r, fig.width = 6, fig.height = 2.5, warning = F}
d %>% 
  pivot_longer(-cen1) %>% 
  mutate(name = factor(name, levels = c("y", "y1", "y_na"))) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(size = .25, binwidth = 2, boundary = 106,
                 fill = "skyblue4", color = "white") +
  xlab(NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~ name, ncol = 3)
```

The original un-censored data are in `y`. The censored data, for which all values above 106 were simply recoded 106, is `y1`. The `y_na` column shows what the distribution would look like if the censored values were omitted.

Here's how we might fit a model which only uses the uncensored values, those in `y_na`.

```{r fit25.1a, warning = F, message = F, results = "hide"}
# define the stanvars
mean_y <- mean(d$y_na, na.rm = T)
sd_y   <- sd(d$y_na, na.rm = T)

stanvars <- 
  stanvar(mean_y, name = "mean_y") + 
  stanvar(sd_y,   name = "sd_y")

# fit the model
fit25.1a <-
  brm(data = d,
      family = gaussian,
      y_na ~ 1,
      prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept),
                prior(normal(0, sd_y), class = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      file = "fits/fit25.01a")
```

Check the summary for the naïve model.

```{r}
print(fit25.1a)
```

Relative to the true data-generating process for the original variable `y`, $\operatorname{Normal}(100, 15)$, those parameters look pretty biased. Now let's practice fitting our first censored model. Here we use the `y1 | cens(cen1)` syntax in the left side of the model `formula` to indicate that the criterion variable, `y1`, has been censored according to the process as defined in the `cens()` function. Within `cens()` we inserted our nominal variable, `cens1`, which indicates which cases are censored (`"right"`) or not (`"none"`).

This model is one of the rare occasions where we'll set our initial values for the model intercept. In my first few attempts, `brm()` had great difficulty initializing the chains using the default initial values. We'll help it out by setting them at `mean_y`. Recall that when you set custom initial values in **brms**, you save them in a list with the number of lists equaling the number of HMC chains. Because we're using the default `chains = 4`, well need four lists of intercept start values, `mean_y`. You can set them to different values, if you'd like.

```{r fit25.2a, warning = F, message = F, results = "hide"}
inits <- list(Intercept = mean_y)

inits_list <- list(inits, inits, inits, inits)

fit25.2a <-
  brm(data = d,
      family = gaussian,
      y1 | cens(cen1) ~ 1,
      prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept),
                prior(normal(0, sd_y), class = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      # here we insert our start values for the intercept
      inits = inits_list, 
      file = "fits/fit25.02a")
```

Now check the summary for the model accounting for the censoring.

```{r}
print(fit25.2a)
```

To get a better sense of what these models are doing, we might do a posterior predictive check with `predict()`.

```{r, fig.width = 6, fig.height = 4.5, warning = F}
# original
p1 <-
  d %>% 
  pivot_longer(cols = c(y, y_na)) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(size = .25, binwidth = 2, boundary = 106,
                 fill = "skyblue4", color = "white") +
  labs(subtitle = "Data",
       x = NULL) +
  scale_x_continuous(breaks = 3:5 * 25) +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~ name, ncol = 1)

# posterior simulations
set.seed(25)
p2 <-
  predict(fit25.1a,
          summary = F,
          ndraws = 4) %>% 
  data.frame() %>% 
  mutate(sim = 1:n()) %>% 
  pivot_longer(-sim)  %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(size = .25, binwidth = 2, boundary = 106,
                 fill = "skyblue", color = "white") +
  geom_vline(xintercept = t1, linetype = 2) +
  labs(subtitle = "Simulations",
       x = NULL) +
  scale_x_continuous(breaks = 3:5 * 25) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = range(d$y)) +
  facet_wrap(~ sim, ncol = 2, labeller = label_both)

# combine
library(patchwork)

p1 + p2 + 
  plot_layout(widths = c(1, 2)) +
  plot_annotation(title = "PP check for the naïve model, fit25.1a")
```

On the left column, the have the original uncensored data `y` and the `y_na` data where the censored values were removed. The four lighter histograms on the right are individual simulations based on our naïve model, `fit25.1a`, which only used the `y_na` data. In all cases, the simulated data are biased to be too small compared with the original data, `y`. They also have the undesirable quality that they don't even match up with the `y_na` data, which drops sharply off at the 106 threshold, whereas the simulated data all contain values exceeding that point (depicted by the dashed vertical lines). Now let's see what happens when we execute a few posterior predictive checks with our first censored model, `fit25.2a`.

```{r, fig.width = 6, fig.height = 4.5, warning = F}
p1 <-
  d %>% 
  pivot_longer(cols = c(y, y1)) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(size = .25, binwidth = 2, boundary = 106,
                 fill = "skyblue4", color = "white") +
  labs(subtitle = "Data",
       x = NULL) +
  scale_x_continuous(breaks = 3:5 * 25) +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~ name, ncol = 1, scales = "free_y")

set.seed(25)
p2 <-
  predict(fit25.2a,
          summary = F,
          ndraws = 4) %>% 
  data.frame() %>% 
  mutate(sim = 1:n()) %>% 
  pivot_longer(-sim)  %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(size = .25, binwidth = 2, boundary = 106,
                 fill = "skyblue", color = "white") +
  labs(subtitle = "Simulations",
       x = NULL) +
  scale_x_continuous(breaks = 3:5 * 25) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = range(d$y)) +
  facet_wrap(~ sim, ncol = 2, labeller = label_both)

# combine
p1 + p2 + 
  plot_layout(widths = c(1, 2)) +
  plot_annotation(title = "PP check for the censored model, fit25.2a")
```

Now the posterior predictive draws match up nicely from the original uncensored data `y` in terms of central tendency, dispersion, and their overall shapes. This is the magic of a model that accounts for right (or left) censoring. It can take all those stacked-up 106 values and approximate the underlying distribution, had they been more accurately recorded.

### Interval censoring: When your values are somewhere in the middle.

Time to move onto interval censoring. Kruschke's example in the text included values censored in the interval between 94 and 100. This will require two more thresholds, which we'll call `t2` and `t3`.

```{r}
t2 <- 94
t3 <- 100
```

We should revisit part of the block quote from the **brms** reference manual, above:

> For interval censored data, a second variable (let's call it `y2`) has to be passed to `cens`. In this case, the formula has the structure `y | cens(censored,y2) ~ predictors`. While the lower bounds are given in `y`, the upper bounds are given in `y2` for interval censored data. Intervals are assumed to be open on the left and closed on the right: `(y,y2]`

It's a little unclear, to me, if this is how Kruschke defined his intervals, but since we're working with **brms** we'll just use this convention. Thus, we will define "values in the interval between 94 and 100" as `y >= t2 & y < t3`. Here we'll follow the **brms** convention and define a new variable `y2` for our lower bound and `y3` for our upper bound. Additionally, the new `cen2` variable will tell us whether a case is interval censored (`"interval"`) or not (`"none"`). We have also redefined `y_na` in terms of our `cen2` variable.

```{r}
d <-
  d %>% 
  mutate(y2   = if_else(y >= t2 & y < t3, t2, y),
         y3   = if_else(y >= t2 & y < t3, t3, y),
         cen2 = if_else(y >= t2 & y < t3, "interval", "none")) %>% 
  mutate(y_na = ifelse(cen2 == "none", y, NA))

d
```

Now let's get a better sense of the new data with a few histograms.

```{r, fig.width = 4.25, fig.height = 4.5, warning = F}
d %>% 
  pivot_longer(cols = c(y, y2, y3, y_na)) %>% 
  mutate(name = factor(name, levels = c("y", "y2", "y3", "y_na"))) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(size = .25, binwidth = 2, boundary = 106,
                 fill = "skyblue4", color = "white") +
  labs(subtitle = "Our data have been updated",
       x = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~ name, ncol = 2)
```

Now we fit two models. The first, `fit25.1b`, will only used the values not interval censored, `y_na`. The second model, `fit25.2b`, will use the `cens()` function to fit account for the interval censored data with the `y2`, `y3`, and `cen2` columns. As before, we'll want to use `inits` to help the censored model run smoothly.

```{r fit25.1b, warning = F, message = F, results = "hide"}
# naïve
fit25.1b <-
  brm(data = d,
      family = gaussian,
      y_na ~ 1,
      prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept),
                prior(normal(0, sd_y), class = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      file = "fits/fit25.01b")

# censored
fit25.2b <-
  brm(data = d,
      family = gaussian,
      y2 | cens(cen2, y3) ~ 1,
      prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept),
                prior(normal(0, sd_y), class = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      # here we insert our start values for the intercept
      inits = inits_list, 
      file = "fits/fit25.02b")
```

Review the model summaries.

```{r}
print(fit25.1b)
print(fit25.2b)
```

In this case, both models did a reasonable job approximating the original data-generating parameters, $\operatorname{Normal}(100, 15)$. However, the naïve model `fit25.1b` did so at a considerable loss of precision, as indicated by the posterior standard deviations. To give a better sense, here are the model parameters in an interval plot.

```{r, fig.width = 7, fig.height = 1.75, warning = F}
# wrangle
bind_rows(as_draws_df(fit25.1b),
          as_draws_df(fit25.2b)) %>% 
  mutate(fit = rep(c("fit25.1b", "fit25.2b"), each = n() / 2)) %>% 
  pivot_longer(b_Intercept:sigma) %>% 
  mutate(parameter = if_else(name == "b_Intercept", "mu", "sigma")) %>% 
  
  # plot
  ggplot(aes(x = value, y = fit)) +
  stat_interval(.width = c(.5, .95)) +
  scale_color_manual("ETI", 
                     values = c("skyblue2", "skyblue4"),
                     labels = c("95%", "50%")) +
  labs(x = "marginal posterior",
       y = NULL) +
  theme(axis.ticks.y = element_blank()) +
  facet_wrap(~ parameter, scales = "free_x", labeller = label_parsed)
```

One could also argue the parameters for the censored model `fit25.2b` were a little less biased. However, I'd be leery of making that as a general claim based on this example, alone.

### Complex censoring: When the going gets tough...

Okay, let's put what we've learned together and practice with data that are both right AND interval censored. This is going to require some tricky coding on our part. Since interval censoring is in play, we'll need two `y` variables, again. When the data are indeed interval censored, their lower and upper bounds will be depicted by `y4` and `y5`, respectively. When the data are right censored, `y4` will contain the threshold 106 and `y5` will contain the original value from `y`. All this tricky information will be indexed in our nominal variable `cen3`. Once again, we update our `y_na` variable, too.

```{r}
d <-
  d %>% 
  mutate(y4   = if_else(y >= t2 & y < t3, t2, 
                        if_else(y > t1, t1, y)),
         y5   = if_else(y >= t2 & y < t3, t3, y),
         cen3 = if_else(y >= t2 & y < t3, "interval",
                       if_else(y > t1, "right", "none"))) %>% 
  mutate(y_na = ifelse(cen3 == "none", y, NA))

d
```

Explore the new data with histograms.

```{r, fig.width = 4.25, fig.height = 4.5, warning = F}
d %>% 
  pivot_longer(cols = c(y, y4, y5, y_na)) %>% 
  mutate(name = factor(name, levels = c("y", "y4", "y5", "y_na"))) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(size = .25, binwidth = 2, boundary = 106,
                 fill = "skyblue4", color = "white") +
  labs(subtitle = "Our data have been updated, again",
       x = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~ name, ncol = 2)
```

In the text, Kruschke reported he had 255 uncensored values (p. 732). Here's the breakdown of our data.

```{r}
d %>% 
  count(cen3)
```

We got really close! Now we fit our two models: `fit25.1c`, which ignores the censoring problem and just drops those cases, and `fit25.2c`, which makes slick use of the `cens()` function.

```{r fit25.1c, warning = F, message = F, results = "hide"}
# naïve
fit25.1c <-
  brm(data = d,
      family = gaussian,
      y_na ~ 1,
      prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept),
                prior(normal(0, sd_y), class = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      file = "fits/fit25.01c")

# censored
fit25.2c <-
  brm(data = d,
      family = gaussian,
      y4 | cens(cen3, y5) ~ 1,
      prior = c(prior(normal(mean_y, sd_y * 100), class = Intercept),
                prior(normal(0, sd_y), class = sigma)),
      chains = 4, cores = 4,
      stanvars = stanvars, 
      # here we insert our start values for the intercept
      inits = inits_list, 
      file = "fits/fit25.02c")
```

Compare the model summaries.

```{r}
print(fit25.1c)
print(fit25.2c)
```

The results for the naïve model are a mess and those from the censored model look great! Also note how the latter used all avaliable cases, whereas the former dropped nearly half of them.

Before we can make our version of Figure 25.4, we'll need to extract the posterior draws. We'll start with `fit25.1c`.

```{r}
draws <- 
  as_draws_df(fit25.1c) %>% 
  mutate(mu = b_Intercept,
         `(mu-100)/sigma` = (b_Intercept - 100) / sigma)

head(draws)
```

These subplots look a lot like those from back in [Section 16.2][Outliers and robust estimation: The $t$ distribution]. Since this is the last plot from the book, it seems like we should go all out. To reduce some of the code redundancy with the six subplots of the marginal posteriors, we'll make a custom geom, `geom_hist()`.

```{r}
geom_hist <- function(xintercept = xintercept, binwidth = binwidth, ...) {
  
  list(
    geom_histogram(fill = "skyblue", color = "white", size = .2, binwidth = binwidth, boundary = 106),
    geom_vline(xintercept = xintercept, color = "skyblue3", size = 1/2, linetype = 3),
    stat_pointinterval(aes(y = 0), point_interval = mode_hdi, .width = .95),
    scale_y_continuous(NULL, breaks = NULL)
  )
  
}
```

Now we have our `geom_hist()`, here are the first three histograms for the marginal posteriors from `fit25.1c`.

```{r}
p1 <-
  draws %>% 
  ggplot(aes(x = mu)) +
  geom_hist(xintercept = 100, binwidth = 0.1) +
  xlab(expression(mu))

p3 <-
  draws %>% 
  ggplot(aes(x = sigma)) +
  geom_hist(xintercept = 15, binwidth = 0.08) +
  xlab(expression(sigma))

p4 <-
  draws %>% 
  ggplot(aes(x = `(mu-100)/sigma`)) +
  geom_hist(xintercept = 0, binwidth = 0.015) +
  xlab(expression((mu-100)/sigma))
```

The histogram of the censored data with the posterior predictive density curves superimposed will take a little more work.

```{r, fig.width = 3, fig.height = 2}
n_lines <- 50

p2 <-
  draws %>% 
  slice(1:n_lines) %>% 
  expand(nesting(.draw, mu, sigma),
         y_na = seq(from = 40, to = 120, by = 1)) %>% 
  mutate(density = dnorm(x = y_na, mean = mu, sd = sigma)) %>% 
  
  ggplot(aes(x = y_na)) + 
  geom_histogram(data = d, 
                 aes(y = stat(density)),
                 size = .25, binwidth = 2, boundary = 106,
                 fill = "skyblue4", color = "white") +
  geom_line(aes(y = density, group = .draw),
            size  = 1/4, alpha = 1/3, color = "skyblue") +
  scale_x_continuous("data with posterior predictive lines", limits = c(40, 110)) +
  scale_y_continuous(NULL, breaks = NULL)
```

Now extract the posterior draws from our censored model, `fit25.2`, and repeat the process.

```{r}
draws <- 
  as_draws_df(fit25.2c) %>% 
  mutate(mu = b_Intercept,
         `(mu-100)/sigma` = (b_Intercept - 100) / sigma)

p5 <-
  draws %>% 
  ggplot(aes(x = mu)) +
  geom_hist(xintercept = 100, binwidth = 0.1) +
  xlab(expression(mu))

p7 <-
  draws %>% 
  ggplot(aes(x = sigma)) +
  geom_hist(xintercept = 15, binwidth = 0.1) +
  xlab(expression(sigma))

p8 <-
  draws %>% 
  ggplot(aes(x = `(mu-100)/sigma`)) +
  geom_hist(xintercept = 0, binwidth = 0.01) +
  xlab(expression((mu-100)/sigma))

p6 <-
  draws %>% 
  slice(1:n_lines) %>% 
  expand(nesting(.draw, mu, sigma),
         y_na = seq(from = 40, to = 120, by = 1)) %>% 
  mutate(density = dnorm(x = y_na, mean = mu, sd = sigma)) %>% 
  
  ggplot(aes(x = y_na)) + 
  geom_histogram(data = d, 
                 aes(y = stat(density)),
                 size = .25, binwidth = 2, boundary = 106, 
                 fill = "skyblue4", color = "white") +
  geom_line(aes(y = density, group = .draw),
            size  = 1/4, alpha = 1/3, color = "skyblue") +
  scale_x_continuous("data with posterior predictive lines", limits = c(40, 110)) +
  scale_y_continuous(NULL, breaks = NULL)
```

Finally, combine the subplots, and annotate a bit.

```{r, fig.width = 6, fig.height = 8.5, warning = F}
((p1 | p2) / (p3 | p4) / (p5 | p6) / (p7 | p8)) +
  plot_annotation(title = "This is our final plot, together.",
                  caption = expression(atop(italic("Upper quartet")*": Censored data omitted from analysis; parameter estimates are too small.  ", italic("Lower quartet")*": Censored data imputed in known bins; parameter estimates are accurate."))) &
  theme(plot.caption = element_text(hjust = 0))
```

To learn more about censored data, check out the nice lecture by Gordon Fox, [*Introduction to analysis of censored and truncated data*](https://www.youtube.com/watch?v=aPN10YYrC1M).

### Bonus: Truncation.

Though Kruschke didn't cover it, truncation is often confused with censoring. When you have actually collected all your values, but some are just less certain than you'd like, you have a censoring issue. That is, you can think of censoring as an issue with *measurement precision*. Truncation has to do with *data collection*. When there's some level in your criterion variable that you haven't collected data on, that's a truncation issue. For example, imagine you wanted to predict scores on an IQ test with socioeconomic status (SES). There might be a relation, there. But now imagine you only collected data on people with an IQ above 110. Those IQ data are severely left truncated. If you only care about the relation of SES and IQ for those with high IQ scores, your data are fine. But if you want to make general statements across the full range of IQ values, standard regression methods will likely produce biased results. Happily, **brms** allows users to accommodate truncated criterion variables with the `trunc()` function, which works in a similar way to the `cens()` function. For details on fitting truncated regression models with `trunc()`, check out the *Additional response information* subsection of the `brmsformula` section of the [**brms** reference manual](https://CRAN.R-project.org/package=brms/brms.pdf). The [end of the Fox lecture](https://youtu.be/aPN10YYrC1M?t=6153), above, briefly covers truncated regression. You can also find a [brief vignette on truncation](https://stats.idre.ucla.edu/r/dae/truncated-regression/) by the good folks at the UCLA Institute for Digital Research and Education.

## What Next?

"If you have made it this far and you are looking for more, you might peruse posts at [Kruschke's] blog, [https://doingbayesiandataanalysis.blogspot.com/](https://doingbayesiandataanalysis.blogspot.com/), and search there for topics that interest you." In addition to the other references Kruschke mentioned, you might also check out McElreath's [*Statistical rethinking*](https://xcelab.net/rm/statistical-rethinking/), both first [-@mcelreathStatisticalRethinkingBayesian2015] and second [-@mcelreathStatisticalRethinkingBayesian2020] editions. Much like this project, I have recoded both editions of *Statistical rethinking* in a **bookdown** form [@kurzStatisticalRethinkingBrms2020; @kurzStatisticalRethinkingSecondEd2021]. Click [here](https://bookdown.org/content/3890/) for the first and [here](https://bookdown.org/content/4857/) for the second. You can find other pedagogical material at my academic blog, [https://solomonkurz.netlify.com/post/](https://solomonkurz.netlify.com/post/). For assistance on **brms**-related issues, check out the **brms** section on the Stan forums at [https://discourse.mc-stan.org/c/interfaces/brms/36](https://discourse.mc-stan.org/c/interfaces/brms/36). Nicenboim, Schad, and Vasishth also have a nice new [-@nicenboim2021introduction] ebook called [*An introduction to Bayesian data analysis for cognitive science*](https://vasishth.github.io/bayescogsci/book/), which highlights **brms** in a way that could compliment the material we have been practicing.

Happy modeling, friends!

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
# remove our objects
rm(fit24.1, HDIofGrid, prob_density_vec, prob_mass_vec, HDI_info, HDIofMCMC, draws, hdi_of_icdf, n, d, t1, mean_y, sd_y, stanvars, fit25.1a, inits, inits_list, fit25.2a, p1, p2, t2, t3, fit25.1b, fit25.2b, fit25.1c, fit25.2c, geom_hist, p3, p4, n_lines, p5, p7, p8, p6)
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```


<!--chapter:end:25.Rmd-->

`r if (knitr::is_html_output()) '# References {-}'`


<!--chapter:end:99.Rmd-->

